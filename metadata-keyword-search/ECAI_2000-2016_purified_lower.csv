Year,Title,Abstract,Keywords
2000,encoding information fusion in possibilistic logic: a general framework for rational syntactic merging," the problem of merging multiple sources information is centralin many information processing areas such as databases integrating problems, multiple criteria decision making, expert opinion pooling, etc. recently, several approaches have been proposed to merge classical propositional bases, or sets of (non-prioritized) goals. these approaches are in general semantically defined. like in belief revision, they use priorities, generally based on dalal's distance,for merging the classical bases and return a new classical base as a result. an immediate consequence of the generation of a classical base is the impossibility of iterating the fusion process in a coherent way w.r.t. priorities since the underlying ordering is lost.this paper presents a general approach for fusing prioritized bases, both semantically and syntactically, when priorities are represented in the possibilistic logic framework. we show that the approaches which have been recently proposed for merging classical propositional bases can be embedded in this setting. the result is then a prioritized base, and hence the process can be coherently iterated. moreover, we also provide a syntactic counterpart for the fusion of classical bases. lastly, the paper briefly discusses rationality postulates for fusing prioritized information. "," uncertainty in ai, belief revision, fusion "
2000,kalman-like filtering and updating in a possibilistic setting, this paper proposes a qualitative counterpart of kalman filtering in the possibilistic logic setting. it corresponds to a type of updating involving a prediction step followed by a revision step. this is compared with updating operations based on imaging in the sense of lewis. imaging is reconsidered in the perspective of a generalized view of kalman filtering where it appears as a particular case of kalman filtering. a syntactic counterpart of qualitative filtering is given in terms of weighted knowledge bases. , uncertainty in ai 
2000,"iterated revision by epistemic states: axioms, semantics and syntax"," epistemic states are suitable to represent intelligent agent'sknowledge. this paper discusses iterated revision of epistemic statesby epistemic states from axiomatic, semantics and syntactic points ofview. we first propose a set of general postulates that such iteratedrevision operations should verify, independently from the epistemicstates representation.  we give a representation theorem for a classof revision operation by epistemic states called ""minimally-preservingmodels"" revision operators.  then, at the semantical level, we proposeto use polynomials for encoding epistemic states. we show that thisencoding makes easy the iteration of the revision process using simpleoperations on these polynomials.  lastly, we give an efficientimplementation of revision by epistemic states, when the later aresyntactically represented by prioritized knowledge bases. the revisionby epistemic states proposed in this paper generalizes (axiomatically,semantically and syntactically) iterated revision operations by (asingle and non-prioritized) formula, stemming from the principle ofstrong primacy of new information. "," belief revision, knowledge representation, nonmonotonic reasoning "
2000,declarative representation of revision strategies," in this paper we introduce a nonmonotonic framework for belief revision in which reasoning about the reliability of different pieces of information based on meta-knowledge about the information is possible, and where revision strategies can be described declaratively. the approach is based on a poole-style system for default reasoning in which entrenchment information is represented in the logical language. a notion of inference based on the least fixed point of a monotone operator is used to make sure that all theories possess a consistent set of conclusions. "," belief revision, nonmonotonic reasoning "
2000,compiling stratified belief bases," many coherence-based approaches to inconsistency handling withinpropositional belief bases have been proposed so far. they consistin selecting one or several preferred consistent subbases of thegiven (usually inconsistent) stratified belief base (sbb), thenusing classical inference from some of the selected subbases.unfortunately, deciding the corresponding inference relationsis typically hard from the computational complexity point of view.in this paper, we show how some knowledge compilation techniquesfor classical inference can be used to circumvent the intractabilityof such sophisticated inference relations. for several families ofcompiled sbbs and several selection policies, the complexity of skeptical inference is identified. interestingly, some tractablerestrictions are exhibited. ", automated reasoning 
2000,a conditional logic for iterated belief revision," in this paper we propose a conditional logic ibc for iterated belief revision.  we define an iterated belief revision system by strenghtening the postulates proposed by darwiche and pearl. first, following the line of darwiche and pearl, we modify agm postulates to make belief revision  a function of epistemic states rather than of belief sets.then we propose a set of postulates for iterated revision which, together with the (modified) agm postulates, entail darwiche and pearl's ones.the conditional logic ibc has a standard semantics in terms of selection function models and it provides a natural representation of epistemic states. ibc contains conditional axioms, corresponding to the postulates for iterated revision.we prove that gardenfors' triviality result does not apply to ibc. moreover, we provide a representation result, which establishes a one to one correspondence between iterated belief revision systems and ibc-models. "," knowledge representation, belief revision,, conditional logic "
2000,classical and general frameworks for recovery	," theory recovery is the process of restoring consistency of a theory with respect to some intended semantics.recently, a general framework for theory recovery has been developed using an intended and a backup semantics, where the latter is used to guide the recovery process.in this paper we establish some relationships between this recoveryframework and the well-known agm postulates for theory revision.we show that these agm revision postulates can be easily adapted totheory recovery and agm-style recovery can be embedded in the generalrecovery framework. we also generalize the postulates to deal with recovery of not necessarily closed theories and we show that the general framework specialised to classical intended semanticssatisfies these agm postulates. it turns out that the backup semantics used is a special cumulative, non-inclusive para-consistent semantics. finally, we generalize these agm postulates for recoveryto cases where the intended semantics does not need to be classical.we show that in such cases the postulates allow for recovery by expansions. "," belief revision, nonmonotonic reasoning		 "
2000,how to revise ranked probabilities," in this paper, we introduce and discuss a new framework for the modeling and revision of probabilistic belief. the epistemic states encode degrees of belief together with second-order uncertainty through special spohn-type ranking measures over subjective probability distributions. the revision strategy, which handles incoming information representable by linear probabilistic constraints, is based on modified jeffrey-conditionalization and information distance minimization procedures. "," belief revision, uncertainty in ai "
2000,an algorithm for adaptation in case-based reasoning," the adaptation process is an important and complex step ofcase-based reasoning (cbr) and is most of the time designed fora specific application. this article presents a domain-independentalgorithm for adaptation in cbr. cases are mapped to a set ofnumerical descriptors filled with values and local constraintintervals. the algorithm computes every target solution descriptorby combining a source solution, a matching expressed as variationintervals together with dependencies between the source problem andits solution. it determines for every target solution descriptor aninterval of the admissible values in which actual values satisfyingglobal constraints are fixed. this generic approach to adaptation isoperational and it introduces general and domain-independant adaptationoperators. this study contributes to the design of a general algorithmfor adaptation in cbr. ", case-based reasoning 
2000,focusing search by using problem solving experience ," case-based reasoning (cbr) aims at using experience from the past in order to guide future problem solvingrather than ""starting from scratch"" every time.we propose a cbr strategy particularly suitable for realizing this principle if heuristic search is used as a problem solving method: given a new problem, a cbr method exploits previously solved problems in order to predict a region of the search space which is (provably) probable to contain the solution. the efficiency of a search method applied afterwards for actually finding the solution is then improved by focusing on this region. our results provide a formal basis for the intuitively meaningful (even though not always justified) idea to concentrate on those parts of the search space where solutions to similar problems have already been found. the approach outlined in this paper either can be seen as one of cbr-supported heuristic search or asa formal framework of (search-oriented) cbr. "," case-based reasoning, search "
2000,similarity-based inference as evidential reasoning," we make use of a probabilistic model in order to formalize the basic assumption underlying case-based reasoning (cbr), suggesting that ""similar problems have similar solutions.""taking this model as a point of departure, we propose a similarity-guided inference scheme in which case-based evidence is represented in form of belief functions over the set of solutions,and in which the combination of evidence derived from individual cases is considered in the context of information fusion. our approach is meant to support the overall process of problemsolving by estimating the quality of potential solutions.besides, it reveals that probabilistic methods and relatedtechniques from the field of reasoning under uncertaintyprovide a convenient framework in which parts of the cbr methodology can be formalized. this framework seems particularly suitable since it allows for taking the heuristic and, hence, uncertain character of case-based problem solving into account. "," case-based reasoning, uncertainty in ai "
2000,competence-guided editing methods for lazy learning," lazy learning algorithms retain their raw training examples and defer all example-processing until problem solving time (eg, case-based learning, instance-based learning, and nearest-neighbour methods). for example, a case-based classifier will typically compare a new target query to every case in its case-base (its raw training data) before deriving a target classification. this can make lazy methods prohibitively costly for large training sets. one way to reduce these costs is to filter or edit the original training set, to produce a reduced edited set by removing redundant or noisy examples. in this paper we describe and evaluate a new family of hybrid editing techniques that combine many of the features found in more traditional approaches with new techniques for estimating the usefulness of training examples. we demonstrate that these new techniques enjoy superior performance when compared to traditional and state-of-the-art methods.  "," case-based reasoning, machine learning "
2000,picture perfect - visualisation techniques for case-based reasoning," case-based reasoning systems solve new problems by retrieving and adapting the solutions to similar previously solved problems. the success and performance of any case-based reasoning system depends critically on its repository of prior problem solving experiences, the cases in its case-base. it is perhaps surprising then that the case-based reasoning community has only recently begun to investigate new ways of intelligently supporting the authoring (and on-going maintenance) of evolving case-bases. in this paper we describe and evaluate a technique for visualising the cases in a case-base. we argue that such techniques have a vital role to play in helping authors to understand the structure of an evolving case-base and so improve the efficiency of the authoring process and the quality of the resulting case-bases. ", case-based reasoning 
2000,differentiating diagnostic theories through constraints over an eight-valued logic," in this paper we address the issue of diagnosing propositional theories where only some components are observable. more specifically, the goal is to find tests that allow the differentiation of two alternative theories. this subject is addressed in the context of differential diagnosis of faulty gates in a vlsi circuit where the only observable findings are its input/output bits, but may be extended to other areas, namely in diagnosing a theory of an agent where knowledge about it is based on its response to external stimuli.to model these problems we developed an eight-valued logic that describes the dependency of the findings on the competing theories. additionally, we implemented a constraint solver to handle this eight-valued logic domain, and to solve efficiently the problem of obtaining differentiating tests that allow the elimination of one of the alternative theories. we discuss the limitations of the currently more advanced techniques to handle disjunctive constraints, and propose a new method, iterative time-bounded search (itbs) to overcome them. the method is exemplified and tested in the problem of generating differential test patterns for digital circuits. "," constraint-based reasoning, diagnosis, constraint satisfaction, constraint programming, search "
2000,tractable sets of the generalized interval algebra," to offer a generic frame which groups together several interval algebra generalizations, we simply define a generalized interval as  a tuple of intervals. after introducing the generalized relationswe focus on the consistency problem of generalized constraint networks and we present sets of generalized relations for which this problem is tractable, in particular the set of the strongly-preconvex relations. "," interval algebra, generalized intervals, constraint networks, complexity "
2000,a formalization of structured temporal objects and repetition," we propose an approach to formally representing structured temporal objects. these new temporal objects are recursively made up of convex intervals and allen's convex relations. the major emphasis in our approach is on temporal repetition.to that effect we define the time loop. time loops allow usto abstract common elements in repetitive temporal patterns.a loop is parameterized by a cycle which is a structured temporal element possibly including subloops, a relationbetween instances of the cycle and a repetition factor or dimension. atemporal assertions are true during these temporalobjects, and thus this formalism allows one to concisely represent and reason with assertions which have an inherenttemporal structure. hence, this formalism allows one toconcisely represent,for example, the scheduling of regular meetings in a university, where a meeting may in turn becomposed of structured temporal elements.  "," knowledge representation, temporal reasoning "
2000,a property of path inverse consistency leading to an optimal pic algorithm," in constraint networks, the efficiency of a search algorithm is strongly related to the local consistency maintained during search.for a long time, it has been considered that forward checking was the best compromise between the pruning effect and the amount of overhead involved. but recent works, comparing the search algorithms on a large range of networks, show that maintaining arc consistency during search (mac) outperforms forward checking on large and hard problems. it is conceivable that on very difficult instances, using an even more pruningful local consistency may pay off. to know which local consistency is the most promising, a study comparing both theirpruning efficiency and the time needed to achieve them has been done [de97]. this work shows that pic1, the path inverse consistency algorithm presented in [fe96], has very bad average and worst case time complexities.  in this paper, we give a property of pic and we propose and evaluate a pic algorithm based on this property that has an optimal worst case time complexity. experiments show that maintaining pic during search outperforms mac on hard sparses cns. ", constraint satisfaction 
2000,a framework for dynamic constraint reasoning using procedural constraints," many complex real-world decision problems, such as planning, containan underlying constraint reasoning problem.  the feasibility of asolution candidate then depends on the consistency of the associatedconstraint problem instance.  the underlying constraint problems areinvariably dynamic, as higher level decisions result in variables andconstraints being added and removed.  there is also no limit on thetypes of constraints that may arise in the constraint network, asthey depend on the domain to which the system is being applied. additionally, real-world problems often have constraints overcontinuous variables.  such applications therefore present a number ofsignificant challenges for a constraint reasoning mechanism.in this paper, we introduce a general framework for handling dynamicconstraint networks with real-valued variables, by using procedures torepresent and effectively reason about general constraints.  thisframework is based on a sound theoretical foundation.  additionally,the framework provides hybrid reasoning capabilities, as alternativesolution methods like mathematical programming can be incorporatedinto the framework, in the form of procedures. "," constraint satisfaction, planning "
2000,on the limit of branching rules for hard random unsatisfiable 3-sat," we study the limit of branching rules in davis-putnam (dp) procedure for hard random unsatisfiable 3-sat and try to answer the question: what would be the search tree size if every branching variable were the best possible? the issue is of practical interest because many efforts have been spent for designing better branching rules. our experimental results suggest that the branching rules used in the current state-of-the-art dp procedures are already close to the optimal, and in particular, that the first of the ten ijcai-97 challenges for propositional reasoning and search formulated by selman et al., namely, proving a hard 700 variable random 3-sat formula is unsatisfiable, probably cannot be answered by dp procedure unless something significantly different from branching can be made effective for hard random unsatisfiable 3-sat "," search, constraint satisfaction, automated reasoning, theorem proving, branching, davis-putnam procedure, sat "
2000,search pruning conditions for boolean optimization," this paper proposes new algorithms for the binate covering problem(bcp), a well-known restriction of boolean optimization. binatecovering finds application in many areas of computer science andengineering. in artificial intelligence, bcp can be used for computingminimum-size prime implicants of boolean functions, of interest inautomated reasoning and non-monotonic reasoning. moreover, binatecovering is an essential modeling tool in electronic designautomation. the objectives of the paper are to briefly reviewbranch-and-bound algorithms for bcp, to describe how to applybacktrack search pruning techniques from the boolean satisfiability(sat) domain to bcp, and to illustrate how to strengthen those pruningtechniques by exploiting the actual formulation of bcp. experimental results, obtained on representative instances, indicate that theproposed techniques provide significant performance gains fordifferent classes of instances. "," constraint satisfaction, search "
2000,incremental forward checking for the disjunctive temporal problem," this paper studies algorithms for the disjunctive temporal problem(dtp) a quite general temporal reasoning problem introduced in\cite{stergiou-koubarakis-98}.  this problem involves the satisfaction of a setof constraints represented by disjunctive formulas of the form $x_1-y_1 \leq r_1 \vee x_2 -y_2 \leq r_2 \vee \dots \vee x_k -y_k \leqr_k$.  the paper starts sketching why dtps are potentially very usefulin plan management applications, then analyzes the current solutionsto dtp, and introduces a constraint-satisfaction problem solvingalgorithm where different aspects of current dtp's literature areintegrated.  this basic algorithm is then improved exploiting thequantitative temporal information in the ``distance graph''. using this knowledge an incremental version of the forward checking isobtained and shown to be competitive with current best results. the whole approach allows to understand pros and cons of the currentalgorithms for the dtp and suggests further future developments asdiscussed in the final part of the paper. "," temporal reasoning, constraint satisfaction, planning "
2000,local search on random 2+p-sat," random 2+p-sat interpolates between the polynomial-time problemrandom 2-sat when p = 0 and the np-complete problem random 3-satwhen p = 1.  at some value p = p_0 ~= 0.41, a dramatic changein the structural nature of instances is predicted by statisticalmechanics methods. this is reflected by a change in the typical costscaling for a complete search method tableau, seenexperimentally.  we show empirically the same change of of behaviourin the local search algorithm, novelty+, a recent variantof wsat.  between p = 0.3 and p = 0.5 we see typical costscaling of novelty+ at the 50% satisfiability point apparently change from slow polynomial growth to superpolynomial.that this behaviour is seen in two such different algorithms lendscredibility to the hypothesis that there is change of typical-casecomplexity around p_0.previous work linked the emergence of a backbone of fullyconstrained variables to the cost peak seen in random k-sat.initial experiments suggest that for those instances whose cost wastypical, backbone size is no larger for p = 0.5 than for p = 0.3,implying that this property is not wholly responsible for anytypical cost scaling change.  a preliminary study shows that thebackbones of instances at p = 0.5 are more sensitive to the removal of clauses than those at p = 0.3. this ``backbone fragility'', which has previously been linked to local search cost, may cause the drastic increase. "," search, constraint satisfaction "
2000,solving permutation constraint satisfaction problems with artificial ants," we describe in this paper ant-p-solver, a generic constraint solverbased on the ant colony optimization (aco) meta-heuristic. the aco metaheuristic takes inspiration on the observation of real ants collective foragingbehaviour. the idea is to model the problem as the searchof a best path in a graph. artificial ants walk trough this graph, ina stochastic and incomplete way, searching for good paths. artificialants communicate in a local and indirect way, by laying a pheromonetrail on the edges of the graph.ant-p-solver has been designed to solve a general classof combinatorial problems: permutation constraint satisfactionproblems, the goal of which is to find a permutation of n knownvalues, to be assigned to n variables, under some constraints. manyconstraint satisfaction problems involve such global permutationconstraints. ant-p-solver capabilities are illustrated, and comparedwith other approaches, on three of these problems, i.e., the n-queens,the all-interval series and the car-sequencing problems.  ", constraint satisfaction 
2000,estimating the hardness of optimisation," estimating computational cost is a fundamental issue intime-bounded computation. we present a method for estimating thehardness of optimisation problems (find a minimal cost solution toinstance i) by observing that of the corresponding decision problems(has i a solution of cost less than threshold t).  provided tis not too close to the optimal, decision is typically much easierthan optimisation, yet knowing the hardness of the former is usefulfor predicting the latter. the present paper reports an experimentalinvestigation of this idea, with encouraging results. an investment ofa few percent of the work required for optimisation suffices forestimation within a small factor, even using a very simpleimplementation of the method. ", search 
2000,diagnosis and diagnosability analysis using pepa," in this paper we propose the use of process algebras (and specifically pepa)as powerful frameworks for model-based diagnosis.in fact, they provide machinery and tools for buildingcomponent-oriented models, for characterizing and computingdiagnoses, and for analyzing properties such as the diagnosability of the system under investigation. "," diagnosis, model-based reasoning "
2000,a comparative analysis of ai and control theory approaches to model-based diagnosis," two distinct and parallel research communities have been working alongthe lines of the model-based diagnosis approach: the fdi community andthe dx community that have evolved in the fields of automatic controland artificial intelligence, respectively. this paper clarifies andlinks the concepts that underlie the fdi analytical redundancy approach and the dx logical approach. the formal match of the twoapproaches is demonstrated and the theoretical proof of theirequivalence is provided under various assumptions. "," diagnosis, model-based reasoning "
2000,qualitative simulation of large and complex genetic regulatory systems," modeling and simulation techniques developed within qualitativereasoning might be profitably used for the analysis of genetic regulatory systems. a major problem of current qualitative simulationtechniques is their lack of upscalability. we describe a method that is able to deal with large and complex systems, and discuss its performance in simulation experiments with random regulatory networks. "," qualitative reasoning, bioinformatics "
2000,consistency-based diagnosis of configuration knowledge bases," configuration problems are a thriving application areafor declarative knowledge representation that currently experiences a constantincrease in size and complexity of knowledge bases. automated support ofthe debugging of such knowledge bases is a necessary prerequisite foreffective development of configurators. we show that this task can beachieved by consistency based diagnosis techniques. based on the formaldefinition of consistency based configuration we develop a frameworksuitable for diagnosing configuration knowledge bases. during the testphase of configurators, valid and invalid examples are used to test thecorrectness of the system. in case such examples lead to unintendedresults, debugging of the knowledge base is initiated. the examples usedfor testing are combined to identify faulty chunks of knowledge.starting from a clear definition of diagnosis in the configurationdomain we develop an algorithm based on conflicts and exploit theproperties of positive examples to reduce consistency checks. ourframework is general enough for its straightforward adaptation todiagnosing customer requirements. given a validated knowledge base ourapproach can then be used to identify unachievable conditions duringconfiguration sessions.  "," configuration, diagnosis "
2000,uncertain temporal observations in diagnosis," in diagnosis, the notion of observation varies according to the class of considered systems. in discrete-event systems, an observation usually consists of a sequence, or a set of sequences, of totally ordered observable events. this paper extends the concept of discrete-event observation in several ways. first, observable events (messages) may be uncertain in nature, both in behavioral models and in system observations. uncertain messages are specified by variables ranging on finite sets of observable labels. second, messages relevant to a system observation are accommodated within a dag, the observation graph, whose edges define a partial temporal ordering among (uncertain) messages. this way, an observation graph implicitly defines a finite set of  system observations in the traditional sense. consequently, solving a diagnostic problem amounts to solving at one time several traditional diagnostic problems. finally, the (possibly distributed) reconstruction of the system behavior is further complicated by the fact that homonymous observable labels can be shared by different components. this raises the need of dealing with null messages. the method is appropriate for several real systems, where messages may get lost, are noisy, or attached timestamps are generated by different clocks. "," diagnosis, model-based reasoning, knowledge representation "
2000,timed automata model to improve the classification of a sequence of images," the aim of this paper is to propose the use of a dynamic plot model to improve landcover classification on a sequence of images. this new approach consists in representing the plot as a dynamic system and in modeling its evolution (knowledge about crop cycles) using the timed automata formalism. in order to refine results obtained by a traditional classifier, observations given by a preliminary classification of images are matched with expected states provided by an automaton simulation. the paper presents the modeling captured by the timed automata formalism and the general method, which is based on prediction and postdiction mechanisms, that have been adopted to improve the classification of a sequence of images. finally, the interest of the method is demonstrated through experimental results. "," model-based reasoning, temporal reasoning, image interpretation, landcover classification, dynamic plot modeling "
2000,qualitative modelling of linear networks in engineering applications," there is a real need for electrical circuit modelling and analysis tools at anintermediate level between analogue and state-based simulators.numerical tools do not capture the abstractions valued by engineers andlogical/functional models do not maintain sufficient affinities with theelectrical properties in a system.qualitative models have had some notable successes in filling this gapbut a number of difficulties pose a barrier to further progress.we describe current issues and some key problems in qualitative analysis of steady-state systems.we then present a new qualitative circuit analysismethod based on a many-valued resistance quantity space with an orders-of-magnitude relation.the results of this technique are shown to solvekey problem cases.   we argue that the new schemeopens up considerably more modelling possibilities and the methodcan be tailored to the application domain, giving powerful modelling options. "," qualitative reasoning, model-based reasoning "
2000,verification programs for abduction," we call verification the process of finding the actual explanation of a given set of manifestations.  we consider an abductive setting, in which explanations are sets of assumptions.  to filter out erroneous explanations, a verification program should propose which assumptions to check.  given the abductive setting of manifestations, assumptions, and a theory relating them, we study the complexity of providing a minimal set of assumptions to be checked in order to identify the actual explanation.  we study also the case in which assumptions to be checked are given in a tree-like order. "," abduction, diagnosis, automated reasoning "
2000,modeling java programs for diagnosis," a key advantage of model-based diagnosis is the ability to use ageneric model for the production of system descriptions that can beused to derive diagnoses for differently structured individual systemsfrom a domain.  this advantage is nowhere more apparent than in thesoftware error diagnosis (or debugging) area, where given a model,system descriptions can be automatically derived from source code.however, effective models for diagnosing programs have so far beenlimited to special-purpose languages.  we describe a value-based modelfor java programs that enables us to explicitly deal with imperativeprogram execution (including loop execution), and compare the outcomeof our approach to the results obtained by using program slicing, atraditional technique from the software debugging community, and asimple dependency-based model for java. "," diagnosis, model-based reasoning "
2000,state-based vs simulation-based diagnosis of dynamic systems ," in model-based diagnosis there is an increasing interest in the diagnosis of dynamic systems.some recent contributions in the literature show that in some cases such systems can be diagnosed with pure state-based diagnosis,i.e. reasoning on single states of the systemrather than on transitions of the system from one state to another.in this work we discuss how in a different context the same resultsdo not hold, and show how reasoning on the causality in the systemand using simulation can provide more precise diagnostic resultswith respect to state-based diagnosis.essential to this result are component fault modelsthat characterize the discontinuity in the behavior associated withabrupt faults, i.e. the sudden transition of a component fromthe correct mode of behavior to a faulty behavior. "," diagnosis, model-based reasoning, causal reasoning, qualitative reasoning "
2000,describing problem solving methods using anytime performance profiles," we propose the use of anytime performance profiles to describe thecomputational behaviour of problem solving methods. a performance profile describes how thequality of the output of an algorithm gradually increases as afunction of the computation time.  such anytime descriptions ofproblem solving methods are attractive because they allow atrade-off to be made between available computation time andoutput-quality.  it turns out that many problem solving methodsfound in the literature have a natural anytime behaviour, whichhas remained largely unexploited until now.in this paper we propose an axiomatic description of performanceprofiles.  furthermore, we give a fixed schematic form for these axiomatic descriptions. finally, we apply our proposal to anumber of realistic problem-solving methods, namely hierarchical classification (used in mdx), parametricdesign (methods from xcon and vt), and consistency-based diagnosis(the gde-method). "," knowledge acquisition, verification and validation of knowledge-based systems, resource-bounded reasoning "
2000,optimal sequential decisions in liver transplantation based on a pomdp model," the present investigation aims at the construction of a sequential decision procedure based on a partially observable markov decision process (pomdp) model. an optimal clinical management strategy based on a risk assessment of patients is to be found. a decision theoretic cost model different from the general approach has been selected for this clinical management (and classification) task: costs were determined by specifying a minimum acceptable sensitivity and specificity of the overall procedure. the aim is to find the earliest possible decision epoch where a final decision can be made under these quality restrictions. solution method is non-linear optimisation combined with a robust partial classification method. the probabilities necessary for the model are estimated from data of a clinical study in liver transplantation patients. decision epochs were at donor organ assessment, immediately before surgery and postoperatively in the intensive care unit. parameters obtained within decision epochs were combined to scores by artificial neural networks (anns). the encouraging results show the applicability of the model in the clinical setting.  "," diagnosis, planning, probabilistic networks, neural networks, machine learning, uncertainty in ai "
2000,selection of perturbation experiments for model discrimination," when a system is described by several competing models, furtherinformation about its behavior is required to distinguish betweenthem. one way to obtain such information is to perform suitably chosen perturbation experiments. this paper introduces a method for the selection of optimal perturbation experiments for discrimination among a  set of competing dynamical models. the models are assumed to have the form of semi-quantitativedifferential equations. the method employs an optimization criterion based on the entropy measure of information. "," qualitative reasoning, model-based reasoning "
2000,tree-based heuristics in modal theorem proving, we use a strong form of the tree model property to boost theperformance of resolution-based first-order theorem provers on theso-called relational translations of modal formulas.  we provide boththe mathematical underpinnings and experimental results concerning ourimproved translation method. ," automated reasoning, theorem proving, modal reasoning, tree model property "
2000,a foundation for region-based qualitative geometry," we  present   a  highly expressive   logical  language for  describingqualitative configurations of spatial regions,  based on tarski's geometry of solids, in which the  `parthood' relation and theconcept of `sphere' are taken  as primitive. we give a completeaxiom system, whose models can  be interpreted classically in terms ofcartesian spaces  over r. we show  that within  this systemthe  concept   of  sphere and  the  `congruence'  relation  areinterdefinable.  we investigate the 2nd-order  character of the theoryand prove incompletenss of some weaker 1st-order variants. "," ontologies, spatial reasoning, qualitative reasoning, representation "
2000,explaining alc subsumption," the usability of knowledge representation systems, including onesbased on description logics (dls) is considerably enhanced by theability to explain inferences to non-sophisticated users.  for dls,the ability to explain subsumption inferences is particularly useful.this is relatively natural for systems based on structural subsumptionalgorithms.  however, systems that deal with more complex logics, suchas alc, generally use tableaux algorithms, and this does not lead to anatural explanation of subsumption inferences as they are based on``refutation/unsatisfiability'' proofs of single formulae.  we proposeusing a slightly extended tableaux algorithm that, by keeping track ofthe ``undesirable'' steps involved in both the reduction of thesubsumption problem to an unsatisfiability problem and thenormalisation into negation normal form, allows the structure of theoriginal subsumption inference to be preserved.  interestingly, thetableaux proof can be represented as a sequence of rules in a simplevariant of sequent calculus.  a set of templates are then used togenerate ""surface"" explanations (in one or more steps) from each ruleapplication.  the result is a parsimonious yet understandable proofpresentation. ", description logics 
2000,using an ontology conceptualisation method to capture an advice giving system's knowledge," in order to produce effective advisory messages, knowledge coming fromdifferent research fields has to be reconciled. domain knowledge isonly part of what is needed: sociological theories of behaviour changeare of great help, and, as the messages have to be conveyed with somewording, argumentation theories are definitely crucial.  the issue ofintegrating diverse knowledge representations is among those whichhave been raised in recent years by researchers in ontologicalengineering, and various methodologies to help conceptualisingknowledge have been suggested by a number of research groups.  in thispaper we take advantage from this community experience, and apply oneof the methodologies proposed to the design of an advice giving systemwhich uses argumentation techniques to produce counselling messages inthe nutrition education domain. "," ontologies, cognitive modelling, knowledge representation "
2000,"identity, unity, and individuality: towards a formal toolkit for  ontological analysis"," we have identified several notions important to ontology in philosophy, that, with suitable formalization, have proven extremely useful tools for ontological analysis, and have become a central part of a methodology for ontology-driven conceptual analysis we are building. we present, in some detail, formalizations of two of these notions, identity and unity, and discuss how they combine to form an overall theory of individuality. we conclude with a brief example of how they can be used to check the ontological consistency of taxonomies. "," ontologies, philosophical foundations, knowledge-based systems "
2000,qualitative representation of planar outlines," a new boundary-based scheme for qualitatively representing planaroutlines is described, consisting of a set of seventeen ``atomic''tokens, and based on a combined discretisation of tangentbearing, curvature, and the rate of change of curvature.by grouping together strings of atomic tokens, higher-level primitivecurve tokens can be specified (pcts), that correspond to localisedcurve features of greater abstraction.  we show how theprimitives of existing boundary-based schemes may be defined as pcts,and how associated token ordering graphs can be constructed thatvisually encode token-string syntax, based on the ordering constraintsimplicit in a set of pct specifications. because of the atomic natureof its building blocks, we propose that the scheme can be developedinto a general framework for constructing sets of task-specificprimitives, for use in application areas such as computer vision andqualitative spatial reasoning. "," knowledge representation, spatial reasoning, qualitative reasoning "
2000,extensions of constraint solving for proof planning," the integration of constraint solvers into proof planning has pushed the  problem solving horizon.  proof planning benefits from the general  functionalities of a constraint solver such as consistency checks, constraint  inference, as well as the search for instantiations.  however, off-the-shelf  constraint solvers need to be extended in order to be be integrated  appropriately: in particular, for correctness, the context of constraints and  eigenvariable-conditions have to be taken into account. moreover, symbolic and  numeric constraint inference have to be combined. this paper discusses the  extensions, their conceptional realization, and the implementation in the  constraint solver cosie. "," theorem proving, constraint-based reasoning "
2000,qualitative spatial reasoning about line segments," representing and reasoning about orientation information is animportant aspect of qualitative spatial reasoning. we present a novelapproach for dealing with intrinsic orientation information byspecifying qualitative relations between oriented line segments, thesimplest possible spatial entities being extended and having anintrinsic direction.we identify a set of 24 atomic relations which form a relationalgebra and for which we compute relational compositions based ontheir algebraic semantics.reasoning over the full algebra turns out to be np-hard.potential applications of the calculusare motivated with a small example which shows the reasoningcapabilities of the dipole calculus using constraint-based reasoning methods. "," spatial reasoning, qualitative reasoning, constraint-based reasoning "
2000,description logics for the representation of aggregated objects," aggregated objects play an important role in many knowledge representation applications. for the adequate representation of aggregated objects, it is crucial to represent part-whole relations. we discuss properties of part-whole relations and extend the description logic alc with means for the adequate representation of part-whole relations and thus of aggregated objects. "," description logics, knowledge representation  "
2000,spatial reasoning in rcc-8 with boolean region terms," this work belongs to the field of qualitative spatialrepresentation and reasoning. we extend the expressivepower of the region connection calculus rcc-8 by allowingapplications of the 8 binary relations of rcc-8 not onlyto atomic regions but also to boolean combinations of them.it is shown that the satisfiability problem for theextended language in arbitrary topological spaces is still np; however, it becomes pspace-complete if onlyeucledean spaces are regarded as possible interpretations.in particular, in contrast to pure rcc-8, the languageis capable to distinguishing between connected annon-connected topological spaces.  "," spatial reasoning, qualitative reasoning "
2000,data set editing by ordered projection," we present an editing algorithm based on the projection of the examples in each dimension.the algorithm, that we have called eop, has some interesting characteristics: important reduction of the number of examples from the database; lower computational cost in respect of other typical algorithms due to the absence of distance calculations; conservation of the decision boundaries, especially from the point of view of the application of axis-parallel classifiers; reduction of the decision tree size or the number of decision rules. the performance of eop is showed by comparing the results provided by c4.5 before and after applying it on databases with continuous attributes. the use of eop as preprocessing method for the later application of any axis-parallel learning algorithm convert it in a valuable tool in the field of data mining. "," editing algorithms, axis-parallel classifiers, data mining "
2000,lazy propositionalisation for relational learning," a number of inductive logic programming (ilp) systems have addressed  the  problem  of  learning  first  order  logic  (fol)  discriminant  definitions by first reformulating  the fol learning problem into an  attribute-value one and  then applying efficient learning techniques  dedicated  to  this  simpler  formalism.   the  complexity  of  such  propositionalisation methods is now  in the size of the reformulated  problem  which  is exponential  when  tackling highly  indeterminate  relational   problems.   we  propose   a  method   that  selectively  propositionalises  the  fol  training  set by  interleaving  boolean  reformulation  and  algebraic resolution.   it  avoids,  as much  as  possible,  the generation  of  reformulated examples  which are  not  relevant to the discrimination task, and still ensures that explicit  correct and complete definitions  are learned. we present propal, an aq-like  algorithm, exploiting this  lazy propositionalisation method and  then provide  a first empirical  evaluation on a  standard benchmark  dataset for ilp, the mutagenesis problem. "," inductive logic programming, propositionalisation, boolean learning, test incorporation "
2000,a kohonen-like decomposition method for the traveling salesman problem - knies_decompose," in this paper a new self-organizing map (som) called the kohonen network incorporating explicit statistics (knies) is presented which is used to solve both the traveling salesman problem (knies_tsp) and the hamiltonian path problem (knies_hpp). the primary difference between the som and the knies is the fact that every iteration in the training phase includes two distinct modules - the attracting module and the dispersing module. in the attracting module a subset of the neurons migrate towards the data point that has been presented to the network. this phase is essentially identical to the learning phase of the som. however, subsequent to this phase the rest of the neurons which have not been involved in the attracting module participate in a dispersing (repellent) migration. indeed, these neurons now move away from their current positions in a manner that ensures that the global statistical properties of the data points are imitated by the neurons. thus, although in the som the neurons asymptotically find their places both statistically and topologically, in the knies they collectively maintain their means to be the means of the data points which they represent.	solving large instances of the tsp via self-organizing map is time consuming (when the number of cities is larger than 100 for example). decomposing the problem into subproblems is an intelligent approach to overcome this difficulty. the subproblems can be solved faster because the number of cities for each subproblem is much smaller than the original problem. once the solutions to each subproblem are obtained they may be combined in an elegant way to find a solution for the original problem. the idea of decomposition is not new and was applied to solve large traveling salesman problems. what is new in this paper is that an all-neural decomposition approach is introduced meaning that all the steps of the approach are performed by neural networks. "," neural networks, self-organizing maps, traveling salesman problem "
2000,perfect refinement operators can be flexible," a (weakly) perfect ilp refinement operator was described in [badea and stanciu 99]. it's main disadvantage however is that it is static and inflexible: for ensuring non-redundancy, some refinements of a hypothesis are disallowed in advance, regardless of the search heuristic which may recommend their immediate exploration. (similar problems are faced by progol and other complete and non-redundant systems). on the other hand, there are systems, like foil, which give up completeness for maximum flexibility. but if the heuristic fails to guide the search to a solution, such a system cannot rely on a complete refinement operator to explore alternative paths.in this paper we construct a _dynamically_ perfect refinement operator which combines the advantages of completeness, non-redundancy and flexibility, and which represents one of the best tractable ilp operators one can hope for. ", inductive logic programming 
2000,similarity-based heterogeneous neuron models ," this paper introduces a general class of neuronmodels, accepting heterogeneous inputs in the form of mixtures ofcontinuous (crisp or fuzzy) numbers, linguistic information, anddiscrete (either ordinal or nominal) quantities, with provision alsofor missing information. their internal stimulation is based on an explicit similarity relationbetween the input and weight tuples (which are also heterogeneous).the framework is comprehensive and several models canbe derived as instances --in particular, two of the commonly usedmodels are shown to compute a specific similarity functionprovided all inputs are real-valued and complete. an example family of modelsdefined by composition of a gower-based similarity with asigmoid function is shown to lead to network designs(heterogeneous neural networks) capable of learning fromnon-trivial data sets with a remarkable effectiveness, comparableto that of classical models. "," neural networks, machine learning, knowledge representation "
2000,a similarity-based approach to relevance learning," in several information retrieval (ir) systems there is a possibility for user feedback. many machine learning methods have been proposed that learn from the feedback information in a long-term fashion. in this paper, we present an approach that builds on user feedback across multiple queries in order to improve the retrieval quality of novel queries. this allows users of an ir system to retrieve relevant documents at a reduced effort. two algorithms for long-term learning across multiple queries in the scope of the retrieval system latent semantic indexing have beenimplemented in a system, regressor, in order to test these ideas. the algorithms are based on k-nearest-neighbor searching and back propagation neural networks. training examples are query vectors, and by using latent semantic indexing, the examples are reduced to a fixed and manageable size.in order to evaluate the methods, we performed a set of experimentswhere we compared the performance of latent semantic indexing andregressor. the results demonstrate that regressorautomatically improves on the performance of latent semantic indexing by utilizing the feedback information from past queries. "," information retrieval and presentation, machine learning, neural networks "
2000,solving pomdps using selected past events," we present new algorithms for solving partially observed markov decisionprocesses. the general idea is to use information given by recent pastevent to build an extended space, thus designing a new markovian processwhich can be more easily solved. we formalize these notions by introducing""exhaustive observable"" and giving two theorems underlying the algorithms.experiments are then conducted with each algorithms to show their validity. "," reinforcement learning, planning "
2000,an agent service brokering algorithm for winner determination in combinatorial auctions," deregulation of telecommunications has meant an increase in third-party service provision, personalized service delivery and integrated networks and media. the efficient allocation of services, without human intervention, to satisfy advanced service requirements spanning several networks is a crucial task. this can be modeled as a winner determination problem in combinatorial auctions where there are multiple services, service providers and winner determination criteria (like cost, bandwidth, delay, etc) but we have shown the problem is np-complete.this paper describes a new two-stage algorithm for optimal anytime winner determination. in the first stage, a hierarchical task network planner is used to decompose a task into subtasks that can be solved by the available services. in the second stage, a genetic algorithm with heuristics is used to find the optimal combination of service providers to provide the services identified. we present our algorithm used to solve the second stage in detail and the results from various experiments. the results show the ga finds optimal solutions much quicker than a modified depth-first search with pruning. we also show the genetic algorithm a) finds optimal solutions quicker when deal lengths have a random distribution and b) initial anytime performance is better when deal lengths have an exponential distribution.  "," multi-agent systems, genetic algorithms, ai architectures "
2000,learning to use operational advice," we address the problem of advice-taking in a given domain, in particular for building a game-playing program. our approach to solving it strives for the application of machine learning techniquesthroughout, i.e., for avoiding knowledge elicitation by any other means as much as possible. in particular, we build upon existing workon the operationalization of advice by machine and assume that advice is already available in operational form. the relative importance of this advice is, however, not yet known and can therefore not be utilized well by a program. this paper presents an approach to determine the relative importance for a given situation through reinforcement learning. we implemented this approach for the game of hearts and gathered some empirical evidence on its usefulness through experiments. the results show that the programs built according to our approach learned to make good use of the given operational advice. "," machine learning, reinforcement learning "
2000,efficient asymptotic approximation in temporal difference learning," td(lambda) is an algorithm that learns the value fonction associated to a policy in a markov decision process (mdp). we propose in this paper an asymptotic approximation of online td(lambda) with accumulating eligibility trace, called atd(lambda). we then use the ordinary differentialequation (ode) method to analyse atd(lambda) and to optimizethe choice of the lambda parameter and the learning stepsize, and we introduce atd, a new efficient temporal difference learning algorithm. "," td(lambda), reinforcement learning, machine learning, uncertainty in ai "
2000,learning efficiently with neural networks: a theoretical comparison between structured and flat representations," we are interested in  the relationship between learning efficiency andrepresentation in  the case of supervised neural  networks for patternclassification  trained by  continuous error  minimization techniques,such as gradient descent.  in  particular, we focus our attention on arecently introduced architecture called recursive neural network (rnn)which is  able to  learn class membership  of patterns  represented aslabeled directed  ordered acyclic  graphs (doag).  rnns  offer severalbenefits   compared  to   feedforward  and   recurrent   networks  forsequences.  however, how  rnns compare  to  these models  in terms  oflearning efficiency still needs investigation. in this paper we give atheoretical answer by giving a  set of results concerning the shape ofthe error surface and  critically discussing the implications of theseresults  on the relative  difficulty of  learning with  different datarepresentations.   the  message  of   this  paper  is  that,  wheneverstructured representations are available,  they should be preferred to``flat''  (array based)  representations  because they  are likely  tosimplify learning in terms of time complexity. "," machine learning, neural networks "
2000,automatic generation of local internet catalogues using hierarchical radius-based competitive learning," this paper presents a way to cluster (local) html document sets in an hierarchical way. the hierarchical clustering is performed using the hierarchical radius-based competitive learning (hrcl) neural network that has been developed by the authors and is made public for the first time. after a detailed discussion of the algorithm, hrcl clustering as well as retrieval results will be presented. the hrcl clustering results in a hierarchical multi-resolution view of the underlying (local) html data collection, consisting of clusters (with its cluster centroids), sub-clusters, sub-subclusters and so forth. comparisons with the self-organizing map (som) as well as with the single-pass statistical clustering already used in the smart retrieval show that hrcl is - in contrast to the latters - able to obtain both, a real vector quantization as well as a good descrition of distributions of the globular input clusters. moreover, hrcl retrieval results can excede the retrieval while using the som. results of all generated hrcl hierarchies will be combined and rendered in the way of an internet catalogue resembling the known yahoo directory (an example will be shown in the paper). the internet search can finally be accelerated using the automatically generated (sub-)cluster centroids. "," search, neural networks, information retrieval and presentation "
2000,q-surfing: exploring a world model by significance values in reinforcement learning tasks," reinforcement learning addresses the problem of learning to select actions in unknown environments. due to the poor performance of reinforcement learning in more complex and thus more realistic tasks with large state spaces and sparse reinforcement, much effort is done to speed up learning as well as on finding structure in problem spaces. models are introduced in order to improve learning by allowing to plan on the internal world model. this implies that a directed exploration in the model is a very important factor in relation to better learning results.  in this paper we present an algorithm which explores the model by computing so-called significance values for each state. using these values for model planning, during early stages knowledge propagation is enhanced, during later stages values in important states retain higher values and might therefor be useful for future decomposition of state spaces. empirical results in a simple grid navigation task will demonstrate this process. "," reinforcement learning, reuse of knowledge "
2000,learning to reason about actions," we focus on learning representations of dynamical systems that can becharacterized by logic-based formalisms for reasoning about actions and change, where system's behaviours are naturally viewed as appropiate logical consequences of the domain's description.to this end, inductive logic programming is reformulated using logic programming for dynamic systems. the study of dynamic domains is started with domains modelable with classical action theories and is progressively enhanced to manage more complex behaviours. "," machine learning, reasoning about actions and change, inductive logic programming, cognitive robotics "
2000,discovering conceptual relations from text," non-taxonomic relations between concepts appear as a major building block in common ontology definitions. in fact, theirdefinition consumes much of the time needed for engineering anontology. we here describe a new approach to discover non-taxonomic conceptual relations from text building on shallow text processing techniques. we use a generalizedassociation rule algorithm that does not only detect relationsbetween concepts, but also determines the appropriatelevel of abstraction at which to define relations. this iscrucial for an appropriate ontology definition in order thatit be succinct and conceptually adequate and, hence, easy tounderstand, maintain, and extend. we also perform an empiricalevaluation of our approach with regard to a manually engineeredontology. for this purpose, we present a new paradigm suited to evaluate the degree to which relations that are learned matchrelations in a manually engineered ontology. "," text mining, ontologies "
2000,towards the re-identification of individuals in data files with non-common variables," record linkage is used to establish relationships between records of two different data files. in this work, record linkage is studied for files that correspond to the same set of individuals but that do not share a common set of variables. under this circumstance, classical techniques can not be applied. we present an approach to this problem based on clustering techniques and knowledge integration ones. in this way, common underlying structures in both files can be detected and re-identification is possible. this approach is based on some basic assumptions that are made explicit in this work.  ", data mining and knowledge discovery 
2000,team-solvability: a model-theoretic perspective," at present, the extension of {\em formal learning theory} to themulti-agent case considers ``teams'' of agents sharing a commonend. success is achieved if one or more of the agents is successful,and cooperation is not involved in the team formation. unfortunately, this is rarely the idea of ``successful team'' wehave in mind. one generally expects agents' behaviour to influenceeach other in a way that is not captured by the present paradigms.a real problem in extending single agent learning methods tomulti-agent setting is thus determining {\em paradigms of cooperation}. this paper makes a contribution to the solution of this problem.first, we advance a paradigm of cooperation as a kind of two-personrepeated game and compare it to a major paradigm of solvability forisolated agents. second, we pay attention to a subset of {\em unsuccessful} agents whotake advantage from teamwork. for these agents, cooperation is provedto be a key of success.the formal results are raised within the model-theoretic tradition offormal learning theory. "," discovery, inductive logic, multi-agent systems "
2000,"arguments, dialogue, and negotiation"," in the past few years there have been a number of proposals formechanisms for negotiation between agents that make use ofargumentation. while stressing the representational advantages of anargumentation based approach, these proposals have largely been vagueon the subject of how the generation and interpretation of argumentsfits into the process of negotiation. in short, the protocol underwhich the argumentation takes place has been missing. this paperaddresses this gap, proposing a particular protocol which is suitablefor negotiation, and illustrating its use on an example from theliterature. ", multi-agent systems 
2000,"integrating individual, organizational and market level reasoning for agent coordination"," in this paper we articulate a multi-level view of agentcoordination and provide solutions for an integrated agent architecture that addresses all the levels. at the individual agent level, we model the decision making problem faced by individual agents that need todiscover their highest utility goals and the plans to achieve them.individual level plans normally contain goals that lie outsidethe agent's control. to achieve them, the agent needs to team upwith other agents in the organization. at the organizational level,we show how organizational structures can be used to form the minimum cost teams needed to achieve such goals.individual and organizational reasoning rely on knowing the utilitiesof the options available to agents. often, these utilities are not given in advance, they must bediscovered dynamically by market driven interaction.at the market level, we give a constraint optimization formulation to multi attribute utility theory and introduce interaction processes that allow agents to discover how to cooperate to optimize their objectives. all levels translate their specific models into a common reasoning infrastructure integrating randomized and systematic search.  "," multi-agent systems, autonomous agents "
2000,run-time selection of coordination mechanisms in multi-agent systems," this paper presents a framework that enables autonomous agents todynamically select the mechanism they employ in order tocoordinate their inter-related activities. adopting this framework means coordination mechanisms move from the realm of being imposed upon the system at design time, to something that the agents select atrun-time in order to fit their prevailing circumstances and theircurrent coordination needs. empirical analysis is used to evaluatethe effect of various design alternatives for the agent's decisionmaking mechanisms and for the coordination mechanisms themselves. "," distributed ai, multi-agent systems, coordination "
2000,"founding agents' ""autonomy"" on dependence theory"," i argue that the notion of ""autonomy"" in agents - which can be evendefinitory - and in particular of ""social autonomy"" (autonomy relatively toother agents) should be defined and operationalised in terms of dependencyrelationships. i generalise our original theory of dependence relations -where dependence is relative to some lacking resource or ability - toinclude deontic conditions (permission, authorisation) and alsoinformation, instructions, control, etc. this view of autonomy, and thisgeneralised notion of ""resource"" are strictly based on the theory of actionand of its necessary inputs and powers, and in particular on thearchitecture of cognitive agents (specifically bdi ones). i claim that thisframework gives a principled perspective to the theory of autonomy andpredicts the various possible dimensions of dependence and regulation. "," autonomous agents, philosophical foundations, multi-agent systems, cognitive modelling "
2000,resolution in a logic of rational agency," a resolution based proof system for a temporal logic of possible belief ispresented and justified. this logic represents a combination of thebranching-time temporal logic ctl and the modal logic kd45. since such combinations of non-classical logics are often used in agent theories for specifying complex properties of rational agents, the resolution system presented here provides a basis for the verification of such specifications. "," automated reasoning, temporal reasoning, multi-agent systems, theorem proving, deduction "
2000,architecture for agent programming languages," as the field of agent-based systems continues to expand rapidly, oneof the most significant problems lies in being able to compare and evaluate the relative benefits and disadvantages of different systems.in part, this is due to the various different ways in which these systems are presented.  one solution is to develop a set of architectural building blocks that can be used as a basis for further construction (to avoid re-inventing wheels), and to ensure a strong and effective, yet simple and accessible, means of presentationthat allows for comparison and analysis of agent systems. in this paper, we address this issue in providing just such an architectural framework by using the 3apl agent programming language as a startingpoint for identification and specification of more general individual agent components.  this provides three additional benefits: it moves the work further down the road of implementation, contributes to a growing library of agent techniques and features, and allows a detailed comparison of different agent-based systems specified in similar ways. ", autonomous agents 
2000,"logical systems for reasoning about multi-agent belief, information acquisition and trust"," in this paper, we consider the influence of trust on theassimilation of acquired information into an agent's belief. byuse of modal logic tools, we characterize the relationship amongbelief, information acquisition and trust both semantically andaxiomatically. the belief and information acquisition arerespectively represented by kd45 and kd normal modal operators,whereas trust is expressed by a modal operator with minimalsemantics. one characteristic axiom of the basic system is ifagent $i$ believes that agent $j$ has told him the truth of $p$and he trusts the judgement of $j$ on $p$, then he will alsobelieve $p$. in addition to the basic system, some variants andfurther axioms for trust and information acquisition are alsopresented to show the expressive richness of the logic. "," multi-agent systems, knowledge representation, belief revision, distributed ai "
2000,theory and properties of a selfish protocol for multi-agent meeting scheduling using fuzzy constraints ," this paper develops an agent-based methodologyfor meeting scheduling. in such a multi-agent system,each agent acts on behalf of a user. for each  user the meetingscheduling problem is modeled by afuzzy constraint satisfaction problem, and an appointmentis made by negotiations among agents.a negotiation procedure concerns with two key components: the protocolfor organizing negotiations among agents, and the operator for  fusingagents' individual evaluations for a feasible time slot.in particular,we  propose a kind of selfish protocol,and  present an axiomatic frameworkfor fusion operators. in addition, a  meeting scheduling example is used to illustratethe proposed methodology. "," scheduling, multi-agent systems, constraint satisfaction, uncertainty in ai, distributed ai "
2000,intention reconsideration in theory and practice," intelligent systems operating in complex dynamic environments need the ability to integrate robust plan execution with higher level reasoning. this paper describes work to combine low level navigation techniques drawn from mobile robotics with deliberation techniquesdrawn from intelligent agents. in particular we discuss the combination of a navigation system based on fuzzy logic with a deliberator based on the belief/desire/intention (bdi) model. we discuss some of the subtleties involved in this integration, and illustrate it with an example. ", autonomous agents 
2000,scheduling meetings using distributed valued constraint satisfaction algorithm," scheduling meetings is generally difficult in that it attempts to satisfy the preferences of all participants. however, all participants can agree to a schedule in which a portion of their preferences are not satisfied, since preferences are regarded in terms of their relative importance. in this paper, we formalize a meeting scheduling as a distributed valued constraint satisfaction problem (dvcsp) and propose an algorithm for solving over-constrained problems formalized as a dvcsp by means of constraint relaxation based on importance. our algorithm can relax lower priority constraints and schedule meetings that satisfy as many of the important constraints as possible under over-constrained conditions. we show a group schedule management system, consisting of multiple agents, and cols which can concretely specify users' preferences as constraints. our experiments show that our algorithm can discover a semi-optimal solution to over-constrained meeting scheduling problem in practical time. we can conclude that our algorithm is cost effective in comparison to another method that can find an optimal solution. "," multi-agent systems, constraint satisfaction, scheduling "
2000,achieving coordination through combining joint planning and joint learning," there are two major approaches to activity coordinationin multiagent systems. first, by endowing the agents withthe capability to jointly plan, that is, to jointly generate hypothetical activity sequences. second, by endowing the agents with the capability to jointly learn, that is, to jointly choose the actions to be executed on the basis of what they know from experience about the interdependencies of their actions. this paper describes a new algorithmcalled jpjl (``joint planning and joint learning'')that combines both approaches. the primary motivation behind this algorithm is to bring together the advantages of joint planning and joint learning while avoiding their disadvantages. experimental results are provided that illustrate the potential benefits and shortcomings of the jpjl algorithm. "," multi-agent systems, distributed ai, learning, planning "
2000,languages for negotiation," this paper considers the use of logic-based languages for multi-agentnegotiation. we begin by motivating the use of such languages, andintroducing a formal model of logic-based negotiation. using thismodel, we define two important computational problems: the successproblem (given a particular negotiation history, has agreement beenreached?) and the guaranteed success problem (does a particularnegotiation protocol guarantee that agreement will be reached?) wethen consider a series of progressively more complex negotiationlanguages, and consider the complexity of using these languages.  weconclude with a discussion on related work and issues for the future. ", multi-agent systems 
2000,multilingual generation for translation in speech-to-speech dialogues and its realization in verbmobil," this  paper presents the generation   module  of the  speech-to-speechdialogue translation  system  verbmobil.  spontaneous   speech,  largemultilingual   vocabulary,    difficulty  of  the   translation  task,robustness and real-time constraints make  the design of such a modulevery challenging.   in order to  overcome these  difficulties, we havedeveloped a system based on a  general kernel and the declarativity ofthe knowledge sources.    this  fully implemented  system  proves  thepractical relevance of  several techniques such as  constraint-solvingfor the microplanning  task  or hpsg-to-tag compilation for  syntacticrealization.  in  addition to the successful  deployment of our moduleinto the  verbmobil  system, the kernel    has been adapted   to otherdomains and tasks.  "," natural language processing, human language technology, constraint satisfaction "
2000,repairing queries in a mediator approach," in this paper, we study unsatisfiable queries posed to a mediatorin an information integration system and expressed in a logicalformalism. first, we characterise conflicts as the minimal causesof the unsatisfiability of a query. then, thanks to our cooperativequery answering process, we produce its set of repairs. arepair is a query that does not generate any conflict and that hasa common generalisation with the initial query and is semanticallyclose to it. "," cooperative answering, query failure, information integration systems, diagnosis "
2000,essence: a portable methodology for acquiring information extraction patterns," one important issue when constructing informationextraction systems is how to obtain the knowledge needed foridentifying relevant information in a document. in most approaches to this issue, the human expert intervention is necessary in many steps of the acquisition process. in this paper we describe essence, a new methodology that reduces significantly the need for human intervention. it is based on ela, a new algorithm for acquiring information extraction patterns. the distinctive features of essence and ela are that 1) allow to automatically acquire ie patterns fromunrestricted text corpus representative of the domain, due to 2)the ability of identifying surrounding context regularities for semantically relevant concept-words for the ie task by using non domain specific lexical knowledge tools andsemantic relations from wordnet, and 3) restricting the humanintervention to only the definition of the task and the validation and typification of the set of ie patterns obtained. the use of a general purpose ontology and syntactic tools of generalapplication allows the easy portability of the methodology andreduces the expert effort. results of the application of thismethodology for acquiring extraction patterns in a muc-like task are also shown. "," information extraction, natural language processing, machine learning, knowledge acquisition "
2000,bringing information extraction out of the labs: the pinocchio environment, pinocchio is   an environment   for  developing  informationextraction  applications.  new   applications and  languages  can becovered  by  just  writing declarative  resources.    information isrepresented uniformly throughout  the architecture: all the modules usethe same  input  structure and   the same   type of declarativeresources. modules are implemented via the same basic processors andshare a common environment for resource development  and debugging. the result  is  an  environment  easy to  use with limited training and skills.   ," natural language processing, information extraction from texts "
2000,naive bayes and exemplar-based approaches to word sense disambiguation revisited," this paper describes an experimental comparison between two standard supervised learning methods, namely naive bayes and exemplar-based classification, on the word sense disambiguation (wsd) problem. the aim of the work is twofold.firstly, it attempts to contribute to clarify some confusing information about the comparison between both methods appearing in the related literature. in doing so, several directions have been explored, including: testing several modifications of the basic learning algorithms and varying the feature space. secondly, animprovement of both algorithms is proposed, in order to deal withlarge attribute sets. this modification, which basically consists in using only the positive information appearing in the examples, allowsto improve greatly the efficiency of the methods, with no loss in accuracy. the experiments have been performed on the largestsense-tagged corpus avaliable containing the most frequentand ambiguous english words. results show that the exemplar-based approach to wsd is generally superior to the bayesian approach, especially when a specific metric for dealing with symbolic attributes is used. "," natural language processing, machine learning, human language processing, bayesian and case-based learning "
2000,a topic segmentation of texts based on semantic domains ," thematic analysis is essential for a lot of natural language processing (nlp) applications, such as text summarization or information extraction. it is a two-dimensional process that has both to delimit the thematic segments of a text and to identify the topic of each of them. the system we present here possesses these two characteristics. based on the use of semantic domains, it is able to structure narrative texts into adjacent thematic segments, this segmentation operating at the paragraph level, and to identify the topic they are about. moreover, semantic domains, that are topic representations made of words, are automatically learned, which allows us to apply our system on a wide range of texts, related to varied domains. "," natural language processing, text analysis, topic segmentation "
2000,a domain knowledge manager for dialogue systems," as information services and domains grow more complex the complexity of dialogue systems increases. they tend to need more and  more domain knowledge and the domain reasoning mechanisms also have to become more sophisticated. to handle these requirements dialogue and task knowledge used by the dialogue manager should be separated from domain knowledge, which instead is handled by a separate module, a knowledge co-ordinator. in this paper we present a proposal of how such a knowledge co-ordinator can be modelled and used in dialogue systems. "," natural language processing, knowledge representation "
2000,a practical system for human-like parsing," this paper describes a human-like natural language parser calledplink.  it works by parsing left-to-right through a sentence andkeeping a complete representation of the partially read sentence.  itdoes this by combining a sophisticated unification-based grammar andgrammar rule selection heuristics.plink also functions in real world applications.  to do this, it mustprocess texts that are not grammatical and does this by combining ageneral grammar and taking advantage of preference levels in the ruleselection heuristics. it has been evaluated on two parsing metrics:parseval [black et al. 1991] and a dependency based metric [lin 1996].plink performs well but below the state of the art.like humans, plink parses in linear time, and generates oneinterpretation that is both syntactic and semantic.  it ispsycholinguistically inspired. "," human language technology, natural language processing, cognitive modelling "
2000,model generation for discourse representation theory," we present a tableaux-based model generation calculus for discourse representation theory (drt), which gives us anincremental approach to discourse processing in the presence of worldknowledge. we show the usefulness of such as system for several discoursephenomena. "," natural language processing, automated reasoning, theorem proving  "
2000,linear regression based alignment of parallel texts using homograph words," about 15% of the vocabulary found in large texts of the official journal of the european communities is the same in its various official languages. if we take, for example, the portuguese-spanish pair, the rate rises to more than 30% since these are similar languages and, for the opposite reason, it drops to about 10% for the pair portuguese-german. this is a wealthy source of information for parallel texts alignment that should not be left unused.bearing this in mind, this paper describes a language independent method that makes use of those words, which are homograph for a pair of languages, in order to align parallel texts. this work was originally inspired and extends work done by pascale fung & kathleen mckeown, and melamed. in order to filter out words that may cause misalignment, we use confidence bands of linear regression analysis instead of statistically unsupported heuristics. we do not get 100% text alignment precision mostly due to term order policies in the different languages. the parallel segments obtained have an average length of four words for case law texts. "," natural language processing, text mining, knowledge acquisition, machine learning "
2000,a theoretical analysis of context-based learning algorithms for word sense disambiguation," word sense disambiguation (wsd) is a central task in the area of  naturallanguage processing. in the past few years several context-basedprobabilistic and machine learning methods for wsd have been presented inliterature. however, an important area of research that has not been giventhe attention it deserves is a  formal analysis of the parameters affectingthe performance of the learning task faced by these systems.  usuallyperformance is estimated by measuring precision and recall of a specificalgorithm for specific test sets and environmental conditions. therefore, acomparison among different learning systems and an objective estimation ofthe difficulty of the learning task is extremely difficult.in this paper we propose, in the framework of computational learningtheory, a formal analysis of the relations between accuracy of acontext-based wsd system, the complexity of the context representationscheme, and the environmental conditions (e.g. the complexity of languagedomain and concept inventory). "," natural language processing, machine learning "
2000,a family of defeasible reasoning logics and its implementation," defeasible reasoning is a direction in nonmonotonic reasoning that isbased on the use of rules that may be defeated by other rules. it is asimple, but often more efficient approach than other nonmonotonicreasoning systems. this paper presents a family of defeasible reasoning formalisms built around nute's defeasible logic. we describe themotivations of these formalisms and derive some basic properties andinterrelationships. we also describe a query answering system that supports these formalisms and is available on the world wide web. "," nonmonotonic reasoning, knowledge representation, logic programming "
2000,logic programs with compiled preferences," we describe an approach for compiling preferences into logic programsunder the answer set semantics. an ordered logic program is anextended logic program in which rules are named by unique terms, andin which preferences among rules are given by a set of dedicatedatoms. an ordered logic program is transformed into a second,regular, extended logic program wherein the preferences are respected,in that the answer sets obtained in the transformed theory correspondwith the preferred answer sets of the original theory. our approachallows both the specification of static orderings (as found in mostprevious work), in which preferences are external to a logic program,as well as orderings on sets of rules. in large part then, we areinterested in describing a general methodology for uniformlyincorporating preference information in a logic program. since theresult of our translation is an extended logic program, we can makeuse of existing implementations, such as dlv and smodels. to this end,we have developed a compiler, available on the web, as a front-endfor these programming systems. "," logic programming, knowledge representation "
2000,an argumentation-theoretic characterization of defeasible logic, defeasible logic is an efficient non-monotonic logic that is definedonly proof-theoretically. it has potential application in some legaldomains. we present here an argumentation semantics for defeasible logicthat will be useful in these applications. our development differs atseveral points from existing argumentation frameworks since there areseveral features of defeasible logic that have not been addressed inthe literature. ," nonmonotonic reasoning, knowledge representation, deduction "
2000,characterizing general preferential entailments," a preferential entailment is defined by a binaryrelation, or ``preference relation''. this relation can be either among interpretationsor among sets of interpretations.the relation can be also among ``states'' which are ``copies of interpretations'',or ``copies of sets of interpretations''.this provides four kinds of preferential entailments.the paper deals mainly with propositional logic, however this work appliesalso to the first order case and indications aregiven in order to describe the situation in first order logic.what we do here is to provide a characterization result for the most general version described above, and to compare with the known characterizations of the ``simplest'' versions.it appears that the apparently most complicated notion possesses by far the simplest characterization result. a by-product of our results is that the definitioncan be simplified without loss of generality: we can define directly the relation among sets of interpretations, eliminating the need for ``states''in this case. thus only three kinds of ``preferential entailment'' remain. "," nonmonotonic reasoning, knowledge representation, automated reasoning "
2000,equivalent sets of formulas for circumscriptions," circumscription is a way of using classical logic in order to modelize rules with exceptions and implicit knowledge.formula circumscription is easier to use in order to modelize a given situation.we describe when two sets of formulas give the same resultwhen circumscribed, introducing two kinds of equivalence.for ordinary equivalence, the two sets give the same circumscription, andfor the strong equivalence, when completed by any arbitrary set, the two setsgive the same circumscription.the strong equivalence corresponds simply to havingthe same closure for logical ``and'' and  ``or''.for the ordinary equivalence, there exists also always a greatest set.our answer to these two equivalence problems for the case of propositional formula circumscription is exhaustive.this gives rise to various notions of formulas positive with respect to a given set of formulas.when starting from ordinary propositional circumscription, things remain simple enough, and we provide a syntactical description of all these equivalent sets, even in the infinite case. "," nonmonotonic reasoning, knowledge representation, automatic reasoning "
2000,gadel: a genetic algorithm to compute default logic extensions," in the area of default logic, after many theoretical works, some operational systems are now able to deal with real world applications.however, due to the theoretical complexity of the problem, finding a default logic extension in a practical way is not yet possible inwhole generality.  our work presents a new methodology to implement an automated default reasoning system based on genetic algorithms techniques.  the aim of this paper is not to exhibit a program able to compute extensions of every kind of default theories in a minimaltime, but to present a new promising approach of the problem.  weprovide here a formal description of the components required for adefault logic extension search based on genetic algorithms principles.we give also a formal result to ensure the correctness of our approachand some experimental results that are very interesting w.r.t.  otherexisting systems. "," nonmonotonic reasoning, genetic algorithms "
2000,warp - a reactive planner integrated in an environmental decision support system for wastewater treatment plant management," in this paper we present an environmentaldecision-support system (named dai-depur+) and a reactive linear planner (named warp), and theirapplication to the wastewater domain. theenvironmental decision-support system is:able to receive and process on- and off-line information about awastewater treatment process, capable of understanding this information for applying anexistent model or for creating a new model of the treatment-systemdynamics,competent in using this understanding for autonomously planningthe control of a wastewater treatment plant (wwtp) and for supportinghuman management of a wwtp.the warp planner is a real-time, continuously-active, intelligentsystem being developed for representing and using experts' proceduralknowledge for accomplishing pre-defined goals. at the present time it isa single-level, mainly data-driven planner. "," planning, reactive control, real-time systems, knowledge-based systems "
2000,plan recognition through goal graph analysis," we present a novel approach to plan recognition based on a two-stage paradigm of graph construction and analysis. first, a graph structure called a goal graph is constructed to represent the observed actions, the state of the world, and the achieved goals as well various connections between these nodes at consecutive time steps. then, the goal graph is analysed at each time step to recognise those achieved goals consistent with the actions observed so far and the valid plans for the recognised goals or part of the recognised goals. we describe two algorithms for goal graph construction and analysis in this paradigm, that are both provably sound, polynomial-time and polynomial-space. we have tested these algorithms in two domains with up to 245 goal schemata and 100000 possible goals, in which excellent performance has been achieved in terms of efficiency, accuracy and scalability. ", planning 
2000,extending talplanner with concurrency and resources," we present talplanner, a forward-chaining planner based on the use  of domain-dependent search control knowledge represented as  temporal formulas in the temporal action logic (tal). tal is a  narrative based linear metric time logic used for reasoning about  action and change in incompletely specified dynamic environments.  tal is used as the formal semantic basis for talplanner, where a  tal goal narrative with control formulas is input to talplanner  which then generates a tal narrative that entails the goal formula.  we extend the sequential version of talplanner, which has previously  been shown to have impressive performance based on benchmark  testing, in two respects: 1)talplanner is extended to generate  concurrent plans, where operators have varied durations and  internal state; and 2) the expressiveness of plan operators is  extended for dealing with several different types of resources. the  extensions to the planner have been implemented and   concurrent planning with resources is demonstrated using an  extended logistics benchmark. "," reasoning about actions and change, planning, knowledge representation "
2000,flexible graphplan	," traditionally, planning problems are cast in terms of imperative  constraints that are either wholly satisfied or wholly violated. it  is argued herein that this framework is too rigid to capture the  full subtlety of many real problems. a new flexible planning problem  is defined which supports the soft constraints often found in  reality. a solution strategy using the graphplan framework is  described and it is shown how flexible plan extraction can be cast  as the solution of a sequence of linked dynamic flexible constraint  satisfaction problems (dfcsps). a recently developed dfcsp  algorithm, flexible local changes, is exploited to solve this  sequence. for a given flexible problem, this framework can  synthesise a range of plans that trade the compromises made in a  plan versus plan length. the proposed technique is evaluated on a  range of flexible problems and against leading boolean solvers on  benchmark problems. "," planning, constraint satisfaction "
2000,"continual planning with time-oriented, skeletal plans"," in dynamically changing environments a planning systemdoes not have all the required information at the first place andthe world state can change, rendering the original plan invalid.consequently, taking planning to complex, real-world domainscalls for close coupling of planning and plan execution, known as continual planning.in domains where a high degree of adaptability is crucial and little a priori knowledge is available, explorative approaches are suitable. in other domains where comprehensive knowledge is available and human life depends on plan execution,like in medical treatment planning, only approved and validated procedures are admitted. even more, these procedures constitute a rich asset in the planning task. to combine the utilization of such knowledge with the flexibility necessary in real-world applications,the asgaard system integrates time-oriented, skeletal planningwith real-time monitoring. it features a monolithic frameworkfor the creation, verification, execution, and critiquing ofplans with extensive covering of temporal modeling anddata abstraction. in this paper, we describe the monitoring andplan adaptation capabilities of asgaard. "," planning, meta-heuristics for ai "
2000,graph based representation of dynamic planning ," dynamic planning concerns the planning and execution of actions in a dynamic, real world environment. its goal is to take into account changes generated by unpredicted events occurred during the execution of actions. in this paper we develop the theoretic model of dynamic planning presented in [moraitis and tsoukias, 99]. this model proposes a graph representation of possible, efficient and best plans of agents acting in a dynamic environment. agents have preferences among consequences of their possible actions performed to reach a fixed goal. environmental changes and their consequences are taken into account by several approaches proposed in the so-called ""reactive planning"" field. the dynamic planning approach we propose, handles in addition changes on agents´ preferences and on their methods to evaluate them and it is modeled as a multi-objective dynamic programming problem.  "," planning, actions and change, decision theory, autonomous agents "
2000,heuristic planning with resources," in this paper we present grt-r, an enhanced version of the grt planner with the ability to explicitly handle resources. grt is a domain independent heuristic planner for strips worlds, which work in the space of the states. the heuristic computes off-line, in a pre-processing phase, estimates for the distances between the domain's facts and the goals. these estimates are utilized during the search process, in order to obtain values for the distances between the intermediate states and the goals. we propose to explicit represent resources in a numerical way. we consider resources that are only consumed and they cannot be produced either by internal or external actions. grt-r uses vectors of values, where the first one estimates the distance between a fact and the goals, while the remaining values estimate the amount of resources needed to achieve that fact. grt-r assigns each fact with a number of such vectors, each one of them having minimum value to either at least one resource or the distance. these vectors correspond to alternative ways to achieve the fact. concerning the search process, grt-r assigns similarly each intermediate state with a vector of values. this vector is based on the vectors of the state's facts, with the intention minimize the distance between the state and the goals, without to exceed the available resources. performance results show that grt-r copes well in domains that embody resrources, like the mystery introduced in the aips-98 planning competition. "," planning, search, automated reasoning "
2000,incorporation of temporal logic control into plan operators," domain-specific control information is often essential in solvingdifficult planning problems efficiently. temporal logics area declarative and expressive way of expressing such controlinformation for almost any form of planning. in this paper weinvestigate the integration of temporal logic control informationinto plan operators. for a given control formula phi and operators o,we produce a new set o_phi of operators that works like o underthe control of phi. we show that for a subset of temporal formulaethe compilation causes only a low-polynomial increase in the numberof operators, that the size of plans is not affected, and thatthe control information speeds up planners in an amount that iscompetitive with what is achieved with temporal logic control asformula progression. "," planning, automated reasoning "
2000,a graph-based approach for pocl planning," in this paper we show that despite the great success of some planningapproaches, partial-order planning is still an efficientand valid approach for tackling planning problems. the goal of this paper is to show that the effort needed by a partial-order planner (pop) to solve a problem can be dramatically reduced.by properly exploiting the problem knowledge it is possible toobtain an approximate plan which is afterwards used as initial plan input to a pop. this plan will always contain actions which must necessarily appear in a valid solution and, therefore, the task of the pop will be simply to add the missing actions thus leading to a significant reduction in search space. in this paper, we will focus on the modifications achieved on a standard partial-order planner to adapt it to this new planning approach. "," planning, knowledge adquisition "
2000,dispatchability conditions for schedules with consumable resources  ," earlier work on scheduling by autonomous systems has demonstrated thatschedules in the form of simple temporal networks, with intervals ofvalues for possible event-times, can be made ""dispatchable"", i.e.,executable incrementally in real time with guarantees against failuredue to unfortunate event-time selections. in the present workwe show how the property of dispatchability can be extended tonetworks that include constraints for consumable resources. we describeconditions that can be placed on resource use by activities duringexecution to avoid oversubscription while insuring scheduledispatchability. we also describe strategies that can be used inconjunction with these conditions to increase flexibility of resourceallocation. this work indicates that flexible handling of resourceuse can be safely extended to the execution layer to provide moreeffective deployment of consumable resources. "," scheduling, real-time systems, constraint satisfaction "
2000,temporal constraint networks in action," this paper presents an study for the logical formalization oftemporal constraint networks (tcn), and how it can be introducedin a nonmonotonic logic for reasoning about actions and change called al2. the resulting logic, alt, combines the representational capabilities of both formalisms, allowing temporal constraints in the conditions of causal rules that describe the domain behavior. weshow how one of the main features of al2, called {\em pertinence}, provides an appropriate basis for directly relating time points notonly to action occurrences, but also to relevant changes in fluents.  "," temporal reasoning, reasoning about actions and change "
2000,turning high-level plans into robot programs in uncertain domains," the actions of a robot like lifting an object are often best thought of aslow-level processes with uncertain outcome. a high-level robot plan can be seenas a description of a task which combines these processes in an appropriate wayand which may involve nondeterminism in order to increase a plan'sgenerality. in a given situation, a robot needs to turn a given plan into anexecutable program for which it can establish, through some form of projection,that it satisfies a given goal with some probability. in this paper we will showhow this can be achieved in a logical framework. in particular, low-levelprocesses are modelled as programs in \pgolog, a probabilistic variant of theaction language \golog. high-level plans are like ordinary \golog\ programsexcept that during projection the names of low-level processes are replacedby their \pgolog-definitions. "," cognitive robotics, reasoning about actions and change, planning "
2000,a modal logic for epistemic tests," we study a modal logic of knowledge and action, focussing on knowledge gathering actions. such tests increase the agent'sknowledge. we propose a semantics, and associate an axiomatics and a rewriting-based proof procedure. "," reasoning about actions and change, knowledge acquisition, diagnosis, logics of knowledge and action "
2000,an embedding of congolog in 3apl," several high-level programming languages for programming agents androbots have been proposed in recent years. each of these languageshas its own features and merits. it is still difficult, however, tocompare different programming frameworks and evaluate the relativebenefits and disadvantages of these frameworks. in this paper, wepresent a general method for comparing agent programming frameworksbased on a notion of bisimulation, and use it to formally comparethe languages congolog and 3apl. congolog is a concurrent languagefor high-level robot programming based on the situation calculus.congolog provides a logical perspective on robot programming, butalso incorporates a number of imperative programming constructs likesequential composition. 3apl is an agent programming language and itssemantics offers a more operational perspective on agents. thelanguage is a combination of logic and imperative programming andprovides operators for beliefs, goals and plans of an agent. weshow that congolog and 3apl are closely related languages byconstructing an embedding of congolog in 3apl. this embedding showshow congolog programs can be translated into equivalent 3apl programs.a number of interesting issues need to be resolved to construct theembedding. these include a comparison of states in 3apl with situationsin congolog, the form of basic action theories, complete vs. incompleteknowledge, and execution models concerning the flow of control ofagent programs. "," autonomous agents, logic programming, foundations "
2000,id-logic and the ramification problem for the situation calculus," the goal of this paper is to extend the general solution to the ramification problem for the situation calculus to the case where causal rules specifying indirect effects of actions may form cycles or cycles through negation.we formulate causal dependencies as rules of inductive definitions. then we give a semantics to a logic for representing inductive definitions by translating definitions to sentences of fixed-point logic. we describe a regression-based procedure for generating successor state axioms from inductive rules, and consider several benchmark examples. finally, we show that boundedness is a sufficient condition for translating causal theories to successor state axioms. "," reasoning about actions and change, causal reasoning, knowledge representation "
2000,bayes rules in finite models," bayesianism has a strong normative claim for uncertainty management,but is not undisputed.  most foundational justifications involve someregularity assumption (continuity of an auxiliary function) that hasbeen recently criticized, particularly for finite models.we show how such assumptions can be replaced by precise and weakerassumptions for finite but extensible domains. these assumptions are weaker than those used in alternative justifications, which is shown by their inadequacy for infinite domains. they are also more compelling. we propose and prove sufficient the following common sense assumtions:refinability:if we have already made a particular splitting of a statement intosub-cases, by adding new statements implying it, it should always bepossible to refine another statement in the same way, and with thesame plausibilities in the new refinement. this modification shouldnot lead to inconsistency.information independence:if a statement is refined by several new symbols, it should bepossible to state that they are information independent, so thatknowledge of one does not affect the plausibility of the other.any real-valued, strictly monotone and consistent plausibility measureof a finite model satisfying these assumptions is equivalent toprobability. for any non-equivalent measure, there is a finiteextension that is inconsistent.  we conjecture that a similar analysisis applicable to lindleys betting paradigm justification ofprobability as canonical uncertainty measure. "," uncertainty in ai, philosophical foundations, bayesian learning "
2000,non-linear modeling of a production process by hybrid bayesian networks," this paper shows how non-linear functions can be approximated by hybrid bayesian networks. the basic idea is to make a piecewise linear approximation with several base points. this approach is applied to an engineering domain and the accuracy is compared to gibbs sampling. great accuracy is shown even at non-continuous functions. due to the general underlying principle,it is possible to adapt this type of network to other domains.  "," probabilistic networks, bayesian learning "
2000,solving the inverse representation problem," in this paper, we present an approach to extract most relevant information from a (semi-)quantitative knowledge base, e.g., from a probability distribution. relevance here is meant with respect to some appropriate inductive inference process, like maximum entropy inference (me-inference) in probabilistics. so in particular, the method developed in this paper is apt to solve the inverse maxent problem, computing from a distribution in a non-heuristic way a set of conditionals that me-represents that distribution. since we only make use of one special characteristic of me-inference, this method may as well be applied to other, similar inference processes.  "," data mining and knowledge discovery, knowledge representation, uncertainty in ai, conditionals, maximum entropy "
2000,empirical comparison of probabilistic and possibilistic markov decision processes algorithms," classical stochastic markov decision processes (mdps) and possibilistic mdps aim at solving the same kind of problems, involvingsequential decision under uncertainty.the underlying uncertainty model(probabilistic / possibilistic) and preference model (reward / satisfaction degree) change, but the algorithms, based on dynamic programming, are similar. so, a question maybe raised about when to prefer one model to another, and for which reasons.the answer may seem obvious when the uncertainty is of an objectivenature (symmetry of the problem, frequentist information) and when theproblem is faced repetitively and rewards accumulate. it is less clearwhen uncertainty and preferences are qualitative, purely subjective and when the problem is faced only once.in this paper we carry out an empirical comparison of both types ofalgorithms (stochastic and possibilistic), in terms of ``quality'' of the solutions, and time needed to compute them. ", uncertainty in ai 
2000,visual design support in dynamic probabilistic networks for driver modelling, understanding inference in probabilistic networks is a crucialpoint during the design phase. their causal structure and locallydefined parameters are intuitive to human experts. the globalsystem induced by the local parameters can lead to results notintended by the human expert. comprehending the behavior ofdynamic probabilistic networks (dpn) for tuning the model isa time consuming task. therefore this paper introduces toolssupporting the design phase. the application of these tools isshown by means of a dpn for human driver modelling. ," probabilistic networks, temporal reasoning, user modeling "
2000,symmetry breaking in constraint programming," we describe a method for symmetry breaking during search  (sbds) in constraint programming.it has the great advantage of not interfering with heuristic choices. it guarantees to return a unique solution from each setof symmetrically equivalent ones,  which isthe one found first by the variable and value ordering heuristics.  we describe an implementation of sbds in ilog solver, and applications to low autocorrelation binary sequences and the  $n$-queens problem.  we discuss how sbds can be applied when there are too many symmetries to reason with individually, and give applications in graph colouring and ramsey theory. "," constraint programming, constraint satisfaction "
2000,towards understanding conceptual differences between minimaxing and product propagation," advantages and disadvantages of minimaxing versus product-propagationback-up rules for game tree searching have been intensively discussedin the literature. so far, examinations have almost exclusively beencarried out through experiments, demonstrating slight superioritiesfor one or the other of these back-up rules, depending on the particular game chosen or on assumptions underlying simulations onabstract game trees. in contrast to these purely quantitativeinvestigations, we aim at elaborating differences in strength of theseback-up rules by characterizing properties of critical situations inwhich the differences between these back-up rules prove relevant.evidence from the examinations carried out suggest that degrees ofquality of the static evaluation function used, ranges of the heuristicvalues associated with the positions compared, and frequencies of valuecombinations are major properties influencing the suitability of eitherback-up rule. the results provide hints for assessing degrees ofcompetence of minimaxing and product propagation, which can be exploited beneficially for motivated combinations of the two back-uprules, according to game tree properties observed in a particular game. "," search, automated reasoning "
2000,towards real-time search with inadmissible heuristics," we discuss the effect of heuristic functionsthat violate admissibility and consistencyon real-time search algorithms.although the flexibility and the leaning abilityof lrta*, the popular real-time search algorithm,makes it an attractive candidate for the genericmodel of intelligent autonomous agents,its behavior is far from rational, in thatit is too optimistic in the face of uncertainty.such behavior is known to be relaxed by incorporatingpessimism into the algorithm, via the use of inadmissible (or overestimating) heuristic functionsand it is also known that the use of such functionsoften improves the performance of real-time search.however, there is no theory that well describes when and why these algorithms benefit from such property of heuristic functions yet.in this paper, as a step towards fully understand and utilize the advantage of such nonstandard heuristic functions,we present several proofs of the properties oflrta* and the moving-target search algorithm, whenthe heuristic function violates admissibility or consistency. "," search, real-time systems "
2000,is there a constrainedness knife-edge?," recent work on search has identified an intriguing feature dubbed theconstrainedness knife-edge by walsh (proc. aaai-98, 406--411)whereby overconstrained problems seem to become on average even moreconstrained as the search goes deeper, while underconstrained onesbecome less constrained. the present paper shows that while theknife-edge phenomenon itself is real, many of the claims that havebeen made about it are incorrect. it is not always associatedwith criticality, it is not a function of the problem so muchas of the algorithm used to solve it, and it does not help toexplain the peculiar hardness of problem instances near phasetransitions. despite the negative findings, the upshot is that walsh'swork has opened a fascinating line of research which will surely repayfurther investigation. ", search 
2000,dynamic user modeling in a web store shell," we describe a framework for the dynamic revision of user models in an adaptive web store shell, which tailors the suggestion of goods, as well as the interaction style, to the characteristics and interests of the individual user.the behavior of the user is monitored, by segmenting it on the basis of the focus spaces explored in the browsing activity, and the actions performed by the user are summarized into abstract facts. these facts are used for revising the user model via the interpretation process performed by a bayesian network which relates user features to user behavior. "," user modeling, intelligent user interfaces "
2000,beat tracking with musical knowledge," when a person taps a foot in time with a piece of music,they are performing beat tracking.beat tracking is fundamental to the understanding of musical structure,and therefore an essential ability for any system which purports to exhibitmusical intelligence or understanding.we present a multiple agent beat tracking system which performs thistask, estimating the locations of musical beats in musical performance data.this approach to beat tracking requires no prior information about theinput data, such as the tempo or time signature; all required information isderived from the performance data.for constant tempo performances, previous beat tracking systems have provedsuccessful; however, these systems fail when there are large variations intempo.we examine the role of musical knowledge in guiding the beat tracking process,and show that a system equipped with knowledge of musical salience is ableto track the beat of music even in the presence of large tempo variations.results are presented for a large corpus of expressively performedclassical piano music (13 complete sonatas), containing a full range oftempos and much variability in tempo within sections.with the musical knowledge disabled, the beats are trackedabout 75% correctly; the inclusion of musical knowledgeraises this figure to over 90%. "," art and music, multi-agent systems "
2000,a frame representation of user requirements for automated data visualization," we propose a new visualization system that automatically creates a graph from a large amount of numerical data corresponding to user's requirements. the system has the following functions:to understand the user's requirements, to select the necessary data from a datatable, to convert and aggregate the selected data to a suitable granularity, and to visualize the data with an appropriate graphical style. in this paper, we introduce a semantic frame in order toformally deal with the user's requirement. the semantic frame consists of two subframes: one is mainly for selecting and restructuring data, and the other is mainly for choosing an appropriate graph-type and emphasizing focal characteristics. we also explain an aggregationalgorithm for making the necessary datatable. with this system, a user only inputs his/her requirements described in natural language, and then he/she is able to get an appropriategraph from an enormous amount of data even if the data granularity is not well suited to therequirement. "," intelligent user interfaces, knowledge representation, knowledge-based systems "
2000,composition through agent negotiation: a step towards fluid interface," one of the next challenges in research on human computerinterfaces (hci) is to give users the means to perceive andmanipulate easily huge quantities of information in the minimumamount of time. intelligent user interfaces (iui) have beenproposed as a means to solve this problem. the intelligencein the interface makes the system more flexible and moreadaptable. one subset of iui is adaptive interfaces. an adaptiveinterface modifies its behaviour according to some definedconstraints (technical, psycho-sensory and user-defined ones) inorder to best satisfy globally all of them. in this paper weintroduce the concept of fluid interfaces, a particular kind ofadaptive interface on which the flow of information composesitself smoothly without any predefined fixed pattern but accordingto several parameters. we introduce the problems raised byself-composition in user interface, then describe our propositionof an agent architecture to achieve fluidity. information ismodelled through agents that negotiate according to technical,psycho-sensory and user-defined constraints in order to compose adynamic display. we detail a prototype that validates the modeland introduce a european project which is derived from the modeland tackles industrial applications. "," intelligent user interfaces, adaptive systems, information presentation, multi-agent systems "
2000,action categorization from video sequences," this article presents a framework for extracting relevantqualitative chunks from a video sequence. the notion of qualitative descriptors, used to perform the qualitative extraction, will be first described.a grouping algorithm operates on the qualitative descriptionsto generate a real-time qualitative segmentation of theimage flow. then, simple pattern recognition methods are used to extract abstract description of basic actions such as ""push"", ""take"" or ""pull"". the method proposed hereprovides an unsupervised learning technique to generateabstract description of actions from a video sequence.  "," perception, qualitative reasoning, signal understanding, vision "
2000,autonomous environment and task adaptation for robotic agents," this paper investigates the problem of improving the performance ofgeneral state-of-the-art robot control systems by autonomouslyadapting them to specific tasks and environments. we proposemodel- and test-based transformational learning (mttl) as acomputational model for performing this task.  mttl uses abstractmodels of control systems and environments in order to proposepromising adaptations. to account for model deficiencies resultingfrom abstraction, hypotheses are statistically tested based onexperimentation in the physical world.we describe xfrm-learn, an implementation of mttl, and apply it to theproblem of indoor navigation.  we present experiments in which xfrm-learn improves the navigation performance of a state-of-the-art high-speed navigation system for a given set of navigation tasks by at least 29 percent. "," robotics, autonomous agents, adaptive systems "
2000,constructing teleo-reactive robot programs," this paper addresses the problem of synthesising programs for teleo-reactive robots. a robot of this kind possesses an internal ruleset, or program, which determines how it reacts to external stimuli. although the robot possesses no inherent goal, its program is designed so as to predispose it towards achieving some overall desired effect, possibly in cooperation with others. it will usually have only a limited perception of the world in which it operates, and its reactive rules will not express any direct link between what it perceives and what its behaviour will achieve. because of this, composing a suitable program by hand can require some ingenuity. this paper presents a formal framework within which such programs may be systematically constructed. it describes such a construction process and illustrates its application, including simulation in a prolog environment. "," cognitive robotics, multi-agent systems, autonomous agents, automated reasoning "
2000,can representation be liberated from symbolism: modeling robot actions with roboticles," since its origins, artificial intelligence has been faced withthe challenge to control robot operations through the so calleddeliberative thinking paradigm. robot actions are governedby a reasoning process which needs robots to acquire informationfrom the environment to update their internal world model causingthe failure to generate an useful action in a finite amount of time.the framework of roboticles, appearing in this paper and borrowedfrom the theory of complex dynamical system, is a tool to deal with quantities like energy} or effort, to symbolize the amountof sensor information a robot is fed with, later dissipated by theaction of its effectors. the dynamical law works as a triggeringmechanism which controls the flow of energy between sensors andeffectors so that its current value can be interpreted, in somesense, as the internal world model handled by the agent.environment changing, detected through sensor signals, results inmoving the representation point of the system on the energy surface.moreover, actions issued by robot effectors dissipate energy in a wayto maintain the working point of the system in a stationary state wherethe energy supplied by sensor signals is balanced by the effortdelivered to the effectors. "," autonomous agents, robotics, dynamical systems, adaptive systems "
2000,a knowledge-based approach for lifelike gesture animation," the inclusion of additional modalities into the communicative behaviorof virtual agents besides speech has moved into focus of human-computer interface researchers, as humans are more likely toconsider computer-generated figures lifelike when appropriate nonverbal behaviors are displayed in addition to speech. in thispaper, we propose a knowledge-based approach for the automatic generation of gesture animations for an articulated figure. it combines a formalism for the representation of spatiotemporal gesture features, methods for planning individual gestural animationsw.r.t. to form and timing, and formation of arm trajectories.finally, enhanced methods for rendering animations from motor programs are incorporated in the execution of planned gestures. the approach is targetted to achieve a great variety of gestures aswell as a higher degree of lifelikeness in synthetic agents. "," lifelike and believable characters, intelligent user interfaces "
2000,on-line model modification for adaptive object recognition," this paper presents and validates a method for adaptive object recognition in image sequences under dynamic perceptual conditions, and consequently, under changing object characteristics. the approach builds a close-loop interaction between object recognition and model modification systems. object recognition applies a modified rbf classifier in order to recognize objects on a current image of a sequence.  the feedback reinforcement generation mechanism evaluates the classification results when compared to the previous images and activates classifier modification, if needed. classifier modification selects a strategy and employs four behaviors in adapting the classifier's structure and parameters. these behaviors include accommodation, translation, generation, and extinction applied to selected classifier components. accommodation modifies the component's boundary/spread. translation shifts a given component over the feature space. generation creates a new component of the rbf classifier. extinction eliminates components that are no longer in use. the evolved rbf model is verified in order to confirm applied model modifications. experimental results are presented for indoor and outdoor image sequences. the approach is validated and compared with traditional non-adaptive methods for object recognition. this validation tracks an error rate, classifier complexity, and the utilization of model modification behaviors over the image sequences.  "," adaptive systems, vision, machine learning, adaptive object recognition "
2000,activity recognition from video sequences using declarative models," we propose here a new approach for video sequence interpretation based on  declarative models of activities. the aim of the video sequence  interpretation is to recognize incrementaly certain situations, like states  of the scene, events and scenarios, in a video stream, in order to  understand what happens in the scene.  the input of the activity  recognition is an {\it a priori} model of the scene and human tracked in  it.  the activity recognition is composed of two subproblems. first,  end-users have to declare all the activities in a configuration phase.  secondly, the declared models must be automatically recognized.  to solve  the first problem, we propose a homogeneous declarative formalism to  describe all the activities (states of the scene, events and scenarios).  the activities are described by the conditions between the objects of the  scene. to solve the second problem, we translate it into a constraints  satisfaction problem. then, we use a classical csp algorithm to recognize  the activities in video sequences. finally, we present some results to show  the robustness of the approach. "," reasoning about actions and change, knowledge-based systems, knowledge representation, vision "
2000,gaining insights into web customers using web intelligence," the web is fast maturing into an important marketing medium that provides businesses with the ability to undertake one-to-one marketing and provide truly personalised services to their customers. log files provide a rich source of information about customers that can be used for achieving these goals. however, the knowledge required to undertake such activity is far removed from what is provided by the current breed of analysis tools for web log files. in this paper we describe an innovative web intelligence tool, easyminer, that provides marketers in e-marketing departments with access to data mining technology that can sift through large amounts of data collected automatically on customer interaction with the businesses web site and bring to light useful marketing knowledge. "," web intelligence, web mining, sequence discovery, web session segmentation "
2000,a dialogue environment for accessing public administration data: the tamic-p system," the purpose of this paper is to show that by a combination of state ofart technologies it is    possible to realize effective   and powerfuldialogue   systems for information-seeking  applications.   tamic-p, aprototype  natural  language based interface  will  be described.  thesystem  is designed to  support  public administration desk  operatorsduring an  information  session with  a  citizen.  three aspects  worktogether  to determine the  system usability: (i)  a strong multimodalapproach, allowing  full  interchangeability between natural  languageand direct  manipulation;  (ii) a robust  dialogue component, allowingcomplex  sequences   of  turns;  (iii) the  adaptive    integration ofdifferent information  sources,  such as  personal  data  and  relatednorms. "," natural language access to databases, dialogue systems, multimodality "
2000,a knowledge-based system for the design of rubber compounds in motor racing," this work presents p-race, a knowledge-based system designed tosupport the formulation of rubber compounds of tyre tread, inorder to take part (and win) in motor racing. because of thedifferent competence involved in the decision making process (thecompound designer and the race engineer), multiple knowledgerepresentations have been adopted, and integrated into a uniquecase-based reasoning (cbr) computational framework. the case-basedapproach captures the episodic knowledge characterising most ofthe race engineer reasoning activity. moreover, a dedicatedrepresentation formalism (abstract compounds machine - acm) hasbeen created in order to allow the core knowledge about rubbercompounds to be explicitly represented, computed and integrated inthe cbr architecture. the most meaningful and innovativecontribution of p-race consists of a general case-basedarchitecture where the adaptation step is performed by the acmchemical formulation model. this system has been developed for themotorsports department of pirelli tyres, where it is currently inuse. "," case-based reasoning, rubber compounds design, abstract compound machine model "
2000,flexible text classification for financial applications: the facile system," this    paper describes  an   advanced system   for multilingual  textclassification adaptable to different user needs.  the system has beeninitially developed  as  an applied  research  project involving  bothresearch centres,  industrial bodies and  end-user organisations.  theproject was a considerable success story in  the     financial  field.three  different successful applications were released at the users' site involved in the project. moreover the system was adopted by the  main italian financial news agency where it is used  to provide classified  news for an  external pay-to-view service.  the system  has been running continuously  since january 1998.   its architecture integrates  modules  based on both someinnovative artificial intelligence methodologies and commercial tools.this   shows  that state-of-the-art ai  techniques  aremature enough to provide real world applications. "," natural language processing, text classification "
2000,personalisation technologies for the digital tv world," the information overload problem is becoming a serious issue on the internet and users are finding it increasingly difficult to quickly locate the right information at the right time. content personalisation techniques may provide a useful solution by enabling information services to respond to the implicit and explicit preferences of individual users. in this paper we describe an innovative personalised tv listings service (ptv), which has been successfully deployed on a number of irish web sites and which has attracted over 15,000 users in the first year of operation. "," information personalisation, case-based reasoning, automated collaborative filtering, intelligent internet applications "
2000,an autonomous cooperative system for material handling applications," the key to the creation of flexible automation is to use inherentredundancy in the capabilities of the system being controlled.this requires the control system to be continuously self-configurableunder varying conditions.  this behavior is expensive and difficult to accomplish using conventional control systems.  in making control systems truly flexible, control hardware, networks, and software willneed to be combined with artificial intelligence capabilities to achieve self-configuration and high flexibility. the ability of the control system to react to and predict changes will ultimately determine the economic viability of that system.  in this paper, an autonomous cooperative system to control material handling systems is presented.  each section of the material handling system is provided with sufficient intelligence and autonomy to flexibly control the operations of the physical equipment.  control is carried out while the overall operation of the system is optimized through cooperation among the controlled sections. the operation of the material handling system is observed during conditions of equipment and product changes.  the results from simulation show how an autonomous cooperative system can be used to reduce the impact of changes in material handling applications. "," autonomy, cooperation, distributed control, negotiation, flexibility, material handling systems "
2000,tiger: continuous diagnosis of gas turbines," the tiger system is in continuous use on 20 gas turbines across four continents.  continuously performing fault diagnosis of industrial gas turbines used for power generation.  tiger receives several hundred data inputs at a once per second interval and performs several layers of fault detection and diagnosis using expert system techniques in a temporal reasoning rulebase.  development of the system has now extended for more than six years and more than 80 man-years.   the diagnostics are supported by an extensive user interface, data archive, trending and display system.  across the 20 current locations, tiger has helped to identify and diagnose over 700 incidents representing 200 different fault categories.  because of the high value of gas turbines and the high expenses in operating them, tiger's benefit is considerable and has been documented at $150,000 or more for many of the locations.  "," diagnosis, knowledge based system, power generation, application "
2000,artificial intelligence techniques for diabetes management: the t-iddm project," we present a successful application of artificial intelligence (ai)methodologies in the context of a telemedicine service for diabeticpatients management, developed within the eu-funded t-iddmproject. the system architecture is distributed, and composed by apatient unit and by a medical unit, connected through atelecommunication link. several ai methods have been exploited toimplement the t-iddm functionality. the data base relies on anexplicit representation of the domain ontology. temporal abstractionsand other intelligent data analysis techniques are used to analyse thepatient's monitoring data; the case based reasoning (cbr) methodologyis applied to perform the knowledge management task. finally, cbr isintegrated with rule based reasoning to provide physicians with amulti-modal reasoning decision support tool. the t-iddm service isbeing tested through a small on field trial in pavia; the firstresults, though preliminary, seem to substantiate the hypothesis thatthe use of an ai-based telemedicine system could present an advantagein the management of type 1 diabetic patients, leading to a more tightcontrol of the patients' metabolic situation, in a cost-effective way. "," intelligent data analysis, case based reasoning, knowledge based systems, telemedicine, diabetes "
2000,autosteve: automated electrical design analysis, (missing) , (missing) 
2000,remote agent: an autonomous control system for the new millennium," on may 17th 1999, the remote agent (ra) became the first artificialintelligence based closed loop autonomous control system to take control of aspacecraft. the ra commanded nasa's new millennium deep space one probe whenit was 65 million miles away from earth. for a period of one week this systemcommanded the ds1's ion propulsion system, its camera, its attitude controland navigation systems. a primary goal of this experiment was to provide anonboard demonstration of spacecraft autonomy. this demonstration included bothnominal operations with goal-oriented commanding and closed-loop planexecution, and fault protection capabilities with failure diagnosis and recov-ery, on-board replanning following unrecoverable failures, and system-levelfault protection.this paper describes the remote agent experiement (rax) and the model basedapproaches to planning and scheduling, plan execution and fault diagnosis andrecovery technologies developed at nasa ames research center and the jetpropulsion laboratory. "," autonomy, closed loop control, constraint-based planning, scheduling, temporal networks "
2000,valens: a knowledge based tool to validate and verify an aion knowledge base," we present valens (valid engineering support), a tool for the validation and verification (v&v) of a knowledgebase (kbs). validation techniques become more and more important when knowledge based systems are widely used to automate business critical processes. the tool we present can be used during and after the development of a kbs and focuses on the logical verification of the kbs. the techniques used to verify a kbs are metarules, an inference engine to verify hypotheses posted by metarules (proof by processing) and metainformation. "," verification, validation, rulebased knowledgebases "
2002,toward an automatically generated theory of coordination — empirical explorations, we define the role and mechanisms of coordination in intelligent agent societies (iass). we then outline our approach and the underlying design principles aimed at the automatic generation of a theory of coordination. such theory would assist in designing new iass and provide trouble-shooting tools for suboptimally functioning iass. we also describe in this paper the decisions that have been made in this endeavor. we have been able to show via a simplified model that the approach is feasible and can produce results. the project is one of the few examples of work in experimental ai. ," multi-agent systems, autonomous agents, distributed ai "
2002,southamptontac: designing a successful trading agent," this paper describes the design, implementation and evaluation of southamptontac, an agent that participated in the second international trading agent competition. in the course of the competition's approximately 600 games, southamptontac achieved the highest mean score and the lowest standard deviation. "," autonomous agents, multi-agent systems, auctions "
2002,multi-agent models for searching pareto optimal solutions to the problem of forming and dynamic restructuring of coalitions," the first part of this paper presents a coalition formation method for multi-agent systems which finds a pareto optimal solution without aggregating the preferences of the agents. this protocol is adapted to problems requiring coordination by coalition formation, where it is undesirable, or not possible, to aggregate the preferences of the agents. the second part proposes an extension of this method enabling dynamic restructuring of coalitions when changes occur in the system.   "," multi-agent systems, distributed ai "
2002,avoiding resource conflicts in intelligent agents," an intelligent agent should be rational, in particularit should at least avoid pursuing goals which are definitely conflicting.  inthis paper we focus on resource conflict in agents that use a planlibrary organised around goals.  we characterise different types of resourcesand define resource requirements summaries.  we give algorithms for derivingresource requirements, using resource requirements  to detect conflict, andmaintaining dynamic updates of resource requirements. we also discuss ways ofresolving resource conflict.  our approach does not represent time, rather itkeeps resource summaries current. this enables an agent's decisions to be madeon the basis of up-to-date information and allows us to develop efficientruntime (online) algorithms.   "," autonomous agents, automated reasoning, multi-agent systems "
2002,polynomial algorithms for clearing multi-unit single-item and multi-unit combinatorial reverse auctions," this paper develops new algorithms for clearing multi-unitsingle-item and multi-unit combinatorial reverse auctions.specifically, we consider settings where bidders submit their bidsin the form of a supply function and the auctions havesub-additive pricing with free disposal. our algorithms are basedon a greedy strategy and we show that they are of polynomialcomplexity. furthermore, we show that the solutions they generateare within a finite bound of the optimal. "," multi-agent systems, autonomous agents "
2002,on modal logic interpretations of games, multi-agent environments comprise decision makers whose deliberationsinvolve reasoning about the expected behaviour of other agents. apposite concepts of rational choice have been studied and formalizedin game theory and our particular interest is with their integration in a logical specification language for multi-agent systems.this paper concerns the logical analysis of the game-theoretical notions of a (subgame perfect) nash equilibrium and that of a (subgame perfect) best response strategy.extensive forms of games are conceived of as kripke frames and aversion of propositional dynamic logic is employed to describe them. we show how formula schemes of our language characterize those classes of frames in which the  strategic choices of the agents can be said to be nash-optimal.our analysis focusses on extensive games of perfect information without repetition. ," multi-agent systems, game theory, dynamic logic, decision theory "
2002,situated cooperative agents: a powerful paradigm for mri brain scans segmentation," to cope with the difficulty of 3d mri brain scans segmentation, specification and instantiation of a priori models should be constrained by local images characteristics. we introduce situated cooperative agents for the extraction of domain and control knowledge from image grey levels. their dedicated behaviours, i.e segmentation of one type of tissue, are dynamically adapted function of their position in the image, topographic relationships and radiometric information gradually gained during local region growing processes. acquired knowledge is gathered and shared via qualitative maps. incremental refinement of the segmentation is obtained through the combination, distribution and opposition of solutions concurrently proposed by the agents. "," autonomous agents, multi-agent systems, vision "
2002,an argumentation-based semantics for agent communication languages," in recent years, the importance of defining a standard framework foragent communication language (acl) has been widely recognized.however, classical proposals (mentalistic semantics and social semantics) fail to meet the objectives of verifiability andflexibility requiered in complex interactions involving heterogenousagents possibly designed by different programmers.in this paper we propose an acl which allows heterogenous agentsto engage flexibly in the different kinds of dialogues identifiedby walton and krabbe. to this end, we propose a two-layered semantics which includes a commitment level and a reasoning level based on argumentation.  "," autonomous agents, argumentation, dialogue "
2002,verification of protocols for automated negotiation ," this paper presents the verifying aspect of our work for verified, unambiguous and sharable protocols with desirable properties to facilitate automated negotiation. a protocol is represented as an abstract theory in a multi-modal meta-language, called anml, thereby enabling the verification and proof of certain protocol correctness properties. furthermore, such logical theories allow the use of ai techniques for reasoning about goal satisfaction. the two case studies discussed in this paper are two protocols proposed in fipa auml, which can be shown to contain errors by using our framework. we provide improved representations in anml and when possible in auml.  "," multi-agent systems, autonomous agents, reasoning about actions and change, verification and validation "
2002,adaptive combination of behaviors in an agent," agents are of interest mainly when confronted with complex tasks. we propose a methodology for the automated design of such agents (in the framework of markov decision processes) in the case where the global task can be decomposed into simpler -possibly concurrent- sub-tasks. this is accomplished by automatically combining basic behaviors using reinforcement learning methods. the main idea is to build a global policy using a weighted combination of basic policies, the weights being learned by the agent (using simulated annealing in our case). these basic behaviors can either be learned or reused from previous tasks since they will not need to be tuned to the new task. furthermore, the agents designed by our methodology are highly scalable as, without further refinement of the global behavior, they can automatically combine several instances of the same basic behavior to take into account concurrent occurences of the same subtask. "," reinforcement learning, reasoning under uncertainty, autonomous agents, reuse of knowledge, machine learning  "
2002,agents and their cities," in this paper we look at the developments of agent technology and the challenges, which will enable agent technology to play a key role in the internet space. this paper describes the design of high-level interoperable agent services through the concept of agentcities, its motivations and drivers. it introduces the different initiatives, which aim at creating an open network of agent platforms, in which complex agent services can be developed, deployed and tested. finally, the key challenges to be addressed to make this initiative a success are evaluated – that of - creating high-level interoperability of heterogeneous services in a dynamic environment to enable large-scale access to e-services within agent environments. "," multi-agent systems, ontologies "
2002,engineering issues in inter-agent dialogues ," this paper presents a logical framework for modeling of complex dialogues between intelligent and autonomous agents. our overall approach builds on the assumption that an agent is composed of a set of modules, each of them conveying the appropriate knowledge to carry out a certain dialogue type, such as deliberation, negotiation, persuasion, etc. much attention has been paid in keeping our framework as operational as possible, in that the architecture of agents and their conversational protocol are thoroughly interrelated. due to the proposed knowledge structure, set of application-independent rules (called dialogue policies) and the combination of backward and forward reasoning, the framework can generate automatic dialogues between agents. "," multi-agent systems, autonomous agents, dialogues between agents, multi-agent communication "
2002,building trading agents: challenges and strategies," with the advent of the internet, trading in electronic market places has become common practice for an increasing number of businesses and individuals. one of the most efficient ways of negotiating for goods and services is via auctions. although implementing an agent to take part in an auction for a single good is relatively simple, developing an agent to participate in simultaneous auctions offering complementary and substitutable goods is a complex task. in this paper we discuss the challenges of the trading agent competition which features a complex benchmark e-market problem and we present the strategies of two agents that participated successfully in the competition, one of them being placed among the eight finalists. "," autonomous agents, game playing, multi-agent systems "
2002,how situated agents can learn to cooperate by monitoring their neighbors' satisfaction," this paper addresses the problem of cooperation between learning situated agents. we present an agent’s architecture based on a satisfaction measure that ensures altruistic behaviors in the system. these cooperative behaviors are obtained by reaction to local signals emitted by the agents following their satisfaction. then, we introduce in this architecture a reinforcement learning module in order to improve individual and collective behaviors. the satisfaction model and the local signals are used to define a compact representation of agents’ interactions and to compute rewards of the behaviors. thus agents learn to select behaviors that are well adapted to their neighbor’s activities. simulations of heterogeneous robots working on a foraging problem demonstrate the interest of the approach. "," autonomous agents, reinforcement learning, multi-agent systems, robotics "
2002,competing in a queue for resource allocations among non-cooperative agents," in this paper, we investigate a multi-agent non-cooperative game for resource allocations based on an m/d/1 queuing model. specifically, agents with common goals to maximize utility are deployed to compete with each other to bid or bribe for quicker service provided by the server. the bid/bribe of each agent in the queue is not revealed, but the outcomes, in terms of the pair of (bid/bribe; total waiting time), are publicly available from the server. agents choose from one of three available strategies: random strategy, nash equilibrium strategy and linear regression strategy, for their decision-makings. bayesian update is integrated into the linear regression technique for searching an optimal bid/bribe. besides, weighted average, second order autoregressive process (ar(2)), and random walk are utilized to predict service speed. after each agent obtained service, it re-evaluates its strategy and adjusts it accordingly. results show that in the long run, the dominant strategy depends on the service speed. when the service speed is low, random strategy dominates the society. but if the service speed is high, linear regression strategy dominates. the model can be extended to study agent-based social simulations and decentralized scheduling for resource allocations in an open multi-agent system. "," multi-agent systems, distributed ai, reasoning about actions and change "
2002,"strong, fuzzy and smooth hierarchical classification for case-based problem solving "," this paper explains how case-based problem solving can have benefitfrom a hierarchical organisation of problems based on a generalityrelation. three adaptation-guided retrieval processes are described.the strong classification in a problem hierarchy is a classicaldeductive process. it is based on the generality relation betweenproblems which organises the hierarchy. the fuzzy classification is afuzzification of the strong classification. it is based on a fuzzygenerality relation between problems, which can be seen as anon-symmetrical similarity measure. the smooth classification extendsthe fuzzy classification: it is also based on a similarity ordissimilarity measure but takes into account problem and solutionadaptation knowledge. these processes have been successfullyimplemented in two case-based reasoning systems: resyn/cbr in thedomain of organic synthesis and kasimir/cbr in the domain of cancertreatment. "," case-based reasoning, reuse of knowledge, automated reasoning, knowledge-based systems "
2002,recommendation engineering," we present a new approach to product recommendation that addresses the limitations of the standard case-based reasoning (cbr) approach of retrieving a list of cases that are most similar to a target query.instead, the target query is used to construct a retrieval network inwhich cases selected for initial presentation to the user are representative of cases that differ from the target query in similar ways. by following links in the retrieval network, the user can examine alternative solutions with no need to await the retrieval of new cases. other advantages of the approach include increased diversity among the cases initially presented to the user and the ability to explain why cases are recommended in much the same way as a human salesperson might explain their relevance.   "," case-based reasoning, information retrieval "
2002,grounded models as a basis for intuitive and deductive reasoning: the acquisition of logical categories," grounded models differ from axiomatic theories in establishing explicit connections between language and reality that are learned through language games. this paper describes how grounded models are constructed by visually grounded autonomous agents playing different types of language games, and explains how they can be used for intuitive reasoning.     it proposes a particular language game that can be used for simulating the generation of logical categories (such as negation, conjunction, disjunction, implication or equivalence), and describes some experiments in which a couple of visually grounded agents construct a grounded model that can be used for spatial reasoning.  "," cognitive modelling, machine learning "
2002,a multi-agent and emergent approach to learner modelling," user modelling has been a central issue in the development of user-adaptive systems, which behaviour is usually based on the user's preferences, goals, interests and knowledge. this is particularly the case when the user is a student and the system is a computer-based learning environment. the aim of this paper is to present a mechanism for diagnosing and modelling learner's conceptions based on a theoretical model of conceptions. our approach takes the diagnosis of learner's conceptions as the emergent result of the collective actions of agents sharing an environment, transforming it and as a consequence influencing new actions. we apply techniques from voting theory for group-decision-making. this approach is at the core of the design of a distance learning platform for the learning of geometry. "," user modeling, multi-agent systems, cognitive modelling "
2002,an intelligent inference approach for user interaction modeling in a generic agent based interface system," research has shown that a typical user exhibits a certain pattern when interacting a computer system. this work infers the user interaction habits automatically from the actions that the user and the application produce. the user interaction habits can enable an interface agent to provide assistance to a user or operate the application on the user’s behalf. all these actions are automatically segmented into several meaningful action sequences, which are stored in the user’s interaction history. an inference algorithm is used to construct the user’s interaction model from his/her interaction history. probabilities of each action’s next possible action are calculated. interface agents then provide assistance based on a user’s interaction model. the interaction model is modified automatically as the interface agents collect more actions. this method is embedded within a generic agent based interface system. information on the prototype system, some experiments and future work are presented. "," interaction model, interaction habits, user model, intelligent interface agents, task, inference "
2002,stochastic constraint programming," to model combinatorial decision problems involving uncertaintyand probability, we introduce stochastic constraint programming. stochastic constraint programs contain both decision variables (which we can set) and stochastic variables (which follow a probability distribution). they combine together the best features of traditional constraint satisfaction, stochastic integer programming, and stochastic satisfiability. we give a semantics for stochastic constraint programs, and propose a number of complete algorithms and approximation procedures. finally, we discuss a number of extensions of stochastic constraint programmingto relax various assumptions like the independence between stochastic variables, and compare with other approachesfor decision making under uncertainty.              "," constraint programming, constraint satisfaction "
2002,synthesis of object models from partial models: a csp perspective," in this work we present an approach for the synthesis of object models(expressed as constraint satisfaction problems, csps) from views orpartial models (expressed, in their turn, as csps as well).the approach we propose is general enough to consider different typesof features and relationships in the views. this is achieved byintroducing the notion of model representation, where features,relationships and their domains are expressed. the (complete) modelcan be synthesized through a proper algorithm, which provides alabeling between the (complete) model and the partial models'components.  the generated csp representing the synthesized model mustsatisfy (or, better, entail) any constraint among features and anyrelationship occurring in each partial model. the framework is applied for synthesizing object models (i.e., cspdescriptions).  we provide two basic approaches for synthesizing aminimal or a correct model, and we experiment them by considering somecase studies in artificial vision. "," constraint satisfaction, vision, constraint programming "
2002,arc consistency in sat," i introduce a new encoding of binary constraint satisfaction problems (csps) into boolean satisfiability (sat). it is the first encoding which enables arc consistency in the originalcsp to be established by unit propagation in the translated sat instance.   with the curent generation of highly effective sat solvers, i show that the encoding can be used to solve hard instancesof random binary csps.   i also show that translation and solutionin sat is about an order of magnitude slower for the establishmentof arc consitency in binary csps.  finally, i show experimentally that local search algorithms suchas walksat perform much better on the new encoding than on thestandard encoding. "," satisfiability testing, constraint satisfaction, search "
2002,progressive focusing search," this paper deals with value ordering heuristics usedin a complete tree search algorithm. their aim is to guidethe search towards a solution. first,we show the limits of the traditional prospective approach, which usesthe size of the domains of the still unassigned variables. in a veryadvantageous context, where arc consistency is maintained and allowsthe time spent by the dynamic value ordering to be negligible, thespeedup is poor when the problems are hard.then, we present a new value ordering heuristic based on a learning fromfailure scheme. instead of making a choice a priori, an interleavingsearch follows every sub-tree to gather information. after thislearning phase, the algorithm focuses on the most promising one.  thisnew algorithm, named progressive focusing search, is compared tointerleaved depth first search and appears to be efficient for problems onthe phase transition complexity peak. "," constraint satisfaction, search "
2002,pseudo-tree search with soft constraints," pseudo-tree search is a well known algorithm for csp solving. it exploitsthe problem structure to detect and solve independent subproblems. itsmain advantage is that itsrun time complexity is bounded by a problem structural parameter.in this paper, we extend this idea to soft constraint problems.we show that the same general ideas apply to this domain.however, a naive implementationis not competitive with state-of-the-art algorithms, becausesolving independent problems separately may yield a poor algorithmicefficiency. we introduce pt-bb, a branch-and-bound algorithm that performs pseudo-treesearch. it features the use of local upper bounds, with which a goodperformance is obtained. we show that pt-bb combines nicely withrussian doll search, yielding an efficient algorithm. "," constraint satisfaction, search "
2002,an algorithm for multi-criteria optimization in csps," constraint satisfaction and optimization are important areas ofartificial intelligence. however, in many real-life applications,more functions should be optimized at the same time; the userneeds to be provided a set of solutions and a posteriori choosethe most preferable.in this paper, we propose an algorithm for solving multi-criteriaoptimization problems in this setting. the algorithm is complete,i.e., it finds all the non-dominated solutions, and does not makeany assumption on the structure of the constraints nor on the typeof the objective functions. it exploits point quad-trees for therepresentation of the non-dominated frontier, in order toefficiently access the data. we describe the implementation andgive experimental results showing that our algorithm outperformswidely used methods. "," constraint programming, constraint satisfaction, search "
2002,an empirical study of the stable marriage problem with ties and incomplete lists," we present the first complete algorithm for the smti problem, the stable marriage problem with ties and incomplete lists.we do this in the form of a constraint programming encoding of the problem.with this we are able to carry out the first empirical study of the complete solution of smti instances.in the stable marriage problem (sm) we have n men and n women. each man ranksthe n women, giving himself a preference list. similarly each woman ranksthe men, giving herself a preference list. the problem is then to marrymen and women such that they are stable i.e. such that thereis no incentive for individuals to divorce and elope. this problem is polynomial timesolvable. however, when preference lists contain ties and are incomplete (smti) theproblem of determining if there is a stable matching of size n is then np-complete, as is the optimisation problem of finding the largest or smallest stable matching. in thispaper we present constraint programming solutions for the smti decision and optimisation problems,  a problem generator for random instances of smti, and an empirical study of this problem. "," constraint programming, constraint satisfaction, search "
2002,a theoretical framework for constraint hierarchy solvers," in the paper we propose a framework describing constraint hierarchy solvers and thus providing a theoretical foundation for construction of such solvers. the framework is based on the decomposition of the constraint hierarchy into constraint cells, i.e., chunks of constraints that are solved together. using various decompositions we can describe a scale of constraint hierarchy solvers from the refining method (the constraint cells correspond to the hierarchy levels) till the local propagation methods (each constraint cell contains exactly one constraint). we highlight important features of the decompositions that form sufficient conditions in soundness and completeness theorems. "," constraint programming, constraint satisfaction, nonmonotonic reasoning "
2002,computing minimal conflicts for rich constraint languages," we address here the following question: given an inconsistent theory,find a minimal subset of it responsible for the inconsistency. such conflicts are essential for problem solvers that make use of conflict-driven search, for interactive applications where explanations are required, or as supporting tools for consistency maintenance in knowledge-bases. conflict computation in ai applications was usually associated with dependency recording as performed by tmss. this techniques, however, have a rather limited applicability for languages that go beyond the expressiveness power of propositional logic. for more powerful languages and solvers constraint suspension appeared, until now, to be the only available alternative for the computation of minimal conflicts.we present here an algorithm for computing minimal conflicts that can be used with powerful constraint languages, e.g. possibly including finite and non-finite variable domains, algebraic and fd constraints,etc. the conflicts are extracted post mortem from the proof (a tree with inferences of the form a & b => c) that lead to the derivationof the inconsistency by an informed search that computes andgeneralizes conflicting relations. the algorithm is based on a simplebut powerful principle that allows to recursively decompose theminimization problem into smaller sub-problems. this principle canalso lay the foundation for efficient constraint suspension algorithms that can be used in case no intermediary results arecached during the constraint solving, i.e. in case no proofstructures are available.  "," constraint programming, model-based reasoning, automated reasoning "
2002,how to establish arc-consistency by reactive agents ," the objective of this paper is to obtain the full global arc consistency of a csp as a result of interactions between simple and reactive agents. thus, a multi-agent model is proposed and discussed in terms of correctness, termination and complexity. this model consists of constraint agents in interaction by exchanging inconsistent values. a comparative analysis with ac-7 is also done. "," constraint satisfaction problems, multi-agent systems, arc-consistency "
2002,"combining hypertree, bicomp, and hinge decomposition"," solving constraint satisfaction problems (csp) is in generalnp-complete. if the structure of the csp is a tree, then thecomputation can be done very effectively in a backtrack-freemanner. there are several methods for converting csps in theirtree-structured equivalent, e.g., hinge decomposition. more recentlyhypertree decomposition was developed and proved to subsume all otherpreviously developed structure-based decomposition methods. in thispaper we report recent results of a hypertree-decompositionimplementation. we further have combined hypertree decomposition, biconnected component decomposition (bicomp),hinge decomposition to improve running time and to make hypertreedecomposition applicable on larger csp instances. the formalrequirements and the empirical results of the combined algorithms arereported.  "," constraint satisfaction, search "
2002,building state-of-the-art sat solvers," the area of propositional satisfiability (sat) has been the subject ofintensive research in recent years, with significant theoretical andpractical contributions. from a practical perspective, a large numberof very effective sat solvers have recently been proposed, most ofwhich based on improvements made to the originaldavis-putnam-logemann-loveland (dpll) backtrack search sat algorithm.the new solvers are capable of solving very large, very hardreal-world problem instances, which more traditional sat solvers aretotally incapable of.despite the significant improvements in state-of-the-art backtracksearch sat solvers, a few relevant questions remain.is a well-organized and well-implemented dpll algorithm enough per se,or should the algorithm definitely include additional searchtechniques? which search techniques are indeed effective for most problem instances? which search techniques cooperate effectively andwhich do not?this paper is a first step towards answering the previous questions.we start by describing the search techniques that have been proposedin recent years for backtrack search sat solvers. afterwards, weempirically evaluate the different techniques, using representativereal-world problem instances. finally, and to conclude, we address theproblem of organizing  effective dpll-based sat solvers. "," satisfiability testing, constraint satisfaction, search, constraint programming, theorem proving, automated reasoning "
2002,improving the evolutionary coding for machine learning tasks," the most influential factors in the quality of the solutions found by an evolutionary algorithm are a correct coding of the search space and an appropriate evaluation function of the potential solutions. the coding of the search space for the obtaining of decision rules is approached, i.e., the representation of the individuals of the geneticpopulation. two new methods for encoding discrete and continuous attributes are presented. our ""natural coding"" uses one gene per attribute (continuous or discrete) leading to a reduction in the search space. genetic operators for this approached natural coding are formally described and the reduction of the size of thesearch space is analysed for several databases from the uci machine learning repository. "," genetic algorithms, machine learning "
2002,evolving bidding strategies for multiple auctions," due to the proliferation of online auctions, there is an increasing need to monitor and bid in multiple auctions in order to procure the best deal for the desired good. against this background, this paper reports on the development and evaluation of a heuristic decision making framework that an autonomous agent can exploit to tackle the problem of bidding across multiple auctions with varying protocols (including english, dutch and vickrey). the framework is flexible, configurable and enables the agent to adopt varying tactics and strategies that attempt to ensure the desired item is  delivered in a manner consistent with the user's preferences. in this context, however, the best strategy for an agent to use is very much determined by the nature of the environment and by the user's preferences. given this large space of possibilities, we employ a genetic algorithm to search (offline) for effective strategies in common classes of environment. the strategies that emerge from this evolution are then codified into the agent's reasoning behaviour so that it can select the most appropriate strategy to employ in  its prevailing circumstances. "," autonomous agents, genetic algorithms "
2002,the use of a genetic algorithm in the calibration of estuary models," this paper describes an artificial intelligence (ai) system for estuarine model design. it is created by the combination of case-based reasoning and genetic algorithm techniques. this application aims to make the utilisation of complicated and expensive hydrodynamic models flexible, cost-effective and accessible to non-specialists. by organising the available knowledge of estuarine modelling into an interactive and dynamic framework, the ai system provides the user with the necessary guidance and information for numerically solving hydro-environmental problems related to estuaries. as soon as a new problem is given to the system, the case-based module for estuarine modelling (cbem) is activated. this module accesses information about estuarine models and estuaries to which numerical solutions have been previously applied. after comparison and evaluation, the case-based search engine returns from its memory the most effective modelling scheme available for the solution of the new problem. the system then calls the genetic algorithm (ga) module which optimises the physical parameters of the selected modelling procedure to suit the new application. the main focus of this paper is on the description of the ga module. this module is developed by combining the classical evolutionary approach with problem-specific information to carry out the required parameter optimisation. the effectiveness of this procedure is illustrated using a one–dimensional hydrodynamic model for the upper milford haven estuary in uk. the comparison between manual and genetic algorithm based calibrations for this specific case suggests that the ga routine can very effectively calibrate estuarine models under realistic situations. this means a significant reduction in the time normally necessary for the implementation of a numerical modelling scheme. this paper describes an artificial intelligence (ai) system for estuarine model design. it is created by the combination of case-based reasoning and genetic algorithm techniques. this application aims to make the utilisation of complicated and expensive hydrodynamic models flexible, cost-effective and accessible to non-specialists. by organising the available knowledge of estuarine modelling into an interactive and dynamic framework, the ai system provides the user with the necessary guidance and information for numerically solving hydro-environmental problems related to estuaries. as soon as a new problem is given to the system, the case-based module for estuarine modelling (cbem) is activated. this module accesses information about estuarine models and estuaries to which numerical solutions have been previously applied. after comparison and evaluation, the case-based search engine returns from its memory the most effective modelling scheme available for the solution of the new problem. the system then calls the genetic algorithm (ga) module which optimises the physical parameters of the selected modelling procedure to suit the new application. the main focus of this paper is on the description of the ga module. this module is developed by combining the classical evolutionary approach with problem-specific information to carry out the required parameter optimisation. the effectiveness of this procedure is illustrated using a one–dimensional hydrodynamic model for the upper milford haven estuary in uk. the comparison between manual and genetic algorithm based calibrations for this specific case suggests that the ga routine can very effectively calibrate estuarine models under realistic situations. this means a significant reduction in the time normally necessary for the implementation of a numerical modelling scheme.  "," case-based reasoning, genetic algorithms, reuse of knowledge "
2002,evolutionary computation in mas design," this paper explores the existing gap between multiagentspecification and implementation and the potential help thatevolutionary programming techniques can bring in.  we present amethodology to help the programmer in the transition from a setof desired global properties expressed as an equation-based modelthat a multi-agent system (mas) must fullfil to an actual society ofinteracting agents.  the evolutionary techniques are seen, withinthis methodology, as a procedure to tune the parameters of thepopulation of agents in order that their aggregated behaviourmaximaly approaches the desired global properties. "," multi-agent systems, design, genetic algorithms "
2002,using multiobjective genetic programming to infer logistic polynomial regression models," in designing non-linear classifiers, there are important trade-offs to be made betweenpredictive accuracy and model interpretability or complexity. we introduce theuse of genetic programming to generate logistic polynomial models, a relativelyinterpretable non-linear parametric model; describe an efficient algorithm based on a division into gp structuredesign and quasi-newton coefficient setting; demonstrate that niched pareto multiobjective geneticprogramming can be used to discover a range of classifiers with different complexity versus ``performance''trade-offs; introduce a technique to integrate a new ``roc dominance'' (receiver operating characteristicperformance) concept into the multiobjective setting; and suggest some modifications to the niched pareto gafor use in genetic programming. the technique successfully generates classifiers with diverse complexityand performance characteristics. "," genetic algorithms, data mining and knowledge discovery "
2002,boltzmann machine for population-based incremental learning," we propose to apply the boltzmann machine (bm) to population-basedincremental learning (pbil). we will replace the statistical modelused in pbil, which assumes that the binary variables of theoptimisation problem are independent, with that of a bm. from thelogarithm of the expectation of the function to maximise, we derivespecific learning rules for the bm.  these learning rules involveexpectations with respect to the distribution of the bm and also toits selected version, in the spirit of pbil or genetic algorithms.new populations are sampled from the bm using traditional gibbssampling. the proposed bm-pbil algorithm alternates gibbs sampling,selection, and update of the parameters.  we evaluate bm-pbil withdifferent classes of functions, compare it to the original pbil, andidentify classes for which it is superior to pbil, in particularfunctions with jumps and quadratic functions. "," genetic algorithms, neural networks, search "
2002,context refinement - investigating the rule refinement completeness of seek/seek2," the pioneer systems for rule refinement are seek and seek2. unlike teiresias, which also has been designed for the acquisition of new inference rules, the systems seek and seek2 are devoted to the refinement of rules for a rheumatology rule-base (a medical diagnosisapplication). this article investigates the general refinement completeness of seek/seek2. a rule refinement system is completeif it solves every possible refinement problem. seek2 hasrefinement heuristics for coping with generalization and specialization problems. complete rule refinement systems should also have refinement capabilities for tackling a third refinement class tobe called context refinement. on the syntactic level, the rheumatology rules which are subject of the seek/seek2 refinements have no logical negation in their if-parts. on the semantic, findings which are to be interpreted by the rheumatology rule-base are represented in positive form only. this seems to be the reason for the incompleteness of seek2 with regard to context problems, i.e., there was no need for context refinement heuristics. a complete rulerefinement system must employ methods for contextualization, as well as generalization and specialization.  "," knowledge acquisition, satisfiability testing, intelligent user interfaces, meta-heuristics for ai, verification and validation, knowledge discovery "
2002,design as interactions of problem framing and problem solving," this paper introduces a conceptual model of framing in design. the proposed ‘recursive model’ takes into account a reflective nature of designing, and it is based on the interplay between two conceptually distinct knowledge sources – an explicit specification of a design problem and a solution to it. the approach is novel in the former investigated aspect that is presented as a semi-formal operation of framing, i.e. interpretation of a given problem using certain conceptual primitives. we argue that the interpretation of design problems has not enjoyed the same rigorous investigation as the problem solving received in both design theory and methodology. in this paper, we focus on the conceptual interaction of the aforementioned activities (problem specification vs. problem solving). furthermore, we discuss two schemas following from the model and featuring refinement of design frame and problem re-framing. "," design, problem framing, nonmonotonic reasoning, knowledge-based systems "
2002,the epistemology of scheduling problems ," scheduling is a knowledge-intensive task spanning over many activities in day-to-day life. it deals with the temporally-bound assignment of jobs to resources. although scheduling has been extensively researched in the ai community for the past 30 years, efforts have primarily focused on specific applications, algorithms, or 'scheduling shells' and no comprehensive analysis exists on the nature of scheduling problems, which provides a formal account of what scheduling is, independently of the way scheduling problems can be approached. research on kbs development by reuse makes use of ontologies, to provide knowledge-level specifications of reusable kbs components. in this paper we describe a task ontology, which formally characterises the nature of scheduling problems, independently of particular application domains and in-dependently of how the problems can be solved. our results provide a comprehensive, domain-independent and formally specified refer-ence model for scheduling applications. this can be used as the ba-sis for further analyses of the class of scheduling problems and also as a concrete reusable resource to support knowledge acquisition and system development in scheduling applications. "," scheduling, ontologies, reuse of knowledge, knowledge-based systems "
2002,cake: a computer-aided knowledge engineering technique," we introduce a computer-aided knowledge engineering (cake) technique for incrementally constructing, managing and querying knowledge structures. these knowledge structures can be used to represent incomplete or approximate world models used for sofbots or robots to reason with in addition to being used for applications on the semantic web. conceptually, the knowledge structures represent a generalization of deductive databases where relations and properties are approximate in nature and the query mechanisms can be contextualized to locally close different parts of the database under an open-world assumption. pragmatically, the knowledge structures are represented using a diagrammatic technique with a formal semantics. a query to a knowledge structure can easily be compiled into a query to a rough relational database. the underlying semantics and computationmechanism insures that any reasoning expressed by the cake diagramsis computable in deterministic polynomial time. the diagrams themselves can be viewed as confederations of granular agents, each responsible for managing a relation or property, or default rules and their adjudication under conflict. this view contributes to the incremental construction, modularity and compositionality of the technique which is demonstrated in the paper. "," knowledge representation, common-sense reasoning, nonmonotonic reasoning, deduction, knowledge-based systems "
2002,personalising on-line configuration of products and services, this paper presents a framework for the management of personalisedconfiguration interactions in business-oriented domains. the overall goal is to assist the user during the configuration task by suggesting suitable choices and providing her with extra-helpful information for making informed decisions.our framework integrates user modelling and personalisation techniqueswith constraint-based configuration techniques and is applied within the cawicoms prototype toolkit for the development of adaptive web-based configuration systems.the personalisation techniques presented in this paper havebeen experimented on the configuration of telecommunication switches. ," intelligent user interfaces, user modeling, configuration, design "
2002,clime: lessons learned in legal information serving," this paper presents lessons learned from the clime  project (1998-2001), aimed at improving the access and understanding of large bodies of legal information through the internet. our approach involves explicit representation of the 'content' of the legal sources consisting of three types of knowledge: (1) the domain being regulated (a domain ontology); (2) normative statements about the domain; (3) resolution mechanisms when normative statements conflict (meta knowledge). the 'legal information server' (lis) provides both conceptual retrieval (cr) and normative assessment (na) on these knowledge structures, given a user's input case.we found that, in contrast to 'traditional' knowledge systems, cases put to a lis are often incomplete and not at the right (most specific) level of detail. therefore we do not only offer a normative judgement based on applicable norms, but also potential exceptions. these exceptions can be (partly) computed off-line based on the representation of the legal sources. our approach has the advantage that knowledge engineers can focus on individual norms, instead of how these may interact with other norms. furthermore, we adopted an incremental modelling approach that shows promise. we begin with the domain ontology for cr and extend it afterwards for na. experiments showed a strong bootstrapping effect, speeding up modelling. finally, we conclude that the best moment for the considerable investment of modelling regulations would be during their drafting. "," knowledge-based systems, ontologies, information retrieval, reuse of knowledge "
2002,similarity between queries in a mediator," answering queries on remote and heterogeneous sources is acomplex task.mediators provide techniques that exploit a domain ontology.to propose a cooperative approach to repair queries withoutanswers, we introduce the notion of solutionwhich is a query close to the user's query.we describe how ontologies can be used to evaluate thesimilarity of two predicates, and then of two queries.the work presented has been developed in the picsel project.examples come from the domain of tourism, the experimentationdomain chosen for this project. "," cooperative answering, information integration, similarity, ontologies, knowledge representation "
2002,knowledge representation for program reuse," in recent years, programs and knowledge about programs have become animportant part of the ``patrimony'' (or intellectual property) ofcompanies. in order to efficiently manage this knowledge it is necessary tomodel it.  however, the scope of this knowledge is wider than the simplecode sources.  we propose a novel framework to encompass such knowledge. inthis framework, we have defined a general ontology for program managementand the yakl knowledge description language, for documenting,modelling and capitalising on the knowledge about the use of code.  thepaper presents the concepts of the ontology and their concreterepresentation in yakl.yakl provides a common language to experts, which is understandableacross domains. it is an open language that can be extended oradapted to suit different needs.  it can also be used in an incrementalmanner: from simple code documentation to an operational knowledge base,that can be run by different inference mechanisms. "," reuse of knowledge, cognitive modelling, knowledge representation "
2002,case retrieval of software designs using wordnet," software design is one of the most important phases in system development, due to crucial decisions that are made during this phase. the need for software being developed in less time puts a lot of pressure in the design phase. one way to solve this problem is to reuse previous design solutions. in software design reuse the retrieval of relevant designs is a key issue.case-based reasoning reuses past experiences to solve new problems, providing a reasoning framework for design reuse. but designing software involves reasoning at a more abstract level than coding software, thus a software design reuse tool must be able to work with a broad range of abstract concepts. a possible solution is the use of a common sense ontology, capable of providing this kind of knowledge, otherwise the system would have to demand a lot of knowledge from the designer. this paper presents an approach to software design retrieval based on case-based reasoning combined with a common sense ontology – wordnet. we describe the case retrieval algorithm, the case similarity metrics and experimental results. "," case-based reasoning, design, ontologies, reuse of knowledge "
2002,updating a hybrid rule base with changes to its symbolic source knowledge," neurules are a kind of hybrid rules that combine a symbolic (production rules) and a connectionist (adaline unit) representation. one way that neurules (target knowledge) can be produced is by converting symbolic rules (source knowledge). however, source knowledge may change, so that updating corresponding target knowledge is necessary. changes concern insertion of new and removal of old symbolic rules. in this paper, methods for updating target knowledge to follow changes made in corresponding source knowledge are presented. the methods are efficient in the sense that they do not require retraining of the whole affected part of the target knowledge, but of as small portion of it as possible.  "," knowledge maintenance, knowledge-based systems, knowledge representation "
2002,extending and unifying chronicle representation with event counters, this paper is dedicated to the chronicle recognition approach used to design an evolution monitoring system for supervising dynamic systems for which time information is relevant. we propose to extend and also to unify the chronicle representation through event counters. the main motivation of such an extension of the chronicle representation raises from alarm processing: counting the occurrences of alarms can be useful to evaluate the severity of a problem and also to discriminate some kind of faults. the paper describes the representation and the corresponding algorithms for processing this extension according to the purpose of supervision in terms of performance. ," temporal reasoning, knowledge representation, diagnosis "
2002,the complexity of checking redundancy of cnf propositional formulae," a knowledge base is redundant if it contains parts thatcan be inferred from the rest of it. we study the problemof checking whether a cnf formula (a set of clauses) isredundant, that is, it contains clauses that can be derivedfrom the other ones. any cnf formula can be made irredundantby deleting some of its clauses: whatresults is an  irredundant equivalent subset (i.e.s.) westudy the complexity of some related problems: verification,checking existence of a i.e.s. with a given size, checkingnecessary and possible presence of clauses in i.e.s.'s, anduniqueness. "," knowledge representation, deduction, computational complexity "
2002,compilation and approximation of conjunctive queries by concept descriptions," in this paper, we characterize the logical correspondence between conjunctive queries and concept descriptions.we exhibit a necessary and sufficient condition for the compilation of a conjunctive query  into an equivalent$\ale$ concept description. we provide a necessary and sufficient condition for the approximation of conjunctive queries by maximally subsumed$\aln$ concept descriptions. "," knowledge representation, description logics "
2002,expressivity and control in limited reasoning," real agents (natural or artificial) are limited in their reasoningcapabilities. in this paper, we present a general framework formodeling limited reasoning based on approximate reasoning anddiscuss its properties. we start from cadoli and schaerf's approximate entailment. we firstextend their system to deal with the full language of propositionallogic.  a tableau inference system is proposed for the extendedsystem together with a sub-classical semantics; it is shown that thisnew approximate reasoning system is sound and complete with respectto this semantics. we show how this system can be incrementally used to move from one approximation to the next until the reasoninglimitation is reached. we note that although the extension is more expressive than theoriginal system, it offers less control over the approximationprocess. we then suggest how we can recover control while keepingthe increased expressivity. "," resource-bounded reasoning, automated reasoning, common-sense reasoning, deduction  "
2002,optimised reasoning for shiq	," we present an optimised version of the tableau algorithmimplemented in the fact knowledge representation system which decides satisfiability and subsumption in shiq, a very expressive description logic providing, e.g., inverse and transitive roles, number restrictions, and general axioms. we prove that the revised algorithm is still sound and complete, and demonstrate that it greatly improves fact's performance - in some cases by more than two orders of magnitude. "," description logics, automated reasoning, knowledge representation  "
2002,automatic learning in proof planning," in this paper we present a framework for automated learning withinmathematical reasoning systems. in particular, this framework enablesproof planning   systems to automatically learn new proof methods fromwell chosen   examples of proofs which use a similar reasoning strategy to prove related theorems. our framework consists ofa representation formalism for methods and a machine learningtechnique which can learn methods   using this representationformalism. we present the implementation of this framework withinthe omega proof planning system, and some   experiments we ran onthis implementation to evaluate the validity of our approach. "," automated reasoning, deduction, machine learning, knowledge acquisition "
2002,approximating propositional knowledge with affine formulas," we consider the use of affine formulas, i.e., conjonctions of linearequations modulo 2, for approximating propositional knowledge. theseformulas are very close to cnf formulas, and allow for efficientreasoning ; moreover, they can be minimized efficiently. we show thatthis class of formulas is identifiable and pac-learnable from examples,that an affine least upper bound of a relation can be computed inpolynomial time and a greatest lower bound with the maximum number ofmodels in subexponential time. all these results are better than thosefor, e.g., horn formulas, which are often considered for representingor approximating propositional knowledge. for all these reasons weargue that affine formulas are good candidates for approximatingpropositional knowledge. "," knowledge representation, knowledge acquisition, machine learning "
2002,hypothesising object relations from image transitions," this paper describes the construction of aqualitative spatial reasoning system based on the sensor data of amobile robot. the robot's spatial knowledge is formalised via three sets ofaxioms. first of all, axioms formalisingrelations between pairs of spatial regions are presented. assuming adistance function as a primitive element, the main purpose of this initial axiom set is theclassification ofrelations between images of objects (from the robot's vision system)according totheir degree of connectivity. changes in the sensor data, due to the movementeither of objects in the robot's environment or of the robot itself,are thus represented by transitions between theserelations. the second set of axioms formalises these transitions. thepredicates defining the transitions between image relations are relatedto possible interpretations for the sensor data in terms ofobject-observer relations; this issue is handled by the third set ofaxioms. these three axiom sets constitute three layers of logic-basedimage interpretation via abduction on transitions in the sensor data. "," spatial reasoning, knowledge representation, cognitive robotics "
2002,a graph-based knowledge representation language for concept description," in this paper, we propose an expressive concept description language, gdl, for real world applications combining features of both conceptual graphs (cgs) and description logics (dls). regarding concept descriptions in cgs, namely existential, positive and conjunctive graphs, gdl is the closure of this language under the boolean operators. now regarding dls, gdl is an extension of alc with graph structures in concept descriptions.gdl extends alc with the intersection, composition, converse of roles and role identity: we show how these constructs can be expressed with the existential graph (eg) and the graph rule (gr) constructs.we provide a sound and complete tableaux algorithm to prove the satisfiability of gdl in nexptime. "," description logics, conceptual graphs, knowledge representation "
2002,querying semistructured data using a rule-oriented xml query language," the goal of the paper is to propose a semistructured data model for representing xml documents and a language for querying semistructured database representing xml data. the language is based on a path calculus and its extension involving rules (in datalog style) and skolem functions. two kinds of matching between query variables and database objects are discussed: a rigid and a flexible matching. the flexible matching is of special importance since the xml data does not conform to a rigid schema, its structure is often not known in advance and its structure may change frequently. we propose a method, based on path expressions with forward and backward elongations, supporting valuations of query variables according to a rigid or a flexible matching. the containment problem for this two matchings is discussed. the main idea of an experimental implementation is outlined.  "," information retrieval, knowledge representation "
2002,sensing and revision in a modal logic of belief and action," we propose a modal logic of belief and nondeterministic actions, where the agent might ignore action laws. sensing is in terms of test actions. the result of tests might be inconsistent with the agent's beliefs (contrarily to knowledge), and therefore such a logic must allow belief revision. we propose a new solution in terms of successor state axioms, which does not resort to orderings of plausibility. our solution allows for regression in the case of deterministic actions. "," cognitive robotics, belief revision, reasoning about actions and change "
2002,qualitative spatio-temporal reasoning with rcc-8 and allen's interval calculus: computational complexity," there exist a number of qualitative constraint calculi that are usedto represent and reason about temporal or spatial configurations. however, there are only very few approaches aiming to create a spatio-temporal constraint calculus. similar to bennett et al., we start withthe spatial calculus rcc-8 and allen's interval calculus in order toconstruct a qualitative spatio-temporal calculus. as we will show, thebasic calculus is np-complete, even if we only permit base relations.when adding the restriction that spatial changes are continuous over time, the calculus becomes more useful, but the satisfiability problemappears to be much harder. nevertheless, we are able to show that satisfiability is still in np. finally, we specify an inference algorithm making use of tractable fragments of the combined calculus. "," spatial reasoning, temporal reasoning, constraint satisfaction, knowledge representation  "
2002,reasoning about spatio-temporal relations at different levels of granularity," this paper discusses aspects of qualitative reasoning about approximate spatio-temporal location at multiple levels of granularity. we start by defining systems of granularities which are tree-like hierarchical structures which are used as frames of reference in order to specify approximate location of objects. we then define levels of granularities within those tree structures based on the notion of cuts proposed in [rs95]. levels of granularity form a lattice structure. following [sw98] we define the notion of stratified map spaces over such a granularity lattice. stratified maps are descriptions of objects in a certain domain at different levels of granularity. the structure of stratified map spaces allows us to perform reasoning about location which is specified at different levels of granularity like: assume that john is in the same place in which mary is (e.g., in hyde park) and that mary is also in the same place in which paula is. it is then our aim to derive that john and paula are in the same place, namely london. rs95 p. rigaux and m. scholl. multi-scale partitions: application to spatial and statistical databases. in m. egenhofer and j. herrings, editors, advances in spatial databases (ssd'95), number 951 in lecture notes in computer science. springer-verlag, berlin, 1995. sw98 j.g. stell and m.f. worboys. stratified map spaces: a formal basis for multi-resolution spatial databases. in t. k. poiker and n. chrisman, editors, sdh'98 proceedings 8th international symposium on spatial data handling, pages 180-189. international geographical union, 1998 "," spatial reasoning, temporal reasoning, qualitative reasoning, granularity "
2002,an attribute weight setting method for k-nn based binary classification using quadratic programming," in this paper, we propose a new attribute weight setting method for k-nn based classifiers using quadratic programming, which is particular suitable for binary classification problems. our method formalises the attribute weight setting problem as a quadratic programming problem and exploits commercial software to calculate attribute weights. experiments show that our method is quite practical for various problems and can achieve a competitive performance. another merit of the method is that it can use small training sets. "," machine learning, data mining and knowledge discovery "
2002,arex – classification rules extracting algorithm based on automatic programming," the paper presents a hybrid classification method of bnf grammar-based genetic programming and evolutionary decision tree induction, customized for the rule induction according to a layered hierarchical scheme – the arex approach. it incorporates two original, independent evolutionary algorithms which together solve the problem of automatic classification rules induction. the method is applied to five real world databases (from medicine and software engineering) and the results are compared to those obtained with c5/see5 to evaluate the method’s efficiency. ideally, this paper will inspire future research in this same area and along similar lines. "," data mining and knowledge discovery, machine learning, genetic algorithms "
2002,music performer recognition using an ensemble of simple classifiers," expressive music performance is the interpretation of a piece of music according to the artist's understanding of the structure (or 'meaning') of the piece. every skilled performer intentionally modifies important parameters, such as tempo and loudness, in order to stress particular notes or passages. in this paper the problem of identifying the most likely music performer, given a set of piano performances of the same piece by a number of skilled candidate pianists, is addressed. we propose a set of features for representing the stylistic characteristics of a pianist. in addition to features that express deviations from the printed score in terms of timing, articulation, and dynamics, we introduce features that are based on the deviation of the pianist from the performance norm (i.e., average performance of a piece, derived from a different set of performers). we show that the norm deviation measures are more stable and reliable in comparison to score deviation measures. moreover, features that exploit the melody lead phenomenon (i.e., notes in a chord that are not played simultaneously) are introduced. a database of piano performances of 22 pianists playing two pieces by f. chopin in a special piano is used in the presented experiments. due to the limitations of the training set size (i.e., only a few training examples per class are available) and the characteristics of the input features (i.e., the score deviation features are affected by slightly changed training sets) we propose a classification model that takes advantage of various techniques of constructing meta-classifiers: subsampling the training set, subsampling the input features, and a weighted majority scheme. we show that the proposed ensemble performs better than any of the constituent simple classifiers when training and test set are taken from different musical pieces and is able to cope efficiently with a very difficult task, even for human experts. "," art and music, machine learning "
2002,effective stacking of distributed classifiers," one of the most promising lines of research towards discovering global predictive models from physically distributed data sets is local learning and model integration. local learning avoids moving raw data around the distributed nodes and minimizes communication, coordination and synchronization cost. however, the integration of local models is not a straightforward process. majority voting is a simple solution that works well in some domains, but it does not always offer the best predictive performance. stacking on the other hand, offers flexibility in modeling, but brings along the problem of how to train on sufficient and at the same time independent data without the cost of moving raw data around the distributed nodes. in addition, the scalability of stacking with respect to the number of distributed nodes is another important issue that has not yet been substantially investigated. this paper presents a framework for constructing a global predictive model from local classifiers that does not require moving raw data around, achieves high predictive accuracy and scales up efficiently with respect to large numbers of distributed data sets. "," data mining and knowledge discovery, machine learning, distributed ai "
2002,a new clustering algorithm based on the ants chemical recognition system," in this paper, we introduce a new method to solve the unsupervised clustering problem, based on a modelling of the chemical recognition system of ants. this system allow ants to discriminate between nestmates and intruders, and thus to create homogeneous groups of individuals sharing a similar odor by continuously exchanging chemical cues. this phenomenon, known as ""colonial closure"", inspired us into developing a new clustering algorithm and then comparing it to a well-known method such as k-means method. our results show that our algorithm performs better than k-means over artificial and real data sets, and furthermore our approach requires less initial information (such as number of classes, shape of classes, limitation in the types of attributes handled). "," artificial ants, clustering, machine learning, autonomous agents, multi-agent systems "
2002,an incremental algorithm for tree-shaped bayesian network learning," incremental learning is a very important approach to learning whendata is presented in short chunks of instances. in such situations,there is an obvious need for improving the performance and accuracy ofknowledge representations or data models as new data is available. itwould be too costly, in computing time and memory space, to usethe batch algorithm processing again the old data together withthe new one.we present in this paper an incremental algorithm for learningtree-shaped bayesian networks. we propose an heuristic able to triggerthe updating process when data invalidates, in some sense,the current structure. the algorithm rebuilds the network structure fromthe branch which it is found to be invalidated. we will experimentallydemonstrate that the heuristic is able to obtain almost optimaltree-shaped bayesian networks while saving computing time. "," incremental learning (not in the ecai keywords), bayesian learning, machine learning  "
2002,learning information extraction rules: an inductive logic programming approach," the objective of this work is to learn informationextraction rules by applying  inductive logic programming(ilp) techniques to natural language data. the approach isontology-based, which means that the extraction rules conclude withspecific ontology relations that characterise the meaning of sentencesin the text. an existing ilp system, foil, is used to learn attribute-value relations. this enables instances ofthese relations to be identified in the text.in specific, we explore the linguistic preprocessingof the data, the use of background knowledge in the learning process,and the practical considerations of applying a supervised learningapproach to rule induction, i.e. in terms the human effort in creatingthe data set, and in the inherent biases in the use of small data sets. "," information extraction, inductive logic programming, ontologies "
2002,on the representation and combination of evidence in instance-based learning," in instance-based learning the classification of a novel instancerelies upon experience given in the form of similar instances whoselabels are already known. each of these instances can hence beseen as an individual piece of evidence. in this paper, we elaborate on issues concerning the representation and combination of such piecesof evidence. particularly, we argue that the information provided bysimilar instances must not be considered as independent. we propose anew inference principle that derives an evidence function specifying the available evidence in favor of each potential label. this principle,which is built upon a probabilistic (random field) model, takesinterdependencies between stored instances into account and suggests a generalization of weighted nearest neighbor estimation. "," case-based reasoning, machine learning, reasoning under uncertainty, probabilistic reasoning "
2002,reinforcement learning integrated with a non-markovian controller," recently a novel reinforcement learning algorithm calledevent-learning or e-learning was introduced. the algorithmbased on events, which are defined as ordered pairsof states. in this setting, the agent optimizes the selection ofdesired sub-goals by a traditional value-policy functioniteration, and utilizes a separated algorithm called thecontroller to achieve these goals. the advantage ofevent-learning lies in its potential in non-stationaryenvironments, where the near-optimality of the value iteration isguaranteed by the generalized epsilon-stationary mdpmodel. using a particular non-markovian controller, the sdscontroller, an epsilon-mdp problem arises in e-learning. we illustrate the properties of e-learning augmented by the sds controller by computer simulations. "," reinforcement learning, machine learning, robotics  "
2002,multiple and partial periodicity mining in time series databases ," periodicity search in time series is a problem that has been investigated by mathematicians in various areas, such as statistics, economics, and digital signal processing. for large databases of time series data, scalability becomes an issue that traditional techniques fail to address. in existing time series mining algorithms for detecting periodic patterns, the period length is user-specified. this is a drawback especially for datasets where no period length is known in advance. we propose an algorithm that extracts a set of candidate periods featured in a time series that satisfy a minimum confidence threshold, by utilizing the autocorrelation function and fft as a filter. we provide some mathematical background as well as experimental results. ", data mining and knowledge discovery 
2002,object identity as search bias for pattern spaces," in the context of frequent pattern discovery, we present agenerality relation, called theta-oi-subsumption, which isbased on the assumption of object identity in spaces of patternsto be intended as existentially quantified conjunctive formulae.the resulting generality order <=oi seems appropriate fororganizing efficiently the space of datalog patterns overstructured domains. indeed we prove the existence of idealrefinement operators for <=oi-ordered spaces and themonotonicity of <=oi with respect to pattern support. featuresof such spaces are illustrated by means of an example of frequentpattern discovery in spatial data. "," data mining and knowledge discovery, knowledge representation, search "
2002,on determinism handling while learning reduced state space representations," when applying a reinforcement learning technique to problems withcontinuous or very large state spaces, some kind of generalization isrequired. in the bibliography, two main approaches can be found. onone hand, the generalization problem can be defined as anapproximation problem of the continuous value function, typicallysolved with neural networks. on the other hand, other approachesdiscretize or cluster the states of the original state space toachieve a reduced one in order to learn a discrete valuetable. however, both methods have disadvantages, like the introductionof non-determinism in the discretizations, parameters hard to tune bythe user, or use of high number of resources. in this paper, we usesome characteristics of both approaches to achieve state spacerepresentations that allow to approximate the value function indeterministic reinforcement learning problems. the method clusters thedomain supervised by the value function being learned to avoid thenon-determinism introduction. at the same time, the size of the newrepresentation stays small and it is automaticallycomputed. experiments show improvements over other approaches such asuniform or unsupervised clustering. "," reinforcement learning, machine learning  "
2002,mining maximal frequent itemsets by a boolean approach," we propose a new boolean based approach for mining frequent patterns in large transactional data sets.  a data set is viewed as a truthtable with an output boolean function. the value of this function is set to one if the corresponding transaction exists in the data set, zero otherwise. the output function represents a condensed form of all the transactions in the data set and is represented by an efficient compact data structure. it is then explored with a depth firststrategy to mine maximal frequent itemsets.  we have developed a prototype, and first experiments have shown that it is possible to rely on a condensed representation based on boolean functions to minefrequent itemsets from large databases. "," maximal frequent itemsets, association rules, data mining, boolean algebra,, binary decision diagrams. "
2002,semi supervised logistic regression," semi-supervised learning has recently emerged as a new paradigm in the machine learning community. it aims at exploiting simultaneously labeled and unlabeled data for classification. we introduce here a new semi-supervised algorithm. its originality is that it relies on a discriminative approach to semi-supervised learning rather than a generative approach, as it is usually the case. we present in details this algorithm for a logistic classifier and show that it can be interpreted as an instance of the classification expectation maximization algorithm.we also provide empirical results on two data sets for sentence classifcation tasks and analyze the behavior of our methods. "," machine learning, semi-supervised learning. "
2002,multi-pattern wrappers for relation extraction from the web," numerous sources of data are available on the web, for instance, product catalogs, multiple directories, conference and event sites, etc. the extraction of information from the content of these sources is a challenging problem and a hard task since they are heterogeneous and dynamic. this paper presents a new method for extracting wrappers and relations from the web using both page encoding and context generalization. its starting point is a training set of instances of the relation the user wishes to extract. multiple patterns are then extracted considering the occurrences of the input instances in the data source. the generalization of these patterns allows us to identify new occurrence of the relation in the same data source. the main features of this method are its genericity and robustness faced to the diversity of sources. its efficiency is shown by the experimental results on different sources, i.e., search engines, shopping, product catalogs, paper listings, etc. "," information extraction, wrapper induction, relation discovery, machine learning "
2002,from margins to probabilities in multiclass learning problems," we study the problem of multiclass classification within the frameworkof error correcting output codes (ecoc) using margin-based binaryclassifiers. an important open problem in this context is how tomeasure the distance between class codewords and the outputs of theclassifiers.  in this paper we propose a new decoding function that combinesthe margins through an estimate of their class conditionalprobabilities. we report experiments using supportvector machines as the base binary classifiers, showing the advantageof the proposed decoding function over other functions of the margincommonly used in practice. we also present new theoretical results bounding theleave-one-out error of ecoc of kernel machines, which can be used totune kernel parameters.an empirical validation indicates that the bound leads to goodestimates of kernel parameters and the corresponding classifiers attainhigh accuracy.  "," machine learning, error correcting output codes, support vector machines,, statistical learning theory "
2002,modelling contextual meta-knowledge in temporal model based diagnosis," applying model based diagnosis (mbd) techniques in medical domains reveal the need to use deep causal knowledge modelling frameworks as well as temporal management techniques to capture the dynamic component of disease evolution (with the latter being very important in other domains). despite the intense research activity in the field of temporal mdb, there are three issues that not been analysed in depth: (a) modelling complex interaction of contextual information, (b) evaluation of hypotheses possibility degrees and (c) the structure of explanations. our aim is to present a general framework for temporal mdb which approaches these problems and to demonstrate the suitability of the fuzzy temporal constraints networks formalism (ftcn) for representing the domain temporal dimension. "," diagnosis, model based reasoning, temporal reasoning, causal reasoning "
2002,dealing with discontinuities in the qualitative simulation of genetic regulatory networks, methods developed for the qualitative simulation of dynamical systemshave turned out to be powerful tools for studying genetic regulatorynetworks. we present a generalization of a simulation method basedon piecewise-linear differential equation models that is able to deal with discontinuities. the method is sound and has been implemented in a computer tool called gna. ," qualitative reasoning, bioinformatics, model-based reasoning "
2002,can ai help to improve debugging substantially? debugging experiences with value-based models," finding and fixing faults in programs is usually an expensive andtedious task.  consequently the development of intelligent debuggingtools that aid the programmer in this task is a topic of majorindustrial interest. this work describes two representations forapplying model-based diagnosis to java programs, a technique thatpermits locating (and partly correcting) faults without requiring aformal specification of the desired program behavior, sinceinteraction can be limited to test cases and observations ofvariable correctness.  one of the models uses a special transformation to provide more accurate diagnoses on programs with loops and thisis borne out by the experiments.  the presented results onactual debugging performance show clearly superior accuracy toclassical debugging techniques, and better discrimination thandependency-based programs models.  we discuss the results in terms of theproperties of the two models and the various example programs andpresent avenues for further improvement. "," debugging, diagnosis, model-based reasoning "
2002,towards an integrated debugging environment," with recent research showing that consistency based diagnosiscan be used to model programs written in imperative programminglanguages for debugging purposes, it has been possible to developdebugging environments that provide interactive support to thedeveloper, homing in on individual faults within a few interactions.in addition to complexity results, this paper discusses how theresults of the straightforward application of diagnosis modelscan be improved upon by incorporating information obtained frommultiple test cases (i.e., input/output vector specifications).the information from executing such test cases can be used tosupport heuristic statement selection by assigning faultprobabilities, and for elimination of diagnosis candidates. wealso discuss how the extended algorithms can be integrated withthe consideration of multiple diagnosis models during the diagnosisprocess. "," diagnosis, model-based reasoning "
2002,diagnosis of discrete-event systems with model-based prospection knowledge," diagnosis of discrete-event systems is a complex and challenging task. within the class of active systems, such a complexity is exacerbated by the possibility of queuing (possibly uncertain) events within connection links, thereby making essential the simulation of link behavior during the reconstruction of the system reaction. however, reconstructing such a reaction without any prospection in the search space is generally bound to detrimental backtracking. to cope with this short-sightedness, we present a technique which allows the automatic generation of prospection knowledge relevant to the mode in which events are produced and consumed in links. such a far-sighted diagnosis requires that a collection of prospection graphs be generated off-line, based on the system model, which  are then exploited on-line to guide the search process. as a result, both time and space can be considerably reduced on-line. the approach is worthwhile whenever time constraints are far more severe on-line (when the diagnostic engine is running) than off-line (when no diagnostic process is ongoing), which is commonplace in a large variety of real systems. "," diagnosis, model-based reasoning, discrete-event systems, knowledge compilation "
2002,parsing natural language using guided local search," in this paper an application of guided local search (gls) tothe problem of natural language parsing is presented.  thegiven parsing approach is situated in a constraint basedparsing paradigm that allows natural language processing ina robust and resource adaptive way.  some extensions of glsare introduced, most notably a multi-threaded search where acouple of agents cooperate with each other in parallel,showing synergetic effects. the resulting algorithm iscompared to competing techniques within the framework ofweighted constraint dependency grammars.  an experimentalevaluation shows gls being on par with similar approaches. "," natural language processing, constraint satisfaction, search, meta-heuristics for ai "
2002,saliency and the attentional state in natural language generation," the importance of saliency and attention in natural language generation is often underestimated. this paper aims to demonstrate why it is necessary to view and operationalise saliency in a similar way as has happened with intention. the discussion draws on both theoretical and practical considerations, and describes a generation system which implements the model. "," discourse modelling, natural language processing "
2002,adaptivity in web-based call," this paper presents the design, implementation and some original features of a web-based learning environment - style (scientific terminology learning environment). style supports adaptive learning of english terminology with a target user group of non-native speakers. it attempts to improve computer-aided language learning (call) by intelligent integration of natural language processing (nlp) and personalised information retrieval (ir) into a single coherent system. "," computer-aided learning, natural language processing, user modeling, knowledge-based systems "
2002,a lexical network and an algorithm to find words from definitions," this paper presents a system to find automatically words from a definition or a paraphrase. the system uses a lexical database of french words that is comparable in its size to wordnet and an algorithm that evaluates distances in the semantic graph between hypernyms and hyponyms of the words in the definition. the paper first outlines the structure of the lexical network on which the method is based. it then describes the algorithm. finally, it concludes with examples of results we have obtained. "," abduction, conceptual graphs, knowledge representation, natural language processing, ontologies "
2002,natural language texts for a cognitive vision system," text-to-logic conversion is studied in a system approach which extracts a conceptual representation of temporal developments within a road traffic scene recorded by a video camera. a fuzzy metric temporal horn logic (fmthl) facilitates a schematic representation of road vehicle behavior at intersections. geometric results of a model-based vehicle detection and tracking subsystem are used to interpret this generic conceptual fmthl representation in order to obtain a conceptual description of the specific developments in the recorded traffic scene. one task accepts a natural language (nl) english text formulation of the generic conceptual knowledge and converts this into the internal fmthl representation. this requires to distinguish algorithmically between discourse-related and scheme-related nl statements. a separate second task creates a synthetic video sequence from a nl text, using as much as possible the same apparatus as the first one. a comparison between genuine road traffic video sequences and those generated synthetically allows to test both the text-to-logic transformation and the use of geometric knowledge represented within the entire system for image sequence evaluation and subsequent interpretation. results obtained by the execution of both tasks are discussed. "," natural language processing, knowledge representation, reasoning about actions and change, knowledge-based systems, vision, description logics, qualitative reasoning, temporal reasoning "
2002,towards answer extraction: an application to technical domains," the shortcomings of traditional information retrieval are most evidentwhen users require  exact information rather than  relevant documents.this practical need is pushing the  research community towards systemsthat can  exactly pinpoint those parts of  documents  that contain theinformation requested.  answer extraction (ae)  systems aim to satisfythis need. this  paper presents one such  system (extrans) which worksby  transforming documents and queries  into a semantic representationcalled minimal logical form  (mlf) and derives  the answers by logicalproof from the documents.  mlfs use underspecification to overcome theproblems associated with a complete  semantic representation and offerthe possibility of monotonic, non-destructive extension. "," natural language processing, answer extraction, information extraction, information retrieval "
2002,anything to clarify? report your parsing ambiguities!," experience with spoken dialogue systems and an in-depth analysis ofhuman-computer dialogues has shown that one of the main reasons forfailed interactions is the difficulty to integrate new utterances inthe dialogue context. this paper addresses the issue of groundingutterances in task-oriented human-computer dialogues. it focuses onthe aspect of handling ambiguities that result from the parsing ofinput from a speech recogniser. an approach to parsing spontaneousspeech is presented that is capable of computing the origins ofambiguities. the parser first segments input into chunks and thenanalyses their dependency relations on the basis of case framescontained in a semantic lexicon. in this way, pragmatic constraintscan be applied early while semantic representations areconstructed. ambiguities usually are handed over to the dialogue manageras n-best parses that are not related to each other. in our approach,ambiguity reports mark the differences between the semantics of thedifferent readings. disambiguation is either achieved by exploitingthe application situation or by initiating clarification dialoguesthat are suitable for the dialogue situation. this approach is animportant step towards flexible handling misunderstandings caused byspeech recognition and parsing in human-computer dialogues. "," discourse modelling, natural language processing "
2002,predicting the components of german nominal compounds," compounding is a common cross-linguistic mean to form complex words. ingerman and many other languages, compounds are commonlywritten as single orthographic strings. because compounding is aproductive process, a considerable amount of compounds cannot, even inprinciple, be listed in a lexicon. this poses a challenge toword prediction systems (such as those embedded in most currentaugmentative and alternative communication systems) that aim topredict what the next word a user wants to type will be on the basisof corpus-extracted n-gram counts. we present a solution tothis problem based on the idea that compounds should not be predictedas units, but as the concatenation of their components. in particular,we designed a word prediction system in which the prediction of germantwo-element nominal compounds (by far the most common compound type ingerman) is split into the prediction of the modifier (left element)and the prediction of the head (right element). both components arepredicted on the basis of uni- and bigram statistics collectedtreating modifiers and heads as independent units, and on the basis ofthe type frequency of nouns in head and modifier context in thetraining corpus. we show that our system brings a dramatic improvementin keystroke saving rate over a word prediction scheme in whichcompounds are treated as units (from 50.8% to 58.1% for a test setof compound targets). in particular, our results indicate that thetype frequency of nouns in head/modifier context in the trainingcorpus is a very good predictor of which nouns will occur inhead/modifier context in new text. "," natural language processing, human language technology "
2002,processing information of geogaphical databases within a dialogue system : a pivot system," a dialogue system within a geographical information system has to understand and to generate texts in natural language.in particular, it applies spatial reasoning in order to identify or to localize the landmarks of the geographical environment.therefore, the domain knowledge manager (i.e. the module that gatherinformation), have to access to the contents of the geographical database, which represent the information usable by the gis.owing to the lack of a standard format, this manager has to deal with various contents and various organizations.in order to overcome this difficulty and to differentiate precisely the cases for which the dialogue system has to interrogate the user, we use, within this module, a pivot system.in this paper, we present this system and the possibilities that it offers for the dialogue system.      "," natural language processing, spatial reasoning, knowledge representation  "
2002,acquisition of conceptual domain dictionaries via decision tree learning, knowledge based systems usually rely on large size domain models needed to support reasoning and decision-making. the development of realistic models represents a critical and labour intensive phase. automatic terminology acquisition (ta) has been proposed as the task of automatically extracting specialized dictionaries from raw texts useful for application purposes like precise information retrieval and machine translation. in this paper we argue that ta provides a significant contribution in the development of ontological components of a knowledge base. we therefore propose an automatic knowledge acquisition architecture for the ta process based on robust methods for text processing and on algorithms for learning decision trees. an incremental semi-automatic approach is proposed to enable the first steps in the development of a domain ontology.the novel aspects of the method rely on the use of syntagmatic and lexical properties of terms combined with analogous (negative) evidences observable for non-terms.the underlying assumptions as well as the different adopted linguistic representations have been extensively investigated over a large test set. the scale of the target test data provides empirical evidence of the superiority of the method over more quantitative approaches. the proposed architecture is thus a viable approach to the development of conceptual domain dictionaries.  ," natural language processing, knowledge acquisition, machine learning, information extraction  "
2002,empirical investigation of fast text classification over linguistic features," recently, an original extension of the well-known rocchio model (i.e. the generalized rocchio classifier (grc)) as a feature weighting method for text classification has been presented. the assessment of such a model requires a statistically motivated parameter estimation method and wider empirical evidence. in this paper, three different corpora have been adopted in two languages. results suggest that grc, integrating linguistic information, is a viable more efficient alternative to state-of-art tc systems. "," information retrieval, natural language processing, information extraction, machine learning   "
2002,chaotic time series prediction with neural networks - comparison of several architectures," this paper presents experimental comparison between selected neuralarchitectures for chaotic time series prediction problem. severalfeed-forward architectures (multilayer perceptrons) are comparedwith partially recurrent nets (elman, extended elman, and jordan)based on convergence rate, prediction accuracy, training time requirements and stability of results.results for chaotic logistic map series presented in the paper indicate that prediction accuracy of mlps with two hidden layers is superior to other tested architectures. although potential superiority of mlps needs to be confirmed on other chaotic time series before any general conclusions can be drawn, it is conjectured here that on the contrary to the common beliefs in several cases feed-forward nets may be better suited for short-term prediction task than partially recurrent nets.it is worth noting that significant improvement in prediction accuracy for all tested networks was achieved by rescaling the data from interval(0,1) to (0.2, 0.8). moreover, it is experimentally shown that with a proper choice of learning parameters all tested architectures produce stable (repeatable) results.open problems left for future research include verification of the above results on another chaotic time series and on financial or business data. "," time series prediction, logistic map series, neural networks "
2002,neuro-fuzzy knowledge representation for toxicity prediction of organic compounds," in the present paper, models based on neuro-fuzzy structures are developed to represent knowledge about a large data set containing descriptors about organic compounds, commonly used in industrial processes.the neuro-fuzzy models here proposed include both, qsar equations, and original numerical values.the proposed methods use various techniques to insert knowledge by training, and to map empiric fuzzy rules in neuro-fuzzy structures.the combinations of generalized fuzzy computation, neuro-fuzzy models, and strategies to insert data in the developed architectures, are used as a powerful processing tool in toxicity prediction. "," knowledge-based systems, neural networks, knowledge representation "
2002,non-negative matrix factorization extended by sparse code shrinkage and weight sparsification non-negative matrix factorization algorithms," properties of a novel algorithm called non-negative matrixfactorization (nmf), are studied. nmf can discover substructuresand can provide estimations about the presence or the absenceof those, being attractive for completion of missing information.we have studied the working and learning capabilities ofnmf networks. performance was improved by adding sparse codeshrinkage (scs) algorithm to remove structureless noise. we havefound that nmf performance is considerably improved by scs noisefiltering. for improving noise resistance in the learning phase, weight sparsification was studied; a sparsifying prior was applied on the nmf weight matrix. learning capability versus noise content wasmeasured with and without sparsifying prior. in accordance with observation made by others on independent component analysis, wehave also found that weight sparsification improved learning capabilities in the presence of gaussian noise. "," neural networks, machine learning "
2002,enhancing first-pass attachment prediction ," this paper explores the convergence between cognitive modeling andengineering solutions to the parsing problem in nlp.  natural languagepresents many sources of ambiguity, and several theories of humanparsing claim that ambiguity is resolved by using past (linguistic)experience.  in this paper we analyze and refine a connectionistparadigm (recursive neural networks) capable of processing acyclicgraphs to perform supervised learning on syntactic trees extractedfrom a large corpus of parsed sentences.  following a widely acceptedhypothesis in psycholinguistics, we assume an incremental parsingprocess (one word at a time) that keeps a connected partial parse treeat all times.  by implementing a parsing simulation procedure, wecollect a large amount of data that shows the viability of the rnn asinformant of a disambiguation process.  we analyze what kind ofinformation is exploited by the connectionist system in order toresolve different sources of ambiguity, and we see how thegeneralization performance of the system is affected by the treecomplexity and the frequency of specific subtrees.  we finally proposesome enhancements to the architecture in order to achieve a betterprediction accuracy. "," natural language processing, neural networks  "
2002,a simple algorithm for learning stable machines, we present a simple algorithm for learning stable machines which is motivated by recent results in statistical learning theory.the algorithm is similar to breiman's bagging despite some important differences in that it computes an ensemble combination of machines trained on small random subsamples of an initial training set. a remarkable property is that it is often possible to just use the empirical error of these combinations of machines for model selection. we report experiments using support vector machines and neural networks validating the theory.  ," machine learning, statistical learning theory, bagging, support vector machines, neural networks "
2002,defeasible logic with dynamic priorities," defeasible logic is a nonmonotonic reasoning approach based on rules andpriorities. its design supports efficient implementation, and it shows promiseto be successfully deployed in applications.so far only static priorities have been used, provided by an externalsuperiority relation. in this paper we show how dynamic priorities can beintegrated, where priority information is obtained from the deductive processitself. dynamic priorities have been studied for other related reasoningsystems, such as default logic and argumentation. we define a proof theory,study its formal properties, and provide an argumentation semantics. "," nonmonotonic reasoning, knowledge representation, logic programming "
2002,uncontroversial default logic," many variants of default logics exist. two of the maindifferences among them are the choice between local or globalconsistency and the choice of accepting maximally successfulsets of defaults. proving a result that is valid in allvariants amounts to showing either a proof that holds forall semantics or a different proof for each semantics. inthis paper, we characterize theories that do not dependat all on what makes the semantics different. a result thatis proved on such theories holds not only for all consideredsemantics, but also on any other semantics thatdiffers on the classical one because of the two choices.these theories are also of interest for practical applicationsof default logic, as an implemented system should be ableto detect (for example, to warn the user) any theorywhose semantics is debatable. "," nonmonotonic reasoning, knowledge representation "
2002,linking makinson and kraus-lehmann-magidor preferential entailments," twelve years ago, various  notions of preferential entailment have been  introduced.  the main reference is a paper by kraus, lehmann and magidor (klm), one of themain competitor being amore general version defined by makinson (mak). these two versions have already been compared, but it is time to revisit these comparisons. here are our main results:(1)  these two notions areequivalent, provided that we restrict our attention, as done in klm,to the cases where the entailment respects logical equivalence (on the leftand on the right).(2) a serious simplification of the description of the fundamental casesin which mak is equivalent to klm, including a natural passagein both ways. (3) the two previous results are given for preferential entailments moregeneral than considered in some texts, but they applyalso to the original definitions and, for this particular case also, themodels can be simplified.  "," nonmonotonic reasoning, knowledge representation, reasoning about actions and change, common-sense reasoning "
2002,notions of attack and justified arguments for extended logic programs," the concept of argumentation may be used to give a formal semanticsto a variety of assumption based reasoning formalisms. inparticular, various argumentation semantics have been proposed forlogic programming with default negation.for extended logic programming, i.e. logic programming with twokinds of negation, there arise a variety of notions of attack on anargument, and therefore a variety of different argumentationsemantics.the purpose of this paper is to shed some light on these varioussemantics, and examine the relationship between different semantics.we identify a number of different notions of attack for extendedlogic programs, and compare the resulting least fixpoint semantics,defined via acceptability of arguments. we investigate the validityof the coherence principle and notions of consistency for thesesemantics. "," nonmonotonic reasoning, logic programming  "
2002,iterated revision and the axiom of recovery: a unified treatment via epistemic states," the axiom of recovery, while capturing a central intuitionregarding belief change, has been the source of much controversy. weargue briefly against these counterexamples---while agreeing that some of their insight deserves to be preserved---and present alternative recovery like axioms in a framework that uses epistemic states, which encode preferences,  as the object of revisions. this makes iterated revision possible and makes explicit the connection between iterated belief change and the axiom of recovery. we provide a representation theorem that connects the semantic conditions that we impose on iterated revision and the additional syntactical properties mentioned. we also show some interesting similarities between ourframework and that of darwiche-pearl.in particular, we show that the intuitions underlying the controversial (c2) postulate are capturedby the recovery axiom and our recovery-like postulates (the latter can be seen as weakenings of (c2). "," belief revision, knowledge representation, common sense reasoning, philosophical foundations  "
2002,feature integration as an operation of theory change," we formalise a relationship between two previouslyunconnected areas: theory change and featureinteraction. this will provide an interesting newapplication area for the logic of theory change, and atheoretical underpinning for the feature interaction problemwhich has a largely practical basis.update is an operation of theory change which is closelyrelated to belief revision. the principal difference lies inthe fact that belief revision models changing beliefs abouta static world whereas update models a changing world.a feature is a unit of functionality which extends ormodifies the behaviour of the system into which it isintegrated. the feature interaction problem arises when twoor more features interact, causing the system to exhibitunexpected, and often undesirable, behaviour. manyapproaches to feature integration and interaction detectionhave been proposed. in this research we use a featureconstruct for the model checker smv.we show that there is a strong connection between update andfeature integration by, preliminarily, formulating smv andthe feature construct in propositional logic. we then go onto prove that the eight rationality postulates for updatehold in the context of this theoretical formulation. "," belief revision, verification and validation, nonmonotonic reasoning "
2002,network-based truth maintenance system," we present a network-based truth maintenance system (ntms) for problem solvers based on bayesian belief network (bn) technology. bn technology has been proven to be effective in various domains, e.g. assessing battlefield situations, such as the enemy’s likely point of interdiction. nodes and links in a bn capture semantic relationships among various domain related concepts. in the absence of firmer knowledge, default assumptions provide the beliefs of some nodes in a bn. before posting incoming evidence into a bn node, a truth maintenance procedure is invoked to check for information consistency between the node’s current expected state and the new observed state. in case of inconsistency, the truth maintenance procedure revises some default assumptions, by isolating those nodes causing inconsistency, via a sensitivity analysis procedure that exploits the strengths of bn causal dependency. we have applied our approach for trustworthy situation assessment in the context of a stability and support operation (saso) scenario. "," belief revision, nonmonotonic reasoning, probabilistic reasoning "
2002,robot free will," we introduce the perspex machine which unifies projective geometry and turing computation and results in a supra-turing machine. we show two ways in which the perspex machine unifies symbolic and non-symbolic ai. firstly, we describe concrete geometrical models that map perspexes onto neural networks, some of which perform only symbolic operations. secondly, we describe an abstract continuum of perspex logics that includes both symbolic logics and a new class of continuous logics. we argue that an axiom in symbolic logic can be the conclusion of a perspex theorem. that is, the atoms of symbolic logic can be the conclusions of sub-atomic theorems. we argue that perspex space can be mapped onto the spacetime of the universe we inhabit. this allows us to discuss how a robot might be conscious, feel, and have free will in a deterministic, or semi-deterministic, universe. we ground the reality of our universe in existence. on a theistic point, we argue that preordination and free will are compatible. on a theological point, we argue that it is not heretical for us to give robots free will. finally, we give a pragmatic warning as to the double-edged risks of creating robots that do, or alternatively do not, have free will. "," philosophical foundations, vision "
2002,a pragmatic theory of induction," this paper develops a qualitative, logical, theory of induction. it begins hempel's attempt to produce a ``purely syntactical'' theory of confirmation and the demise of this attempt as a result of goodman's paradox. ideas from the informal, pragmatic, solutions to this paradox proposed by goodman and quine are then adopted, adapted and extended in order to produce a formal, pragmatic, theory of induction. according to this theory, induction takes place in an evolving context of inference; which includes an evolving system of kinds and, typically, a background theory. the theory is illustrated by giving a formal solution to goodman's paradox, and a further difficulty raised by davidson is discussed. "," inductive logic, philosophical foundations, induction "
2002,extending ff to numerical state variables," the ff system obtains a heuristic estimate for each state during aforward search by solving a relaxed version of the planning task,where the relaxation is to assume that all delete lists are empty.we show how this relaxation, and ff's heuristic function, cannaturally be extended to planning tasks with constraints and effectson numerical state variables. first results show that theimplementation, metric-ff, is competitive with other approaches tonumerical planning, performing well against one of the most recentapproaches on a numerical version of the logistics domain. ", planning 
2002,solving power supply restoration problems with planning via symbolic model checking," the past few years have seen a flurry of new approaches for planning under uncertainty, but their applicability to real-world problems is yet to be established since they have been tested only on toy benchmark problems. to fill this gap, the challenge of solving power supply restoration problems with existing planning tools has recently been issued.  this requires the ability to deal with incompletely specified initial conditions, fault conditions, unpredictable action effects, and partial observability in real-time.  this paper reports a first response to this nontrivial challenge, using the approach of planning via symbolic model-checking as implemented in the mbp planner. we show how the problem can be encoded in mbp's input language, and report very promising experimental results on a number of significant test cases. "," planning, reasoning under uncertainty "
2002,combining two heuristics to solve a supply chain optimization problem," in this paper, we consider a real-life supply chain optimization problem concerned with supplying a product from multiple warehouses to multiple geographically dispersed retailers. each retailer faces a deterministic and period-dependent demand over some finite planning horizon. the demand of each retailer is satisfied by the supply from some predetermined warehouse through a fleet of vehicles which are only available within certain time windows at each period. our goal is to develop a co-ordinated schedule such that the system-wide total cost over the planning horizon is minimised. this problem in essence is an amalgamation of two classical np-hard optimizatin problems: the dynamic lotsizing problem and the vehicle routing problem.in this paper, we propose an efficient rolling forward heuristic that combines two heuristics to solve this problem. numerical experiment results show that our approach can achieve, on average, within 10% of the lower-bound proposed by by chan, federgruen and simchi-levi (1998) for some specific instances generated from solomon benchmarks. "," scheduling, planning, meta-heuristics for ai, search "
2002,a temporal planning system for durative actions of pddl2.1," many planning domains have temporal features that can be expressed using durations associated with actions. unfortunately, the conservative model of actions used in many existing temporal planners is not adequate for some domains which require more expressive models. level 3 of pddl2.1 introduces a model of durative actions that includes local conditions and effects to be satisfied at different times during the execution of the actions, thereby giving the planner greater freedom to exploit concurrent actions. this paper presents a temporal planning system which combines the principles of graphplan and tgp in order to plan with such actions. although the system is consequently more complex than tgp, the experimentalresults demonstrate it remains feasible as a way to deal with durativeactions. "," planning, temporal reasoning "
2002,representation of decision-theoretic plans as sets of symbolic decision rules," in recent years, it is increasingly recognised that action planning in real-world domains requires an accurate treatment of uncertainty. the theory of partially-observable markov decision processes has been found to provide a powerful framework for studying this type of planning. within this framework, plans are often expressed as rooted trees. however, for various reasons it is often more convenient to express plans as collections of decision rules. for instance, domain experts are often able to formulate a number of reliable decision rules that could serve as a starting point in finding an optimal plan. this paper investigates the representation of decision-theoretic plans as sets of symbolic decision rules. it is shown under which conditions such plans are internally consistent, coherent, en complete. "," planning, reasoning under uncertainty, knowledge representation "
2002,supporting goal based interaction with dynamic intelligent environments," modern technical infrastructures and appliances provide a multidude of opportunities for simplifying and streamlining the everyday life. however, many of the systems available today -- such as the typical feature-loaded audio and video components -- are not always efficiently usable for the average person. but, in an environment where features abound, the easy access of these features more and more becomes the key quality criterion for the user.we present a planner-based approach to helping the user to interact with such complex infrastructures. specifically, we concentrate on the application domain of networked infotainment systems and home control. also, we describe the architectural concept, which makes it possible to integrate classical artificial intelligence technology -- such as planning and scheduling -- into the domain of networked consumer appliances within the scope of a multimodal assistance system.the work we present is part of the embassi-project, a joint project with 19 partners from industry and academia, that aims at establishing an interoperable system infrastructure for multimodal and multimedia assistance systems. "," ai architectures, ontologies, planning, scheduling "
2002,dai-depur: an environmental decision support system for the control and supervison of municipal wastewater treatment plants," integrated operation of  municipal wastewater treatment plants is still far from being solved. a reasonable proposal should link advanced and robust control algorithms to  knowledge-based techniques, allocating the detailed engineering to numerical computations, while delegating the logical analysis and reasoning to supervisory intelligent systems. this paper describes the development and implementation of a environmental decision support system to supervise and control the operation of a real wastewater treatment plant. the system integrates different reasoning modules, overcoming the limitations in the use of each single technique, while providing an agent based architecture with additional modularity and independence. it is structured into five separated levels: data gathering, diagnosis, decision support, planning and action. the different tasks of the system are performed in  a seven-step cycle: data gathering and update, diagnosis, supervision, prediction, communication, actuation, and evaluation phase. in spite of certain reservations of the scientific community about the use of these techniques, the system is successfully performing real-time support to the operation of the granollers facility since september 1999. results of the first four-month validation period are shown and discussed. an example of the system behavior is also shown in the paper. the conclusions indicate the key steps which are necessary to transfer the system to another facility. ", pais 
2002,personalized adaptive navigation for mobile portals," the ability of users to navigate efficiently to content is a key portal usability test, one that wap portals are currently failing. today wap users waste time navigating a sea of menus to locate content. a solution to this problem is presented that adapts portal navigation structures for the needs of individual users. we show how this can radically improve the usability of wireless portals, resulting in significant revenue benefits for mobile operators. ", pais 
2002,benefits of a knowledge-based system for parenteral nutrition support: a report after 5 years of routine daily use," calculating the daily changing composition of parenteral nutrition forsmall newborn infants is troublesome and time consuming routine workin neonatal intensive care. the task needs expertise and experienceand is prone to inherent calculation errors. in 1996 we introduced a knowledge-based system called vie-pnn at the neonatalintensive care unit (nicu). it supports the daily calculation of nutrition plans for combined parenteral and enteral nutritionof newborn infants utilizing textbook knowledge and clinicalrules of expert neonatologists. vie-pnn uses a html-basedclient-server architecture and is integrated into the intranet of thelocal patient data management system (pdms).the system is now in daily routine use for more than 5 years at2 nicus. its main benefits are considerable time savings forclinicians and an increased quality of care. the main factors forsuccess are its ease of use, its robustness, the integration into the pdms, and the maintainability by the clinical experts. most important, physicians highly value the time savings the systemprovides. ", pais 
2002,a framework for rapid development of advanced web-based configurator applications," for the last two decades product configurators relying on artificial intelligence techniques have been successfully applied in industrial environments. these systems support configuration of complex products and services in shorter time with fewer errors and therefore reduce costs of a mass-customization business model. the eu-funded cawicoms project aims at the next generation of web-based configurator applications that cope with the new challenges of today's open, networked economy: this includes support for heterogeneous user groups in an open market environment as well as integration of configurable sub-products provisioned by specialized suppliers.this paper describes the cawicoms workbench for the development of configurator applications that support both adaptive and personalized user interaction as well as distributed configuration of products in a supply chain. the developed tools and techniques rely on a harmonized knowledge representation and knowledge acquisition mechanism, open xml-based protocols, and advanced distributed reasoning techniques.we applied the workbench based on the real-world business scenario of distributed configuration of services in the domain of ip-based virtual private networks. ", pais 
2002,international insurance traffic with software agents," this paper presents a business case on digital cross-border information flow within a european network of insurance companies.underlying the business case is the multi-agent paradigm.the paper explores a structured approach for designing agent behavior based on the 5 capabilities (5c) agent  model.the central value of the 5c model is the conceptual separation of concerns.it breaks apart five different dimensions of capabilities upon which an intelligent agent must draw.the work shows that functional as well as technical constraints can be reflected in an intuitive manner along these five dimensions.the outcome of the business case was the kir system, a network of information exchanging agents handling green card insurance traffic within europe.the strength of the agent paradigm combined with the astonishing simplicity of the application design acted as an eye-opener; with two major consequences.first, the customer has given the green light to develop and implement the kir system at european level.second, the customer has become a strong believer in the value of solutions in the insurance domain based on intelligent agents. ", pais 
2002,idd: integrating diagnosis in the design of automotive systems," in this paper we overview the achievement of the idd european project, which aims at defining a new framework for the design of automotive systems. in particular, starting from the weaknesses of the current design process, especially as regards issues related to diagnosis (diagnosability analysis, generation of the fmea ? failure mode effect analysis, generation of on-board diagnostic rule), the project aims at defining a new process in which this issues are integrated within the design of a system and of its control strategies. the project also aims at defining and implementing a software toolkit supporting the new process. the toolkit integrates applications for design and simulation (e.g., matlab simulink) and model-based reasoning systems for diagnosis-related tasks. ", pais 
2002,programme driven music radio," this paper describes the operation of and research behind a networked application for the delivery of personalised streams of music at trinity college dublin.  smart radio is a web based client-server application that uses streaming audio technology and recommendation techniques to allow users build, manage and share music programmes. while it is generally acknowledged that music distribution over the web will dramatically change how the music industry operates, there are few prototypes available to demonstrate how this could work in a regulated way. the smart radio approach is to have people manage their music resources by putting together personalised music programmes. these programmes can then be recommended to other listeners using a combination of collaborative and content-based recommendation strategies. we describe how we use a novel two-stage approach to find recommendations that are pertinent to a listener’s current listening preferences, something which collaborative techniques are insensitive to.  we describe additional constraints required to provide a service personalised to each listener. the smart radio system currently runs within the computer science intranet with permission from the irish music rights organisation. it is a prototype system for an ""always on"" high bandwidth internet connection such as adsl. ", pais 
2002,alarm correlation in traffic monitoring and control systems: a knowledge-based approach," the aim of the paper is to highlight the importance of alarmcorrelation in traffic monitoring and control systems, and topropose a knowledge-based solution for developing a module of atraffic monitoring and control system. the alarm correlationmodule (mca) integrated with the system for automatic monitoringof traffic (samot) will be described. the aims of the mca are toanalyze, filter and correlate traffic flow anomalies detected byvideo image processing (vip) boards, to create adequate imagesequences to be shown on operators close-circuit tvs and todisplay adequate messages in variable message panels to keepmotorists informed. thus, the mca provides samot with an automaticprocessing tool that, based on traffic operators' experience andknowledge, supports operators in the interpretation of trafficsituations and in the event-driven control of traffic anomalies.the mca knowledge base implements a model of traffic flowconcerning the most relevant traffic patterns and taking intoaccount time and space dependence of detected traffic anomalies.the mca integrated in samot is a successful example of theknowledge-based approach applied to traffic monitoring andcontrol. the mca has been developed in collaboration with thesamot provider (project automation s.p.a.) and the italian highwaycompany (societa' autostrade s.p.a.). after a 6 months trialperiod, the system is now installed and functioning on two of themost crowded italian highways (i.e. a7 and a10). ", pais 
2002,a neural network approach for forestal fire risk estimation," this paper describes an intelligent system for the prediction of forest fire risk in galicia, a region in north-westspain. the system has been designed to calculate a risk fire index for each of the 360 squares of 10x10 kms into whichthe area map has been divided digitally. in our research, the problem was approached using a feedforward neuralnetwork. the information used to train the network was gathered at five meteorological stations on a daily basis from1985 to 1999, and consisted of basically meteorological data, namely temperature, humidity and rainfall, in conjunctionwith previous fire records for the areas represented by squares. network topologies were tested using 125,156 trainingdata and validated over 13,906 test samples, and that achieving the best performance was the 6-9-1 topology. finally,our results indicate that the system performs satisfactorily, with a sensitivity of 0.857 and a specificity of 0.768. ", pais 
2002,fault diagnosis of a continuous process using imprecise quantitative knowledge and causal models," abstract. this paper presents a systematic methodology for building causal models that can be used for fault detection and isolation. the aim of a causal model is to capture the influences between the variables of a continuous process and to generate qualitative and quantitative knowledge that is interpreted by a diagnostic module. following a model-based approach for fault detection, the diagnostic module compares the predicted outputs of the causal model with the measured values. each influence of the causal model is associated with component(s) of the process. this qualitative knowledge is used to isolate the source fault on a set of components of the process. the application to a fluid catalytic cracking process pilot plant is briefly described and a fault scenario is finally presented. the work is done in the context of the eu-funded chem project ""advanced decision support system for chemical/petrochemical manufacturing processes"". ", pais 
2002,ai on the ocean: the robosail project," sailing represents a new area of research within the field of applied ai. this paper gives a global overview of an ongoing broad-scoped research programme, called the robosail project (www.robosail.com). part of this project is the realization of a semi-autonomous intelligent pilot and a decision support system, assisting sailors in optimizing sailboat performance. the project encompasses a wide range of various real-world ai problems. in solving these problems, we argue that our approach to symbol grounding by means of using jargon amounts to a significant reduction in the problem's complexity. the first implementation of this system has been successfully tested by some of the leading sport teams in the world of short-handed sailing. ", pais 
2002,online diagnosis of engine dyno test benches: a possibilistic approach," a causal diagnosis solution relying on experts' knowledgeconcentrates on the different malfunctions that may disturb the dataacquisition process and tells the engine dyno test bench tuning engineer which malfunction has occurred or which malfunctions are most likely suspected. the use of fuzzy sets and possibility theory provides better feedback and knowledge representation. the general architecture of the system is described, and a prototype of the fault-diagnosis part of this system is presented. it concerns the implementation of an (off-line) automatic knowledge formalization system and the implementation of the (on-line) possibilistic causal diagnosis process. ", pais 
2002,propagation of multiple observations in qpns revisited," the sign-propagation algorithm for inference with a qualitative probabilistic network has been designed to handle a single observationat a time.  multiple observations can in essence be dealt with by entering them consecutively and combining the results of the successive propagations, or by entering them for a newly added dummynode.  we demonstrate that both approaches can yield weaker resultsthan necessary.  we identify the causes underlying this unnecessaryweakness and adapt the propagation algorithm so as to provide for thestrongest possible results upon inference. "," qualitative reasoning, reasoning under uncertainty, probabilistic reasoning "
2002,graph partitioning techniques for markov decision processes decomposition," recently, several authors have proposed serial decomposition techniquesfor solving approximately or exactly large markov decision processes.many of these techniques rely on a clever partitioning of the state space in roughly independent parts.the different parts only communicate through relatively few communicating states.the efficiency of these decomposition methods clearly depends on the size of the set of communicating states : the smaller, the better.however, the task of finding such a decomposition with few communicating states is often (if not always) left to the user.in this paper, we present automated decomposition techniques for mdps.these techniques are based on methods which have been developed for graph partitioning problems. "," reasoning under uncertainty, markov decision processes "
2002,on the complexity of the mpa problem in probabilistic networks," the problem of finding the best explanation for a set of observationsis studied within various disciplines of artificial intelligence.for a probabilistic network, finding the best explanation amounts tofinding a value assignment to all the variables in the network thathas highest posterior probability given the available observations.this problem is known as the mpa, or maximum probability assignment,problem.  in this paper, we establish the computational complexity ofthe mpa problem and of various closely related problems.  among otherresults, we show that, while the mpa-p problem, where an assignmentwith probability at least p is to be found, is np-hard, itsfixed-parameter variant is solvable in linear time. "," probabilistic reasoning, computational complexity "
2002,building bayesian networks through ontologies," to support building and maintaining knowledge-based systems forreal-life application domains, sophisticated knowledge-engineeringmethodologies are available. as more and more bayesian networks arebeing developed for complex applications, their construction andmaintenance calls for the use of tailor-made knowledge-engineeringmethodologies. we have designed such a methodology and have studiedits use within the domain of oesophageal cancer. based upon expertknowledge and a previously constructed bayesian network, we havebuilt an ontology for this domain, from which we have constructed,in a sequence of steps, a new network. the use of our methodologyhas allowed us to address, in a structured fashion, the variousintricate modelling issues involved. "," probabilistic reasoning, ontologies, knowledge-based systems "
2002,possibilistic logic representation of preferences: relating prioritized goals and satisfaction levels expressions," the preferences of an agent can be expressed in various ways. the agentmay indicate goals having different levels of priority for him, or provides classes of choices with their level of satisfaction for him. the first type of specification can be captured in possibilistic logic under the form of constraints on a necessity measure. it is shown in this paper that the second manner for expressing preferences can be encoded as constraints on a so-called ""guaranteed possibility"" measure (a min-decomposable function with respect to disjunction). the paper shows how each representation is semantically associated with a possibility distribution (which plays the role of a value function), and how necessity-based possibilistic logic representations can betranslated directly into a guaranteed possibility-based representations and vice-versa. in logical terms, it corresponds to the transformation of a generalized dnf into a generalized cnf. reasoning in guaranteed possibility-based logic is also discussed. moreover, the two types of representations can also be shown to be equivalent to sets of conditional preference statements. thus, different basic modes ofpreference expression can be captured in the same framework. "," reasoning under uncertainty, decision theory "
2002,bayesian network modelling by qualitative patterns," in designing a bayesian network for an actual problem, developersneed to bridge the gap between the mathematical abstractions offeredby the bayesian-network formalism and the features of the problem tobe modelled. a notion that has been suggested in the literature tofacilitate bayesian-network development is causal independence.  itallows exploiting compact representations of probabilisticinteractions among variables in a network. however, only very fewtypes of causal independence are in use today, as only the mostobvious ones are really understood. we believe that qualitativeprobabilistic networks (qpns) may be useful in helping understand causalindependence. originally, qpns have been put forward as qualitativeanalogues to bayesian networks. in this paper, we deploy qpns indeveloping and analysing a collection of qualitative, causalinteraction patterns, called qc patterns.  these are endowedwith a fixed qualitative semantics, and are intended to offerdevelopers a high-level starting point when developing bayesiannetworks. "," probabilistic reasoning, qualitative reasoning, causal reasoning, reasoning under uncertainty "
2002,bayesian networks for probabilistic weather prediction," several standard approaches have been introduced for meteorological time series prediction (analog techniques, neural networks, etc.). however, when dealing with multivariate spatially distributed time series (e.g., a network of meteorological stations over the iberian peninsula) the above methods do not consider all the available information (they consider special independency assumptions to simplify the model).in this work, we introduce bayesian networks (bns) in this framework to model the spatial and temporal dependencies among the different stations using a directed acyclic graph. this graph is learnt from the available databases and allows deriving a probabilistic model consistent with all the available information. afterwards, the resulting model is combined with numerical atmospheric predictions which are given as evidence for the model. efficient inference mechanisms provide the conditional distributions of the desired variables at a desired future time. we illustrate the efficiency of the proposed methodology by obtaining precipitation forecasts for 100 stations in the north basin of the iberian peninsula during winter 1999. we show how standard analog techniques are a special case of the proposed methodology when no spatial dependencies are considered in the model. "," probabilistic reasoning, bayesian learning, data mining and knowledge discovery "
2002,anchoring action representation to perception in a mobile robot," the framework proposed in (chella et al. 1997,2000) for therepresentation of inner perceptual knowledge of a robot is generalizedto include a representation of actions, in order that a mobile robot can exploitits inner perceptual representations to anchor itsactions to perception. such extension is aimed to allow the robotto simulate its own future actions, to anticipate their consequences andevaluate them in order tochoose the most appropriate action to perform. the proposed framework isillustrated by describing its performances in acat-mouse scenario. "," robotics, perception, vision, reasoning about actions and change "
2002,learning to ground fact symbols in behavior-based robots," a robot running a hybrid control system (its architecture comprising a deliberative and a reactive part) must permanently update its symbolic situation model to allow its ongoing deliberation to operate. previous work has shown that this update can be improved by using, possibly among other sources, the robot's sensor information as filtered through recent activation value histories of robot behaviors. in that work, characteristic patterns in groups of behavior activation values are used to define chronicles, which allow true facts about the current situation to be hypothesized. chronicle definitions are hand-crafted as part of the domain modeling.in this paper, we demonstrate that analogs of chronicle definitions can be learned. we use an echo state network, which is a particular type of recurrent neural network. to train it, the same activation value data are used as before in chronicle definitions. the training process is fast. the detection process is cheap to run on-line on board the robot. the new method is demonstrated on data from a robot simulator. it provides the robot programmer an alternative tool for getting recent symbolic situation fact hypotheses. "," robotics, perception, neural networks, machine learning "
2002,mip: a new hybrid multi-agent architecture for the coordination of a robot colony activities," in this paper the development and the implementation of one new robotic architecture for the coordination and the planning of a robot colonies in a dangerous and unknown environment are outlined. the name of this new architecture is metaphor of italian politics (mip architecture). the structure of this architecture is dynamics.  it takes inspiration from the political organizations of the democratic governments, where the leader isn't only one robot but it is constituted by a government of robots. in the mip architecture the robots team is coordinated by a prime minister, a minister of the defence and a minister of communication while a second group of robots represents the robot citizens that are the executor of each mission. the model of the agents is hybrid, so every robot can assume every political position inside this dynamic structure. an intelligent procedure for the government regeneration has been developed in order to avoid the collapse of a mission. to validate the effectiveness of the our approach we have developed a framework based on the mission lab software developed at the mobile robot lab of the georgia institute of technology. the architecture was tested using as framework a bomb disposal expert robots team. "," ai architectures, robotics, multi-agent systems, planning "
2002,a deformation tolerant version of the generalized hough transform for image retrieval," we propose a new version of the famous ballard's generalized hough transform (ght) for image retrieval by shape similarity. indeed, the ght is a very powerful pattern recognition technique, robust to noise and occlusion situations, utilized in hundreds of different machine vision problems. nevertheless, it is conceived for an exact matching between the model and the input image, while, in image retrieval, the user description of a figure is inherently abstract and approximate, thus locally different from each data base image. in this paper we present a version of the ght locally tolerant to deformations which successfully fits image retrieval peculiaritieswithout accuracy loss. the proposed method has been implementedand tested using images randomly chosen from the web with very goodexperimental results. "," vision, information retrieval, information extraction  "
2002,shape analysis and classification using landmarks: polygonal wavelet transform," shape analysis has played a central role in many problems in visionand perception, being an active multidisciplinary research field. inthis context, this paper introduces a new shape analysis approachusing the well known wavelet transform and exploring shaperepresentation by landmarks. this work shows how to obtain a timesignal from the landmarks representation, which undergoes the wavelettransform, as well as a useful geometrical interpretation using twospecial mother wavelets, i.e. the first and the second derivatives ofthe gaussian. successful experimental results obtained from real dataare also discussed. ", vision 
2002,pixel-based behavior learning, in this paper we address the problem of learning behaviors for autonomous mobile robots.we particularly focus on methods which enable a human user to train a robot in its real destination environment without giving an a-priori model. using complex visual input typical of real situations in office environments we  show that very simple visual features can be used to represent the perception/action relation specific to a given behavior. from this point we  propose a learning model relying on a statistical collection of two-pixels features for representing a behavior. we then present the experiments made on a real robot and propose extensions of the model for active-perceptionand behavior selection. ," robotics, machine learning, vision, autonomous agents "
2002,deriving textual descriptions of road traffic queues from video sequences," based on geometric results obtained by an algorithmic video (sequence) evaluation, a generic conceptual representation will be instantiated into a representation of the specific temporal developments within the recorded scene. such an instantiated conceptual representation will in turn provide the input for a text generation subsystem. this contribution outlines a system implementation which relies on a fuzzy metric-temporal (horn) logic to realize the system-internal genericrepresentation. the instantiation step is realized as the search for a suitable interpretation of the corresponding set of logic formulae where the geometric results obtained by image sequence evaluation provide the set of individuals. the practicality of this approach is demonstrated by abstraction processes which aggregate suitably selected vehicles from recorded road traffic scenes into a vehicle queue. "," vision, knowledge representation, natural language processing, qualitative reasoning, reasoning about actions and change, knowledge-based systems, temporal reasoning, description logics "
2002,modeling interaction using learnt qualitative spatio-temporal relations and variable length markov models ," motivated by applications such as automated visual surveillance andvideo monitoring and annotation, there has been a lot of interest inconstructing cognitive vision systems capable of interpreting the highlevel semantics of dynamic scenes. in this paper we present a novel approach for automatically inferring models of object interactions that can be used to interpret observed behaviour within a scene. a real-time low-level computer vision system, together with an attentional control mechanism, are used to identify incidents or events that occur in the scene. a data driven approach has been takenin order to automatically infer discrete and abstract representations(symbols) of primitive object interactions;effectively the system learns a set of qualitative spatial relations relevant to the dynamicbehaviour of the domain. these symbols then form the alphabet of a vlmm which automatically infers the high level structure of typical interactive behaviour. the learnt behaviour model has generativecapabilities and is also capable of recognizing typical or atypicalactivities within a scene. experiments have been performed within thetraffic monitoring domain; however the proposed method is applicable to the general automatic surveillance task since it does not assume a priori knowledge of a specific domain.  "," vision, learning, machine learning "
2004,negotiation in state-oriented domains with incomplete information over goals," state oriented domains (sods) are domains where agents are concerned
with moving the world from an initial state into one of a set of
target states. negotiation in this environment was explored by
rosenschein and zlotkin, who provided an analysis of incentive
compatible mechanisms over a variety of two-agent, single-encounter
types. their model included the concept of an agent's worth (the
agent's benefit from achieving its goal), using it as a baseline for
utility calculation of a negotiation's outcome.  one scenario left
unexamined, however, was the case where agents know one another's
worths, but not one another's goals. this situation creates the
possibility of agents' lying to one another solely about goals, to
influence the outcome of a negotiation.

in this paper, we explore this specific case of known worths and
unknown goals in two-agent state oriented domains, in a variety of
encounter types. through analysis and examples, it is shown that an
agent can benefit from declaring less costly goals, but that there
are certain limits to the lies an agent can beneficially declare.
we also analyze the connection of this work to classic game theory
results, including general work on incentive compatible mechanisms
and the revelation principle.
 "," multi-agent systems, distributed ai, negotiation, autonomous agents, incentive compatible mechanisms, game theory "
2004,negotiating the distribution of obligations with sanctions among autonomous agents," in this paper we consider the problem of the distribution of
obligations directed to collective entities like groups and
organizations among the individuals belonging to those entities. we
model the distribution of obligations among a set of agents as a
negotiation process among the agents. we distinguish different
types of collective obligations according to whether the
responsibility of a violation during the negotiation or the
execution of the task is attributed to the
 individual agents or to the whole set of agents. "," multi-agent systems, decision and game theory, normative systems, negotiation "
2004,"agent planning, negotiation and control of operation"," this paper presents a framework that integrates three 
aspects of agency: planning,
for proactive behaviour, negotiation, for social behaviour
and resource achievement, and control of operation, for
reconciling rationality with reactivity. 
agents are designed and programmed in a computational 
logic-based language where these aspects are accommodated
in a declarative and modular way. we show how this 
framework can be applied to agent problems requiring
negotiation and resource achievement and 
present some of its formal properties. the
framework has been implemented 
based on a communication platform
for agent interaction and on well-established logic programming
technologies for agent reasoning. "," multi-agent systems, autonomous agents, logic programming "
2004,game-theoretic agent programming in golog," we present the robot programming language gtgolog, 
which integrates explicit agent programming in golog
with game-theoretic multi-agent planning in markov 
games. it is a generalization of dtgolog to a multi-agent 
setting, where we have two competing single agents or 
two competing teams of agents. the language allows for 
specifying a control program for a single agent or a 
team of agents in a high-level logical language. the 
control program is then completed by an interpreter 
in an optimal way against another single agent or 
another team of agents, by viewing it as a markov 
game, and computing a nash strategy. we illustrate 
the usefulness of this approach along a robotic 
soccer example. "," cognitive robotics, reasoning about actions and change, reasoning under uncertainty, multi-agent systems, game playing "
2004,fire: an integrated trust and reputation model for open multi-agent systems," trust and reputation are central to effective interactions in open multi-agent systems in which agents, that are owned by a variety of stakeholders, can enter and leave the system at any time. this openness means existing trust and reputation models are cannot readily be used. to this end, we present fire, a trust and reputation model that integrates a number of sources to produce a comprehensive assessment of an agent's likely performance. specifically, fire incorporates interaction trust, role-based trust, witness reputation, and certified reputation to provide a trust metric in virtually all circumstances. fire is empirically benchmarked and is shown to help agents effectively select appropriate interaction partners. "," trust, reputation, multi-agent systems "
2004,the kgp model of agency," 
this paper presents a new model of agency, called the $kgp$ ({\bf
k}nowledge, {\bf g}oals and {\bf p}lan) model. this model draws
from the classic $bdi$ model proposing a hierarchical agent
architecture with a highly modular structure that synthesises
together various reasoning and sensing capabilities of the agent
in an open and dynamic environment. the novel features of the
model include: its innovative use of computational logic (cl) in a
way that facilitates both the formal analysis of the model and its
computational realisability directly from the high-level
specification of the agents (a first implementation platform for
the development of $kgp$ agents exists); the modular separation of
concerns and flexibility afforded by the model in designing
heterogeneous agents and in developing independently the various
components of an agent; and the declarative agent control provided
through a context-sensitive cycle cl theory component of the agent
that regulates its operational behaviour, according to the current
circumstances of operation thus breaking away from the
one-size-fits-all control of operation. "," autonomous agents, logic programming, multi-agent systems "
2004,negotiation strategies for autonomous computational agents," autonomous agents are being used in an increasingly number of applications. these agents operate in complex environments and, over time, conflicts inevitably occur among them. conflict resolution is crucial for achieving coordination. the predominant process for resolving conflicts is negotiation. recent growing interest in electronic commerce has also given increased importance to negotiation. 

artificial intelligence researchers have recently started to investigate the design of autonomous negotiating agents. some researchers developed or adopted a model of individual behavior and used the model as a starting point for the development of a negotiation model. however, most researchers have focused solely on developing negotiation models. they have addressed only part of the overall task of building autonomous negotiating agents. in particular, they have paid little attention to the problem of integrating existing or new models of individual behavior with their negotiation models. this fundamental problem is still an open problem.

this paper presents a generic negotiation model for autonomous agents that handles multi-party, multi-issue, and single or repeated rounds. the main components of the model are: (i) a prenegotiation model, (ii) a negotiation protocol, (iii) an individual model of the negotiation process, (iv) a set of negotiation strategies, and (v) a set of negotiation tactics. the model is based on computationally tractable assumptions, accounts for a tight integration of the individual capability of planning and the social capability of negotiation, and formalizes a set of human negotiation procedures.

the paper also introduces the type of real world applications we are interested in, by describing a multi-agent supply chain system. 
 "," multi-agent systems, negotiation, e-commerce, conflict management, autonomous agents "
2004,diagnosability analysis of distributed discrete event systems," this paper addresses the diagnosability problem of distributed discrete event systems. until now, the problem of diagnosabilty  has always been solved by considering centralised approaches, monolithic systems. these approaches do not use the fact that real monitored systems are generally modelled in a distributed manner. in this paper, we propose a framework for the diagnosability analysis of such systems. in this framework, we study the diagnosability of the system using the fact that it is based on a set of communicating components, we  also want to provide more accurate information in order to better understand why the system is not diagnosable. "," diagnosability, failure diagnosis,, discrete event systems "
2004,complex networks emergence through local optimisation ," the emergence of complex network structures, such as the celebrated power-law and small-world networks reflecting relationships between autonomous agents emerge in a wide range of distributed systems. many researchers have proposed models to explain and reproduce this striking phenomenon. however, most of their assumptions are non-realistic and are motivated by their interest of reproducing, rather than explaining, the emergence of such structures. one example of this is the need for agents to have knowledge about all other agents interactions in order to carry on preferential attachment, the most used mechanisms for generation power-laws. a more realistic point 
of departure is based on the agents having access only to local knowledge and not to the global properties of the system and the corresponding network. we propose a general model based on a set agents with dissimilar attractiveness level, who are seeking the best set of agents to interact with in terms of the outcome received 
from the payoff matrix of the cooperative game they are engaged in. thus, agents perform an local optimisation process bounded by realistic and plausible assumptions: local and imperfect information.
furthermore, our model is able to generate several kinds of complex networks: from power-law to small-world including central-periphery networks. this diverstiy depends on a new parameter that we have identified: system's harshness.
 "," complex networks, power-law networks, scale-free networks, small-world networks, social simulation, multi-agent systems, distributed ai, game-theory "
2004,automatic verification of deontic and epistemic properties of multi-agent systems by model checking via obdd's," we present an algorithm and its implementation for the
verification of correct behaviour and epistemic states in multiagent
systems. the verification is performed via model checking techniques based
on obdd's. we test our implementation by means of a communication
example: the bit transmission problem with faults. "," verification and validation, multi-agent systems "
2004,many hands make light work: localized satisfiability for multi-context systems," in this paper, we tackle the satisfiability problem for multi-context systems. first, we establish a satisfiability algorithm based on an encoding into propositional logic. then, we propose a distributed decision procedure that maximally exploits the potential amenity of localizing reasoning and restricting it to relevant contexts. we show that the latter approach is computationally superior to our translation-based procedure, and outline how off-the-shelf reasoning procedures, like binary decision diagrams and propositional sat solvers, can be used to implement our algorithm. "," distributed ai, satisfiability, knowledge representation "
2004,agreements without disagreements , multi-agent coalition formation method ," multi-agent, coalition formation "
2004,serse: searching for semantic web content," searching the semantic web (sw) for digital content is arguably more complex than searching the current
web. in fact, a sw search system must deal with a large number of
distributed, heterogeneous resources, that may
reference many different ontologies.
in order to manage this complexity we have integrated different technologies such as peer-to-peer, ontologies, and
multi-agent technology in the design of serse, a multi-agent system for searching the sw.
  in serse, agents are organised in a p2p network according to a semantic overlay, where the neighbourhood is determined by the
semantic proximity of the ontological definitions that are known to
the agents.  the integration of these technologies poses some
problems. on the one hand, the more ontological knowledge the agents
have, the better we can expect the system to perform. on the other
hand, global knowledge would constitute a point of centralisation
which might potentially degrade the performance of a p2p system.  in
addition, maintenance of such semantic overlay can also degrade the performance.
the paper presents serse together with
an analytical evaluation of the workload of the system due to
maintenance and some experimental results that evaluate the performance
in response to changes in the size of semantic
neighbourhood.
 "," multi-agent systems, ontologies "
2004,balancing coordination and synchronization cost in cooperative situated multi-agent systems with imperfect communication," in this paper, we propose a new markov team decision model to the decentralized control of cooperative multi-agent systems with imperfect communication: messages may be delayed or lost. we introduce the concept of informational classes, which captures system's communication semantics and uncertainties about transmitted information. stochastic transmission models, including delayed and lost messages, summarize characteristics of communication devices and protocol. this model provides a quantitative solution to the problem of balancing coordination and synchronization cost in cooperative domains, but its exact solution is computationally infeasible. we propose a generic two-phase heuristic approach. at the off-line phase, a centralized team plan is established. at the on-line phase, decentralized decision-making is based on bayesian dynamic system estimators and decision-theoretic policy generators. these generators use system estimators to express agent's uncertainty about system state and also to quantify expected effects of communication on local and external knowledge. probabilities of external team behavior, a byproduct of policy generators, are used into system estimators to infer state transition. bayesian inference also allows local estimations of external knowledge. under perfect communication assumptions, we solve two previously proposed multi-agent tasks and compare our results with original ones. we then introduce communication limitations (range and reliability) and solve these new settings.  "," cooperative multiagent systems, decentralized markov decision process, decision theory "
2004,expectation reasoning using regret and disappointment," a critical aspect of an agent system is the ability to deal with unexpected situations to determine an appropriate course of action in a changing environment. in this paper, we investigate the incorporation of the mental attitudes of regret and disappointment (which have been studied by economists using utility theory) into the agent's reasoning system in order to improve its ability to deal with unexpected events. mental attitudes in agent systems have generally been expressed in modal logics, such as the belief-desire-intention (bdi) logic and epistemic logic, and, more recently, in a logic of expectation and observation. we show how regret and disappointment can be naturally integrated into a framework based on the attitudes of expectation and observation, and give describe some key properties of the system. "," agent system, expectation reasoning, modal logic, regret, disappointment "
2004,a risk-based bidding strategy for continuous double auctions," we develop a novel bidding strategy that software agents can use to buy and sell goods in continuous double auctions (cdas). our strategy involves the agent forming a bid or ask by assessing the degree of risk involved and making a prediction about the competitive equilibrium price that is likely to be reached in the marketplace. we benchmark our strategy against two of the most common strategies for cdas, namely the zero-intelligence (zi) and the zero-intelligence plus (zip) strategies, and we show that our agents outperform the benchmarks. specifically, they win in 100% of the simulations against the zi agents and, on average, in 75% of the games against the zip agents. "," multi-agent systems, continuous double auctions "
2004,algorithms for distributed exploration," in this paper we propose algorithms for a set of problems where a
distributed team of agents tries to compile a global map of the environment from local observations. we focus on two approaches: one based on behavioral agent technology where agents are pulled (or repelled) by various forces, and another where agents follow a approximate planning approach that is based on dynamic programming. 
we study these approaches under different conditions, such as different types of environments, varying sensor and communication ranges, and the availability of prior knowledge of the map. the results show that the simpler behavioral agent teams perform at least as well, if not better, than the teams based on approximate planning and dynamic programming. we also compare our approaches to theoretical lower bounds on optimal performance.   

the research has not only practical implications for distributed exploration tasks, but also for any distributed search problem that is 
analogous to spatial exploration problems.
 ", multi-agent systems 
2004,passive threats among agents in state oriented domains," previous work in multiagent systems has used tools from game theory
to analyze negotiation among automated agents in cooperative
domains. rosenschein and zlotkin, using these tools, provided a
general mechanism for two-agent negotiation in an isolated state
oriented domain (sod) encounter, and also provided a classification
that divided these encounters into four basic types. other multiagent
systems work considered the notion of threats during negotiation, but
in order to do so introduced additional assumptions on the domain.

this paper presents a new model of threats among negotiating agents
in state oriented domains that requires no additional domain-specific
assumptions. we assume that agents may use a ""threat of passivity""
against other agents --- in other words, threatening to remain
inactive (and not exploit existing cooperative opportunitites),
forcing both agents to satisfy their goals on their own (serially).
the possibility of this negotiation threat adds interesting
complexity to the four basic sod encounter types, further
subdividing them into additional types of encounter. we analyze
these new encounter types that arise when there is the possibility
of ""passive threats"", providing a thorough characterization of their
properties.
 "," multi-agent systems, distributed ai, autonomous agents, negotiation, threats, state oriented domains "
2004,instance-based prediction with guaranteed confidence," instance-based learning (ibl) algorithms have proved to be successful
in many applications. however, as opposed to standard statistical
methods, a prediction in ibl is usually given without characterizing
its confidence. in this paper, we propose an ibl method that allows
for deriving set-valued predictions that cover the correct answer
(label) with high probability. our method makes use of a formal model
of the heuristic inference principle suggesting that similar instances
do have similar labels. the focus of this paper is on the prediction
of numeric values (regression), even though the method is also useful
for classification problems if a reasonable similarity measure can be 
defined on the set of classes. "," machine learning, case-based reasoning "
2004,unsupervised instance-based learning techniques of feature weighting do not perform so badly!," major hypothesis that will be proved in this paper is that unsupervised learning techniques of feature weighting are not significant worse than supervised methods as is commonly supposed in the machine learning community.
this paper tests the power of unsupervised feature weighting techniques for predictive tasks within several domains. the paper analyses several unsupervised and supervised feature weighting techniques, and proposes new unsupervised feature weighting techniques. two unsupervised entropy-based weighting algorithms are proposed and tested against all other techniques. the techniques are evaluated in terms of predictive accuracy on unseen cases, measured by a ten-fold cross-validation process. of course, the label class is not taken into account to find out the weights within the unsupervised methods, but it is for the predictive accuracy computation.
the testing has been done using thirty-four data sets from the uci machine learning database repository and other sources. unsupervised weighting methods assign weights to attributes without any knowledge about class labels, so this task is considerably more difficult. it has commonly been assumed that unsupervised methods would have a worse performance than supervised ones, as they do not use any domain knowledge to bias the process. to really confirm or not this hypothesis, a performance analysis among several supervised and unsupervised methods have been done.
major result of the study is that unsupervised methods really are not so bad! moreover, one of the unsupervised learning methods has shown a very promising behaviour when faced against domains with many irrelevant features, reaching similar performance as some of the best supervised methods. 
 "," feature weighting, unsupervised instance-based learning, machine learning  "
2004,exchanging emotions - som approach," 
today's cellular phones are still based on the age-old views of what a telephone can do. however, in human interaction, only a fraction of communication is based on the actual speech information. how to make the future phones less technical, more personal? in this paper, possibilities of applying affective computing in cellular phones is discussed and the experiments that have been carried out are presented.
   "," affective computing, self-organizing map (som), cellular phones "
2004,applying affective tactics for a better learning," this paper describes the mediating agent, an animated pedagogical
agent inserted in a computational system for distance learning,
which has the goal of motivating the student to learn as well as
promoting a positive mood in the student, which is more
appropriate to learning. in order to accomplish its function, the
agent should recognize the student's emotions to respond
appropriately. thus, it catches some information from observing
the student's behaviour in the interface of system, which allows
it to infer the student's affective state according to the
cognitive approach of emotion. this information is stored in an
affective model and it is used so that the agent may choose an
affective pedagogical tactic to be applied. "," affective computing, emotions and learning, animated pedagogical agents, affectivity in human computer interation "
2004,forming odour categories using an electronic nose," in this work, we consider how to automatically create a correspondence between symbols, in this case linguistic names, and categories of odours using the data from an electronic nose. to accomplish this task, we create a system that first creates categories using unsupervised clustering and then generalizes from these catgeories by evaluating how well an odour name applies to its own sensor representations. since part of the objective is to facilitate human and machine interaction, the names outputed from the system are correlated and with those of a human user. this may mean that in the context of an unsupervised categorization, perceptual differences between the human and the electronic nose may arise, however, these differences can be considered an asset especially when dealing with olfaction, for example when the electronic nose detects carbon monoxide that is normally perceived as odourless for a human. therefore, to cope with the perceptual differences, the system is tuned to provide as explicit information about the categorization of odours as possible. we accomplish this task by adapting existing fuzzy-based algorithms to generate informative odour descriptions. "," perception, cognitive modelling "
2004,a formal tutoring process model for intelligent tutoring systems," the combination computer based training systems with artificial intelligence and cognitive science have led to the development of intelligent tutoring systems (its) nearly 30 years ago. a common agreement has been reached about the constituents of an its. nonetheless, the interpretation of the role of each component in the its is still heterogeneous. together with the absence of formal methods in its, this leads to the situation, that most its' components are strongly domain dependent and not reusable.
in the paper, a formal approach for case-based its is described. the formal model is called the tutoring process model. the model is based on the idea to support the training of at least two cognitive processes in the case-based setting. the integration of the tutoring process model as the central component in the its architecture has led to a homogenization of the architecture. the tutoring process model has been used as the basis of the its docs 'n drugs, a web- and case-based its for clinical medicine. docs 'n drugs is part of the medical curriculum at the university of ulm since three years and has been used by several hundreds of students, yet.
 "," computer aided learning, intelligent tutoring systems, cognitive modelling "
2004,reasoning about emotional agents," in this paper we discuss the role of emotions in artificial
agent design. from cognitive science we know that having emotions may help one to do reasoning and tasks for which rationality seems to be the only factor. for example, emotions moderate the execution and maintenance of the agent's agenda. thus emotions make sense in describing the behaviour of certain intelligent agents, and may help structuring the design of the agent (by means of an architecture that caters for emotional aspects).
consequently - when specifying agent behaviour - it is useful to reason about emotions of an agent, or rather about the emotional states an agent may be in, together with its effects on the agent's actions. we do so by extending the karo framework for reasoning about rational agents appropriately. in particular we formalize in this framework how emotions are related to the action monitoring capabilities of an agent. here we are interested in at least two things: how do actions of agents change their emotional states and how do emotional states determine what action is taken and what effect is obtained from this in a given state. "," intelligent agents, modal logic, emotions "
2004,focusing reasoning through emotional mechanisms," in concrete environments, where uncertainty and dynamism are pervasive and time and resources are limited, reasoning and decision-making processes raise important problems related both to adaptive ability and to the computational complexity of the underlying cognitive mechanisms. in this paper we propose that a symbiotic integration between emotion and cognition is a key aspect to address these problems. to concretize this view we present an agent model where emotion and cognition are modeled as two integrated aspects of intelligent behavior and where affective-emotional mechanisms are used to support adaptability and to focus the reasoning and deliberation mechanisms to cope with their computational complexity. "," cognitive modeling, resource-bounded reasoning, emotion modeling "
2004,a case study of revisiting best-first vs. depth-first search," best-first search usually has exponential space requirements on difficult problems. depth-first search can solve difficult problems with linear space requirements, but it cannot utilize large additional memory available on today’s machines. therefore, we revisit the issue of when best-first or depth-first search is preferable to use. through algorithmic improvements, it was possible for the first time to find optimal solutions of certain difficult problems (the complete benchmark set of fifteen puzzle problems) using traditional best-first search (with the manhattan distance heuristic only). our experimental results show that this search can solve them overall faster than any of the previously published approaches (using this heuristic). note that this kind of search was believed to be incapable of solving randomly generated instances of the fifteen puzzle within practical resource limits because of its exponential space requirements. so, our case study suggests that changes in hardware and algorithmic improvements together can revise the previous assessment of best-first search. "," search, heuristic search, best-first search, bidirectional search "
2004,boosting systematic search by weighting constraints," in this paper, we present a dynamic and adaptive variable ordering heuristic which guides systematic search toward inconsistent or hard parts of a constraint satisfaction problem (csp).
this generic heuristic, denoted wdeg, is able to exploit information about previous states of the search process whereas traditional dynamic ones only exploit information about the current state.
intuitively, wdeg avoids some trashing by first instantiating variables involved in the constraints that have frequently participated in dead-end situations.
such information is recorded by associating a weight with each constraint.
this weight is increased whenever the associated constraint is violated during search.
the extensive experiments that we have conducted prove that our approach is the most efficient current one with respect to significant and large classes of academic, random and real-world instances.  "," constraint satisfaction, systematic search, variable ordering heuristic "
2004,adversarial constraint satisfaction using game-tree search," many decision problems can be modelled as ""adversarial"" constraint satisfaction, and doing so allows us to bring to bear methods from artificial intelligence game playing. in particular, by using the idea of ""opponents"", we can model both collaborative problem solving, where intelligent participants with different agendas must work together to solve a problem, and multi-criteria optimisation, where a single solver must balance different objectives. in this paper, we focus on the case where two opponents take turns to instantiate constrained variables, each trying to direct the solution towards their own objective. we represent the process as game-tree search. we develop constraint propagation methods and variable and value ordering heuristics based on game playing strategies. we examine the performance of various algorithms on general-sum graph colouring problems, for both multi-participant and multi-criteria optimisation. "," constraint satisfaction, search, game tree "
2004,generalized widening," we present a new threat based search algorithm that outperforms other threat based search algorithms and selective knowledge-based alpha-beta for open life and death problem solving in the game of go. it generalizes the iterative widening algorithm which consists in iteratively increasing the threat searched at the root. the main idea of generalized widening is to perform iterative widening at all max nodes of the search tree instead of performing it only at the root. experimental results show it can be three times faster than selective knowledge-based alpha-beta using the same knowledge, and eight times faster than simple iterative widening. the performance against alpha-beta can possibly be greatly enhanced by adding more knowledge in the selection of moves during the verification of the threats.
 ", game playing 
2004,quantified constraint satisfaction and bounded treewidth," in previous work, it has been shown that instances of the constraint satisfaction problem with bounded treewidth are tractable.
we investigate, for the first time, the restriction of bounded treewidth
in the setting of the quantified constraint satisfaction problem (qcsp).
we demonstrate a tractability result, namely, that instances of the qcsp
with bounded treewidth, bounded quantifier alternations, and bounded domain size are tractable. "," quantified constraint satisfaction, tractability, bounded treewidth "
2004,how to use the scuba diving metaphor to solve problem with neutrality ? ," we proposed a new search heuristic using the scuba diving metaphor. this approach is based on the concept of evolvability and tends to exploit neutrality which exists in many real-world fitness landscapes. despite the fact that natural evolution does not directly select for evolvability, the basic idea behind the scuba search heuristic is to explicitly push evolvability to increases. a comparative study of the new algorithm and standard local search heuristics on the travelling salesman problem (tsp) has shown advantage and limit of the scuba search heuristic. in order to tune neutrality we use a tsp where cities are randomly placed on a lattice; and travel distance between cities is computed with the manhattan metric. the amount of neutrality vary with the city concentration on the grid. assuming the concentration below one, this tsp reasonably remains a np-hard problem. "," metaheuristic, fitness landscape, neutrality "
2004,symmetry-breaking as a prelude to implied constraints: a constraint modelling pattern," constraint programming can be used to solve a wide range of problems by first modelling the problem as a set of constraints that characterise the problem's solutions, and then searching for solutions that satisfy the constraints. modelling choices are crucial to how efficiently the resultant model can be solved. models generated by experts often are augmented with constraints that break symmetries in the model and thus remove redundancy from the search space, and with implied constraints, which prune some parts of the search space that contain no solutions. common practice is to choose symmetry-breaking constraints by considering the number of symmetries that they break and the overhead that they incur. this paper argues that, to obtain the best model, the choice of symmetry-breaking constraints must be made not simply on their own merits, but also based on the strength of the implied constraints derivable from them. this approach should be established as a pattern in good constraint modelling practice. in particular, it is demonstrated that sometimes a weaker symmetry-breaking scheme can outperform a stronger one when constraints are added. "," constraint programming, modelling with constraints, constraint patterns, symmetry breaking "
2004,encoding quantified csps as quantified boolean formulae," quantified constraint satisfaction problems (qcsps) are csps in which some variables are universally quantified.  for each possible value of such variables, we have to find ways to set the remaining, existentially quantified, variables so that the constraints are all satisfied.  interest in this topic is increasing following recent advances in quantified boolean formulas (qbfs), the analogous generalisation of satisfiability (sat).   we show that we can encode qcsps as qbfs.  we introduce simple generalisations of both the direct and support encodings of csps into sat.   we then introduce some adaptations of these encodings to make them effective for search in a qbf solver.   we solve some qcsp test instances an order of magnitude faster than using a specialised qcsp solver, taking advantage of the more advanced state of the art in qbf solving.   
our conclusions are twofold.  first, there is considerably more subtlety required in encodings in qbf than in sat.  second, in an area such as qcsp where algorithmic techniques are not yet highly developed, encodings into a better understood problem can give access to extremely advanced search methods with very little implementation effort. "," satisfiability testing, constraint satisfaction, search, automated reasoning "
2004,constrained pure nash equilibria in graphical games," the rational behavior of non-cooperative players is often formalized by means of the game theoretic notion of nash equilibrium. however, there are several practical applications (such as multi agent planning, mechanism design, and routing protocols design) where the computation of any nash equilibrium could not be satisfactory, since we are often interested only in equilibria that satisfy some additional requirements. even though such nash equilibria, called constrained nash equilibria, received a great deal of interest, a comprehensive formalization and a deep investigation of the complexities issues related to their computation are still missing.

in this paper, we present a formal framework for specifying and working with these constraints, by focusing on graphical games, where a player p may be directly interested only on part of the other players, called neighbors of p. we study the computational complexity of the main problems arising in this framework for pure strategies. furthermore, we identify some restrictions on players' interaction that make some of these problems easy, by exploiting the graphical representation of the structure of the game, and a generalization of graph acyclicity called treewidth. in these tractable cases, we also provide highly-parallelizable algorithms for the computation of such
constrained nash equilibria. "," game theory, decision theory "
2004,robust solutions for constraint satisfaction and optimization," super solutions are a mechanism to provide solution robustness 
within constraint programming. super solutions are solutions
in which, if a small number of variables lose their values, we
are guaranteed to be able to repair the solution with only a 
few changes. in this paper, we extend the super solution framework
in several dimensions to make it more useful practically. we 
present the first algorithm for finding super solutions in 
which the repair changes both variables that have lost values
and those that have not. we then extend the framework and algorithms
to permit a wide range of practical restrictions on the breaks and
repairs. consider, for example, a scheduling problem in which the
variables are jobs and the values are the times. we might only be 
able to repair a variable with a larger value as repairing with a
smaller value take us back in time. we also show how to deal with
symmetry when finding super solutions. symmetry is a frequent 
problem in constraint solving. our experimental results suggest
that it is even more important to tackle symmetry when looking
for super solutions. finally, we present results on job shop 
scheduling problems which demonstrate the tradeoff between 
solution robustness and makespan.  "," constraint programming, constraint satisfaction, reasoning under uncertainty "
2004,an effective branch-and-bound algorithm to solve the k-longest common subsequence problem," in this paper, we study the longest common subsequence problem of multiple sequences. because the problem is np-hard, we devise an effective branch-and-bound algorithm to solve the problem. 
results of extensive computational experiments show our method to be effective not only on randomly generated benchmark instances, but also on real-world protein sequence instances. "," search, branch-and-bound, bioinformatics, real-world protein sequence "
2004,decomposition and good recording for solving max-csps," [20] presents a new method called btd for solving valued csps and so max-csps. this method based both on enumerative techniques and the tree-decomposition notion provides better theoretical time complexity bounds than classical enumerative methods and aims to benefit of the practical efficiency of enumerative methods thanks to structural goods which are recorded and exploited during the search. however, in [20], the authors do not provide any experimental result and they do not discuss the way of finding an optimal solution from the optimal cost (because btd only computes the cost of the best assignment). providing an optimal solution is an important task for a solver,  especially when we consider real-world instances. so, in this paper, we first raise these two questions. then we explain how a solution can be efficiently compute and we provide experimental results which emphasizes the practical interest of btd. "," constraint satisfaction, valued csp, tree-decomposition "
2004,a study on the accuracy of heuristic functions," from a practical point of view, most problems cannot
be optimally solved applying brute-force single-agent search algorithms.
thus, a new class of single-agent search algorithms was devised
to find the optimal solution faster and/or consuming less space,
heuristic single-agent search algorithms. however, designing accurate
heuristic functions is still a major challenge. in this paper, a
mathematical model is introduced for characterizing the accuracy of
any heuristic function. this is a step ahead in the analysis of heuristic
functions since this method could be used to aid other procedures to
improve the original heuristic estimations of h(.).
 "," game playing, planning,, search "
2004,improved nogood learning and agent cooperation in asynchronous backtracking with complex local problems,"   distributed constraint satisfaction, in its most general
  acceptation, involves a collection of agents solving local
  constraint satisfaction subproblems, and a communication protocol
  between agents, in order to allow the distributed system converge to
  a global solution. the literature, however, often concentrates on
  the reduction where each agent owns exactly one variable, under the
  rationale that the corresponding algorithms are easily extended to
  the most general case.  while this is mostly true, the specificities
  of agents handling local csps give way to numerous improvements,
  since a tradeoff becomes possible between local and distributed
  search effort.
                                                                                                 
  in this paper, we seek to improve nogood learning and solver
  cooperation in multi-variables distributed constraint satisfaction
  problems. we propose incremental improvements to be implemented on
  top of an abt-like algorithm, and make experimental evaluations of
  the performance improvement they bring. "," constraint satisfaction, distributed ai, multi-agent systems "
2004,tractable symmetry breaking using restricted search trees," we present a new conceptual abstraction in symmetry breaking -- the ge-tree. the construction and traversal of a ge-tree breaks all symmetries in any constraint satisfaction or similar problem. 
we give a polynomial-time algorithm for this construction in the case of csps with arbitrary value symmetries. we have implemented this technique, and supply experimental evidence of its practical effectiveness.
 "," symmetry, constraints, search "
2004,implementing variable elimination," adaptive consistency is a solving algorithm for constraint networks. 
its basic step is variable elimination: it takes a network as input,
and produces an equivalent network with one less variable and one new
constraint (the join of the variable bucket).  this process is
iterated until every variable is eliminated, and then all solutions
can be computed without backtracking.  a direct, naive implementation
of variable elimination uses more space than needed, which renders
this algorithm unapplicable in many cases.  we present a more
sophisticated implementation, based on the projection with memory of
constraints.  when a variable is projected out from a constraint, we
keep the supports that that variable gave to the remaining tuples. 
using this data structure, we compute a set of new constraints
equivalent to the new constraint computed as the join of the variable
bucket, but using less space on the average.  this is especially
relevant for problems where the elimination of the first variables
generate very large constraints.  we provide experimental evidence of
the benefits of our approach. "," constraint satisfaction, constraint inference, adaptive consistency "
2004,guiding a theorem prover with soft constraints," attempts to use finite models to guide the search for proofs by resolution and the like in first order logic all suffer from the need to trade off the expense of generating and maintaining models against the degradation in quality of guidance as investment in the semantic aspect of the reasoning is decreased. previous attempts to resolve this tradeoff have resulted either in poor selection of models, or in fragility as the search becomes over-sensitive to the order of clauses, or in extreme slowness. here we present a fresh approach, whereby most of the clauses for which a model is sought are treated as soft constraints. the result is a partial model of all the clauses rather than an exact model of only a subset of them. this allows our system to combine the speed of maintaining just a single model with the robustness previously requiring multiple models. we present experimental evidence of benefits over a wide range of first order problems domains. "," automated reasoning, theorem proving "
2004,using symmetries for coloring queen graphs," the queen graph coloring problem consists in covering a nxn
chess-board with nxn queens, so that two queens of the same color
cannot attack each other. when the size, n, of the chess-board is
not a multiple of 2 or 3 it is hard to color the queen graph with
only n colors. we have developed an exact algorithm which is able
to solve exhaustively this problem for dimensions up to n=12 and
find one solution for n=14 in 168 hours of computing time. the 454
solutions of 12x12 queen show horizontal and vertical symmetries
in the color repartition on the chess-board. from this
observation, we design a new exact, but incomplete, algorithm
which leads us to color nxn queen problems with n colors for n=15,
16, 18, 20, 21, 22, 24 and 28 in less than 24 hours of computing
time by the exploitation of symmetries and other geometric
properties.

 "," queen graph coloring, exact incomplete algorithm, symmetry, rotation "
2004,uncertain linear constraints," linear constraints occur naturally in many reasoning problems
and the information that they represent is often uncertain.
there is a difficulty in applying many ai uncertainty formalisms to this situation, as their representation of the underlying logic,
either as a mutually exclusive and exhaustive set of possibilities, or as a propositional or a predicate logic, is inappropriate (or at least unhelpful). to overcome this, we express reasoning with linear constraints as a logic, and develop the formalisms based on this different underlying logic. we focus in particular on a possibilistic logic representation of uncertain linear constraints, and a dempster-shafer representation. "," reasoning under uncertainty, spatial reasoning, temporal reasoning, nonmonotonic reasoning "
2004,generation of desired emergent behavior in swarm of micro-robots," emergent behavior of swarm-like-systems results from interactions among system's components and cannot be directly preprogrammed. this kind of behavior is very efficient, flexible and is closely related with collective (or swarm-) intelligence. in the presented work we consider a derivation of local rules, creating such interactions, that lead to desired (technically useful) emergent behavior. as an example of collective emergence we choose forming spatial groups and assembling micro-objects by a swarm of micro-robots. "," emergent behavior, micro-robotic systems, swarm intelligence, generation of local rules "
2004,a genetic algorithm with feminine selection," this paper describes a selection approach for evolutionary algorithms
- called feminine selection -- that is inspired in the fact
that in some animal species the female actively select their
reproduction partners. in these species, the males exhibit their
attributes, sometimes fighting with other males, and the female choose
the one she considers the best. to implement this approach, the
algorithm adopts a sexual reproduction mechanism, that among other
properties precludes the reproduction between two identical genomes,
avoiding premature convergence. the proposed algorithm was implemented
and its results compared with those of the standard goldberg
algorithm.
 ", genetic algorithms 
2004,control of a hydroforming press with bayesian networks," the demands to automatic control for industrial plants are growing due to an increased complexity of the manufacturing processes. to face these challenges, intelligent control is getting more and more important. for example, neural networks and fuzzy logic are regularly used. the usage of bayesian networks is seldom mentioned even if many training algorithms are available and bayesian networks are also able to act under real-time conditions. that means that main preconditions for a self adaptive controller are given. this paper explains how a bayesian network is employed as a controller. the main idea is to use the desired value as if it were already observed and to use marginalization for the calculation of the input. this principle is 
successfully applied to the control of a hydroforming press. as a result the process characteristics in terms of an uniform blank draw-in and the preforming pressure are improved. "," bayesian networks, modelling of manufacturing processes, control, hydroforming "
2004,knowledge-based cinematography and its applications," automated control of a virtual camera is useful for both linear
animation and interactive virtual environments. we have
constructed a knowledge-based system that allows users to
experiment with various cinematic genres and view the results in
the form of animated 3d movies. we have followed a knowledge
acquisition process converting domain expert principles into
declarative rules, and our system uses non-monotonic reasoning in
order to distinguish between absolute rules, default rules, and
arbitrary user choices. we evaluated the tool by generating
various movies and showing some of the results to a group of
expert viewers. "," virtual camera, knowledge-based systems, virtual environment, cinematography, animation, non-monotonic reasoning "
2004,determining the direction of causal influence in large probabilistic networks: a constraint-based approach," determining the direction of causal influence from observational data only is essential in many applications, such as the reconstruction of genetic networks from microarray data. as opposed to many probabilistic network inference algorithms which were designed to induce just statistical models of the data, conditional independence (ci) based algorithms are theoretically able to infer true causal models from observational data only. but unfortunately, the small sample sizes available from current microarray experiments render the determination of causal direction highly inaccurate. here we show that this essential aspect of ci-based algorithms can be significantly improved by double-checking certain key statistical tests and by reconciling potential inconsistencies using a simple constraint propagation scheme. "," causal reasoning, conditional independence based structure inference with small samples "
2004,improving web search through collaborative query recommendation," search engines are the primary means by which people locate
information on the web. unfortunately most web users are not
information retrieval experts and there is a tendency for web
queries to be ambiguous and under-specified. query expansion and
recommendation techniques offer one way to solve the ambiguous
query problem in web search, by automatically identifying and
adding new terms to a vague query in order to focus the search. in
this paper, we describe and evaluate a novel query recommendation
technique based on reusing previous search histories. the central
idea is the recommendation of queries to a user. this is achieved
by selecting, ranking, and then recommending previously successful
queries to users. its novelty stems from the way in which queries
are scored and ranked using relevance and coverage factors in
order to prioritise those queries that are most likely to be
successful in the current search context. we demonstrate that
these recommendations can lead to improved search performance
based on live-user data. "," web search, relevance, ranking, evaluation "
2004,goal specification in presence of non-deterministic actions," one important aspect in directing cognitive robots or agents is to
formally specify what is expected of them. this is often referred
to as goal specification. for agents whose actions have
deterministic consequences various goal specification languages
have been proposed. the situation is different, and less studied,
when the actions may have non-deterministic consequences. for example, a simple goal of achieving p, has many nuances such as making sure that p is acheived, trying ones best to achieve p, preferring guaranteed acheivement of p over possible acheievement, and so on. similarly, there are many nuances in expressing the goal to try to acheieve p, and if it fails then to achieve q. we develop an extension of the branching time temporal logic ctl*, which we call pi-ctl*, and show how the above mentioned goals can be expressed using it, and why they can not be expressed in ctl*. we compare our approach to an alternative approach proposed in the literature.
 "," goal specification, temporal logic, agent directives "
2004,extending a lexicon ontology for intelligent information integration," one of the current research on the semantic web area is
semantic annotation of information sources.  on-line lexical
ontologies  can be exploited as a-priori common knowledge to
provide meaningful machine-readable metadata. nevertheless, the
absence of concepts related to specific domains often cause a loss
of semantics. in this paper we present wneditor, a tool
whose aim is to guide the annotation designer during the creation
of  domain lexical ontologies, extending the pre-existing wordnet
ontology. new lexicons, meanings and relations between terms are
virtually added and managed in respect to the wordnet internal
organization. "," ontologies, data integration systems, semantic annotation, lexicon ontology "
2004,the mereology of stages and persistent entities,"   in this paper a formal theory of the mereological structure of
  persistent entities is presented. persistants are entities which
  exist at more than one instant in time. endurance and perdurance as
  different modes of persistence are identified and distinguished
  according to the ways persistants are located in spacetime and
  according to whether or not statements about parthood among those
  entities can be made in an a-temporal manner.
  
  the underlying framework is a layered mereology. roughly, layers in
  the presented theory are interpreted as three-dimensional
  (time-)indexed slices of spacetime.  two kinds of parthood relations
  are distinguished: (a) parthood relations which are confined to hold
  among entities within a single layer; and (b) parthood relations
  which hold across layers. different kinds of parthood relations
  provide the basis to formally characterize the different modes of
  persistence.

  the proposed framework is purely mereological and avoids commitments
  to a specific temporal ontology. the distinction between temporal
  and atemporal statements about parthood is replaced by the
  distinction of statements using layer-dependent and
  layer-independent notions of parthood.
 "," ontology, knowledge representation, spacetime, space, time "
2004,abduction over unbounded domains via asp," it is known that abduction can be embedded into answer set programming (asp).  this enables sophisticated answer set solvers to be applied to various problems that can be expressed as abduction problems.  however this approach does not scale to abduction over infinite domains, nor to unbounded individual abduction, due to well-known undecidability results.    the approaches to open abduction usually rely on 3-valued semantics to overcome technical difficulties, but this changes the underlying semantics and prevents the application of asp solvers.  in this paper we apply the theory of finitary programs to prove that for an expressive and very interesting class of domain theories, asp-based abduction with unbounded domains can effectively be computed.  we also prove that each observable has a finite set of finite explanations representative of all the observable's infinitely many explanations.  moreover, standard asp engines can be adapted to compute the set of representative explanations. "," answer set programming, abduction, open domains, finitary programs "
2004,analysis of design process dynamics," to enable the development of automated support for design processes, a
challenge is to model and analyse dynamics of such processes in a formal manner. this paper contributes a declarative, logical approach for specification of dynamic properties of design processes, supported by a formal temporal language which has a high expressivity. this language can be used to specify dynamic properties at the level of a design process as a whole, or of parts thereof. at the most detailed level, in an executable sublanguage also simulation models are specified in a declarative, logical manner, which allows to use these specifications in logical analysis as well. the approach is illustrated by an example agent-system design process.
 "," design, verification and validation "
2004,"polynomial time reasoning in a description logic with existential restrictions, gci axioms, and---what else?"," in the area of description logic (dl) based knowledge representation, research on reasoning w.r.t.\ general terminologies has been mainly focused on very expressive dls. recently, though, it was shown for the dl el, providing only the constructors conjunction and existential restriction, that the subsumption problem w.r.t.\ cyclic terminologies can be decided in polynomial time, a surprisingly low upper bound. in this paper we show that even extending el by inverse roles and admitting general concept inclusion (gci) axioms at the same time preserves the polynomial time upper bound for subsumption. we also show that subsumption becomes co-np hard when adding one of the constructors number restriction, disjunction, and 'allsome', an operator used in the dl k-rep.
an interesting implication of the first result is that reasoning over the widely used medical terminology snomed is possible in polynomial time. "," description logics, knowledge representation "
2004,a rank based description language for qualitative preferences," in this paper we develop a language for representing complex qualitative preferences among problem solutions. we use ranked knowledge bases to represent prioritized goals. a basic preference description, that is a ranked knowledge base together with a preference strategy, defines a preference relation on models which represent problem solutions. our language allows us to express nested combinations of preference descriptions using various connectives. this gives the user the possibility to represent her preferences in a natural, concise and flexible manner. "," preference modeling, knowledge representation, nonmonotonic reasoning, qualitative optimization "
2004,an ontology based visual tool for query formulation support,"   in this paper we describe the principles of the design and
  development of an intelligent query interface, done in the context
  of the sewasie (semantic webs and agents in integrated economies)
  european ist project. the sewasie project aims at enabling a uniform
  access to heterogeneous data sources through an integrated ontology.
  the query interface is meant to support a user in formulating a
  precise query -- which best captures her/his information needs --
  even in the case of complete ignorance of the vocabulary of the
  underlying information system holding the data.  the intelligence of
  the interface is driven by an ontology describing the domain of the
  data in the information system. the final purpose of the tool is to
  generate a conjunctive query ready to be executed by some evaluation
  engine associated to the information system.
 "," intelligent user interfaces, ontologies, knowledge-based systems, knowledge representation, description logics, automated reasoning "
2004,fundamental issues of aesthetic knowledge representation," in this paper we deal with the problem of formalizing the notion of aesthetic judgment. many aspects of the representation of beauty have been evaluated and studied in the analytical philosophy and in the linguistic literature. however, to our knowledge, an approach that includes a complete formalization from a knowledge representation point of view has not yet been carried out.

in this paper we approach the problem in a neo-kantian perspective. we shall consider an aesthetic judgment as both an evaluation and a description. we model this conceptualization by means of a formalization in first-order logic, that incorporates the fundamental notions of a social theory of taste called the
theory of distinction originally conceived for explaining fashion phenomena and then provided as a model for taste in general.

we presuppose three sorts: objects, individuals and a subset of
individuals named the elite. the elite individuals establish canons and acknowledge other individuals in terms of good taste, being this kind of judgment expressed as well among generic individuals. "," formal ontology, knowledge representation, computer aided aesthetic design, semantic web "
2004,new advances in compiling cnf into decomposable negation normal form," we describe a new implementation for compiling conjunctive normal form (cnf) into deterministic decomposable negation normal (d-dnnf), which is a tractable logical form that permits model counting in polynomial time. the new implementation is based on latest techniques from both the sat and obdd literatures, and appears to be orders of magnitude more efficient than previous algorithms for this purpose. we compare our compiler experimentally to state of the art model counters, obdd compilers, and previous cnf2ddnnf compilers.  ", knowledge compilation 
2004,similarity-based ontology alignment in owl-lite," interoperability of heterogeneous systems on the web will be admittedly achieved through an agreement between the underlying ontologies. however, the richer the ontology description languages, the more complex the agreement process, and hence the more sophisticated the corresponding tools.
we believe that among current ontology alignment paradigms, similarity-based approaches are both powerful and flexible enough to adapt to the alignment within ontology languages like owl.
our own approach amounts to defining a universal measure for comparing the entities of two ontologies that is based on a simple and homogeneous comparison principle.
instead of using the external textual form, our similarity works on a dedicated graph representation where ontology (language) entities become (typed) vertices and their relationships (typed) edges, and where a vertex is basically described through its adjacent edges.
as one-to-many relationships and circularity in inter-entity dependencies constitute the key difficulties in the resulting context, we insist on the related topics in the similarity definition and computation while illustrating them through an example.  "," ontologies, reuse of knowledge, machine learning "
2004,an investigation into the expressive power of pddl2.1," the planning domain language pddl2.1, used in the 3rd international planning competition, has sparked off some controversy in the planning community as researchers consider its expressive power and the ease with which interesting domain models can be constructed in the language. in this paper we show that the expressive power of pddl2.1 is much greater than is commonly believed. we demonstrate that pddl2.1 can model many of the domain features often claimed to lie beyond its modelling capability. in so doing we provide a means by which powerful domain features and language constructs can be given a semantics in terms of the canonical basis of pddl2.1.  "," pddl, expressive power, planning "
2004,operationalizing domain ontologies : a method and a tool," the work presented in this paper deals with the integration of domain ontologies in knowledge based systems (kbs). we claim that, for using an ontology in a kbs, it must be operationalized, that is transcribed in an operational knowledge representation language, according to a precise scenario of use. the use of an operational ontology of geometry led us to propose a formal method to operationalize an ontology with the conceptual graphs model, in particular by producing operational forms of the axioms, appropriated to the scenario of use. this method has been implemented into a tool called toocom (tool to operationalize an ontology with the conceptual graph model) which allows to edit an ontology and to automatically produce operational forms of the ontology, according to the different scenarii of use.
 "," ontology, operationalization, conceptual graphs "
2004,a semantics for abstraction," the goal of this paper is to propose a model-theoretic formalization
of abstraction, where abstraction is modeled as
two representations, the ground and the abstract representation,
modeling the same phenomenon at different levels of detail.
using the framework of local models semantics, the ground
and abstract representations are modeled as two sets of (local) first
order models, while the relations
holding between them are captured by an appropriate ``compatibility
relation''. the tuning of the compatibility relation allows
for the definition of the many different kinds of abstraction.  "," knowledge representation, common-sense reasoning, context-based reasoning, theories of abstraction "
2004,domain descriptions should be modular," this work is about the metatheory of actions, and here we address the problem of what a good domain description for reasoning about actions should look like. we state some postulates concerning this sore spot, which establishes the notion of a modular domain description. we point out the problems that arise when modularity is violated and propose algorithms to overcome them. "," reasoning about actions, domain description, modularity "
2004,reasoning in description logics with a concrete domain in the framework of resolution," in description logics, concrete domains are used to model concrete
properties such as weight, name, or age, using concrete values
such as integers or strings with built-in predicates such
as <= or =. until now, reasoning with concrete domains has
been studied predominantly in the context of tableaux and automata
calculi. in this paper, we present a general approach for concrete
domain reasoning in the resolution framework. then we use this
approach to devise an optimal decision procedure for shiq(d), the
extension of shiq with a restricted form of concrete domains,
which serves as the logical underpinning of owl-dl.
 "," resolution, concrete domain reasoning, description logics, semantic web "
2004,random bayesian networks with constraints on induced width: generation and applications,"   we present algorithms for the generation of uniformly
  distributed bayesian networks with constraints on
  induced width. the algorithms use ergodic markov chains
  to generate samples. the introduction of constraints on
  induced width leads to realistic networks but requires 
  new techniques. a tool that generates random networks is
  presented and applications are discussed.
 "," bayesian networks, markov chains, probabilistic reasoning "
2004,representing normal programs with clauses," we present a new method for transforming normal logic programs into
sets of clauses. this transformation is based on a novel
characterization of stable models in terms of level numberings and it
uses atomic normal programs, which are free of positive body atoms,
as an intermediary representation.

the corresponding translation function has a unique combination of
properties: (i) a bijective relationship is established between stable
models and classical models, (ii) the models coincide up to the set of
atoms at(p) appearing in a program p, and (iii) the length of the translation as well as the translation time are of order len(p)xlog2|at(p)| where len(p) is the length of the program p.

our preliminary experiments with an implementation of the
transformation, translators called lp2atomic and lp2sat, and sat solvers such as chaff and relsat suggest that our approach is competitive when the task is to compute not just one but all stable models for a normal program given as input.
 "," normal logic program, translation, sat solver "
2004,representing knowledge about norms to reason on texts," inferences based on norms are far richer than those based on logical implications. to take advantage of this potential, the norms of a domain must be made explicit. in this paper, we propose a language to represent the norms of a limited domain, and we use it to reason on the causes of accidents described by car-crash reports.
knowing the norms enables to define what is abnormal; we distinguish basic and derived anomalies; the criterion of success is whether the basic anomaly found by our system coincides with the cause of the accident pointed out by a human reader. "," common-sense reasoning, reasoning about actions and change, natural language semantics, causal reasoning, knowledge representation "
2004,"from knowledge-based programs to graded belief-based programs, part i: on-line reasoning"," knowledge-based programs are a powerful
notion for expressing action policies in which branching
conditions refer to implicit knowledge and call for a deliberation
task at execution time. however, branching contitions in
knowledge-based programs cannot refer to possibly erroneous
beliefs or to graded belief, such as ``if my belief that $\varphi$
holds is high then do some action $\alpha$ else perform some
sensing action $\beta$''. the purpose of this paper is to build a
framework where such programs can be expressed. in this paper we
focus on the execution of such a program (a companion paper
investigates issues relevant to the off-line evaluation and
construction of such programs). we define a simple graded version
of doxastic logic kd45 as the basis for the definition of
belief-based programs. then we study the way the agent's belief
state is maintained when executing such programs, which calls for
revising belief states by obvservations (possibly unreliable or
imprecise) and progressing belief states by an physical action
(which may have both normal and execptional effects). 
 "," kmowledge representation, reasoning about action, belief revision and update, planning, reasoning under uncertainty "
2004,gene network modeling through semi-fixed bayesian network," gene networks describe functional pathways in a given cell or
tissue, representing processes such as metabolism, gene expression
regulation, protein or rna transport. thus, learning gene network
is a crucial problem in the post genome era. most existing works
learn gene networks by assuming one gene provokes the expression
of another gene directly leading to an over-simplified model. in
this paper, we show that the gene regulation is a complex problem
with many hidden variables. we propose a semi-fixed model to
represent the gene network as a bayesian network with hidden
variables. in addition, an effective algorithm to learn the model
is presented. experiments on artificial and real-life dataset
confirm the effectiveness of our approach. "," semi-fix network, bayesian network, gene network "
2004,description logics with concrete domains and functional dependencies," description logics (dls) with concrete domains are a useful tool in
many applications. to further enhance the expressive power of such
dls, it has been proposed to add database-style key constraints. up
to now, however, only uniqueness constraints have been considered in
this context, thus neglecting the second fundamental family of key
constraints: functional dependencies. in this paper, we consider the
basic dl with concrete domains alc(d), extend it with functional
dependencies, and analyze the impact of this extension on the
decidability and complexity of reasoning. though intuitively the
expressivity of functional dependencies seems weaker than that of
uniqueness constraints, we are able to show that the former have a
similarly severe impact on the computational properties: reasoning is
undecidable in the general case, and nexptime-complete in some
slightly restricted variants of our logic.
 ", description logics 
2004,efficient and secure collaborative filtering through intelligent neighbourhood selection," in this paper, we introduce novel neighbourhood formation and
similarity weight transformation schemes for automated collaborative
filtering systems. we define profile utility, which models the
usefulness of user profiles as a function of the items they
contain. for example, item popularity and item rating distribution are
two such possible measures of utility. we demonstrate that our
approach leads to more efficient collaborative filtering when compared
to a benchmark k-nearest neighbour approach, while providing system
accuracy and coverage to the same standard. in addition, our new
approach is secure against malicious attack as outlined in our
previous work. "," collaborative filtering, efficiency, security, robustness, neighbourhood selection "
2004,synonymous theories in answer set programming and equilibrium logic," the study of strong equivalence between logic programs or nonmonotonic theories under answer set semantics is extended to the case where the programs or theories concerned are formulated in different languages.
we suggest that theories in different languages be considered equivalent in the strong sense or synonymous if and only if each is bijectively interpretable (hence translatable) into the other.
since the logic of here-and-there, which provides a suitable foundation for answer set programming, has the beth property, we can easily give model-theoretic conditions that are equivalent to bijective interpretability.
these conditions involve mappings between the models of the two theories that, in particular, preserve the property of being an answer set or equilibrium model.
 "," synonymous theories, equilibrium logic, answer set programming "
2004,"diligent: towards a fine-grained methodology for distributed, loosely-controlled and evolving engineering of ontologies"," ontology engineering processes in truly distributed settings like
the semantic web or global peer-to-peer systems may not be
adequately supported by conventional, centralized ontology
engineering methodologies. in this paper, we present our work
towards the diligent methodology, which is intended to support
domain experts in a distributed setting to
engineer and evolve ontologies with the help of a
fine-grained methodological approach based on rhethorical
structure theory, viz. the diligent argumentation model.  "," ontology processes, distributed ontology building, evolving ontologies "
2004,answer type checking in open-domain question answering," open domain question answering systems have to bridge the
potential vocabulary mismatch between a question and its candidate
answers. one can view this as a recall problem and address
it accordingly. recall oriented strategies to question answering
may generate considerable amounts of noise. to combat this, many
open domain question answering systems contain an explicit
filtering or re-ranking component. in many cases this involves
checking whether the answer is of the correct semantic type.

we compare and explore redundancy-based and knowledge intensive 
strategies for this ""answer type checking"". common to both approaches is 
that the semantic types are structured as concepts in an ontology of 
the domain for a particular question type. we implement these strategies 
for geographical questions and evaluated on over 900 questions. 
overall, we find that type checking, independent of the chosen strategy,
improves the number of correct answers significantly. moreover, each of
the described approaches improve on questions where the other fails. 
as knowledge intensive approaches are labor intensive in the creation and
domain dependent, but efficient, whereas redundancy-based methods are 
more easily extensible to other domains, but sometimes inefficient, 
both methods have their merits.  "," information extraction, ontologies, information retrieval, text mining "
2004,"what part-of is and isn't (... in the bio domain, at least)"," biomedical terminologies lack a clear-cut understanding of the semantics of part-whole relations. in order to design a conceptually adequate and valid ontology for the bio domain, we investigate different forms of ontological dependencies between parts and wholes. furthermore we discuss the abstraction of parthood by spatial inclusion in order to diminish discretionary modeling decisions.
as a consequence, we propose a multi-facetted encoding pattern 
for biological structure at the level of knowledge representation.
 "," ontologies, knowledge representation, bioinformatics, mereotopology "
2004,explaining the result of a decision tree to the end-user," this paper addresses the problem of the explanation of the result given by a decision tree, when it is used to predict the class of a new case. in order to evaluate this result, the end-user relies on some estimate of the error rate and on the trace of the classification. unfortunately the trace doesn't contain the information necessary to understand the case at hand. we propose a new method to qualify the result given by a decision tree when the data are numerical. we perform a geometric study of the decision surface (the boundary of the inverse image of the different classes). this analysis gives the list of the tests of the tree that are the most sensitive to a change in the input data. unlike the trace, this list can easily be ordered and pruned so that only the most important tests are presented. we also show how the metric can be used to interact with the end-user. "," machine learning, user modelling, decision tree, explanation of result "
2004,outlier detection using disjunctive logic programming," assume your general knowledge about the world is encoded in a
disjunctive logic program p via answer set semantics. then,
assume you get factual evidence about some aspects of the current
status of the world encoded in a second disjunctive logic program,
say p'. a fundamental question to be answered is the following:
does the general knowledge encoding p agree with the
evidence about the world as encoded in p'? in this paper we
first define a formal framework suitable to discuss this question
and then illustrate how difficult it is to answer that question.
 "," disjunctive logic programming, knowledge discovery, outlier detection "
2004,a model-based approach to sequence clustering," we present a hidden markov model-based approach to cluster sequences. this problem is addressed in term of learning hidden markov models (hmm) structure from data, with constraints on topology. using a top-down approach, we iteratively simplify an initial hmm that consists of a mixture of as many left-right hmms as training sequences. this simplification is performed by merging of hmm components using a similarity measure specifically designed for left-right hmms. our approach allows to learn, in an unsupervised manner, the number of clusters and the cluster models that best represent training data. this approach is generic and we provide experimental results on two different application fields. first, we apply our system to automatically identify the number and nature of allographs in on-line handwriting signals. second, we apply our system to hypermedia navigation patterns in order to identify user typologies - a key component of user modelling. "," machine learning, sequence clustering, hidden markov models "
2004,discovering complex and sparse events in long sequences ," the hierarchical hidden markov model (hhmm) is a well formalized tool suitable to model complex patterns in long temporal or spatial sequences. the literature offers effective algorithm derived from the classical em able to estimate hhmm parameters from sequences. however, up to now, little has been done in order to automatize the construction of the model architecture. the primary focus of this paper is on a multi-strategy algorithm for inferring the hhmm structure from sequences where the events to capture are present in at least a relevant portion of the cases. the algorithm follows a bottom-up strategy in which elementary facts in the sequences are progressively grouped building the abstraction hierarchy of an hhmm, layer after layer. in this process, clustering algorithms and sequence alignment algorithms, widely used in domains like molecular biology, are exploited. the induction strategy has been designed in order to deal with events characterized by a sparse structure, where gaps filled by irrelevant facts can be intermixed with the relevant ones. irrelevant facts are modeled by ""delays"", i.e. hmms of the noise. delays are hypothesized when there is no significant statistical evidence for hypothesizing the  existence of a specific model. moreover, delays can be replaced in a second time by a specific model after new facts have been acquired. the induction strategy also allows an expert of the domain to integrate previous knowledge in the form of ""chunk of structure"" and in the form of temporal constraints.
the method is evaluated both on artificial and on real data. artificial data consist of a progression of datasets each representing a learning task of increasing difficulty. more specifically, each artificial dataset contains a complex event generated by a handcrafted hhmm plus additional spurious events. the goal is to evaluate the influence of the complexity, and of the noise, on the distance between the hhmm discovered by the algorithm and the original one.
the real data have been proposed by a group of molecular biologists. the preliminary results generated by the algorithm have been considered interesting by the experts of the domain.
 "," hierarchical hidden markov model, machine learning, sequence analysis "
2004,statistical strategies for pruning all the uninteresting association rules," we propose a general framework to formalize the pro\-blem of capturing the intensity
of implication for association rules through statistical metrics.
in this framework we present properties that influence the
interestingness of a rule, analyze the conditions that lead a measure to perform a 
perfect prune at a time, and define a final proper order to sort the surviving
rules. we will discuss why none of the currently employed measures can capture objective 
interestingness, and just the combination of some of them, in a multi-step fashion, can be
reliable. in contrast, we propose a new simple modification 
of the pearson coefficient that will meet all the necessary requirements. we 
statistically infer the convenient cut-off threshold for this new metric 
by empirically describing its distribution function through simulation.
final experiments show the ability of our proposal. "," association rules, statistical metrics, pruning strategy, order on rules "
2004,"comparing conceptual, partitional and agglomerative clustering for learning taxonomies from text"," the application of methods for automatic taxonomy construction
from text requires knowledge about the trade-off between,
(i), their effectiveness (quality of result), (ii),
efficiency (run-time behaviour), and,(iii), traceability
of the taxonomy construction by the ontology engineer. to offer 
such a trade-off, we define an original method to use conceptual clustering based on formal concept analysis for automatic taxonomy construction and we compare it against hierarchical agglomerative clustering and hierarchical partitional clustering. "," knowledge acquisition, ontology learning, term clustering, text mining "
2004,poboc : an overlapping clustering algorithm. application to classification rules learning," this paper presents the clustering algorithm poboc (pole-based  overlapping clustering). it has two main characteristics: the number  of final clusters is unknown a  priori and poboc allows an object to belong to one or several clusters. starting from a similarity matrix over a set of  objects, it first builds a small and homogeneous sets of objects (the poles), and then it assigns the objects to the poles. 
poboc differs from traditional clustering methods which aim at maximizing the intra-cluster similarity and minimizing the inter-cluster similarity. one of its main applications concern the pre-processing task which allows the organization of data in a knowledge discovery perspective. 

the clustering method is evaluated on the rule-based learning (rbl) task. classification rules are generated by organizing the instances of a class so that each cluster is covered with a single rule. poboc is compared to different clustering methods on traditional datasets from the uci  repository. we show that our approach is appropriated to the rbl task, with respect to other clustering technics. moreover, the global classification method leads, on some datasets, to better results than famous classifiers such as decision-tree or nearest neighbour ones.
 "," machine learning, clustering, rule-based learning, data mining "
2004,learning qualitative metabolic models," the ability to learn a model of a system from observations of the 
 system and background knowledge is central to intelligence, and the 
 automation of the process is a key research goal of artificial 
 intelligence.  we present a model-learning system, developed for 
 application to scientific discovery problems, where the models are 
 scientific hypotheses and the observations are experiments.  the 
 learning system, {\sc qoph} learns the {\it structural} relationships 
 between the observed variables, known to be a hard problem.  {\sc qoph} has 
 been shown capable of learning models with hidden (unmeasured) 
 variables, under different levels of noise, and from qualitative or 
 quantitative input data. "," model learning, qualtitative reasoning, ilp, computational biology "
2004,inference attacks in peer-to-peer homogeneous distributed data mining," agent technology could provide a computing paradigm which naturally
fits distributed data mining environments.  in particular, spontaneous
formation of peer-to-peer agent-based data mining systems seems a
plausible scenario in years to come.  
however, the emergence of peer-to-peer environments further aggravates
privacy and security concerns that arise when performing data mining tasks.  
we analyze potential threats to data privacy in a peer-to-peer
agent-based distributed data mining scenario under the hypothesis
that all the distributed datasets follow the same schema, and
discuss inference attacks which could compromise data
privacy in a peer-to-peer distributed clustering scheme known as kdec. "," inference attack, peer-to-peer, distributed data mining, privacy, data clustering "
2004,face recognition using novel lda-based algorithms," facial feature extraction with enhanced discriminatory
power plays an important role in face recognition (fr)
applications. the linear discriminant analysis (lda) is a
powerful tool used for dimensionality reduction and
feature extraction in fr tasks. however, the classification
performance of traditional lda is often degraded, due to
two factors: 1) their classification accuracies suffer from
the small sample size problem (sssp), which widely exists
in fr; 2) their fisher discriminant criterions are not
directly related to the classification ability. recently,
called direct fractional-step lda (df-lda) algorithm
has been proposed to solve this problem. in this paper, the
limitations of df-lda are discussed and a novel dflda
has been proposed to solve those problems. the
novel df-lda has been tested, in terms of classification
accuracy, on the orl and the umist face databases.
results reveal that the proposed method outperforms the
previous existing methods, including: the eigenfaces, fisherfaces,
d-lda, previous df-lda, and efm methods.
 ","  face recognition (fr), feature extraction, linear discriminant analysis (lda), small sample size, problem (sssp). "
2004,a backtracking strategy for order-independent incremental learning," agents that  exist in  an environment  that changes  over time,
and are able to take into account the temporal nature of
experience, are commonly called   incremental learners. it
is widely   known  that incremental   learning systems suffer from
ordering   effects,  a phenomenon observed when different ordered
sequences of examples lead to different results.

the goal of this paper is presenting inthelex_back, a
modification of the incremental learning system inthelex with the
aim of making it order-independent. specifically, a backtracking
strategy is incorporated in its refinement operators, which causes
a change in its refinement strategy and reflects the human
behaviour during the learning process. it consists in remembering
the different versions of the learned theory across modifications
due to new evidence. in this way the system can backtrack on a
previous knowledge level when it discovers to have made a wrong
choice. experiments on an artificial dataset validate the approach
in terms of computational cost and predictive accuracy.
 "," inductive logic programming, incremental learning "
2004,"pushing ""underfitting"" to the limit: learning in bidimensional text categorization"," the analysis of two heuristic supervised learning algorithms for text categorization in two dimensions is presented here. the graphical properties of the bidimensional representation allows
one to tailor a geometrical heuristic approach in order to exploit the peculiar distribution of text documents. in particular, we want to investigate the theoretical linear cost of the algorithms and try to push the performance to the limit. the experiments on reuters-21578 standard benchmark confirm that this approach is an alternative to the standard linear learning models, such as support vector machines, for text classification. moreover, due to the fast training session, this approach may also be considered as a support for text categorization systems for fast graphical investigations of large collections of documents. "," text categorization, machine learning, information models, text representation "
2004,avoiding data overfitting in scientific discovery: experiments in functional genomics," functional genomics is a typical scientific discovery domain characterized by a very large number of attributes (genes) relative to the number of examples (observations). the danger of data overfitting is crucial in such domains. this work presents an approach which can help in avoiding data overfitting in supervised inductive learning of short rules that are appropriate for human interpretation. the approach is based on the subgroup discovery rule learning framework, enhanced by methods of restricting the hypothesis search space by exploiting the relevance of features that enter the rule construction process as well as their combinations that form the rules. a multi-class functional genomics problem of classifying fourteen cancer types based on more than 16000 gene expression values is used to illustrate the methodology.
 "," scientific discovery, feature relevancy, learning from small datasets, functional genomics "
2004,learning techniques for automatic algorithm portfolio selection," the purpose of this paper is to show that a well known machine
learning technique based on decision trees can be effectively used
to select the best approach (in terms of efficiency) in an
algorithm portfolio for a particular case study. in particular, we
are interested in deciding when to use a constraint programming
(cp) approach and when an integer programming (ip) approach, on
the basis of the structure of the instance considered. different
instances of the same problem present different features and
structure, and one aspect (e.g. feasibility or optimality) can
prevail on the other. as a case study, we use the bid evaluation
problem (bep) arising in combinatorial auctions. we have extracted
from a set of bep instances, a number of parameters representing
the instance structure. some of them (few indeed) precisely
identify the best strategy and its corresponding tuning to be used
to face that instance. we will show that this approach is very
promising, since it achieves the correct identification in the 90%
of the cases.
 "," constraint satisfaction and optimization problem, algorithm portfolio, machine learning, combinatorial auction "
2004,anttree: a web document clustering using artificials ants," we present in this work a new algorithm (called anttree) for document hierarchical clustering and automatic generation of portals sites. this model is inspired from the self-assembling behaviour observed in real ants where ants progressively get attached to an existing support and successively to others attached ants. we have simulated the way ants build complex structures by connecting themselves to each others. the artificial ants that we have defined will similarly build a tree. each ant represents one document. the way ants move and build this tree depends on the similarity between the documents. in this paper we have shown how this behaviour can be used to build a hierarchical tree-structured partitioning of a set of documents. we have tested anttree on a set of web pages extracted from internet and we have successfully compared our results presented as a series of html files with hyperlinks, to those obtained by the ahc (ascending hierarchical clustering), we have shown that our algorithm obtains competitive results in term of classification error, the number of classes “found” and computational time, those results are extremely encouraging and the main perspective of this work is to keep on studying this promising model. "," text mining, hierarchical clustering, autonomous agents, artificials ants, portals sites, web document "
2004,rule-mining: a knowledge-based selection of association rules," a reoccuring problem in mining association rules is the selection of
interesting association rules within the overall, and possibly huge
set of extracted rules. 
the majority of previous works in this area uses statistical methods for 
quality estimation and selection of association rules. however, 
strictly bottom-up approaches are oblivious of knowledge though rule
extraction may profit from the usage of knowledge. 
in this paper, we conceive of this problem as a classification task.
the  framework of a probabilistic knowledge-based classifier is 
introduced that uses ontologies in order to carry out the rule-mining task.
 "," text mining, knowledge discovery, probabilistic reasoning, machine learning, natural language processing "
2004,yet more efficient em learning for parameterized logic programs by inter-goal sharing," in the previous research, we presented a graphical em algorithm
for parameterized logic programs, which is based on the
structure sharing with tabled search.  also it is shown that
this general framework achieves the same time complexity 
as that of the specialized algorithms, e.g. the baum-welch
algorithm for hidden markov models(hmms).  the efficiency
is brought by sharing the common paths in the derivation tree
for a given goal, but such sharing is incomplete in the sense
that it is not allowed to share the paths appearing in the
different derivation trees.  in this paper, we introduce
a general idea of `inter-goal sharing' where the different
goals can share the common derivation paths.  inter-goal
sharing achieves the full sharing of derivation paths and
hence makes em learning more compact and efficient in
practical cases.  then, for prism programs, we present a
simple implementation of inter-goal sharing, which can be
justified both logically and statistically.  finally we show
the experimental results with two typical and widely-applied
statistical language models, i.e. hmms and probabilistic
context-free grammars.  for both artificial and real linguistic
data, it is found out that the proposed method runs 2-6 times
more compactly and faster than the previous approach.
 "," em algorithm, tabled search, structure sharing, dynamic programming, statistical language models "
2004,bias windowing for relational learning," a central issue in relational learning is the choice of an
appropriate bias for representing hypotheses. bias must be weak
enough to avoid underfitting and strong enough to avoid
intractability and overfitting. in order to circumvent this
dilemma, we propose a framework inspired from windowing
techniques. a bias window is a restricted subclass of the
relational space determined by some constraint parameters. the
idea is thus to learn a theory in a small window first, and only
switch to larger windows if the result in the small window is
unsatisfactory. to this end, our framework integrates three key
components: a logical notion of window space, an approximation
algorithm that explores this space, and a model selection
principle that monitors the learning progress. experiments on
relational datasets show that, after a short period of
underfitting, windowing converges to hypotheses which are both
compact and effective. ", machine learning 
2004,exploiting association and correlation rules parameterns for improving the k2 algorithm," a bayesian network is an appropriate tool to work with a sort of uncertainty and probability, that are typical of real-life applications. bayesian network arcs represent statistical dependence between different variables.  in the data mining field, association rules can be interpreted as well as expressing statistical dependence relations.  k2 is a well-known algorithm which is able to learn bayesian network.  in this paper we present two extensions of k2 called k2-lift and k2-x2 that exploit two parameters normally defined in relation to association and correlation rules for learning bayesian networks.  the experiments performed show that k2-lift and k2-x2 improve k2 with respect to both the quality of the learned network and the execution time. "," probabilistic reasoning, bayesian learning, machine learning, data mining, bayesian networks "
2004,online learning of probabilistic discriminative obejct tracking , ," computer vision, object tracking, online learning, bayesian approach, fisher linear discriminant analysis, particle filters "
2004,finding social network for trust calculation," trust is a necessary concept to realize the semantic web. but how
can we build “web of trust”? we first argue that a small “web of trust” for each
community is very essential to realize a huge “web of trust.” then, we focus on
an academic community as one of small “web of trust” and show a web mining
approach to automatically generate a social network. each edge is added using
the number of retrieved pages by a search engine which include both persons’
names. moreover, each edge has a label such as “co-authors” and “members of
the same project” by applying classification rules to the page content. the relation
of persons such as “coauthor,” or “same laboratory” can be described by a rdf
format. finally, using the social network, we calculate authoritativeness of a node
as a social trust and an individual trust. "," web mining, semantic web, social network, pagerank "
2004,voted co-training for bootstrapping sense classifiers," this paper introduces voted co-training, a bootstrapping method that combines co-training with majority voting, with the effect of smoothing the learning curves, and improving the average performance. voted co-training was evaluated on a word sense classification problem, with significant improvements observed over basic co-training algorithms. various optimal and empirical parameter selection methods for co-training are also investigated, with various degrees of error reduction.

 "," word sense disambiguation, semantics, natural language processing, bootstrapping, machine learning "
2004,weak oi: ideal refinement of datalog clauses using primary keys," inductive logic programming (ilp) algorithms are frequently used for data mining tasks in multi-relational databases. however, by most ilp algorithms primary key information is disregarded while this information is often available for such databases. this work shows how primary key information can be incorporated in a downward refinement operator. we show how primary keys can be used to define sublanguages of full clausal logic and provide evidence that one can define ideal refinement operators for these languages. we incorporate our refinement operator in  a multirelational data mining algorithm to demonstrate its feasibility. "," data mining, knowledge representation, inductive logic programming, machine learning "
2004,visual learning by set covering machine with efficient feature selection," in this paper, we propose a new visual learning method for real-world object recognition task. 
our method is based on the set covering machine (scm), 
to make the learning time shorter than the methods based on commonly used trial-and-error algorithms,
such as genetic programming and reinforcement learning.
generally, the process of visual learning is quite time-consuming
because image data consists of large amount of information.
we attempt to reduce the learning time by introducing the effective feature selection method
to find small number of useful features in image data.
additionally, we introduced a criterion based on the minimum description length (mdl) principle
to refine the hypothesis.
we perform some experiments to verify the effectiveness of our method.
 "," visual learning, set covering machine, attribute selection, image filtering, adaptive learning "
2004,controlling on-line search for solving markov decision processes via sampling," markov decision processes (mdps) have become a standard these last ten years for solving problems of sequential decision under uncertainty. the usual request in this framework is the computation of an optimal policy that defines the optimal action for each state of the system. for complex mdps, exact computation of optimal policies is often infeasible. several approaches have been developed by the reinforcement learning community to compute near optimal policies for complex mdps by means of function approximation and simulation.

in this paper, we investigate the problem of refining near optimal policies via online search techniques, tackling the local problem of finding an optimal action for a single current state of the system. kearns, mansour & ng (machine learning 2002) consider such an on-line approach based on sampling: at each step, a randomly sampled look-ahead tree is developed to compute the optimal action for the current state. we propose here some control search strategies for constructing such trees. their purpose is to provide good ""anytime"" profiles : they quickly select a good action with a high probability and then smoothly increase the probability of selecting the
optimal action. "," markov decision processes, reinforcement learning, sampling, heuristic search "
2004,time-independent rule-based guideline induction," whereas decision trees are widely studied in the context of knowledge representation and inductive learning, guidelines can be considered as an extension of decision trees that have not received the same amount of attention in the artificial intelligence community, even though they have been proved hightly useful in several decision support problems as the therapy process in health-care.

this paper is about the formal definition of guidelines as a rule-based knowledge structure and also about the introduction of an incremental inductive learning algorithm to develop and update time-independent guidelines. these ideas have been applied to the modeling of clinical practice guidelines and tested with the health-care domain of heart diseases.
 "," machine learning, knowledge representation "
2004,learning augmented naive bayes classifiers," we propose two general heuristics to transform a batch
hill-climbing search into an incremental one. our heuristics, when new data arrive, study the search path of the former learning step to determine whether to revise the current structure or not. heuristics are also able to determine which part of the learned structure should be revised.
then, we apply our heuristics to two well-known bayesian network structure learning algorithms in order to obtain incremental augmented naive bayes classifiers. we experimentally show that our incremental approach saves a significant amount of computing time while yields classifiers of similar quality than the ones learned with the batch approach.
 "," incremental learning, hill-climbing search, naive bayes classifiers "
2004,combining multiple answers for learning mathematical structures from visual observation," learning general truths from the visual observation of simple domains and, further, learning how to use this knowledge are essential capabilities for any intelligent agent to understand and execute informed actions in the real world. the aim of this work is the investigation of the automatic learning of mathematical structures from visual observation. this research was conducted upon a system that combines computer vision with inductive logic programming that was first designed to learn protocol behaviour from observation. in this paper we show how axioms for order and equivalence could be induced from the noisy data provided by a vision system. noise in the data accounts for the generation of a large amount of possible generalisations by the ilp system, most of which do not represent interesting concepts about the observed domain. in order to automatically choose the best answers among those generated by induction, we propose a method for combining the results of multiple ilp processes by ranking the most interesting answers. "," common sense reasoning, cognitive vision, inductive logic programming "
2004,stacked generalization for information extraction," this paper defines a new stacked generalization framework in the context of information extraction (ie) from online sources. the proposed setting removes the constraint of applying classifiers at the base-level. a set of ie systems are trained instead to recognize relevant fragments within text documents, which differs significantly from the task of classifying candidate text fragments as relevant or not. the predictions of the base-level ie systems are stacked and form a set of feature vectors for training a meta-level classifier. therefore, ie is transformed into a common classification task at meta-level. the proposed framework was evaluated on three web domains, using well known ie approaches at base-level and a variety of classifiers at meta-level. results demonstrate the added value obtained by combining the base-level ie systems in the new framework. "," information extraction, machine learning, text mining "
2004,avatars that learn how to behave," it is possible to model avatars that learn to simulate object manipulations and interactions between individuals. many applications may benefit from this technique including safety, ergonomics, film animation and many others. current techniques ""control"" avatars  manually, scripting what they can do by imposing constraints on their physical and cognitive model. in this paper we show how avatars in a controlled environment can learn behaviours as composits of simple actions. the avatar learning process is described in detail for a generic behaviour and tested in simple experiments. local and global metrics are introduced to optimise the selection of a set of actions from the learnt pool. the performance for the learnt tasks is qualitatively compared with a human performance. "," learnt behaviours, animation, learning metric, lifelike characters "
2004,piece-wise model fitting using local data patterns," in this paper we propose a novel classification algorithm that
fits models of different complexity on separate regions of the
input space. the goal is to achieve a balance between global and
local learning strategies by decomposing the classification task
into simpler subproblems; each task narrows the learning problem
to a local region of high example density over the input space.
specifically, our proposed approach is to apply a clustering
algorithm to all training examples that are class-uniform; each
cluster becomes an intermediate concept that is learned by
selecting a model with an (estimated) optimal degree of
complexity. experimental results on real-world domains show
consistent good performance in predictive accuracy with our
piece-wise model fitting strategy. "," classification, model fitting, clustering, local learning, global learning "
2004,m-som-art: growing self organizing map for sequences clustering and classification ," this paper presents a new growing neural network for sequences clustering and classification.
this network is a self-organising map (som), which has the properties of stability and plasticity. 
the stability concerns the preservation of previously learned knowledge and the plasticity concerns the adaptation to any change in the input environment. 
these properties are obtained using adaptive resonance theory.
in order to take into account the temporal information (the dynamics) and the correlation of the patterns contained in the sequences, the inputs of the map are modelled using their associated dynamic covariance matrices.  
this new model is inspired from the field of speaker recognition. 
we have modified a covariance matrix in order to represent a temporal order in the sequence. 
the experimentations show that our approach is better than some other temporal self-organizing map for user web navigation classification. "," self organizing map, stability, plasticity, sequences, classification, clustering "
2004,automatic discovery of  translation collocations from bilingual corpora," we describe a method to automatically discover translation collocations from a bilingual corpus and how these improve a machine translation system. the process of inference of collocations is iterative: an alignment is used to derive an initial set of collocations, these are used in turn to improve the alignment and this new alignment is used to generate new collocations. this process is repeated until no more collocations are found. the final alignment and the set of collocations are used to train a translation model. we use a model that is based on finite state transducers and word clusters and has been modified to work with collocations in addition to single words.

we present experiments in which we show that automatic collocations improve translation quality without prior linguistic information.
 "," collocations, bilingual corpus, machine translation "
2004,improvements on automatic word codification for machine translation," encouragingly accurate translations have recently been obtained using a connectionist translator called recontra (recurrent connectionist translator). in order to deal with tasks of medium or large vocabularies, distributed representations of the lexicons are required in this translator. a simple connectionist model has been recently designed to automatically obtain word distributed representations. in this paper several learning algorithms were used to train this connectionist encoder aiming to improve the translation rates achieved with the corresponding obtained codifications of the vocabularies involved. "," machine translation, automatic word encoder, neural networks, training algorithms "
2004,an argumentative approach to assessing natural language usage based on web linguistic corpora," although spelling and grammar checkers used in word-processing software have helped to significantly reduce the overall amount of burden for checking documents, the problem of judging the appropriateness of language usage in different contexts remains to a large extent still unsolved.

this paper presents a novel approach to providing proactive
assistance for language usage assessment. in the proposed framework, a critiquing system analyzes the textual information presented by the user as an input to a word-processing tool. textual expressions from
the user's document are extracted and used for querying a web search
engine to gather information about their absolute and relative frequencies with respect to a web-based linguistic corpus. such frequencies are used to compute usage indices, which are good indicators of the suitability of an expression in a given context.

the user's preferences consist of a number of (possibly defeasible)
rules and facts which encode different aspects of adequate language usage, defining the acceptability of a given expression on the basis of the computed usage indices. a defeasible argumentation system determines if a given expression
is acceptable by analyzing the user's preferences
and establishing if there exists a warranted argument supporting
this claim. those expressions assessed as unsuitable are further inspected automatically by the system to help the user make the
necessary repairs. "," natural language processing, defeasible argumentation, web-based systems, critiquing systems "
2004,finite-state models for computer assisted translation," current methodologies for automatic translation cannot be expected to produce high quality translations. however, some techniques based on these methodologies can increase the productivity of human translators. the basis of one of these methodologies are finite-state transducers, which are adequate models for computer assisted translation. these models have proved to be very efficient in many pattern recognition and artificial intelligence tasks such as speech recognition, handwriting recognition and machine translation for specific tasks.

these finite-state models present some advantages. on one hand, finite-state models can be learnt from bilingual corpus to infer transducers. on the other hand, there are well-known and efficient
algorithms to perform the parse of the best translation according to
these models (viterbi search). in this paper, the concept of interactive search will be introduced along with some efficient techniques that solve the problem of producing a translation given a sentence to be translated and a prefix typed by the user. needless to say that this system must run under real-time constraints to be useful for human translators.

this approach have been tested on a corpus of printer manuals and the
first results reflect that human translators would only need to type 25% of the characters, instead of the complete translated text, increasing in this way their throughput and reducing their effort.
 "," automatic translation, computer assisted translation, finite-state models, interactive search "
2004,parsing languages with a configurator," recent evolutions of linguistic theories heavily rely upon the concept of constraint. also, several authors have pointed out the similitude existing  between the categories of feature-based theories and the notions of objects or frames. we show that a generalization of constraint programs called configuration programs can be applied to natural language parsing. we propose here a systematic translation of the concepts and constraints introduced by property grammars to configuration problems representing  specific target languages. we assess the usefulness of this translation by studying first a recursive (context free) language with semantics, and a natural language subset with lexical ambiguities. our experiments show the practical efficiency of this approach, which does not require the use of ad hoc algorithms and can be freely used in analysis, generative, or hybrid mode. 
 "," constraint satisfaction, configuration, natural language processing, property grammars "
2004,an application of lexicalized grammars in english-persion translation," increasing the domain of locality by using tree-adjoining-grammars (tag) encouraged some researchers to use tags in applications such as machine translation, especially in the disambiguation process. successful experiments of applying a tag to french-english and korean-english translation encouraged us to use it for another language pairs with very divergent properties, i.e., english and persian. by using a synchronous tag (s-tag) for this pair of languages, we can benefit from syntactic and semantic features of these languages. in this paper, we report on our successful experiments of automatic translation of english into persian. we also present a computational model for disambiguation of lexical selection, based on a decision tree approach. finally, a new automatic method for learning a decision tree from a sample data set is introduced. "," natural language processing, machine translation, tree-adjoining-grammars, lexicalized grammar, english-persian translation, decision-tree "
2004,a comparative analysis of grammar extraction from text corpora ," this paper addresses the issue of what type of grammar knowledge is
extracted from corpora.
in particular the paper analyzes two lexicalized
tree adjoining grammars (ltag) extracted from two types
of corpora, respectively a collection of newspaper articles and a
collection of laws from the civil code.
in order to compare the two grammars extracted we have implemented a
coverage test: the grammar extracted from one corpus has been applied
to the other corpus. the results have been that the
civil code grammar covers the 66% of the newspaper corpus, while the
newspaper grammar only covers the 46% of the civil code corpus, revealing a wider coverage for the civil code grammar.
an explanation of the results relies on a deeper analysis of the
grammar rules:
the newspaper grammar is larger than the civil code grammar
in termumber of rules as of different rules, but a reduced  representative of the language, while the others are more specific to some context;
the civil code grammar is smaller in terms of different rules, but these are more representative of the language and are more uniformly distributed in the corpus sentences.
the conclusion is that the grammar knowledge from the civil code can be more easily exportable to tasks concerning other types of corpora.
in order to validate this conclusion we have implemented a test based on the results of a rule-based parser, that have confirmed the greater generality of the civil code grammar. 
 "," natural language processing, grammar extraction, parsing and coverage "
2004,wordnet sits the s.a.t. a knowledge-based approach to lexical analogy ," one can measure the extent to which a knowledge-base enables intelligent or creative behavior by determining how useful such a knowledge-base is to the solution of standard psychometric or scholastic tests. in this paper we consider the utility of wordnet, a comprehensive lexical knowledge-base of english word meanings, to the solution of s.a.t. analogies. we propose that such analogies test a student’s ability to recognize and estimate a measure of pairwise analogical similarity, and describe an algorithmic formulation of this measure that uses the taxonomic structure of wordnet. we report that the knowledge-based approach yields a precision at least equal to that of statistical machine-learning approaches. "," analogy, wordnet, similarity, creativity "
2004,likely-admissible and sub-symbolic heuristics ," it is well-known in problem solving that while strict admissibility guarantees the optimality of a* algorithms many problems cannot be effectively faced in this way because of the combinatorial explosion. to avoid intractability it has been introduced the notion of epsilon-admissible search, which yields solutions with bounded costs, but it does not investigate the probability of discovering optimal solutions.

in this paper, we introduce the related concept of likely-admissible search, where the admissibility requirement is relaxed in a probabilistic sense and guarantees to end up with optimal solutions with a given probability. interestingly, likely-admissible heuristics can be obtained naturally by statistical learning techniques such as anns, which can learn from examples the expected distance to the target. we designed a novel cost function in order to bias the learning towards admissibility.

our experiments with the 15-puzzle and ida* show that the adoption of likely-admissible sub-symbolic heuristics yields optimal solutions in 50\% of the cases, taking only 1/500 time (1/13000 space) of classic manhattan-based search. this promising result seems to candidate our algorithm as an alternative to disjoint pattern databases[korf2002], which makes a much more significant use of memory resources shifting np-hardness of strict admissibility from time to space.
 "," heuristics, search, problem solving, machine learning, neural networks "
2004,dynamic selection of model parameters in principal components analysis neural networks," one of the best known techniques for multidimensional data analysis is the principal components analysis (pca). a number of local pca neural models have been proposed to partition an input distribution into meaningful clusters. each neuron of these models uses a certain number of basis vectors to represent the principal directions of a particular cluster. most of these neural networks are unable to learn the number of basis vectors, which is specified a priori as a fixed parameter. this leads to poor adaptation to input data. here we develop a method where the number of basis vectors of each neuron is learned. then we apply this method to a well known local pca neural model. finally, experimental results are presented where the original and modified versions of the neural model are compared. "," neural networks, principal components analysis, dimensionality reduction, multispectral imaging "
2004,learning neural network predictors from very large datasets," advances in data collection technologies allow accumulation of large and high dimension datasets and provide opportunity for learning high quality classification and regression models. however, supervised learning from such data raises significant computational challenges including inability to preserve the data in the computer main memory and need to optimize model parameters within given time constraints. for certain types of predicting models techniques have been developed for learning from large datasets, but few of them address efficient learning of neural networks. towards this objective, in this study we proposed a procedure that automatically learns a series of neural networks of different complexities on smaller data chunks and then properly combines them into an ensemble predictor through averaging. based on the idea of progressive sampling the proposed approach starts with a very simple network trained on a very small data chunk and then progressively increases the model complexity and the data chunk sizes until the learning performance no longer improves. our empirical study on three real life large datasets suggests that the proposed method is successful in learning complex concepts from large datasets with low computational effort. "," neural networks, large datasets, ensemble predictor, progressive sampling, data mining "
2004,a generalized quadratic loss exploiting target information for support vector machines," the standard svm formulation for binary classification is based on the hinge loss function, where errors are considered to be independent. due to this, local information in the feature space which can be useful to improve the prediction model is disregarded.
in this paper we address this problem by defining a generalized quadratic loss where the co-occurrence of errors is weighted according to a kernel similarity measure in the feature space. in particular the proposed approach weights pairs of errors according to the distribution of the related patterns in the feature space. 
the generalized quadratic loss includes also target information in order to penalize errors on pairs of patterns that are similar and of the same class. 
we show that the resulting dual problem can be expressed as a hard margin svm in a different feature space when the co-occurrence error matrix is invertible. the existence of the inverse can be assured by generating the co-occurrence error matrix via an exponential
kernel. 
we compare our approach against a standard svm using the hinge loss on some binary classification tasks.
experimental results obtained for different instances of the co-occurrence error matrix on these problems, show an improvement in the performance. "," svm, classification, machine learning, loss function "
2004,artificial agents - personhood in law and philosophy," thinking about how the law might decide whether to extend legal personhood
to artificial agents provides a valuable test-bed for philosophical
theories of mind. we begin with a naïve thesis about the extent to which
philosophical and legal theorising about personhood can be mutually
informing. we then investigate two case studies, drawing on legal
discussions of the status of artificial agents . the first case study
looks at the doctrinal difficulties presented by the contracts entered
into by artificial agents. we conclude that it is not necessary or
desirable to postulate artificial agents as legal persons in order to
account for such contracts. the second case study looks at the potential
for according sophisticated artificial agents with legal personality with
attendant constitutional protections similar to those accorded to humans.
we investigate the validity of attributes that have been suggested as
pointers of personhood, and conclude that they will take their place
within a broader matrix of pragmatic, philosophical and extra-legal
concepts.             "," philosophical foundations, autonomous agents, legal theory of agents "
2004,simplicity in solving the frame problem ," this paper furnishes a unified philosophy for reasoning about action
and change. we appeal to the principle of occam's razor---the best
explanations are the simplest ones---as the underlying theme of
commonsense reasoning and conduct a preliminary investigation of how
an appeal to simplicity allows us to address some of the challenges we
face when reasoning about the effects of actions. in particular, we
apply occam's razor to transformations between worlds, given an action
specification. we formalise this by appealing to kolmogorov
complexity to identify the intended (simplest) transformation given an
action description. we support our claims by showing that the
formalism captures our intuitions on some simple examples.
 "," reasoning about actions and change, common-sense reasoning, nonmonotonic reasoning, knowledge representation, philosophical foundations "
2004,utilizing volatile external information during planning," most ai-planning research has assumed that a planner has all the information it needs at the start of the planning process---but in many practical planning situations, planners may need to get information from external sources (database systems, cad systems, human users, web services, etc.) during the planning process.  the lag time for such queries can slow down the planning considerably.  furthermore, the information returned by the queries may be volatile: it may change before planning is completed, necessitating revisions to the plan.  we describe the following:

1. wrappers that may be placed around conventional (isolated) planners.  the wrapper replaces some of the planner's memory accesses with queries to external information sources. when appropriate, the wrapper will automatically backtrack the planner to a previous point in its operation.

2. query-management strategies for wrappers.  these dictate when to issue queries, and when/how to backtrack the planner.  we describe several provably sound strategies, and describe conditions under which they are complete.

3. mathematical analysis and experimental tests.  our results show conditions under which different query management strategies are preferable, and demonstrate that certain kinds of planning paradigms are more suited than others for planning with volatile information. 
 ", planning 
2004,job shop scheduling with probabilistic durations," proactive approaches to scheduling take into account information about the execution time uncertainty of a scheduling problem in forming a schedule. in this paper, we investigate proactive approaches for the job shop scheduling problem where activity durations are random variables.  the main contributions are (i) the introduction of the problem of finding probabilistic execution guarantees for difficult scheduling problems; (ii) a method for generating a lower bound on the minimal makespan; (iii) the development of the monte carlo approach for evaluating solutions; and (iv) the design and empirical analysis of three solution techniques: an approximately complete technique, found to be feasible only for very small problems, and two techniques based on finding good solutions to a deterministic scheduling problem, which scale to much larger problems. "," scheduling, probabilistic reasoning, reasoning under uncertainty "
2004,"interleaving execution and planning for nondeterministic, partially observable domains"," methods that interleave planning and execution are a practical
solution to deal with complex planning problems in nondeterministic 
domains under partial observability. 
however, most of the existing approaches do not tackle in a 
principled way the important issue of termination of the
planning-execution loop, or only do so considering specific 
assumptions over the domains.

in this paper, we tackle the problem of interleaving planning and
execution relying on a general framework, which is able to deal with
nondeterministic, partially observable planning domains.  
we propose a new, general planning algorithm that guarantees the
termination of the interleaving of planning and execution: either the
goal is achieved, or the system detects that there is no longer a 
guarantee to progress toward it.

our experimental analysis shows that our algorithm can efficiently
solve planning problems that cannot be tackled with a state of the 
art off-line planner for nondeterministic domains under partial
observability, mbp. moreover, we show that our algorithm can 
efficiently detect situations where progress toward the goal can be 
no longer guaranteed.

 ", planning 
2004,planning with numeric variables in multiobjective planning," purely propositional representation traditionally used to express ai
planning problems is not adequate to express numeric variables when
modeling real-world continuous resources, such as fuel consumption,
energy level, etc. this paper presents a heuristic planning approach
that uses a richer representation with capabilities for numeric
variables, including duration on actions, and multiobjective
optimisation. this approach consists of two stages. first, a spike
construction process estimates the values of the variables associated
to propositions/actions. unlike other approaches, we do not relax
numeric effects in the calculus of the estimation, but only numeric
conditions. second, a heuristic search process generates a relaxed
plan according to the estimations of the first stage, and then
performs search in a plan space. the relaxed plan and the heuristic
estimations help the process find a plan while trying to optimise the
multiobjective criterion. "," multiobjective planning, planning with resources, temporal planning, planning "
2004,planning with numerical expressions in lpg," we present some techniques for handling planning problems with numerical expressions that can be specified using the standard language pddl. these techniques are implemented in lpg, a fully-automated planner based on local search that was awarded for its performance at the last international planning competition (2002). 
first, we present the representation used in lpg for handling plans 
involving numerical expressions, that we call numerical action graph
(na-graphs). then, we propose some extensions of the heuristics guiding a search process where the search states are na-graphs. 
finally, we give the results of an experimental analysis showing that our techniques are very effective in terms of cpu-time or plan quality, and they significantly improve the previous version of the planner. "," planning, local search for planning, numerical planning "
2004,when are behaviour networks well-behaved?," agents operating in the real world have to deal with a constantly changing and only partially predictable environment and are nevertheless expected to choose reasonable actions quickly. this problem is addressed by a number of action-selection mechanisms. behaviour networks as proposed by maes are one such mechanism, which is quite popular. in general, it seems not possible to predict when behaviour networks are well-behaved.  however, they perform quite well in the robotic soccer context. in this paper, we analyse the reason for this success by identifying conditions that make behaviour networks goal converging, i.e., force them to reach the goals regardless of the details of the action selection scheme. in terms of strips domains one could talk of self-solving planning domains.


 "," autonomous agents, planning, cognitive robotics "
2004,biasing the structure of scheduling problems through classical planners," the integration of planning and scheduling techniques has become an increasingly hot topic for the ai community in the last years. since planning and scheduling address complementary aspects of problem solving, much attention is recently being paid to the possibility of mutually exchanging the information yielded during the planning and the scheduling search procedures. in this context, we investigate a loosely coupled approach, which consists of cascading a planner and a scheduler. while other implementations of this framework have already been reported, our work aims at analyzing the structural properties of the scheduling problem which results from the planning component, focusing on the bias produced by different planning approaches in the light of makespan-optimizing scheduling. "," planning, scheduling, planning and scheduling integration, problem structure, execution threads "
2004,evaluation strategies for planning as satisfiability," in this paper we report new developments in
satisfiability planning emerging from a shift
of paradigm in evaluation strategies (where
our contribution in this work lies) together
with important qualitative and quantitative
improvements in satisfiability algorithms
that have been taking place in the recent years,
leading to dramatically improved efficiency
of satisfiability planning.

we investigate different evaluation strategies
for planning problems represented as a constraint
satisfaction problem or as a satisfiability problem.
the standard evaluation strategy -- evaluating
the formulae sequentially increasing the length
one step at a time -- guarantees that a plan
corresponding to the first satisfiable formula
is found first, but this strategy is often not
the best possible in terms of runtime.
we present evaluation strategies based on parallel
or interleaved evaluation of several formulae, and
show that on many problems this leads to
substantially improved runtimes, sometimes several
orders of magnitude. the cost of the improved runtimes
is a possible decline in plan quality: some of
the optimality guarantees of the standard evaluation
strategy are lost, but this may be acceptable because
of the improved runtimes. "," planning, satisfiability testing "
2004,improving the initialization and repair heuristics to effectively solve the pickup and delivery problems with time windows ," pickup  and   delivery  problems   with  time  windows   (pdp-tw)  are
challenging  real-world  scheduling problems  in  which each  delivery
vehicle is  assigned to handle different  pairs of delivery-and-pickup
requests.   in our  previous work,  we successfully  adapted  the push
forward insertion heuristic (pfih)  for initialization, and proposed a
new swap operator to effectively solve pdp-tw.  in this paper, we have
improved our  previous search proposal  by modifying the  adapted pfih
method  to   efficiently  obtain  a  better   initial  solution.  more
importantly,  we proposed  an adaptive  swapping  (adp-swap) mechanism
that can  flexibly revise the neighbourhood size  to aggressively look
for  possible  improvements.   our  search  prototype  using  improved
initialization and repair  methods achieved remarkable results against
those of a tabu-embedded metaheuristic  search or our previous work on
a set of modified solomon's  test cases. besides, there are still many
exciting directions for possible  improvements or investigation in our
search framework.
 ","  scheduling, meta-heuristics for ai, automated reasoning. "
2004,lazy adaptive multicriteria planning," this paper describes a learning system for the automatic configuration of domain independent planning systems, based on measurable features of planning problems. the purpose of the lazy adaptive multicriteria planning (lamp) system is to configure a planner in an optimal way, concerning two quality metrics (i.e. execution speed and plan quality), for a given problem according to user-specified preferences. the training data are produced by running the planner under consideration on a set of problems using all possible parameter configurations and recording the planning time and the plan length. when a new problem arises, lamp extracts the values for a number of domain-expert specified problem features and uses them to identify the k nearest problems solved in the past. the system then performs a multicriteria combination of the performances of the retrieved problems according to user-specified weights that specify the relative importance of the quality metrics and selects the configuration with the best score. experimental results show that lamp improves the performance of the default configuration of two already well-performing planning systems in a variety of planning problems. "," machine learning, planning, instance-based learning, multicriteria combination, feature weighting "
2004,flexible demand assignment problem," this paper proposed a state-of-the-art local search for solving flexible demand assignment problem (fda) which considers the balance between revenue and cost in demand assignment. the hardness of finding a general accurate approximation method for solving fda is first proved. different than the published studies, our research splits the fda problem into three core subproblems as operators for neighborhood construction. the three specified subproblems one bin repack, two bins repack and unpack are proposed completely based on mathematical modelling, computational complexity, executive conditions and greedy solving methods. benchmark experimental results have shown that the proposed local search improved to the best published heuristics by 2.34% "," meta-heuristics for ai, planning, search, scheduling "
2004,querying the semantic web with the corese search engine (submission to pais)," the semantic web relies on ontologies for representing domains through their main concepts and the relations between them. such a domain knowledge is the keystone to represent the semantic contents of web resources and services in metadata associated to them. these metadata then enable a search for information based on the meaning of web resources rather than their syntactic forms.
in this paper, we present an ontology-based approach for web querying, using semantic metadata. we propose a query language based on ontologies and emphazize its ability to express approximate queries, which is of prime importance for an efficient information retrieval on the web. we present the corese search engine we hable developed to handle rdf(s) metadata and we illustrate its interest through several real-world applications. "," ontologies, semantic web, search engine, rdf, conceptual graphs, query language "
2004,masfit: multi-agent system for fish trading," traditional wholesale fresh fish markets carry out their sales by means of the traditional dutch auction system, with the potential buyers physically present in the market hall. in this paper we present the multi-agent system for fish trading (masfit) platform which allows customers to participate remotely in several fish markets at a time with the help of software agents while maintaining the traditional auctions. the platform includes all the necessary tools to create, customize and train buyer software agents.
(this is a submission for pais) ", multi-agent systems 
2004,a portal for publishing museum collections on the semantic web," this paper presents a scheme for publishing museum collections on the semantic web.
it is shown how museums with their semantically rich and interrelated collection content could
start creating large, consolidated semantic collection portals together on the web. by semantic web
techniques, it is possible to make collections semantically interoperable and provide the museum
visitors with intelligent content-based search and browsing services to the global collection base. the
idea and its challenges are addressed through a real-world application, museumfinland. it is a semantic portal for finnish museums to publish their collections on the semantic web. "," semantic web, portal, rdf, search, recommendation "
2004,advisor suite - a knowledge-based sales advisory system," in many domains, a large variety of comparable products and
services from different manufacturers is available on the market.
as a result, customers are increasingly overwhelmed when they
have to choose the products that match their personal needs and
preferences because the selection process in many cases requires
deep technical knowledge about the product features. in the
online sales channel, recommender systems have already
been successfully applied as an additional means for increased
customer guidance. real-world sales advisory, however, can be a
knowledge-intensive task. in order to elaborate the real customer
needs, an experienced sales assistant will for instance conduct a
dialogue on the customer's knowledge level, match these needs
with the available products, and will finally explain the
recommendations as well as possible alternatives.

in this paper we present cwadvisor, a now-commercialized system
for building such intelligent, personalized sales advisory
applications. cwadvisor consistently follows a knowledge-based
approach where all required development activities - from the
definition of the user model and the recommendation rules, up to
the personalization of the dialogue flow as well as user
interface generation - are based on simple conceptual models and
declarative knowledge representation. all these tasks are also
supported by a set of graphical tools comprehensible for
non-programmers, which leads to significant reductions in
development and maintenance costs. at the end of the paper we
discuss experiences from several commercial installations of the
presented system.
 ", pais 
2004,"web information extraction: a domain, user adaptive and multilingual approach"," for pais 2004

this paper describes an advanced prototype system for web
information retrieval and extraction adaptable to different
domains, languages and users’ interests. this system has
been developed in the context of a r&d project involving
both academic and industrial organisations. two different
applications were released at the project’s site in four
different languages. the system’s architecture is open,
modular and multi-agent integrating components for collecting
domain-specific web pages using crawling and spidering
technologies, for extracting information from the collected
web pages using natural language processing and machine
learning techniques, and for presenting the extracted
information according to users’ interests employing user
modelling techniques. a customisation infrastructure is also
provided involving an ontology management system and various
customisation tools. "," information retrieval, information extraction, user modelling, machine learning, multilinguality "
2004,an interactive system for hiring and managing graduate teaching assistants," in this paper, we describe a prototype system for managing hiring and
assigning graduate teaching assistants (gtas) to academic tasks based
on their qualifications, preferences, and availability.  this system
is built using constraint processing techniques and is operated
through web-based interfaces.  the system has yielded a significant
improvement of the quality and stability of the final assignments and
a reduction of the workload and frustration of the administrators
involved in this task.  this paper describes the motivation and
practical significance of this system, the design and functionalities
of its components, and the teaching and research opportunities it
enabled. "," constraint satisfaction, search, resource allocation, constraint processing, interactive system, web-based system, intelligent user interfaces "
2004,a system for pacemaker treatment advice," previously it has been shown that the process of programming a cardiac
pacemaker can be described in terms of diagnosis. a set-theoretical
framework of diagnosis has been taken as the basis for the
construction of a system for pacemaker programming that in its present
form is capable of assisting cardiologists. the system has been made
available commercially in 2003 by vitatron for its c series
pacemakers. in this paper, we discuss the practical requirement
imposed by the clinical environment in which pacemaker programming
takes place. the theory of diagnosis that has been used is briefly
reviewed, after which we describe the implemented system and its
limitations. the paper is rounded off by some ideas for future
development.  as far as we know, this is the first system of its kind
in the area of pacemakers commercially available.
 "," medical decision support, pacemaker programming, theory of diagnosis, model-based diagnosis "
2004,model and heuristics for the shortest road layout problem," the road layout problem consists of finding a valid road layout between two locations described by their azimut and tridimensional coordinates in a topographical map. valid layouts should comply with certain road design regulations that normally depend on design parameters such as road speed and type (e.g. highway, or conventional road). the paper presents a discrete state space for this problem that approximates real layouts, and discusses heuristic estimates that allow searching for the shortest approximation in such space using the a* heuristic search algorithm. "," design, search "
2004,smartcare(tm) - automated clinical guidelines in critical care," in critical care environments important medical and economical
challenges are presented by the enhancement of therapeutic
quality and the reduction of therapeutic costs. for this purpose
several clinical studies have demonstrated a positive impact of
the adoption of so-called clinical guidelines. clinical guidelines
represent well-documented best practices in health care and are
fundamental aspects of evidence-based medicine. however, at the
bedside, such clinical guidelines remain difficult to use by the
clinical staff. recently, we have designed and implemented the
knowledgebased smartcare(tm) system that allows automated control
of medical devices in critical care. smartcare(tm) constitutes a
clinical guideline engine since it executes one or more clinical
guidelines on a specific medical device. the underlying methodology
comprises two sequential phases and seamlessly combines knowledge
engineering techniques, e.g. for computerizing clinical guidelines
with expert system techniques e.g. rule-based forward chaining and
temporal reasoning and finally with software engineering techniques,
e.g. for source code generation and integration to the target
platform. smartcare(tm) was initially applied for the automated
control of a mechanical ventilator and is currently being evaluated
in a european multi-centre clinical study started two years ago.
intermediate reports have been extremely positive and suggest a
statistically significant reduction in the duration of mechanical
ventilation using smartcare(tm). the methodology allows smartcare(tm)
to be implemented effectively with other medical devices and/or with
other appropriate guidelines. in this paper we report on the
methodology, architecture and the resulting versatility of
smartcare(tm) for the automated execution of clinical guidelines.
benefits and lessons learned during its development are discussed.
 "," care plans, clinical protocols, shared ontology, intelligent monitoring, knowledge acquisition, knowledge representation, computer aided knowledge engineering (cake). "
2004,autas: a tool for supporting fmeca generation in aeronautic systems ," the goal of the fmeca (failure modes effects and criticality analysis) process is to determine the consequences that failures may have on the function and behavior of a complex system. in the aeronautic industry this process is very important and must comply to international standards. however, most of the process is currently performed manually. this can be problematic since although the basic process is not difficult, taking into account all behaviors and all the interactions between the behaviors of several components of a system can be very complex. in the paper we discuss how mbr (model based reasoning) can support the process as it is defined by the aeronautic industry and we present a toolkit which implement this concept. "," pais, model based reasoning, failure mode effects analysis, diagnosis "
2004,approach: decentralised rotation planning for container barges," (submission to pais) this paper presents the engineering and development process of a multi-agent software tool (approach) that is used for decentralised rotation planning for container barges in the port of rotterdam. for domain analysis and requirements engineering for the tool, a formal organisation dynamics modelling approach was adopted. we present the approach tool demonstrating the added value of automation of rotation planning over traditional planning. the development trajectory of the tool demonstrates the benefits of the used formal modelling approach in a practical real-world setting, resulting in a concrete software tool that can be put into work in the harbour. preliminary analysis has shown that execution times of barge rotation in the harbour decreases when the tool is used. ", multi-agent systems 
2004,lessons from deploying nlg technology for marine weather forecast text generation," sumtime-mousam is a natural language generation (nlg) system that produces textual weather forecasts for offshore oilrigs from numerical weather prediction (nwp) data. it has been used for the past year by weathernews (uk) ltd for producing 150 draft forecasts per day, which are then post-edited by forecasters before being released to end-users. in this paper, we describe how the system works, how it is used at weathernews and finally some lessons we learnt from building, installing and maintaining sumtime-mousam. one important lesson has been that using nlg technology improves maintainability although the biggest maintenance work actually involved changing data formats at the i/o interfaces.  we also found our system being used by forecasters in unexpected ways for understanding and editing data. we conclude that the success of a technology owes as much to its functional superiority as to its suitability to the various stakeholders such as developers and users. "," nlg, text generation "
2004,soft computing techniques applied to industrial catalysis," a soft computing technique based on the combination of neural networks and a genetic algorithm has been developed for the discovery and optimisation of new catalytic materials when exploring a high-dimensional space. the genetic algorithm, based on real codification, allows to deal with different problems in chemical engineering. also, the neural network simulates the reaction process and allows to calculate the fitness functions used by the genetic algorithm. one possible application of this technique is the optimisation of the catalytic performance of new solid materials by exploring simultaneously a big number of variables as elemental composition, manufacture procedure variables, etc. another application is the optimisation of process conditions in catalytic reactors at industrial scale. considering the high temporal and financial costs required for synthesizing and empirically testing potential solid catalysts, the application of soft computing techniques in this field seems really interesting, as the number of experiments could be reduced. the proposed system has been validated using two hypothetical functions, based on the modelled behaviour of multi-component catalyst explored in the field of combinatorial catalysis. moreover, this soft computing technique has been applied to an industrial problem, being possible to obtain an optimised ti-silicate catalyst for the epoxidation of olefins.
 "," soft computing, genetic algorithm, optimization, neural networks, combinatorial catalysis "
2004,paraconsistent preferential reasoning by signed quantified boolean formulae," we introduce a uniform approach of representing a variety of
paraconsistent non-monotonic formalisms by quantified boolean
formulae (qbfs) in the context of four-valued semantics. this
framework provides a useful platform for capturing, in a simple
and natural way, wide range of methods for preferential reasoning.
off-the-shelf qbf solvers may therefore be incorporated for
simulating the corresponding consequence relations. "," paraconsistent and preferential reasoning, quantified boolean formulae, multiple-valued logics "
2004,the use of temporal reasoning and managment of complex events in smart homes," technological advancements have and will revolutionise the support offered to persons in their home environment.  as the population continues to grow and in addition the percentage of elderly within the population increases we now face the challenge of improving individual autonomy and quality of life.   smart home technology offering intelligent appliances and remote alarm-based monitoring are moving close towards addressing these issues.

to date the research efforts on smart home technology have focused on communications and intelligent user interfaces. the trends in these areas must now,  however,  focus on the analysis on the data which is generated from the  devices within the house as a means of producing ‘profiles’ of the users and providing intelligent interaction to support their daily activities.   a key element in the implementation of these systems is the capability to handle time-related concepts. here we report about one experience using active databases in connection with temporal reasoning in the form of complex event detection to accommodate prevention of hazardous situations. 

 "," temporal reasoning, active data bases, smart homes "
2004,a general recursive schema for argumentation semantics," in argumentation theory, dung's abstract framework provides a unifying view of several alternative semantics based on the notion of extension. recently, a new semantics has been introduced to solve the problems related to counterintuitive results produced by literature proposals. in this semantics, an important role is played by a recursive schema in the definition of extensions. this paper proves that all the semantics encompassed by dung's framework adhere to this property, not previously considered in the literature, which we call scc-recursiveness. we argue that this notion plays a general role in the definition and computation of argumentation semantics. "," argumentation semantics, nonmonotonic reasoning, reasoning under uncertainty "
2004,a syntactical approach to revision," the aim of this article is to revisit dalal's operator for belief
revision. dalal has proposed a technique for revising belief bases
based on the minimization of a distance between interpretations.  the
result is a concrete operator that can be considered either from a
semantical point of view (distance between interpretations) or from a
syntactical point of view (number of atoms that have their truth
values changed). dalal has shown that the so-called alchourrón,
gärdenfors and makinson (agm) postulates are satisfied by its
operator. the agm postulates constrain the revision process so that
minimal changes occur in the belief set. in this article, our
contribution is twofold: first, we improve dalal's algorithm by
avoiding multiple satisfiability checking, each one of which is a
np-complete task. our algorithm requires only one np-stage if beliefs
are expressed in a specific syntax, namely the prime implicates and
prime implicants.  second, we propose a new distance based on the
number of prime implicates contradicted by the incoming new
information. we argue that in some cases changing a minimal set of
propositional symbols do not necessarily entail minimal changes.
 ", belief revision 
2004,a unifying semantics for belief change," many belief change formalisms employ plausibility orderings over the set of possible worlds to determine how the beliefs of an agent ought to be modified after the receipt of a new epistemic input. while most such possible world semantics rely on a single ordering, we look at using an extra ordering to aid in guiding the process of belief change. we show that this provides a unifying semantics for a wide variety of belief change operators. by varying the conditions placed on the second ordering, different families of known belief change operators can be captured, including agm belief contraction and revision, the severe withdrawal of rott and pagnucco, the systematic withdrawal of meyer et. al, and the linear liberation and sigma liberation of booth et al. this approach also identifies novel classes of belief change operators that are worth further investigation.
  "," belief revision, possible worlds semantics, agm contraction, withdrawal, liberation, agm revision "
2004,new insights on the intuitionistic interpretation of default logic," an interesting result found by truszczynski shows that godel's
translation of intuitionistic logic into s4 can also be used for
encoding default logic (dl) into s4f. in this work we further
investigate this translation showing that it preserves its utility
for two general nonmonotonic formalisms: turner's ""nested default
logic"" (ndl) and pearce's ""equilibrium logic"" (which encodes logic
programs into the intermediate logic of here-and-there). for
comparison purposes, we propose a variation of dl, we have
called ""intuitionistic default logic"" (idl). this variation consists
in replacing standard default constructs (rule conditional,
justifications, disjunction in the consequent, etc) by the
corresponding intuitionistic operators, using their modal
interpretation in s4f. we show how both ndl and equilibrium logic
are restricted versions of idl, where the former does not allow
nesting or combining the rule conditional operator, whereas the
latter exclusively restricts the shape of classical formulas to
atoms. finally, we also study the property of ""strong equivalence""
for default theories -- that is, coincidence of results even under
the common addition of new defaults. a simple adaptation of ndl
results allows us to show that the s4f-equivalence of the modal
encodings is a necessary and sufficient condition for strong
equivalence. "," nonmonotonic reasoning, logic programming, knowledge representation "
2004,a unit resolution-based approach to tractable and paraconsistent reasoning,"  a family of unit resolution-based paraconsistent inference relations
 $\vdash_{\sigma}^*$ for propositional logic in the clausal case is
 introduced. parameter $\sigma$ is any consistent set of clauses
 representing the beliefs which are intended to be exploited
 through full deduction, i.e., every classical consequence of
 $\sigma$ must be kept w.r.t. $\vdash_{\sigma}^*$, whatever the
 belief base $b$. contrariwise to many paraconsistent inference
 relations, any relation $\vdash_{\sigma}^*$ can be decided in time
 linear in $|b|$ whenever $\sigma$ is of bounded size.  we show that
 $\vdash_{\sigma}^*$ exhibits several valuable properties, including
 strong paraconsistency. we also show how a unit propagation
 algorithm can be simply turned into a decision procedure for
 $\vdash_{\sigma}^*$.
 we finally show how the $\vdash_{\sigma}^*$ family relates to
 several inference relations proposed so far as approximations of
 classical entailment. "," belief revision, resource-bounded reasoning, automated reasoning, satisfiability testing "
2004,a tabulation proof procedure for residuated logic programming," residuated logic programs allow to capture a spate of different
semantics dealing with uncertainty and vagueness. in this work we
 provide a tabulation goal-oriented query procedure,
and show that our tabulation query procedure terminates if and
only if the sequence of iterations of the immediate consequences
operator reach the least fixpoint after only finitely-many steps.
on the basis of this result we show that the tabulation procedure 
terminates for every definite residuated logic program. 
moreover, using the general tabulation packae of the xsb system, 
a general encoding showing how to use this tabled features  in order to
implement residuated logic programming within xsb. "," reasoning under uncertainty, logic programming "
2004,elimination of spurious explanations," the generation of explanations is considered as a main asset of
knowledge based systems. explanations are exploited in
communication processes in order to justify the outcome of a
reasoning procedure. e.g., during a sales process a client wants
to know why an offered solution (i.e., a product or service)
implies particular properties.

in this paper we show that current approaches of generating
explanations fall short. these approaches can lead to spurious
explanations with respect to a proposed (or selected) solution.
we introduce an extension of current explanation principles such
that all explanations of properties regarding a proposed solution
are well-founded explanations. such well-founded explanations can
be computed by additional polynomial computation costs compared to
traditional explanation algorithms.  "," constraint satisfaction, explanation "
2004,reasoning about actions with sensing under qualitative and probabilistic uncertainty," we focus on the aspect of sensing in reasoning about actions 
under qualitative and probabilistic uncertainty.
we extend an action language by
actions with nondeterministic and probabilistic effects, 
and define a formal semantics 
in terms of deterministic, 
nondeterministic, and probabilistic transitions 
between epistemic states.
we then introduce the notions of a conditional plan 
and its goodness in this framework, and we formulate the conditional 
planning problem. we present an algorithm for solving it, 
which is proved to be sound and complete in the sense 
that it produces all optimal plans. we also report  
on a first prototype implementation of this algorithm.
an application in a robotic-soccer scenario
underlines the usefulness of our formalism 
in realistic applications.
 "," reasoning about actions and change, reasoning under uncertainty, cognitive robotics "
2004,iterated belief change for reasoning agents," action formalisms like the fluent calculus have been developed to endow logic-based agents with the abilities of reasoning about the effects of actions, executing high-level strategies, and planning. in this paper we extend the fluent calculus by a method for belief change, which allows agents to revise their internal model if they make observations that contradictthis model. unlike the existing combination of the situation calculus with belief revision, our formalism satisfies all of the standard postulates for (iterated) belief change. furthermore, we have extended the action programming language flux by a computational approach to belief change which is provably equivalent to the axiomatic characterization in the fluent calculus.
 "," reasoning about actions and change, iterated belief revision "
2004,geographic information revision based on constraints," information usually has  many sources and is often incomplete and /
or uncertain, this leads to many inconsistencies. revision is the
operation which consists in identifying these inconsistencies and
then in removing them by changing a minimum of information.
in this paper, we are interested in geographic information
revision in the framework of a flooding problem. we show how to
express and how to revise this problem by simple linear constraints.
we present three revision strategies based on linear constraints
resolution. we apply and compare these approaches on both a real-world flooding
problem and random flooding instances. "," revision, linear constraints, geographic information "
2004,introducing alias information into model-based debugging," model-based diagnosis applied to computer programs has been
studied for several years. although there are still weaknesses
in the used models, especially on dealing with dynamic data
structures, the approach has been proven useful for automatic
debugging. the weaknesses stem from the fact that heap objects
are modeled without considering alias information. our approach
extends the modeling process with a static points-to analysis
that reveals the structure and relations between heap objects.
this points-to information is then used to improve existing
value-based models for java programs such that the diagnosis
engine is able to differentiate between separate data
structures. with this extension the set of diagnoses can be
reduced for certain types of programs.
 "," automatic debugging, diagnosis "
2004,"diagnosis of discrete-event systems by separation of concerns, knowledge compilation, and reuse"," model-based diagnosis of discrete-event systems (dess) requires the reconstruction of the behavior of the system to be diagnosed, which is computationally expensive and, therefore, time-consuming. accordingly, most approaches propose a trade-off between off-line and on-line computation: suitable knowledge, derived off-line from the model of the system, can be exploited on-line based on the actual observation. this way, a large amount of model-based reasoning is anticipated off-line, thereby making the on-line task considerably lighter. the essential novelty of this paper, which aims to support the diagnosis of asynchronous dess, lies in the ability to exploit not only the general-purpose diagnostic knowledge compiled off-line but also the special-purpose knowledge generated on-line for the solution of previous  problems, thereby pursuing processing reuse. to this end, compatibility checking is required: the solution of a new diagnostic problem can exploit the solution of another problem provided the latter subsumes the former. "," diagnosis, model-based reasoning, knowledge compilation, knowledge reuse, discrete-event systems "
2004,debugging program loops using approximate modeling,"   developing model-based automatic debugging strategies has been an
  active research area for several years. we analyze shortcomings of
  previous modeling approaches when dealing with object-oriented
  languages and present a revised modeling approach. we employ
  abstract interpretation, a technique borrowed from program analysis,
  to improve the debugging of programs including loops, recursive
  procedures, and heap data structures.  together with heuristic model
  refinement, our approach delivers superior results than the previous
  models. the principle of our approach is demonstrated on a set of
  examples.
 "," diagnosis, debugging "
2004,on line monitoring and diagnosis of multi-agent systems: a model based approach," in this paper we present an approach for monitoring and diagnosis of  multi-agent systems where mobile robotic agents 
act in an environment and partial  observability is provided by a set of fixed sensors. the robotic agents execute actions 
of a plan synthesized by a supervisor and may compete with each other for the access to limited resources.
this kind of systems exhibits complex dynamics  where weakly predictable interactions among agents may arise.
the paper presents a model-based, centralized approach to on-line monitoring and diagnosis;  partial information about 
the system status is provided by messages coming from sensors (and possibly from robotic agents).
dynamics of the components of the system and their relations are modeled via communicating automata. for efficiency 
reasons, the global system model is factored  in a  number of subsystems dynamically  aggregating a convenient set of 
component models.
the on-line monitoring filters possible evolutions of the system by exploiting messages from sensors and agents and  by 
enforcing global constraints.  when the monitor detects failures in the actions execution, a diagnoser is triggered for 
explaining the failure in terms of faults in the robotic agents and/or troublesome interactions among them.
as a specific case-study we refer to the robocare project "," diagnosis, model-based reasoning, intelligent monitoring, supervision of agent systems "
2004,plausibility structures for default reasoning,"   friedman and halpern have introduced the inference by plausibility
  structures, which provides semantics for various default logics.  this is a
  generalization of known inferences, such as those based on expectations, or
  on possibilistic logic.  we argue that a slightly different inference by
  plausibility would be more appropriate for default reasoning.  it would
  still be appropriate for all the default logics considered by friedman and 
  halpern.  significant cases, such as subsets of normal defaults with
  multiple extensions, or a
  formalism extending circumscription to the cases where (and) is falsified,
  can be translated into the new formalism only. 
  
  in order to prove our results, we complete the list of the reasoning
  properties for the (two versions of the) inference by plausibility.  since
  these properties describe the behavior of a given inference in an intuitive
  and non technical way, this list is important for any potential user.
  moreover, it happens that considering these properties only, without going
  into the technicalities of the plausibility approach, is enough to describe
  the main results given here. "," default logics;, common sense knowledge; "
2004,a qualitative theory for shape representation and matching for design," a new method for qualitative shape recognition and matching applied to the recognition and matching of objects in designs is presented. the paper presents an ordering information approach to the qualitative description of shapes considering qualitatively their angles, relative side length, concavities and convexities of the boundary and colour. the shapes recognised are regular and non-regular closed polygons without or with holes. to describe shapes with holes, topological and qualitative spatial orientation aspects are considered in order to relate the hole with its container. each object is described by a string containing its qualitative salient features (symbolic representation), which is used to match the object against others. the paper also describes how this method can be used in industrial design by explaining an application. given an image whit different objects (representing tiles) and a vectorial image design of a ceramic tile mosaic border the application recognises which tile in the image belongs to the border design and indicates its position and rotational angle to place the tile in the correct position of the border design. therefore this software could be applied in the future to a robot arm who places the tiles in the correct position to create the final ceramic mosaic strip designed. this qualitative method provides several advantages over traditional quantitative representations. the main advantages are the reduction of computational costs and the managing of uncertainty (two manufactured tiles are not geometrically identical, but they represent the same tile in the design). "," qualitative reasoning, qualitative shape "
2004,an algorithm for knowledge base extraction," many approaches have been proposed for reasoning based on conflicting 
information in general and in particular on stratified knowledge 
bases, i.e. bases in which all pieces of information are assigned a 
rank. in this paper, we want to reflect on a particular family of 
algorithmic approaches - known as adjustments - suggested for 
extracting a consistent knowledge base from a possibly inconsistent 
stratified one. we will point out counter-intuitive results provided 
by these approaches and develop an algorithm we call refined 
disjunctive maxi-adjustment which does not have these drawbacks. "," information extraction, inconsistency handling, belief revision, merging, adjustment "
2004,a fuzzy approach to temporal model-based diagnosis ," in intensive care unit (icu) domain, diseases' temporal evolution and patients' contextual information are critical pieces of knowledge that must be considered in the design of a diagnosis task. the uncertainty, inherent in the description of temporal information associated to diseases, requires a temporal representation and reasoning framework to cope with. this temporal framework has to be flexible enough in order to facilitate its integration in a behavioral model. this paper proposes a temporal behavioral model (tbm) that makes this integration possible and permits the specification of contextual information (that may modified the tbm). a diagnosis process is also proposed. this process uses temporal model based techniques and fuzzy temporal constraints networks (ftcn) as underlying temporal framework. some  heuristics, which affect not only to the temporal reasoning dimension  but also to the causal one, have been designed in order to compute efficiently a solution. "," temporal behavioral model, diagnosis task, temporal framework, temporal reasoning "
2004,diagnosis as semiring-based constraint optimization ," constraint optimization is at the core of many problems in artificial
intelligence. in this paper, we frame model-based diagnosis as a
constraint optimization problem over lattices. we then show how it can
be captured in a framework for ""soft"" constraints known as
semiring-csps. the well-defined mathematical properties of a
semiring-csp allow to devise efficient solution methods that are based
on decomposing diagnostic problems into trees and applying dynamic
programming. we relate the approach to sab and tree*, two diagnosis
algorithms for tree-structured systems, which correspond to special
cases of semiring-based constraint optimization. "," diagnosis, model-based reasoning, constraint programming "
2004,iterated belief change and exogenous actions in the situation calculus," we present a novel theory of action and change capable of dealing
with discord between an agent's beliefs and the results of its sensing. previous work by scherl and levesque (2003) and shapiro et al. (2000) have given accounts of iterated belief update and revision,  and the ability to deal with mistaken beliefs. however, they assume that all actions, including exogenous actions beyond the agent's control, are accessible to the agent for the purposes of reasoning. our approach abandons this idealistic stance and allows the agent to hypothesise the occurrence of exogenous actions to account for any discrepancy between belief and sensing. "," reasoning about action and change, knowledge representation, belief revision, cognitive robotics "
2004,models of behavior deviations in model-based systems," tasks like diagnosis, failure-modes-and-effects analysis (fmea), and therapy proposal involve reasoning about deviating variables and parameters. in model-based systems, one tries to capture this kind of inferences by models that describe how deviations from some reference state or behavior are created and propagated through a system. several techniques and systems have been developed that address this issue, in particular in the area of qualitative modeling. in the context of model-based systems, we face the requirement that the system model has to be compositional, i.e. can be generated by aggregating local models of the system constituents (e.g. components) that are stored in a library. however, to our knowledge, a rigorous mathematical foundation and a “recipe” for how to construct such compositional deviation models has not been presented in the literature, despite the widespread use of the idea and the techniques. in this paper, we briefly revisit the concepts and use of deviation models and related techniques. we reveal (sometimes implicit) presumptions and limitations and present a general mathematical formalization of deviation models. based on this, aspects of constructing libraries of deviation models, their properties, and their application in consistency-based diagnosis and prediction-based fmea in a component-oriented framework are analyzed. "," model-based systems, diagnosis, qualitative modeling, failure-mode-and-effects analysis, deviation models "
2004,consistency and optimisation for conditional preferences," tcp-nets are an extension of cp-nets which allow the expression of conditional relative importance of pairs of variables. in this paper it is shown that a simple logic of conditional preferences can be used to express tcp-net orders, as well as being able to represent much stronger statements of importance than tcp-nets allow. the paper derives various sufficient conditions for a subset of the logical language to be consistent, and develops methods for finding a total order on outcomes which is consistent with the set of conditional preferences. this leads also to an approach to the problem of constrained optimisation. "," reasoning with preferences, qualitative reasoning "
2004,a robot task planner that merges symbolic and geometric reasoning," we describe an original planner that has been specially designed to
address intricate robot planning problems where geometric constraints
cannot be simply ``abstracted'' in a way that has no influence on the
obtained plan. this paper presents the ingredients that allowed us to
establish an effective link between representations used by a symbolic
task planner and a realistic motion and manipulation planning library
using probabilistic roadmap methods.  the architecture and the
main plan search strategies are presented. the symbolic representation
not only let us represent constraints but also propose a relaxed
problem used as a heuristic value. at each step of the planning
process both symbolic and geometric data are considered. besides, the
planning process tries to arbitrate between finding a plan with the
level of knowledge already acquired, or ``investing'' more in a deeper
knowledge of the topology of the different configuration spaces it
manipulates. the main topics are illustrated with an example inspired
by the hanoi tower problem. "," planning, geometric reasoning, robotic "
2004,indirect and conditional sensing in the event calculus," controlling the sensing of an environment by an agent has been accepted as necessary for effective operation within most practical domains. usually, however, agents operate in a partially observable domains where not all parameters of interest are accessible to direct sensing.  in such circumstances, sensing actions must be chosen for what they will reveal indirectly, through an axiomatized model of the domain causal structure.  this article shows how sensing can be chosen so as to acquire and use indirectly obtained information to meet goals not otherwise possible.  this paper presents event calculus extended with a knowledge formalism and causal ramifications, and used to show how inferring unknown information about a domain leads to conditional sensing actions. such an extended theory is able to reason about knowledge for the purposes of describing and controlling sensing actions in a partially observable causal domain.  partial observability is usual in most practical circumstances, and restricts an agent in its range of directly sensed parameters of the environment.  in cases where the state of an unobservable environment fluent is required, this information must be inferred using the causal domain model along with other known fluents.  determining which fluents to sense, and when, is the problem to be solved at plan time. "," cognitive robotics, autonomous agents "
2004,an active learning approach for assessing robot grasp reliability," this paper describes a novel application of active learning techniques
in the field of robotic grasping. a vision-based grasping system has
been implemented on a humanoid robot. it is able to compute a set of
feasible grasps and to execute and measure the actual reliability of
any of them.

an algorithm aimed to predict the performance of an untested grasp
using the results observed on previous similar attempts is
presented. the previous experience is stored using a set of
vision-based grasp descriptors.  in addition to this, an algorithm
that actively selects the next grasp to be executed in order to improve
the predictive quality of the accumulated experience is introduced
as well.

an exhaustive database of experimental data is collected and used to
test and  validate both algorithms.  "," robotics, active learning, machine learning "
2004,adaptive robot coordination using interference metrics," one main issue facing robotic groups is effective coordination
mechanisms. many robotic domains contain restrictions such as
limited areas of operation or narrow passages that are liable to
cause spacial conflicts between robots and their environment.
previous work presented a novel definition of interference that
measured the total time robots deal with resolving such conflicts
over space. it was found that a robotic group's productivity was
strongly negatively correlation to this measure. effective
coordination techniques minimize this level of interference and
thus achieve higher productivity. this paper uses this result
towards creating adaptive coordination techniques that are able to
dynamically adjust to the level of interference a given robot in
the group faces. using the robot's internal interference metric as
a guide, we are able to create coordination methods that can
quickly and effectively adjust to a given domain's spacial
limitations. we present a heuristic that is completely distributed
and requires no communication between robots. we found that groups
that used this approach consistently achieved higher productivity
than non-adaptive coordination methods.
 "," cognitive robotics, robotics "
2004,attention-driven parts-based object detection," recent studies have argued that natural vision systems
perform classification by utilizing different mechanisms
depending on the visual input. in this paper we present
a hybrid, data-driven object detection system that combines
parts-based matching and view-based attention for faster detection.
we propose a simple competitive policy that allows incremental
addition of new object classes to the system without requiring
class-vs-class training. using our framework, we show empirical
support for the hypothesis that low-frequency visual information
can be effectively used to direct attention and possibly subsume
further, more costly analysis. we evaluate our approach on face
and car detection problems, while concentrating on the capability 
to learn from small samples. our implementation
is freely available as matlab source code.
 "," vision, perception, machine learning "
2004,using spatio-temporal continuity constraints to enhance visual tracking of moving objects ," a framework is presented for the logical and statistical analysis of dynamic scenes containing occlusion and other uncertainties.
this framework consists of three elements: an object tracker module, an object recognition/classification module and a logical consistency reasoning engine. the principle behind the object tracker and recognition modules is to reduce arror by increasing ambiguity, by mergin objects in close proximity and presenting multiple hypotheses. the reasoning engine deals with error, ambiguity and occlusion in a unified framework to produce a most likely hypothesis which is logically consistent over time, in respect of spatio-temporal continuity constraints. the system results in improved annotation over frame-by-frame methods. the framework has been implemented and applied successfully to the analysis of team sports video recorded a single fixed cammera. "," vision, tracking, spatio-temporal reasoning "
2004,a context-based model of attention," artificial visual systems need an attentional selection
mechanism to constrain costly processing to parts that are likely to contain the
object. an important design decision for artificial vision systems
concerns the locus of selection. to guide the selection mechanism,
traditional models of attention use either an early locus of selection
based on low-level features (e.g., conspicuous edges) or a late locus of
selection based on high-level features (e.g., object templates). an
early locus is computationally cheap and fast but is an unreliable
indicator of the objecthood. a late locus is computationally
expensive and slow and requires the object to be selected object to be known,
rendering selection for identification useless. to combine the
advantages of both loci, we
propose the coba (context based) model of attention, that guides
selection on the basis of the learned spatial context of objects. the
feasibility of context-based attention is assessed by experiments in
which the coba model is applied to the detection of faces in natural
images. the results of the experiments show that the coba model is highly
successful in reducing
the number of false detections. from the results, we may conclude that context-based attentional
selection is a feasible and efficient selection mechanism for artificial visual systems.
 "," visual attention, perception, machine learning, vision, object detection "
2004,learning to focus attention on discriminative regions for object detection, a major task of visual attention is to focus processing on regions of interest to enable rapid and robust object search. instead of integrating generic feature extraction into object specific interpretation we strictly pursue a top-down approach. early features are tuned to selectively respond to task related visual features. in this work we determine discriminative regions from the information content in the local appearances patterns. a rapid mapping from appearances to discriminative regions is estimated using decision trees. the focus of attnetion on discriminative patterns enables then efficient detection and the definition of a sparse object representation. evaluation of complete image analyis under various degrees of partial occlusion and image noise resulted in highly ropbust recognition even in the presence of severe occlusion and noise effects. ," visual attention, object recognition, cascaded detection, decision trees "
2004,vision-language integration in ai: a reality check," multimodal human to human interaction requires integration of the contents/meaning of the modalities involved. artificial intelligence (ai) multimodal prototypes attempt to go beyond technical integration of modalities to this kind of meaning integration that allows for coherent, natural, “intelligent” communication with humans. in this paper, we address the issue of vision-language content integration and attempt to shed some light on how, why and to what extent this type of integration takes place within ai. we present a taxonomy of vision-language integration prototypes which resulted from an extensive survey of the field and which uses integration purposes as the guiding criterion for classification of systems that span many decades and many different ai multimedia-related research areas. we look at the integration resources and mechanisms used in such prototypes and correlate them with theories of integration that emerge indirectly from computational models of the mind. we argue that state of the art vision-language prototypes fail to address core integration challenges automatically, because of human intervention in stages during the integration procedure that are tightly coupled with inherent characteristics of the integrated media. last, we present vlema, a prototype that attempts to perform vision-language integration with minimal human intervention in these core integration stages. "," intelligent user interfaces, intelligent multimedia systems, vision-language meaning integration "
2004,distributed reasoning in a peer to peer setting," in a peer to peer inference system, each peer can reason locally but 
can also solicit some of its acquaintances, which are peers sharing 
part of its vocabulary. in this paper, we consider peer to peer 
inference systems in which the local theory of each peer is a set of 
propositional clauses defined upon a local vocabulary. an important 
characteristic of peer to peer inference systems is that the global 
theory (the union of all peer theories) is not known (as opposed to 
partition-based reasoning systems).
the contribution of this paper is twofold. we provide the first consequence finding algorithm in a peer to peer setting: it is incremental, anytime and computes consequences gradually from the
solicited peer to peers that are more and more distant. we exhibit a sufficient condition on the acquaintance graph of the peer to peer inference system for guaranteeing the completeness of this algorithm.
 "," automated reasoning, deduction, consequence finding, distributed ai, knowledge-based systems, knowledge representation "
2004,learning model free motor control," some robotic tasks, like the control of leg movements for a walking robot, require an accurate control to follow the desired trajectory in the presence of unforeseen external disturbances and variations in the dynamic parameters of the system. to solve this problem with classical control techniques such as pid, a precise tuning of control parameters is necessary, and they must be readjusted when the system working conditions vary. this burden can be avoided using a learning process that automatically learns the appropriate control law and adapts to ongoing system variations. a drawback of many learning systems is that they are not effective for non-toy problems, because of the large amount of experiences they require in the learning task. in this paper we present the results obtained with a categorization and learning algorithm able to perform efficient generalization of the observed situations, and learn accurate control policies in a short time without any previous knowledge of the plant and without the need of any kind of traditional control technique. its performance is evaluated on the trajectory tracking control with simulated dc motors and compared with pid controls specifically tuned for the same problems. "," categorization and learning, reinforcement learning, trajectory tracking control, robot locomotion "
2004,iterated algorithm for the optimal winner determination in combined negotiations , iterated algorithm for the optimal winner determination in combined negotiations ," agents, winner determination "
2004,transmission expansion planning based on tabu search algorithm," this paper presents a new approach for
formulating and solving the transmission expansion
planning (tep) problem. the main improvement is in
introducing the corona power loss in the objective
function and operating constraints. as a result, the
objective function is made up of three terms, namely: the
cost of investment of new transmission lines, the cost of
ohmic power loss of both new and existing lines, and the
cost of corona power loss of new lines. this combination
of terms reveals a nonlinear objective function which is
solved by tabu search (ts). the corona power loss term
has been tried for a practical number of sub-conductors (1,
2, 3, and 4) and practical sub-conductor radii. in order to
test and justify this new formulation, it has been applied
to garver’s 6-bus test sytem the outcome of the
expansion process is the optimal scenario which defines
the number of lines to be added. for each added line, the
number and radii of sub-conductors, phase spacing,
height, and bundle radius are also generated as a result of
the expansion process. when compared to previously
reported tep attempts, simulation results show a
reduction in the total cost of the expanded network. "," transmission lines, tabu search, optimization, corona, power loss, planning "
2004,a declarative characterisation of disjunctive paraconsistent answer sets," in this work, paraconsistent answer sets for extended disjunctive logic programs are presented in terms of a fully declarative approach. in order to do that, we introduce a frame-based semantics settled on a two-dimensional extension of the intuitionistic logic here-and-there. as it is known, frames are a powerful and elegant tool which have been used to characterise and integrate substructural logics. unlike the original definition, in this process, no kind of syntactic transformation is employed. indeed paraconsistent answer sets are defined just by minimising models satisfying some conditions. as less models are involved for this minimisation, our version is even more economical. furthermore considering that paraconsistent answer sets embed both answer sets and stable models, these semantics are also captured via frames.
 "," logic programming, extended disjunctive programs, frame-based semantics, paraconsistent semantics, knowledge representation "
2004,automatic generation of macro-operators from static domain analysis," the attempt of dealing with the complexity of planning tasks by resorting to abstraction techniques is a central issue in the field of automated planning. although the generality of the approach has not been proved always useful on domains selected for benchmarking purposes, in our opinion it will play a central role as soon as the focus will move from artificial to real problems. in this case, it will be crucial to have a tool for automatically generating abstraction hierarchies from a domain description. this paper addresses the problem of how to identify macro operators starting from a ground-level description of a domain, to be used for generating useful abstract-level descriptions. compared to our previous work, this paper reports a step further, in the direction of fully automatizing the process, from both a conceptual and pragmatic perspective. conceptually, we refined the process of macro-operators extraction by dealing with the problem of parameters' unification through the exploitation of domain invariants. pragmatically, we implemented a system that –given a description of the domain specified in pddl 1.x– outputs a set of macro-operators to be used as a starting point for defining abstract operators. experimental results highlight the ability of the system to identify suitable macro-operators that are usually good alternatives to those extracted by a knowledge engineer after a thorough (and sometimes painful!) domain analysis. "," macro-operators, automatic domain analysis, planning by abstraction "
2004,optimal brain surgeon variants for optimization," the determination of the optimal architecture of a multilayer perceptron (mlp) to solve a specific problem is a difficult task. several approaches based on saliency analysis have been developed in this field. in this paper we prove that the obs does not gives an optimal architecture then we propose to apply the obs method several times to achieve this goal. we also present the advantage of hybrid methods for optimization. a new hybrid method is proposed which use the flexible optimal brain surgeon (f-obs) specialized in variable selection method. a comparison of our approaches to standard techniques for architecture optimization designed for mlp is presented.
simulation results obtained on the monk's problem illustrate the specificities of each method described in this paper.
 "," neural networks, pruning, weight saliency, optimization "
2004,trying again to fail-first," in ecai 1998 smith and grant performed a study of the fail-first principle of haralick and elliott. the fail-first principle states that ``to succeed, try first where you are most likely to fail.'' for constraint satisfaction problems, haralick and elliott realized
this principle by minimizing branch depth. smith and grant hypothesized that if failing first is a good thing, then more of it should be better. they devised a range of variable ordering heuristics, each in turn trying harder to fail first. they showed that trying harder to fail-first does not guarantee a reduction in search effort. we failed in our attempt to repeat that study. this was due to a implementation error for one of their heuristics. however, with this fix applied we show that smith and grant's conclusions were indeed correct: failing first does not invariably lead to greater overall efficiency. this is because minimizing branch depth sometimes involves large increases in the branching factor. we also show that the effects of trying harder to fail-first are algorithm dependent. our results indicate that trying to fail is an important principle in search, and they suggest an alternative failure strategy that seeks to minimize the number of nodes in a subtree.
 "," search, constraint satisfaction "
2004,handling conflicts in first-order knowledge bases: application to access control models," there have been several proposals for handling inconsistency in propositional knowledge bases. some of these approaches, called ""coherence-based approaches"" propose to give up some formulas in order to get one or several preferred consistent subbases, and to apply classical entailment on these subbases to deduce plausible conclusions. 
the first part of this paper proposes tools for handling conflicts in first-order knowledge bases.  
we show that the ""blind"" application of propositional approaches to inconsistent first order knowledge bases can lead to undesirable conclusions. a solution based on weakening first order formulas responsible of conflicts is proposed. 
in the second part of this paper, we propose a new access control model called e-orbac and show how conflicts can be solved in this system. "," handling conflicts, stratified first-order knowledge bases, access control "
2004,preferences on queries in a mediator approach," the problem of integrating relevant information obtained from multiple heterogeneous sources is a complex task, with which biologists are now faced. in this paper, we address the problem of querying biomedical databases in a mediator context. we propose to exploit the metadata of the sources to take into account user preferences. the mediator system we present is designed within a tractable logical framework. it allows both transparent and cooperative querying like making it possible to keep track of the origins of the instances provided as answers. our proposal is generic in that it is relevant not only for bioinformatics, but could also be applied to other domains for which metadata are available.
 "," mediator, metadata, biomedical sources "
2004,a formalization of coalition structures in multiagent systems," in this paper we study the properties of coalition structures in multiagent systems.
starting from a notion of power in a multiagent system we provide a formal representation of
coalitions. the basic idea we capture is that in a coalition every agent must contribute with his power
to the goals of some of his partners and, at the same time, he must be dependent on some of them.
moreover connecting properties are taked in account in order to distinguish when a group of agents are tied
in some exchanges as a whole coalition, or can be viewed as the composition of independent subgroups.  "," multiagent systems, game playing "
2004,aclanalyser: a tool for debugging multi-agent systems,"  multi-agent systems are a special kind of distributed systems in which
the main entities are autonomous, in a proactive sense. these systems
are special because their unpredictability. agents can spontaneously engage in complex interactions, guided by their own goals and intentions. when developing such kind of system, there are many problems the designer/programmer has to face. all these problems make virtually impossible to totally debug a complex enough multi-agent system. in this article we describe a debugging tool we have developed in our lab which pretends to alleviate the problems derived from distribution and  unpredictability. "," tools for programming mas, debuggin mas "
2004,on multiclass active learning with support vector machines," in supervised machine learning, a training set of examples which are assigned to the correct target labels is a necessary prerequisite. however, in many applications, the task of assigning class labels cannot be conducted in an automatic manner, but involves human decisions and is therefore time-consuming and expensive. in the case of classification learning, the active learning framework has been considered to address this problem. while most research on active learning in the field of kernel machines has focused on binary problems, less attention has been given to the problem of learning classifiers in the case of multiple classes. we consider three common decomposition methods to state multiclass problems in terms of sets of binary classification problems and propose novel active learning heuristics to reduce the labeling effort. various experiments conducted on real-world datasets demonstrate the merits of our approach in comparison to previous research. "," active learning, support vector machine, multiclass classification "
2004,a speech architecture for personal assistants applied in a knowledge management context," this paper describes the design and the problems encountered while developing a speech and natural language dialog interface for personal assistants. personal assistants are agents that help human users (often referenced as masters) to do their daily work. the particular skills of a personal assistant are devoted to understanding its master, and presenting the information intelligently and in a timely manner. we believe that the use of speech will facilitate the interaction between the user and his machine, since the user may speak with the agent using his own terms, increasing the quality of the assistance. the main difficulty however is to handle spoken natural language, understanding their actual context. to do so, we restrict the exchanges to directives speech act classes: inform, request, or answer. in addition, the knowledge handling is crucial for the effectiveness of such interfaces. we are using a set of task and domain ontologies, separating out domain and task models for reasoning. in this work, we present such an architecture for a multi-agent system applied to knowledge management. as a clear result of the conversational speech interface, we could expect a reduction in the cognitive load of the user and an increase in the quality of assistance. "," intelligent user interfaces, spoken dialogue, personal assistants, knowledge management "
2004,local search heuristics: fitness cloud versus fitness landscape," this paper introduces the concept of fitness cloud as an alternative way to visualize and analyze search spaces than given by the geographic notion of fitness landscape. it is argued that the fitness cloud concept overcomes several deficiencies of the landscape representation. our analysis is based on the correlation between fitness of solutions and fitnesses of nearest solutions according to some neighboring. we focus on the behavior of local search heuristics, such as hill climber and simulated annealing, on the well-known nk fitness landscape. in both cases the fitness vs. fitness correlation is showed to be related to the epistatic parameter k. finally, we show that qualitative analysis of the fitness cloud gives more insight on the search space than the geographical fitness landscape metaphor. "," fitness landscape, metaheuristic "
2004,lakatos-style automated theorem modification," automated deduction systems are, in general, not very flexible. in
most cases, they assume that the conjecture supplied is true and in
all cases, they assume that the conjecture supplied is well formed:
the definitions of the concepts related by the conjecture are correct
and are the ones the user is really interested in. this does not
reflect the more organic processes at work in human mathematics,
whereby sketch conjectures are hypothesised and revised in the light
of counterexamples, and sketch proofs are slowly improved alongside
alterations to the theorem statement until an interesting, proved,
result is achieved.

drawing on ideas from lakatos's philosophy of mathematics, we
demonstrate how greater flexibility in automated reasoning can be
achieved. in particular, we have implemented a system which is able to take open conjectures and either prove them if true, or modify
them to something which is true if the original were false. in the latter case, the system
uses the mace model generator to supply examples supporting the
conjecture and examples falsifying it, then the hr machine learning
system forms a theory using these examples. our system extracts
concepts which define a subset of the supporting examples and this
enables it to perform lakatos-style techniques such as
counterexample-barring, strategic withdrawal and piecemeal
exclusion. this produces modified hypotheses which are then proved by
the otter theorem prover, and shown to the user. we show the
effectiveness of this approach by modifying non-theorems taken from
the tptp library of first order theorems.
 "," theorem proving, machine learning, model generation, automated mathematics, philosophy of mathematics, concept formation "
2004,a uniform tableaux-based method for abduction and contraction in description logics," we present algorithms for concept abduction and concept contraction,
two reasoning services in description logics (dl) recently proposed to
model how several supplies fit a demand (all described by concepts),
and vice versa, in e-commerce.
our method is based on truth-prefixed tableaux, and solves both abduction and 
contraction in a uniform way. recent  tableaux methods for dls could 
be easily extended to truth-prefixed tableaux.

we also analyze the complexity of the 
problems for the dl named aln, and show that the upper bound given by our 
methods meet the complexity lower bounds.
 "," description logics, abduction, tableaux, belief revision "
2004,on the importance of being diverse," web search engines place a significance emphasis on the similarity between the current query and the results when it comes to selecting and ranking a result-list. we argue that this query-similarity emphasis may limit search precision. we draw on related work by the case-based reasoning (cbr) community, which shows how enhancing result diversity can improve the quality of retrieved cases and recommendations. we evaluate the use of such techniques in web search, showing that similar benefits are available. "," web search, similarity, diversity "
2004,axiomatizing noisy-or," the noisy-or function is 
extensively used in probabilistic reasoning, and
usually justified with heuristic arguments.
this paper investigates sets of conditions that 
imply the noisy-or function.
 "," noisy-or function, bayesian networks, probabilistic reasoning, uncertainty representation "
2004,adapting lpgp to plan with exogenous events and goals with deadlines," although there are a number of ai planners that reason about actions with duration, not many deal with exogenous events or goals with deadlines.  this paper describes two approaches that enable the planner lpgp to reason about domains which contain exogenous events and goals with deadlines: the first approach investigates ways in which such domains may be encoded using pddl2.1 level 3, a planning domain definition language developed for the third internationa planning competition (ipc3), while the second approach involves directly modifying lpgp.  both approaches have been tested in a number of domains involving exogenous events and goals with deadlines.  the paper draws conclusions about the relative merit of the two approaches based on the results of these experiments. "," ai planning, temporal reasoning "
2004,compilation of ltl goal formulas into pddl," temporally extended goals are used in planning to express safety and
maintenance conditions.  linear temporal logic is the language often
used to express temporally extended goals.  we present a method for
compiling ltl goal formulas into pddl, which is the standard language
used to define planning domains and problems and is handled by many
planners.  the compilation process first constructs a finite state
machine representing all reachable progressions of the goal formula,
then modifies the planning domain and problem definition so that the
state of the fsm is tracked.  we discuss the complexity issues relating to the size of the representation resulting from the compilation.
 "," planning, temporal logic "
2004,argumentation neural networks: value-based argumentation frameworks as neural-symbolic learning systems ," while neural networks have been successfully used in a number of machine learning applications, logical languages have been the standard for the representation of legal and argumentative reasoning. in this paper, we present a new hybrid model of computation that allows for the deduction and learning of argumentative reasoning. we do so by using neural-symbolic learning systems where non-classical reasoning is representable. we propose a neural argumentation algorithm to translate argumentation networks into standard neural networks. we then show a correspondence between the semantics of the two networks. the algorithm works not only for acyclic argumentation networks but also for circular networks. the approach enables cummulative argumentation through learning, as the strength of the arguments change over time. "," neural-symbolic systems, value-based argumentation frameworks, hybrid systems, argumentation "
2004,teams of genetic predictors for inverse problem solving," genetic programming (gp) has shown to be a good method to predict functions that solve inverse problems.
in this context, a solution given by gp generally consists in a sole predictor.
in contrast, stack-based gp systems manipulate structures that allows to evolve together several predictors, 
that can be considered as teams of predictors.
work in machine learning reports that combining predictors gives good result in both quality and robustness.
in this paper, we use stack-based gp to study different ways to realize cooperation between predictors.
first, preliminary tests and parameters tuning are performed on an academic gp benchmark.
then, the system is applied to a real inverse problem. 
a comparative study with standard methods has shown limits and advantages of teams prediction; 
in particular, when combinations take into account the response quality of each team member. "," genetic algorithms (genetic programming), learning "
2004,from belief change to obligation change in the situation calculus," a  solution to the  frame problem in the  context of belief change has
been defined in the framework of situation  calculus. in this paper we show how this solution can  be adapted to  obligation change. for that purpose ideality levels are assigned to  situations in the same way as plausibility  levels  are  assigned to situations   in  the context of belief change.  however, it is shown that  there  are deep differences between the evolution of beliefs and the evolution of obligations.
 "," deontc logic, revision, sutuation calculus "
2004,discovering relevancies in very difficult regression problems: applications to sensory data analysis," abstract. learning preferences is a useful tool in application fields like information retrieval, or system configuration. in this paper we show a new application of this machine learning tool, the analysis of sensory data provided by consumer panels. these data sets collect the ratings given by a set of consumers to the quality or the acceptability of market products that are principally appreciated through sensory impressions. the aim is to improve the production processes of food industries. we show how these data sets can not be processed in a useful way by regression methods, since these methods can not deal with some subtleties implicit in the available knowledge. using a collection of real world data sets, we illustrate the benefits of our approach, showing that it is possible to obtain useful models to explain the behavior of consumers where regression methods only predict a constant reaction in all consumers, what is useless and unacceptable. "," machine learning, learning preferences, support vector machines, analysis of sensory data "
2004,miss scarlett in the ballroom with the lead piping," temporal logics of knowledge are useful for reasoning about situations where the knowledge of an agent or component is important, and where change in this knowledge may occur over time. here we use temporal logics of knowledge to reason about the game cluedo. we show how to specify cluedo using temporal logics of knowledge and prove statements about the knowledge of the players using a clausal resolution calculus for this logic. "," knowledge representation and reasoning, temporal representation and reasoning, specification and verification "
2004,towards a logical analysis of biochemical pathways," biochemical pathways or networks are generic representations used to model many different types of complex functional and physical interactions in biological systems. for example, metabolism can be viewed and modeled in terms of complex networks of chemical reactions  catalyzed by enzymes and consisting of reactive chains of substrates and products.  often these models are incomplete. for example,  reactions may be missing  and only some products are observed. in such cases, one would like to reason about incomplete network representations and propose candidate
hypotheses, which when represented as additional reactions, substrates, products, or constraints on such,  would complete the network and provide causal explanations for the existing observations. in this paper, we provide a logical model of biochemical pathways and show how hypothesis generation may be used to provide additional information about incomplete pathways. hypothesis generation is achieved using weakest and strongest necessary conditions for restricted fragments of 1st-order theories which represent these incomplete biochemical pathways and explain observations
about the functional and physical interactions being modeled. quantifier elimination techniques are used to automatically generate these hypotheses. part of the modeling process includes the use of approximate databases where inferences can be made about the resultant hypotheses.the techniques are demonstrated using metabolism and molecular synthesis examples. "," knowledge representation, bioinformatics, abduction "
2004,a new mdl-based function for feature selection for bayesian network classifiers," when constructing a bayesian network classifier from data, the accuracy of the resulting classifier can often be improved upon by
selecting a subset of the available features.  while the minimum
description length (mdl) function is generally accepted as a suitable
function for comparing the qualities of alternative classifiers over a
fixed set of features, we show that it is not suited for the task of
feature subset selection.  we introduce a new mdl-based function,
called mdl-fs, and show that it is better tailored to the task of
identifying and removing redundant features.  we present the results
from experiments in which we compare the performance of the two
functions.  these results demonstrate that, with the mdl-fs function,
classifiers are yielded that have an accuracy comparable to the ones
found with the mdl function, yet include fewer attributes. "," bayesian network classifiers, machine learning, feature selection, minimum description length "
2004,context dependence in multiagent resource allocation," a standard assumption in studies of multiagent resource allocation 
problems is that the value an individual agent places on its
assignment remains unchanged by any redistribution of the remaining
resources among the other agents. this assumption renders impossible
analyses of scenarios where the utility an agent attaches to a
particular set of resources is determined by factors other than the
resource set itself. thus an agent's perception of what its
allocation is worth may be tempered by its view of what other agents
in the system may own, e.g. if working within a coalition a particular
allocation may assume a greater value if other coalition members
hold certain resources. in this paper we develop a model for examining
such context dependent valuations and consider various decision
problems related to the existence of context dependent allocations
satisfying various criteria. "," resource allocation, multi-agent systems, computational complexity "
2004,tractability results for automatic contracting," automated negotiation techniques have received considerable attention
over the past decade, and much progress has been made in developing
negotiation protocols and strategies for software agents. however,
comparatively little effort has been devoted to understanding the
computational complexity of such protocols and strategies. building on
the work of rosenschein, zlotkin, and sandholm, we consider the
complexity of negotiation in a particular class of task-oriented
domains. specifically, we consider scenarios in which agents negotiate
to achieve a redistribution of tasks amongst themselves, where the
tasks involve visiting nodes in a graph. focussing on a particular
representaion of the domain (as a spanning tree), we establish a
number of complexity results pertaining to the complexity of
negotiation in this scenario, with our main result to the effect that
the problem of deciding whether a given deal could be reached by a
chain of rational proposals is tractable. "," negotiation, multiagent systems, computational complexity "
2004,evolution of communication in a genetic based multi-agent system: use wise resources," we studied how communication evolves in a multi-agent
system using genetic based machine learning. this work is an extension of a minimal model of communication which consists in making two agents communicating through a medium of communication and playing a naming game with a limited number of situations to recognize. we complexify that model by increasing both the number of agents within the multi-agent system and the number of words that can be used by agents. we studied how confusion may emerge through communication and how agents use their available ``cognitive'' ressources in order to learn to communicate. we pointed out that further study with new measures is needed in order to better understand how communication evolves in a genetic based multi-agent system.  "," evolution of communication, multi-agent systems, genetic based machine learning, classifier systems "
2004,induction and revision of terminologies," we focus on the problem of the refinement of concept definitions in a terminology expressed in description logics.
generalizing operators proposed in the literature tend to induce overly specific definitions and specializing operators have been poorly investigated. the method presented in this paper, based on multilevel counterfactuals, can be used for inducing new concept descriptions or for refining existing (incorrect) definitions. we formally prove the correctness of the algorithm and discuss its applicability. "," machine learning, description logics, ontologies "
2004,empirical evaluation of the effects of concept complexity on generalization error," in this paper we focus on the relationship between concept
complexity and the generalization error of learned concept descriptions.
after introducing the concept of compressibility, we suggest
how it could be usefully exploited in order to estimate from
the training data the kolmogorov complexity of the concept to be
learned. then, we present an empirical apparatus which allows us
to study the relationship between the estimated target concept complexity
and the generalization error of different learning algorithms.
results show a linear relationship between the two variates: the generalization
error appears to increase as the target concept becomes
more complex. while this is expected, quite interesting is the fact that
the relationship seems to be (only) linear. known theoretical bounds,
in fact, show a super-linear behavior. another interesting finding is that, 
while the degree of correlation changes for different learners, the “linear” relationship
seems not to be affected by the particular learning algorithm. "," machine learning, generalization error, complexity "
2004,automatic induction of domain-related information: learning descriptors type domains," learning in complex contexts often requires pure induction to be
supported by various kinds of meta-information on the domain
itself and/or on its representation. providing such information is
a critical issue for the learning task, and is often in charge of
the human expert. it is also a difficult and error-prone activity,
in which mistakes are highly probable because of a number of
factors. this makes it desirable to develop procedures that can
automatically generate such information starting from the same
observations that are input to the learning process.

this paper focuses on a specific kind of meta-information: the
types used in the description language and their related
domains. indeed, many learning systems known in the
literature are able to exploit (and sometimes require) such a kind
of knowledge to improve their performance. an algorithm is
proposed to automatically identify types from observations, and
detailed examples of its behaviour are given. an evaluation of its
performance in domains with different characteristics is reported,
and its robustness with respect to incomplete observations is
studied. "," logic programming, knowledge representation, machine learning "
2004,appropriateness of the case-based approach in an application domain with multiple conflicts among goals," one of the challenges for the complete automation of a flexible manufacturing cell is the correct sequencing of actions to be performed by each constituent machine. in general, this is due to the large number of problems which may appear during the production of an object. in practice, from a graphic description of an object and a given set of tools, a sequence of actions for a specific cnc tool-machine can be generated by an expert. however, this specification is not only manually created, but also does not observe the problems satisfactorily, bringing the need for a detailed review by an expert. in the light of this a prototype system, which allows satisfactory automation of the production process described above,  has been designed and will be presented in this work. the system reuses previously elaborated cases as a model to get new solutions. systems like this are known as case-based systems and constitutes a research topic within the artificial intelligence field which has received great attention from researchers world-wide, mainly for being potentially applicable in many areas.   "," intelligent automation, case-based reasoning, planning "
2004,interactive task planning through multiple abstraction: application to assistant robotics," humans, in particular non-expert people, are the most important element within the environment of assistant robots. in this kind of applications, robots must possess a symbolic world model in order to plan efficiently the tasks requested by users, which should be specified and supervised in a human-like manner. therefore, the internal world model of the robot must be suitable for human interaction with the task planning process. if a single model is used both for task planning and for human-robot communication, it may be non optimal with respect to both operations. in this paper, a task planning scheme is proposed to face efficiently both problems in assistant robot applications through a multihierarchical world model. "," robotics, task planning, world model, human-robot interaction "
2004,qualitative interpolation for environmental knowledge representation," environmental knowledge representation is concerned with
  representing and reasoning about the surrounding spaces, or
  environments, within which intelligent agents go about their
  businesses. a key facet of this is an understanding of qualitative
  features of the landscape. to understand a landscape is to have
  command of a structured model in which important features are
  highlighted and their mutual interrelations made available for
  inference; in constructing such a model we must acknowledge that our
  cognitive engagement with large-scale environments differs in
  important ways from the understanding of `table-top' spaces which
  has been prevalent in ai.  we advocate an approach in which
  knowledge about objects (e.g., landmarks and paths) in the landscape
  is supplemented by knowledge of spread-out characteristics of the
  terrain, represented by means of spatial fluents or qualitative
  fields. in particular we focus on some problems of qualitative
  interpolation (a form of non-monotonic spatial reasoning) arising in
  the context of this approach.
 "," spatial reasoning, qualitative reasoning, spatial interpolation, knowledge representation, gis "
2004,swarm intelligence: agents for adaptive web search," the large amount of available information on the web makes hard for users to locate resources about their information needs. the conventional search tools do not always successfully cope with this problem and, for this reason, the personalized search systems are receiving increasingly attention, as a well-founded alternative to cope with this problem.
in this paper, we present an adaptive and scalable web search system, based on a multi-agent reactive architecture, which drew inspiration from biological researches on the ant foraging behavior. its target is to search autonomously information on particular topics, in huge hypertextual collections, such as the web, exploiting the outstanding properties of the agent architectures. the algorithm has proven to be robust against environmental alterations and adaptive to user's information need changes, discovering valuable evaluation results from standard web collections. "," agents, topic driven crawlers, world-wide web "
2004,a spatial logic of betweenness," there are several approaches for qualitative spatial reasoning, covering different aspects of the domain. in this paper, i present a region-based spatial logic of the ternary betweenness relation. in one of the possible specializations, the theory can be used to represent and reason about the orientational or positional aspects of the spatial domain in a region-based manner. the expressive power of the developed theory is remarkable when compared to the most common mereological or topological calculi in the literature. nevertheless, the main contribution of this paper is that i provide specific models which are sound and complete for the first-order theory of betweenness developed here. 
 "," spatial reasoning, qualitative reasoning "
2004,melodic similarity: looking for a good abstraction level," computing similarities in sequences of notes is a very general problem
with diverse musical applications ranging from music analysis to
content-based retrieval. choosing the appropriate level of
representation is a crucial issue and depends on the type of
application. our research interest concerns developing a cbr system
for expressive music processing. in that context, a well chosen
distance measure for melodies is a crucial issue. in this paper we
propose a new melodic similarity measure based on the i/r model for
melodic structure and compare it with other existing measures. the
experimentation shows that the proposed measure provides a good
compromise between discriminatory power and the level of abstraction
of melody representation.
 ", art and music 
2004,model-based monitoring of software components," the development of component-based software systems opens the possibility of using model-based diagnosis techniques for software systems. we have developed a simple monitoring system based on the modeling of the external behavior of software components by petri nets. with each component is associated a local controller which observes the messages received and sent by the component and compares them with the specified behavior. as the components interact, information is collected on error emission and time constraint violation to infer indicators about the state of  components. "," model-based reasoning, diagnosis, software components, supervision, petri nets "
2004,dynamic skeleton-based wayfinding," in this paper, a system is proposed in which case-based reasoning principles are applied to the wayfinding problem for computer games. case-based reasoning is thought to mimic aspects of human thinking, and it is hoped that by applying this methodology to wayfinding in computer games, more convincing non-player character behaviour can be simulated. additionally, case-based reasoning allows for re-use of knowledge, and performance gains could be achieved through a reduction in redundant computation. this approach incorporates aspects of island search, case-based reasoning and traditional wayfinding techniques. agents store a skeleton set of known paths and adapt these skeleton paths to solve new wayfinding problems. solutions are then incorporated back into the case base to create a dynamic, evolving case-base of skeleton paths.
 "," wayfinding, case-based reasoning, dynamic skeleton set, computer games "
2004,using multiple relaxations in temporal planning," crikey is a planner that separates out the temporal and logical reasoning in temporal planning. this can be seen as a relaxation of the temporal information during the classical planning phase. relaxations in planning are used to guide the search. however, the quality of the relaxation greatly affects the performance of the planner, and in some cases can lead the search into a dead end. this can happen whilst separating out the planning and scheduling problems, leading to the production of an unschedulable plan. whilst planning, crikey can detect these cases and change the relaxation accordingly. this paper describes how it does this and presents some domains where this reasoning is required. "," temporal planning, planning, scheduling, relaxation, separation of problems, pddl2.1 "
2004,improving heuristics through search," we investigate two methods of using limited search to improve admissible heuristics for planning, similar to pattern databases and pattern searches. since one method involves searching in an and/or graph, we also develop a new algorithm for searching such graphs. results indicate that these methods are cost-effective for some kinds of hard planning problems.
 "," planning, heuristic search "
2004,utilizing structured representations in conformant probabilistic planning," a csp based algorithm for the conformant probabilistic planning
  problem (cpp) has been presented by hyafil & bacchus. although
  their algorithm displayed some interesting potential when compared
  with traditional pomdp algorithms, it was developed using a ``flat''
  representation. in this work we revisit this algorithm and develop a
  version that utilizes a structured representation. the structured
  representation can be exponentially more efficient than the flat
  representation when dealing with the structured problems that are
  typical in ai. our new structured version of their algorithm allows
  us to make the main contribution of this work, which is to
  demonstrate that the csp approach to cpp can in fact be much more
  effective than traditional pomdp algorithms for an interesting range
  of problems. this is contrary to previously presented results, and
  makes the application of csp techniques to decision theoretic
  planning a more promising area for future work. "," probabilistic planning, decision theoretic planning, constraint satisfaction problems, structured representations "
2004,carsim: a system to simulate road accidents in a 3d environment from written descriptions," this paper describes a system to create animated 3d scenes of car
accidents from written reports. reports are narratives
written after the accident by one of the drivers or by an accident analyst. the
text-to-scene conversion process consists of two stages. an
information extraction module creates a tabular description of the
accident and a visual simulator generates and animates the scene.

we first describe the overall structure of the text-to-scene
conversion and the template structure. we then explain the
information extraction and visualization modules. we show snapshots of the car animation output and we conclude with the results we obtained. "," text-to-scene conversion, information extraction, visualization "
2004,parameter estimation in large causal independence models," the assessment of a probability distribution associated with a
bayesian network is a challenging task, even if its topology is
sparse. special probability distributions based on the notion of
causal independence have therefore been proposed, as these allow
defining a joint probability distribution in terms of boolean
combinations of local distributions. however, for very large networks
even this approach becomes infeasible: in bayesian networks which need
to model a large number of interactions among causal mechanisms, such
as in fields like genetics or immunology, it is necessary to further
reduce the number of parameters that need to be assessed.  similar
arguments apply to the modelling of temporal processes, which also
involves large numbers of interactions among variables. in this paper,
we propose using equivalence classes of binomial distributions as a
means to define very large bayesian networks.  we analyse the
behaviours obtained by using different symmetric boolean functions
with these probability distributions as a means to model joint
interactions. some surprisingly complicated behaviours are obtained in
this fashion, and their intuitive basis is examined. "," bayesian networks, causal independence, parameter estimation, knowledge representation, probability theory "
2004,temporal plan interdependencies: illustrating various plan components," the need to improve the quality of health care while at the same time controlling or reducing its cost leads to a strong demand for clinical protocols and guidelines. several (semi-) formal languages to represent such guidelines exist. however, to translate clinical guidelines into such a representation is a burdensome and time-consuming task. we aim to support that knowledge acquisition task by providing methods and tools, which focus on the process-oriented nature of such guidelines. in this paper, we provide a systematic analysis of the various plan components and their temporal interdependences (written in asbru) to infer heuristics for our methods and tools. we examine the temporal issues, in particular the flows whose temporal information are available in different ways. i.e. the expressions can be both metric and qualitative nature or dependencies between individual flows can be represented by point or interval relations. based on these several formats we develop different forms of representations to display the flows in asbru. by means of these definitions the modeling process will be simplified and therefore can help saving developing time and costs. "," skeletal planning, temporal representation, process modeling, knowledge-based systems "
2004,a troubleshooting approach with dependent actions," man-made systems often break, and it is highly desired that they are fixed as fast and as cheap as possible. however, achieving this optimally is a very complex problem if there is action dependency, that some actions may fix the same fault. when this is the case, ordering of ordinary efficiencies fails to guarantee an optimal action sequence. thus, it is aimed to find a heuristic method that adjusts action efficiencies for better results. for this end, dependency sets are defined, and it is shown that these sets are what complicate the problem and that an optimal sequence is achievable once they are solved optimally. when adjusting efficiencies, the value in failing an action is utilized, which is modeled as the increase in the weighted efficiencies of remaining actions. the weight for each action is assumed to be its scaled efficiency, which is found by normalizing all efficiencies. this approach is implemented in the 1-sea troubleshooter, and its performance is tested on the basis of expected cost of repair in 14 problems and compared to two other troubleshooters from the literature. the results indicate that the approach is very effective and time-efficient, and promise successful practices in complex systems.
 ", 
2004,reliability and typicalness for estimating confidence level of individual classifications ," in the past decades machine learning algorithms have been
successfully used in many problems, and are emerging as valuable data
analysis tools. however, for a serious practical use, their serious
impediment is, that more often than not, they cannot produce good
(unbiased) assessments of their predictions' quality.

in last years, several approaches for estimating reliability or
confidence of individual classifiers have emerged, many of them
building upon algorithmic theory of randomness, such as (historically
ordered) (1) transduction-based confidence estimation, (2)
typicalness-based confidence estimation, and (3) transductive
reliability estimation. however, all of these approaches have
weaknesses, (1) and (3) are tightly bound with particular
classifiers, for (2) the interpretation of reliability estimations is
not consistent with statistical confidence levels.

in the paper we propose a joint approach that compensates the
described weaknesses by integrating typicalness-based confidence
estimation and transductive reliability estimation in a joint
confidence machine.

by our approach the machine produces true confidence values (in a
usual statistical sense, e.g., a confidence level of 95% means that
in 95% the predicted class is also a true class), as well as provides
us with a general approach that can be used with (almost) any
underlying classifier

we perform a series of tests with several different machine learning
algorithms in several problem domains. our experiments show that the
proposed approach integrates the ""best of both worlds"" in terms of
discriminating performance, generality and comprehensiveness.
 "," confidence, reliability, transduction "
2004,qualitative reasoning feeding back into quantitative model-based tracking," tracking vehicles in image sequences of innercity road traffic
scenes still must be considered to constitute a challenging task.
even if a-priori knowledge about the 3d form of vehicles, of the
background structure, and about vehicle motion is provided,
(partial) occlusion and dense vehicle queues easily can cause
initialisation and tracking failures. a stepwise improvement
of the tracking approach implies numerous and time-consuming
experiments. these difficulties can be eased considerably by
endowing the system with -- at least part of the -- qualitative
knowledge which a human observer activates in order to judge the
results. in the case to be reported here, a system for qualitative
reasoning has been coupled with a quantiative model-based
tracking system in order to explore the feedback from qualitative
reasoning into the geometric tracking subsystem. the
approach and encouraging experimental results obtained for
real-world image sequences are described. "," cognitive vision, model-based tracking, qualitative reasoning "
2004,consistency is consistency is consistency," qualitative spatial and temporal reasoning problems are usually expressed in terms of constraint satisfaction problems, with determining consistency as the main reasoning problem. because of the high complexity of determining consistency, several notions of local consistency, such as path-consistency, k-consistency and corresponding algorithms have been introduced in the constraint community and adopted for qualitative spatial and temporal reasoning. since most of these notions of local consistency are equivalent for allen's interval algebra, the first and best known calculus of this kind, it is believed by many that these notions are equivalent in general---which they are not! in this paper we point out some of these common consistency myths and give counter-examples to all of them. we discuss the differences between the various notions and argue that only one of them, namely path-consistency, is feasible for deciding consistency. we further give a general necessary and sufficient condition for characterizing the cases where consistency can be decided by enforcing path-consistency.  "," spatial reasoning, temporal reasoning, consistency, path-consistency "
2004,a critical-shaking neighborhood search for the yard allocation problem," the yard allocation problem (yap) is a real-life resource allocation problem faced by the port of singapore authority (psa). to overcome its np-hardness practically, we propose an effective heuristic procedure, named critical-shaking neighborhood search, to solve the problem. extensive experiments show that the new method can produce solutions with higher qualities in a much shorter time, as compared with other heuristics proposed in the literature. further to this, it also improves or at least achieves the best solutions to all the benchmark instances of the problem. "," port logistics, planning, scheduling, packing, heuristics, automation "
2004,qualitative modelling of kinematic robots for fault detection and diagnosis," this study presents an approach, the unit circle (uc), to qualitative representation of manipulators. a manipulator is described as a collection of constraints holding among time-varying, interval-valued parameters. the uc representation is presented, and the continuous motion of the end-effector is evaluated by the change of directions of qualitative angle and qualitative length. analytical formulas of qualitative velocity and qualitative acceleration are derived. the characteristic mapping is introduced for fault detection and diagnosis in terms of the uc. in the end simulation results demonstrate the feasibility of the uc approach for fault diagnosis.

the uc representation of robots concerns a global assessment of the systems behaviour, and it might be used for the purpose of monitoring, diagnosis, and explanation of physical systems. this is the first step to fault diagnosis and remediation for beagle 2 using qualitative methods.
 "," qualitative modelling, fault diagnosis, robotics "
2004,modelling the interpretation of novel compounds," the understanding of novel compounds is a special case in which we can explore the deep generativity of natural language understanding.  we report a model, called punc, that captures the comprehension and interpretation of novel noun-noun compounds.  we show how the model constructs multiple interpretations for a given novel compound, and ranks these interpretations for their overall acceptability using the key constraint variables of diagnosticity, plausibility and informativeness.  we discuss how each variable contributes to the interpretations produced, and to the calculation of an interpretation’s acceptability.  we then present a sensitivity analysis of the model’s key variables, demonstrating a graceful degradation in performance as the weightings to these variables are altered. "," cognitive model, conceptual combination, sensitivity analysis "
2004,role swapping in multi-agent sensor webs," this paper reports on the application of multi-agent systems to the design of autonomic sensor networks.  in advance of the current hardware/software capabilities of these sensor networks, we simulate an autonomic sensing network handling the detection of a contaminant, in a hostile environment, where the effective life of the network has to be optimised (through power handling). by casting each sensor as an agent that can swap its role (produce or integrator, both of which have different power usages) when the conditions in the network change we show that networks using role-swapping, as opposed to fixed roles, can be much more effective over time. this emergent behaviour, that arises from economical local interactions, also becomes more effective as the environment for the network becomes progressively harsher. "," multi-agent systems, sensor web, autonomous agents, distributed ai "
2004,embodied conversational agent and influences :  towards individualized agents," in view of creating embodied conversational agent (eca) able to
display different behaviors depending on factors such as context
environment, personality and culture, we propose a taxonomy and a
computational model of the influences these factors may induce.
influences act not only on the type of the signals an agent
conveys but also on the expressivity of the signals. thus, to
individualize ecas, we consider not only the influences acting on
the agent but also the notion of expressivity. expressivities
arise at various levels of the agent representation (meaning,
signal, behavior). in our ecas architecture, the system takes an
input text annotated with multi-modal communicative information
tags. considering the different expressivities representing the
agent and the influences, the system instantiates the
communicative tags into a sequence of \emph{expressive} signals.
so for a same input text, our system associates to a given meaning
different signals across modalities (face, gesture, posture,
gaze), depending on agents' characteristics. in this paper, we
also introduce a model of signals classification for a given
meaning, based on expressivity, and a model of signals modulation. ", autonomous agent 
2004,hig-level observations in java debugging ,"  while there have been considerable developments in modeling
  techniques for automatically locating faults in programs recently,
  user interaction was mostly left aside. in particular, the problem
  of difficult to answer queries when debugging interactively has not
  been addressed. this work extends the standard entropy-based
  measurement selection algorithm proposed by de kleer to deal
  with high-level observations about the intended behavior of java
  programs, specific to a set of test cases.  we show how to
  incorporate the approach into previously developed model-based
  debugging frameworks and to what extent reasoning about high-level
  properties of programs can improve diagnostic results. "," diagnosis, debugging, model-based reasoning "
2004,a skeleton based method for efficient 3d object localization: application to teleoperation  ," our aim is to develop a vision system for teleoperation to localize an object. this system has to be used through internet connection. the recognition problem addressed in this paper is to localize a 3d free-form object from a single 2d view of 3d scene. using a skeletonization process allows to obtain two graphs, the first one representing an object in the scene (2d skeleton) and the second one representing a database object (3d homotopic skeleton). the method encodes geometric and topological information in the form of a skeletal graph and uses graph isomorphism techniques to match the skeletons and find the one-to-one correspondences of nodes in order to estimate the object’s pose. knowing skeleton is a set of lines centred within the 3d/2d objects,  our method transforms the problem of free form object localization into points and lines pose estimation. some experimental results on real images demonstrate the robustness of the proposed method with regard to occlusion, cluster, shadows … "," 3d free form object localization, 2d & 3d skeletonization, graph & sub-graph matching, teleoperation "
2004,mapping clinical guidelines representation primitives to decision theory concepts," supporting therapy selection is a fundamental task for a system for computerized management of clinical guidelines (gl). to this hand, decision theory issues could provide significant advances. in this paper, we propose a systematic analysis of the main gl representation primitives, and of how they could be related to decision theory issues. the knowledge representation contribution we provide can be seen as a basis for implementing a decision support tool within any of the systems described in the literature: as a matter of fact, at a sufficiently abstract level, the gl primitives we treat are shared by all of the systems. such a tool could be adopted when executing a gl on a single patient (in clinical practice), and for simulation purposes. in particular, a decision theory tool based on this analysis is being implemented in the system glare.
 "," knowledge representation, clinical guidelines, decision support, decision theory "
2004,ipss: an hybrid reasoner for planning and scheduling," most complex problem solving tasks are composed of several  subproblems that require specialized techniques to the kind of problems to be solved. however, building integrated architectures with specialized components, such that each one can handle a specific kind of subproblems, is a difficult task. as an example, recently the fields of ai planning and scheduling have witnessed a big interest on the integration of techniques from both areas in order to solve complex problems. these problems require the reasoning on which 
actions to be performed as well as their precedence constrains (planning) in combination with the reasoning with respect to the time at which those  actions should be executed and the resources they use.  in this paper we  describe ipss (integrated planning and scheduling system) a domain  independent reasoner that integrates ai planning and constraint satisfaction (cs) by separating both reasoning tasks. thanks to the use of these techniques, ipss is able to reason about precedence constraints, time  (deadline, time windows, etc) and resource usage/consumption. experimental results show that the separation of the planning and scheduling reasoning
can enable it to outperform state-of-the-art planners with time and
resources reasoning capabilities.
 "," temporal and resources reasoning, constraint satisfaction, planning and scheduling "
2004,robel : synthesizing and controlling robust robot behaviors," we present the robel supervision system which is able to
learn from experience robust ways of performing high level tasks
(such as ""navigate to""). each possible way to perform the task is modeled as a hierarchical tasks network (htn), called modality whose primitives are sensory-motor functions.  an htn planning process synthesizes all the consistent modalities to achieve a task. the relationship between supervision states and the appropriate modality is learned through experience as a markov decision process (mdp) which provides a general policy for the task.  this mdp is independent of the environment and characterizes the robot abilities for the task. "," robotics, supervision, learning, robustness, planning, navigation, markov decision process, hierarchical task network "
2004,integrating local-search advice into refinement search (or not)," recent work has shown the promise in using local-search ""probes"" as a basis for directing a backtracking-based refinement search. in this approach, the decision about the next refinement step is based on an interposed phase of constructing a complete (but not necessarily feasible) variable assignment. this assignment is then used to decide on which refinement to take, i.e., as a kind of variable- and value-ordering strategy.

in this paper, we investigate the efficacy of this hybrid search approach in the combinatorial domain of job shop scheduling. first, we evaluate methods for improving probe-based guidance, by basing refinement decisions not only on the final assignment of the probe-construction phase but also on information gathered during the probe-construction process. we show that such techniques can result in a significant performance boost.

second, we consider the relative strengths of probe-based search control and search control that is biased by more classically motivated variable- and value-ordering heuristics (incorporating domain-specific knowledge). our results indicate that - while probe-based search performs better than an uninformed search - use of domain-specific knowledge proves to be a much more effective basis for search control than information about constraint interactions that is gained by local-search probes, and leads to substantially better performance. "," constraint satisfaction, constraint programming, meta-heuristics, scheduling, search, automated reasoning "
2004,identifying relational concept lexicalisations by using general linguistic knowledge," the success of the ontological approach in the semantic web is strictly related to the possibility of applying in large natural language semantic models. web documents are first of all documents and the activity of making explicit their content through the ontological language, i.e. extracting concept instances and their relationships, is fairly similar to the classical information extraction task. however, to apply information extraction models, semantic web ontologies need to be equipped with a sort of ""linguistic interface"" representing the one-to-many mappings between coarse-grained relational concepts of the ontology and the corresponding linguistic realisations.
with an eye on the problem of constructing coarse-grained relational concept catalogues, this paper analyses the extent and nature of the general semantic knowledge required for this task analysing how different general-purpose classification algorithms react to the use of this knowledge. 
for exploiting ambiguous semantic information within the feature vector model, we propose an original model, the semantic fingerprints. "," information extraction, ontology learning, machine learning "
2004,"adaptive, multilingual named entity recognition in web pages"," the identification of interesting web sites and web pages and
the extraction of information from them is an interesting but
complex task. most of the information on the web today is in
the form of html documents, which are designed for presentation
purposes and not for machine understanding and reasoning.
the extraction task becomes even harder in a multilingual context,
where web pages in different languages need to be analysed.
the majority of existing systems needs to be manually configured
for new domains, a process that requires substantial effort
and time. this paper presents an adaptive, multilingual named
entity recognition and classification (nerc) technology, which
can be easily customised to new domains and to new languages.
our evaluation results demonstrate the viability of our approach. "," information extraction, named entity recognition, machine learning, multilinguality "
2004,extending defeasible logic and defeasible prolog," defeasible logic promotes enthymemic, argumentative reasoning on incomplete set of premisses that are retracted on the presence of contrary information. defeasible prolog (d-prolog) is a prolog metainterpreter designed by donald nute to implement nonmonotonic inference based on a system of defeasible logic. in this paper, it is shown how to give proof conditions for the 'even-if' conditions of defeasible logic. this is done by allowing the pre-emption of defeaters, in other words by preventing some rules from rebutting other, more specific rules. these proof conditions are implemented to d-prolog. computational results are presented for the given examples, and fundamental goals of defeasible reasoning are assessed. "," logic programming, nonmonotonic reasoning "
2004,context aware personalised service delivery," in this paper we explore the potential of recommendation
systems in an environment where users can access a variety of services from different locations. in particular we explore the use of adaptive technologies to recommend new services to users based on context, which we define in terms of four factors: time, location, user behaviour and user profile. a multi-agent service delivery architecture is used as a platform for the recommendation system. user behaviour in the service environment is analysed and used to predict the next service that a user will invoke; the user's preferences and past behaviour are then utilised to recommend specific aspects of that service to the user. "," personalisation, agents, learning, semantic web, mobile "
2004,cooperation based on communication: an approach for an autonomous driving system," abstract
autonomous driving in real road traffic is still a unsolved challenge. while sensor systems and image processing certainly make up a great part of the needed system, intelligent algorithms for the actual driving behaviour must likewise be developed. communication and cooperation between different vehicles on the road can be a great help for achieving such a system. in the cartalk2000 project, a new system for inter-vehicle communication is introduced. this system, which allows the building of wireless ad-hoc networks between all equipped vehicles on the road, can be very efficiently used for drive assistant systems as well as fully autonomous driving systems. the following paper describes the cooperative driving system developed during the project, which enables the autonomous accomplishment of typical freeway situations like lane merging, overtaking up to fully autonomous freeway driving.
 "," autonomous agents, multi-agent systems, autonomous driving "
2004,evaluating global adequacy in fuzzy classifications," the study of hybrid connectives linearly compensated h=ëc + (1-ë)c* , where c is a t-norm and c* represents the dual connective of c, to define aggregation operators for fuzzy classifications is a key point not only in fuzzy sets theory but also in learning processes. although these operators are not associative, the fact that they can be decomposed into associative functions, give rise easily to n-ary aggregation functions by straightforward iteration. among the most used t-norms there are those of frank's family, which are simultaneously t-norms and copulas. the purpose of this paper is to give a characterization of the hybrid connective h, via the properties of the connective c. necessary and sufficient conditions on h that define c as a copula are given. the characterized hybrid connectives h are used to compute global adequacy degree of an object in a class from marginal adequacy degrees in a learning system. "," hybrid connectives, reasoning under uncertainty, machine learning, classification algorithms, qualitative reasoning "
2004,equilibrium strategies for task allocation in dynamic multi-agent system," in this paper we address a model of self interested agents competing over performing tasks. the agents are situated in an uncertain environment while different types of tasks are dynamically arriving from a central manager. the agents differ in their skills to perform a task under different world states. in such environments, previous models concerning cooperative agents aiming for a joint goal are not applicable, as self interested agents has a motivation to deviate from the joint allocation strategy in order to increase their private benefits. a stable solution, is a set of strategies, derived from equilibrium where no agent can benefit from changing its strategy given the other agents' strategies and the allocation protocol set by the central manager. specifically we focus on a protocol in which upon arrival of a new task, the central manager starts a reverse auction among the agents, and the agent who bids the lowest payment wins. we introduce the model, formulate its equations and suggest equilibrium strategy for the agents. identifying some specific characteristics of the equilibria, we manage to suggest an efficient algorithm for enhancing the agents' equilibrium strategies calculation. comparison to the central allocation mechanism, and the effect of environmental settings over the perceived equilibrium are given through simulation. "," multi-agent systems, autonomous agents, distributed ai "
2004,diagnosis of discrete-event systems using bdds," we improve the efficiency of sampath's diagnoser approach
by exploiting compact symbolic representations of the
system and diagnoser in terms of binary decision diagrams.
we present an algorithm for synthesising the symbolic diagnoser
with promising results on test cases derived from a telecommunication
application.
 "," failure diagnosis, discrete-event systems, diagnoser, symbolic representations, binary decision diagrams "
2004,concurrent planning by decomposition," planning is known to be a difficult task. one of the approaches that can be used to reduce this difficulty is problem decomposition. this paper introduces a new decomposition technique for planning problems in strips domains which is based on the idea of landmark, that is, literals that must be true in any solution plan. landmarks are ordered and grouped in different sequential sets named intermediate goals (ig), being each ig a sub-goal to solve. then, we build the initial state (is) corresponding to each ig, such that each pair (is, ig) is viewed as an independent problem. this way a multiprocessor system can be used to solve these problems concurrently. "," planning, problem decomposition "
2004,an intrinsic information content metric for semantic similarity in wordnet," information content (ic) is an important dimension of word knowledge when assessing the similarity of two terms or word senses. the conventional way of measuring the ic of word senses is to combine knowledge of their hierarchical structure from an ontology like wordnet with statistics on their actual usage in text as derived from a large corpus (e.g., [15]). in this paper we present a wholly instrinsic measure of ic that relies on hierarchical structure alone. we report that this measure is consequently easier to calculate,
yet when used as the basis of a similarity mechanism it yields judgments that correlate more closely with human assessments than other, extrinsic measures of ic that additionally employ corpus analysis. we report a resulting correlation value of 0.84 between human and machine similarity judgments on the dataset of miller and charles [13], which is suggestively close to the upper-bound of 0.88 postulated by resnik in [16]. "," semantic similarity, natural language processing, wordnet "
2004,perception based cognitive diagnosis," the development of intelligent educational systems faces the challenging problem of cognitive diagnosis. this necessitates the development of approaches for analyzing user performance and inferring cognitive states.  we will focus on the transformation of model-based technical diagnosis into model-based cognitive analysis, which is intrinsically interactive. the purpose of cognitive analysis, in contrast to device diagnosis, leads to development of novel approaches to interpreting student performance. we support the view that zadeh’s computational theory of perception compliments qualitative methodologies by providing an additional level of information granulation and a computational inference engine. the computational theory of perception is considered complimentary to qualitative methods when processing and reasoning with perception-based information, and this perspective will allow us to reformulate performance analysis. 

the adopted view throughout this article is that concepts in any domain, as well as their interrelations, are better communicated through a variety of models, each providing partial definition or exemplification from a different perspective. the problem-solving knowledge involving each concept will be described with a set of models arranged in a multi-model space along various modelling dimensions.  the representation techniques underlying these models will be described in the paper. "," computer-aided learning, cognitive diagnosis, perception-based reasoning, model-based reasoning, knowledge representation "
2004,kernel machine based feature extraction algorithms for regression problems," in this paper we consider two novel kernel machine based feature extraction algorithms in a regression settings. the first method is derived based on the principles underlying the recently introduced maximum margin discimination analysis (mmda) algorithm. however, here it is shown that the orthogonalization principle employed by the original mmda algorithm can be motivated using the well-known ambiguity decomposition, thus providing a firm ground for the good performance of the algorithm. the second algorithm combines kernel machines with average derivative estimation and is derived from the assumption that the true regressor function depends only on a subspace of the original input space. 
the proposed algorithms are evaluated in preliminary experiments conducted with artificial and real datasets. "," feature extraction, ensemble learning, kernel methods, support vector machine "
2004,variants of a* for planning," significant advances have occurred in plan synthesis in the last
nine years. many of the recently developed efficient planners use
a variation of a* search algorithm called weighted a*. in this paper we report on three sound and complete classical planners awa*-pd, awa*-ac and awa*-ac-le. awa*-ac is adjusted weighted a* with action conflict-based adjustment. awa*-pd is adjusted weighted a* with deleted preconditions-based adjustment. awa*-ac-le is a variant of awa*-ac which performs lazy evaluation (le). the novel ideas in these planners are (i) node-dependent weighting for g(n) and h(n) terms in the path cost equation of a*, (ii) conditional two-phase heuristic evaluation, and (iii) lazy heuristic evaluation which does not construct relaxed plans to compute heuristic values for all nodes. we report on an empirical evaluation of the planners on several challenging new domains containing dead ends. we also report on an empirical comparison of the planners with planners hsp-2 and ff. the variants of a* solve several problems from these domains faster than hsp-2 and ff.
 ", planning 
2004,configuration of web-services as parametric design," the configuration of web-services is particularly hard given the noisy, unreliable and open nature of the web. furthermore, such composite web services are likely to be complex services, that will require adapation for each specific use.
current approaches to web-service configuration are almost exclusively 
based on pre/post-condition-style reasoning, resulting in a
planning-style approach to service configuration, configuring a
composite web service ``from scratch'' every time.
in this paper, we propose instead a knowledge intensive
brokering approach to the creation of composite web-services.
in our approach, we
(i) identify a specific vocabulary of web service categories to be
used in a specific service configuration task;
(ii) we use fixed process model to combine these service categories;
this fixed process model can be re-used between multiple
configurations of similar composite services;
(iii) we exploit detailed knowledge about how to configure such 
service categories to obtain the required composite web service.
we illustrate our proposal by applying it to a specific
family of web-services, namely ``classification services'',
and we describe a specific implementation and execution of our approach. 
 "," web services, semantic web, design, configuration, reuse of knowledge, knowledge-based systems "
2004,a spanish-catalan translator using statistical methods," the development of a spanish-catalan statistical machine
translation system has been described. this approach tries to
solve the problem using a pure inductive method, without using
linguistic knowledge. to obtain the translator we follow the next
steeps: first, we obtain a bilingual corpus from internet. second,
we fragment the corpus into units (sentences and tokens). third,
we align the sentences from the two different languages. then, we
use the aligned corpus to train statistical models. finally, we
use these models to translate. that is, given a source sentence,
we search the most probable target sentence. we have compared our
translator with the most used spanish-catalan translators and we
have obtained similar translation results than the other
commercial system. it is accessible at
http://ttt.gan.upv.es/~jtomas/trad. "," machine translation, statistical pattern recognition, human language technology  "
2004,a qualitative reprsentation of trajectory pairs a qualitative reprsentation of trajectory pairs a qualitative representation of trajectory pairs ," this paper presents a theory for reasoning about movements of objects in a qualitative framework, by relating the movement of objects with respect to one another. the most widely researched area of qualitative spatial reasoning is mereotopology, for example the region connection calculus (rcc). the relations in rcc can be connected in order to construct a conceptual neighborhood diagram which describes the continuous transitions between these spatial relations. however no distinctions are made between disconnected objects, which results in a lack of expressiveness in domains and applications where most objects are disconnected, such as mobile physical objects. although there are qualitative calculi for orientation, there is no well developed mechanism for describing trajectories of mobile objects. in this paper we present a qualitative spatio-temporal representation for moving objects based on describing their trajectories relative to other objects. a conceptual neighborhood diagram shows how the basic trajectories may be composed over time assuming continuous motion. we illustrate the applicability and naturalness of the representation with an example. "," qualitative reasoning, spatial reasoning, temporal reasoning "
2004,defining equivalence classes for the elicitation of probability constraints for bayesian networks," among the tasks involved in building a bayesian network for a
real-life application, the task of eliciting all probabilities
required is generally considered the most daunting.  this task can be
supported by the construction of a qualitative network as an
intermediate model of the domain under study.  such a network
specifies qualitative features of the probability distribution to be
represented, which can be taken as constraints on the probabilities
for the bayesian network.  in this paper, we analyse the possible
combinations of features, resulting in a small number of equivalence
classes.  based upon these classes, we present a method for eliciting
the qualitative features of a domain's probability distribution.  we
report on an initial study of the use of our method in the domain of
neonatology. "," bayesian networks, probabilistic relationships, knowledge acquisition "
2004,postponing branching decisions," solution techniques for constraint satisfaction and optimisation problems often make use of backtrack search methods, exploiting variable and value ordering heuristics. in this paper, we propose and analyse a very simple method to apply in case the value ordering heuristic produces ties: postponing the branching decision. to this end, we group together values in a tie, branch on this sub-domain, and defer the decision among them to lower levels of the search tree. we show theoretically and experimentally that this simple modification can dramatically improve the efficiency of the search strategy. although in practise similar methods may have been applied already, to our knowledge, no empirical or theoretical study has been proposed in the literature to identify when and to what extent this strategy should be used. "," search, constraint programming "
2004,contextualized abstraction for assertion-level theorem proving," in this paper we propose a context-based approach to abstract theorem 
proving. the challenges stem from the need to identify an abstract 
level for theorem proving where (less important) information can be 
temporarily ignored so that a (plan for a) proof of the abstracted 
problem can be devised to guide the (re)construction of the 
object-level proof. contextualization is realized by preserving the 
logical structures of the formulas of the original representation 
while pushing the less important subformulas, according to a relevance 
relation, into the hierarchical subcontexts. this representation 
allows the problem to be gradually unfolded during the proof search 
process by hierarchically exploring the subcontexts required to 
provide support for the hypotheses used in the proof plan. the 
underlying inference machinery is also equipped with an assertion
application module which allows mathematical assertions such as 
axioms, definitions, theorems, and even global and local assumptions 
to be applied directly to a proof situation to obtain their logical 
consequences (from the applied proof situation) and fill in the gaps opened up by an abstract-level proof step.
 "," proof planning, knowledge representation, theorem proving, meta-heuristics for ai "
2004,automatic recognition of famous artists by machine," the paper addresses the question whether it is possible for a machine
to learn to distinguish and recognise famous musicians (concert
pianists), based on their style of playing. to that end, we extract a
number of low-level features related to expressive timing and
dynamics from the original audio cd recordings by famous pianists,
and apply various machine learning algorithms to the task of learning
classifiers based on these features. experiments show that the
computer can learn to identify the performer in a new recording with a
probability significantly higher than chance, despite the fact that
the features only capture a very limited amount of information about
performance style. some pianists exhibit a particularly high degree of
recognisability.
an analysis of the learned classifiers reveals a number of
performance features that seem particularly relevant to style
differentiation, and an application of the classifiers to
performances of music of a very different style shows that the machine 
seems to have captured truly fundamental aspects of artistic style.
one limitation of the current approach is that sequential information
is totally ignored in the learning process, and we briefly report on
ongoing work that tries to address this problem via an interesting
conversion of music performances to strings. "," art and music, machine learning "
2004,soft constraints with partially ordered preferences," representing and reasoning with an agent's preferences is important
in many applications of constraints formalisms. such preferences are often only partially ordered. one class of soft constraints formalisms, semiring-based csps, allows a partially ordered set of preference degrees, but this set must form a distributive lattice; whilst this is convenient computationally, it restricts the representational power. this paper constructs a logic of soft constraints where it is only assumed that the set of preference degrees is a partially ordered set, with a maximum element 1 and a minimum element 0. when the partially ordered set is a distributive lattice, this reduces to the idempotent semiring-based csp approach, and the lattice operations can be used to define a sound and complete proof theory. this case can also be viewed as a lattice-valued possibilistic logic. for a general partially ordered set of preference degrees, we show how the machinery that exists for the distributive lattice case can be used to perform sound and complete deduction, using a particular embedding of the partially ordered set in a distributive lattice. "," reasoning about preferences, constraint satisfaction, soft constraints, possibilistic logic, qualitative reasoning   "
2004,an ant colony genetic algorithm," in this paper, a hybrid genetic algorithm called ant colony genetic algorithm is presented. the initial population of the method is generated from the each subspace which is divided from the feasible solution space of the optimization problem evenly. and every subspace is marked by initial pheromone. during the genetic operation, selection operator is under the effect of subspace’s pheromone remaining. because of the initial population of points that are scattered uniformly over the feasible solution space, so that the algorithm can evenly scan the feasible solution space once to located good points for further exploration in subsequent operator. in addition, the effect of pheromone of each subspace in the selection can improve the speed of convergence rate of algorithm. "," hybrid genetic algorithm, ant colony system, pheromone remaining "
2004,ngp: numerical graph planning," most of the real world problems involve numerical handling. new planning systems such as ff-metric and sapa are able to handle numerical resources, but in most cases the resources are only used as auxiliary numerical constraints added to the symbolic planning domain. the extension to mips to solve numerical domains uses as for ff-metric and sapa two different types of heuristic one for the propositional part of the planning problem and another one for its numerical information. in this paper we present a new planning system ngp (numerical graph planning) that is able to solve totally numerical and/or symbolic planning domains. we propose a new action representation to support numerical conditions and effects, where we allow a non-restricted function application for numerical update. ngp guides its search using a heuristic derived from a planning graph construction. in ngp we instantiate symbolic actions parameters from the outset for only one time during the planning process, but we progressively instantiate the numerical actions parameters every time a new graph layer is reached. ngp explicitly represents the possible range of numerical parameters for the numerical functions of the domain. "," planning, heuristic search "
2004,topological localization using decentralized concept lattices," this paper presents a new decentralized method for selecting visual landmarks in a structured environment. different images, issued from the different places, are analyzed, and primitives are extracted to determine whether there is presence or not of features in the images. subsequently, landmarks are selected as a combination of these features with a mathematical formalism called galois -or concept- lattices. a general approach developped formely is here improved and algorithms polished, but the main drawback of such an approach is the exponential complexity of lattice building algorithms. a decentralized approach is therefore defined and detailed in this paper~: it leads to smaller lattices, and thus to better performance as well as an improved legibility. "," landmarks, vision, localization, topological, concept, lattice "
2006,background default knowledge and causality ascriptions,"a model is defined that predicts an agent's ascriptions of causality (and related notions of facilitation and justification) between two events in a chain, based on background knowledge about the normal course of the world. background knowledge is represented by nonmonotonic consequence relations. this enables the model to handle situations of poor information, where background knowledge is not accurate enough to be represented in, e.g., structural equations. tentative properties of causality ascriptions are explored, i.e., preference for abnormal factors, transitivity, coherence with logical entailment, and stability with respect to disjunction and conjunction. empirical data are reported to support the psychological plausibility of our basic definitions.",nan
2006,comparing sets of positive and negative arguments: empirical assessment of seven qualitative rules,"many decisions can be represented as bipolar, qualitative sets of arguments: arguments can be pros or cons, and ranked according to their importance, but not numerically evaluated. the problem is then to compare these qualitative, bipolar sets. in this paper (a collaboration between a computer scientist and a psychologist), seven procedures for such a comparison are empirically evaluated, by matching their predictions to choices made by 62 human participants on a selection of 33 situations. results favor cardinality-based procedures, and in particular one that allows for the internal cancellation of positive and negative arguments within a decision.",nan
2006,a similarity and fuzzy logic-based approach to cerebral categorisation,this work proposes a formal modelling of categorisation processes attempting at simulating the way information is categorised by neural populations in the human brain. the formalism mainly relies on a similarity-based approach to categorisation. it involves weighted rules that use inference and fusion techniques borrowed from fuzzy logic. the approach is illustrated by a simulation of the mcgurck effect where the combination of contradictory auditory and visual stimuli creates an auditory perceptive illusion.,nan
2006,imitation of intentional behaviour,"in this paper a computational simulation of the imitation of intentional behaviour is presented. important assumptions which make the problem computationally tractable are introduced and motivated. it is shown that horizontal, iterated learning, and vertical transmission schemes can be used to learn from imitation using the proposed framework.",nan
2006,dramatization meets narrative presentations,"in recent times, information presentation has evolved towards sophisticated approaches that involve multi-modal aspects and character-based mediation.this paper presents a novel methodology for creating information presentations based on a dramatization of the content exposition in two respects. on one side, the author plots a character's monologue that aims at achieving presentation goal and exhibits an engaging inner conflict; on the other side, the system architecture dynamically assembles the elementary units of the plot scripted by the author by implementing a tension between contrasting communicative functions.the methodology has been applied in the implementation of a virtual guide to an historical site.",nan
2006,mama: an architecture for interactive musical agents,"in this paper, we present mama — an architecture for interactive musical agents. this system uses a theory of musical acts, based on speech act theory to support the agents interaction. we discuss the basics of a representation language which these agents can use to represent and reason about music. we present a case study system based on these ideas, and discuss its ability to support distributed execution of a minimalist score.audio recordings of the system are available at http://homepages.inf.ed.ac.uk/s0239182/mama",nan
2006,bayesian modelling of colour's usage impact to web credibility,"colour plays an important role in web site design. the selection of effective chromatic combinations and the relation of colour to the perceived aesthetic and emotional value of a web site is the focus of this paper. the subject of the reported research has been to define a model through which to be able to associate colour combinations with specific desirable emotional and aesthetic values. the presented approach involves application of machine learning techniques on a rich data set collected during a number of empirical studies. the data obtained were used to train a bayesian belief network which associated a simple chromatic model to perceived aesthetic and emotional dimensions. a set of tools that have been build in the process to support the methodological framework and ensure its reusability with minimal effort, are also described.",nan
2006,evaluating perception of interaction initiation in virtual environments using humanoid agents,"we evaluated user perception of certain attention behaviours, eye head and body orientation in particular, of humanoid agents in a virtual environment for the purposes of interaction initiation. users were shown a number of scenarios involving agents making gaze, gesture and locomotion behaviours and were asked to report their impressions of how attentive the agents were and their perceived likely-hood to engage in interaction. these evaluation studies are critical for furthering our understanding of the early phases of human-computer interaction where distance may be involved and there is uncertainty as to the intention of the other to interact. amongst other results, we establish here that the human perception of interest, interaction seeking and openness from a humanoid agent is based, in part, on the time-varying behaviour of its eyes, head, body and locomotion directions. application domains include social robotics and embodied conversational agents.",nan
2006,cognitive situated agents learn to name actions,"we present experiments in which a population of situated autonomous cognitive agents learns by means of “language games” a common lexicon of symbols that designates basic motions (translation, rotation) in a 2d world. we study the quality of the lexicon learnt when we give ours agents a cognitive functionality akin to working memory, and we explain how they are able to agree on a set of common names for basic actions, if their world representation is built using sensorimotor contingencies informations.",nan
2006,tracking the lexical zeitgeist with wordnet and wikipedia,"most new words, or neologisms, bubble beneath the surface of widespread usage for some time, perhaps even years, before gaining acceptance in conventional print dictionaries [1]. a shorter, yet still significant, delay is also evident in the life-cycle of nlp-oriented lexical resources like wordnet [2]. a more topical lexical resource is wikipedia [3], an open-source community-maintained encyclopedia whose headwords reflect the many new words that gain recognition in a particular linguistic sub-culture. in this paper we describe the principles behind zeitgeist, a system for dynamic lexicon growth that harvests and semantically analyses new lexical forms from wikipedia, to automatically enrich wordnet as these new word forms are minted. zeitgeist demonstrates good results for composite words that exhibit a complex morphemic structure, such as portmanteau words and formal blends [4, 5].",nan
2006,enhancing constraints manipulation in semiring-based formalisms,"many “semiring-like” structures are used in soft constraint satisfaction problems (scsps). we review a few properties of semirings that are useful for dealing with soft constraints, highlighting the differences between alternative proposals in the literature.we then extend the semiring structure by adding the notion of division as a weak inverse operation of product. in particular, division is needed to apply constraint relaxation when the product operation of the semiring is not idempotent. the division operator is introduced via residuation and it is also able to deal with partial orders, generalizing the approach given for valued csps.",nan
2006,evaluating asp and commercial solvers on the csplib,"this paper deals with three solvers for combinatorial problems: the commercial state-of-the-art solver ilog opl, and the research asp systems dlv and smodels. the first goal of this research is to evaluate the relative performance of such systems, using a reproducible and extensible experimental methodology. in particular, we consider a third-party problem library, i.e., the csplib, and uniform rules for modelling and selecting instances. the second goal is to analyze the effects of a popular reformulation technique, i.e., symmetry breaking, and the impact of other modelling aspects, like global constraints and auxiliary predicates. results show that there is not a single solver winning on all problems, and that reformulation is almost always beneficial: symmetry-breaking may be a good choice, but its complexity has to be carefully chosen, by taking into account also the particular solver used. global constraints often, but not always, help opl, and the addition of auxiliary predicates is usually worth, especially when dealing with asp solvers. moreover, interesting synergies among the various modelling techniques exist.",nan
2006,automatic generation of implied constraints,"a well-known difficulty with solving constraint satisfaction problems (csps) is that, while one formulation of a csp may enable a solver to solve it quickly, a different formulation may take prohibitively long to solve. we demonstrate a system for automatically reformulating csp solver models by combining the capabilities of machine learning and automated theorem proving with csp systems. our system is given a basic csp formulation and outputs a set of reformulations, each of which includes additional constraints. the additional constraints are generated through a machine learning process and are proven to follow from the basic formulation by a theorem prover. experimenting with benchmark problem classes from finite algebras, we show how the time invested in reformulation is often recovered many times over when searching for solutions to more difficult problems from the problem class.",nan
2006,maintaining generalized arc consistency on ad-hoc n-ary boolean constraints,"binary decision diagrams (bdds) can compactly rep- resent ad-hoc n-ary boolean constraints. however, there is no gen- eralized arc consistency (gac) algorithm which exploit bdds. for example, the global case constraint by sicstus prolog for ad-hoc constraints is designed for non-boolean domains. in this paper, we introduce a new gac algorithm, bddc, for bdd constraints. our empirical results demonstrate the advantages of a new bdd-based global constraint – bddc is more efficient both in terms of mem- ory and time than the case constraint when dealing with ad-hoc boolean constraints. this becomes important as the size of the ad- hoc constraints becomes large.",nan
2006,a study on the short-term prohibition mechanisms in tabu search,"tabu search (ts) is a well known local search method which has been widely used for solving ai problems. different versions of ts have been proposed in the literature, and many features of ts have been considered and tested experimentally. the feature that is present in almost all ts variants is the so called (short-term) tabu list which is recognised as the crucial issue of ts. however, the definition of the parameters associated with the tabu list remains in most ts applications still a handcrafted activity.in this work we undertake a systematic study of the relative influence of few relevant tabu list features on the performances of ts solvers. in particular, we apply statistical methods for the design and analysis of experiments. the study focuses on a fundamental theoretical problem (graph colouring) and on one of its practical specialisation (examination timetabling), which involves specific constraints and objectives. the goal is to determine which ts features are more critical for the good performance of ts in a general context of applicability.the general result is that, when the quantitative parameters are well tuned, the differences with respect to qualitative parameters become less evident.",nan
2006,random subset optimization,"some of the most successful algorithms for satisfiability, such as walksat, are based on random walks. similarly, local search algorithms for solving constraint optimization problems benefit significantly from randomization. however, well-known algorithms such as stochastic search or simulated annealing perform a less directed random walk than used in satisfiability. by making a closer analogy to the technique used in walksat, we obtain a different kind of randomization called random subset optimization. experiments on both structured and random problems show strong performance compared with other local search algorithms.",nan
2006,search for compromise solutions in multiobjective state space graphs,"the aim of this paper is to introduce and solve new search problems in multiobjective state space graphs. although most of the studies concentrate on the determination of the entire set of pareto optimal solution paths, the size of which can be, in worst case, exponential in the number of nodes, we consider here more specialized problems where the search is focused on pareto solutions achieving a well-balanced compromise between the conflicting objectives. after introducing a formal definition of the compromise search problem, we discuss computational issues and the complexity of the problem. then, we introduce two algorithms to find the best compromise solution-paths in a state space graph. finally, we report various numerical tests showing that, as far as compromise search is concerned, both algorithms are very efficient (compared to moa*) but they present contrasted advantages discussed in the conclusion.",nan
2006,"minion: a fast, scalable, constraint solver","we present minion, a new constraint solver. empirical results on standard benchmarks show orders of magnitude performance gains over state-of-the-art constraint toolkits. these gains increase with problem size – minion delivers scalable constraint solving. minion is a general-purpose constraint solver, with an expressive input language based on the common constraint modelling device of matrix models. focussing on matrix models supports a highly-optimised implementation, exploiting the properties of modern processors. this contrasts with current constraint toolkits, which, in order to provide ever more modelling and solving options, have become progressively more complex at the cost of both performance and usability. minion is a black box from the user point of view, deliberately providing few options. this, combined with its raw speed, makes minion a substantial step towards puget's ‘model and run’ constraint solving paradigm.",nan
2006,asynchronous forward-bounding for distributed constraints optimization,"a new search algorithm for solving distributed constraint optimization problems (discops) is presented. agents assign variables sequentially and propagate their assignments asynchronously. the asynchronous forward-bounding algorithm (afb) is a distributed optimization search algorithm that keeps one consistent partial assignment at all times. forward bounding propagates the bounds on the cost of solutions by sending copies of the partial assignment to all unassigned agents concurrently. the algorithm is described in detail and its correctness proven. experimental evaluation of afb on random max-discsps reveals a phase transition as the tightness of the problem increases. this effect is analogous to the phase transition of max-csp when local consistency maintenance is applied [3]. afb outperforms synchronous branch & bound (sbb) as well as the asynchronous state-of-the-art adopt algorithm, for the harder problem instances. both asynchronous algorithms outperform sbb by a large factor.",nan
2006,distributed log-based reconciliation,"computer supported cooperative work (cscw) defines software tools and technology to support groups of people working together on a project, often at different sites [5]. in this work, we present four distributed algorithms for log-based reconciliation, an important np-hard problem occurring in cscw. our methods remove the classical drawbacks of centralized systems like single point of failure, performance bottleneck and loss of autonomy. the problem is formalized using the distributed constraint satisfaction paradigm (discsp). in the worst case, the message passing complexity of our methods range from o(p2) to o(2p) in a system of p nodes. experimental results confirm our theoretical analysis and allow us to establish quality and efficiency trade-off for each method.",nan
2006,extracting mucs from constraint networks,"we address the problem of extracting minimal unsatisfiable cores (mucs) from constraint networks. this computationally hard problem has a practical interest in many application domains such as configuration, planning, diagnosis, etc. indeed, identifying one or several disjoint mucs can help circumscribe different sources of inconsistency in order to repair a system. in this paper, we propose an original approach that involves performing successive runs of a complete backtracking search, using constraint weighting, in order to surround an inconsistent part of a network, before identifying all transition constraints belonging to a muc using a dichotomic process. we show the effectiveness of this approach, both theoretically and experimentally.",nan
2006,preference-based inconsistency proving: when the failure of the best is sufficient,"inconsistency proving of csps is typically achieved by a combination of systematic search and arc consistency, which can both be characterized as resolution. however, it is well-known that there are cases where resolution produces exponential contradiction proofs, although proofs of polynomial size exist. for this reason, we will use optimization methods to reduce the proof size globally by 1. decomposing the original unsatisfiability problem into a conjunction of satisfiable subproblems and by 2. finding an ordering that separates the solution spaces of the subproblems. this principle allows operation research methods to prove the inconsistency of overconstrained linear programs even if domains are infinite. we exploit the principle for testing the satisfiability of global user requirements in product configuration problems.",nan
2006,return of the jtms: preferences orchestrate conflict learning and solution synthesis,"we use a lexicographical preference order on the problem space to combine solution synthesis with conflict learning. given two preferred solutions of two subproblems, we can either combine them to a solution of the whole problem or learn a ‘fat’ conflict which cuts off a whole subtree. the approach makes conflict learning more pervasive for constraint programming as it well exploits efficient support finding and compact representations of craig interpolants.",nan
2006,multi-objective propagation in constraint programming,"bounding constraints are used to bound the tolerance of solutions under certain undesirable features. standard solvers propagate them one by one. often times, it is easy to satisfy them independently, but difficult to satisfy them simultaneously. therefore, the standard propagation methods fail. in this paper we propose a novel approach inspired in multi-objective optimization. we compute a multi-objective lower bound set that, if large enough, can be used to detect the inconsistency of the problem. our experiments on two domains inspired in real-world problems show that propagation of additive bounding constraints using our approach is clearly superior than previous approaches.",nan
2006,last conflict based reasoning,"in this paper, we propose an approach to guide search to sources of conflicts. the principle is the following: the last variable involved in the last conflict is selected in priority, as long as the constraint network can not be made consistent, in order to find the (most recent) culprit variable, following the current partial instantiation from the leaf to the root of the search tree. in other words, the variable ordering heuristic is violated, until a backtrack to the culprit variable occurs and a singleton consistent value is found. consequently, this way of reasoning can easily be grafted to many search algorithms and represents an original way to avoid thrashing. experiments over a wide range of benchmarks demonstrate the effectiveness of this approach.",nan
2006,dynamic orderings for and/or branch-and-bound search in graphical models,"and/or search spaces have recently been introduced as a unifying paradigm for advanced algorithmic schemes for graphical models. the main virtue of this representation is its sensitivity to the structure of the model, which can translate into exponential time savings for search algorithms. since the variable selection can have a dramatic impact on search performance when solving optimization tasks, we introduce in this paper a new dynamic and/or branch-and-bound algorithmic framework which accommodates variable ordering heuristics. the efficiency of the dynamic and/or approach is demonstrated empirically in a variety of domains.",nan
2006,compact representation of sets of binary constraints,"we address the problem of representing big sets of binary constraints compactly. binary constraints in the form of 2-literal clauses are ubiquitous in propositional formulae that represent real-world problems ranging from model-checking problems in computer-aided verification to ai planning problems. current satisfiability and constraint solvers are applicable to very big problems, and in some cases the physical size of the problem representations prevents solving the problems, not their computational difficulty. our work is motivated by this observation.we propose graph-theoretic techniques based on cliques and bicliques for compactly representing big sets of binary constraints that have the form of 2-literal clauses. an n, m biclique in a graph associated with the constraints can be very compactly represented with only n+m binary constraints and one auxiliary variable. cliques in the graph are associated with at-most-one constraints, and can be represented with a logarithmic number of binary constraints. the clique representation turns out to be a special case of the biclique representation. we demonstrate the effectiveness of the biclique representation in making the representation of big planning problems practical.",nan
2006,pessimistic heuristics beat optimistic ones in real-time search,"admissibility is a desired property of heuristic evaluation functions, because when these heuristics are used with complete search methods, such as a* and rbfs, they guarantee that an optimal solution will be found. since every optimistic heuristic function is admissible, optimistic functions are widely used. we show, however, that with incomplete, real-time search, optimistic functions lose their appeal, and in fact they may hinder the search under quite reasonable conditions. under these conditions the exact opposite is to be preferred, i.e. pessimistic heuristic functions that never underestimate the difficulty of the problem. we demonstrate that such heuristics behave better than optimistic ones of equal quality on a standard testbed using rta* search method.",nan
2006,inverse consistencies for non-binary constraints,"we present a detailed study of two inverse consistencies for non-binary constraints: relational path inverse consistency (rel pic) and pairwise inverse consistency (pwic). these are stronger than generalized arc consistency (gac), even though they also only prune domain values. we propose algorithms to achieve rel pic and pwic, that have a time complexity better than the previous generic algorithm for inverse consistencies. one of our algorithms for pwic has a complexity comparable to that for gac despite doing more pruning. our experiments demonstrate that inverse consistencies can be more efficient than gac on a range of non-binary problems.",nan
2006,guiding search using constraint-level advice,constraint satisfaction problems are traditionally solved using some form of backtrack search that propagates constraints after each decision is made. the efficiency of search relies heavily on the use of good variable and value ordering heuristics. in this paper we show that constraints can also be used to guide the search process by actively proposing the next choice point to be branched on. we show that search effort can be reduced significantly.,nan
2006,beyond singleton arc consistency,"shaving algorithms, like singleton arc consistency (sac), are currently receiving much interest. they remove values which are not part of any solution. this paper proposes an efficient shaving algorithm for enforcing stronger forms of consistency than sac. the algorithm is based on the notion of weak k-singleton arc consistency, which is equal to sac if k=1 but stronger if k>1. this paper defines the notion, explains why it is useful, and presents an algorithm for enforcing it. the algorithm generalises lecoutre and cardon's algorithm for establishing sac. used as pre-processor for mac it improves the solution time for structured problems. when run standalone for k>1, it frequently removes more values than sac at a reasonable time. our experimental results indicate that at the sac phase transition, it removes many more values than sac-1 for k=16 in less time. for many problems from the literature the algorithm discovers lucky solutions. frequently, it returns satisfiable csps which it proves inverse consistent if all values participate in a lucky solution.",nan
2006,symmetry breaking using value precedence,"we present a comprehensive study of the use of value precedence constraints to break value symmetry. we first give a simple encoding of value precedence into ternary constraints that is both efficient and effective at breaking symmetry. we then extend value precedence to deal with a number of generalizations like wreath value and partial interchangeability. we also show that value precedence is closely related to lexicographical ordering. finally, we consider the interaction between value precedence and symmetry breaking constraints for variable symmetries.",nan
2006,coordination through inductive meaning negotiation,"this paper is on negotiation, precisely on the negotiation of meaning. we advance and discuss a formal paradigm of coordination and variants thereof, wherein meaning negotiation plays a major role in the process of convergence to a common agreement. our model engages a kind of pairwise, model-theoretic coordination between knowledge-based agents, eventually able to communicate the complete & local meaning of their beliefs by expressions taken from the literals of a common first-order language. we focus on the framework of inductive inference—sometimes called “formal learning theory,” and argue that it offers a fresh and rigorous perspective on many current debates in artificial intelligence in the context of multiple agents in interaction.",nan
2006,reaching agreements for coalition formation through derivation of agents' intentions,"this paper addresses the coalition formation problem in multiagent systems. although several coalition formation models exist today, coalition formation using these models remains costly. as a consequence, applying these models through several iterations when required becomes time-consuming. this paper proposes a new coalition formation mechanism (cfm) to reduce this execution cost. this mechanism is based on four principles: (1) the use of information on task relationships so as to reduce the computational complexity of the coalition formation; (2) the exploitation of the coalition proposals formulated by certain agents in order to derive their intentions, (this principle makes the search for solutions easier, which in turn may result in earlier consensus and agreements-the intention derivation process is performed on a new graph structure introduced in this paper); (3) the use of several strategies for propagating the proposals of the agents in the coalition formation process; and (4) the dynamic reorganization of previous coalitions.",nan
2006,cheating is not playing: methodological issues of computational game theory,"computational game theory is a way to study and evaluate behaviors using game theory models, via agent-based computer simulations. one of the most known example of this approach is the famous classical iterated prisoner's dilemma (cipd). it has been popularized by axelrod in the beginning of the eighties and had led him to set up a successful theory of cooperation.this use of simulations has always been a challenging application of computer science, and of agent-based approaches, in particular to social sciences. it may be viewed as empirical game theory. these kind of approach is often necessary since, in the general case, classical analytical ones do not give suitable results. these tools are also often used when full game-theoretic analysis is intractable.the usual method to evaluate behaviors consists in the collection of strategies, through open contests, and the confrontation of all of them as in a sport championship. then it becomes, or at least seems to become, easy to evaluate and compare the efficiency of these behaviors.evaluating strategies can however not be done efficiently without the insurance that algorithms used are well formed and that they can not introduce bias in their computation. it can not be done without tools able to prevent or, at least, measure deviation from the object of the study. unfortunately people using such simulations often do not take care seriously about all those aspects, because they are not aware of it, and sometimes because they are. we will try to show effects of bad simulations practice on the simplest example.we show methodological issues which have to be taken care of, or avoided in order to prevent trouble in simulation results interpretation. based on some simple illustration, we exhibit two kinds of bias that could be introduced. we classify them as voluntary or involuntary mistakes. the former ones can be explained by poor design of experimentations whereas the latter can defeat the purpose of the evaluation using simple ideas of agreement and cooperation. we also show the implications on interpretations and conclusions that such errors may produce.we state that scoring/ranking methods are part of the game, and as such have to be described with the game. many points described may seem to be widely known. we think that with the growth of interest of such methods they have to be detailed and exposed clearly.",nan
2006,mediation in the framework of morpho-logic,"in this paper we introduce a median operator between two sets of interpretations (worlds) in a finite propositional language. our definition is based on morphological operations and hausdorff distance. it provides a result which lies “halfway” between both sets and accounts for the “extension” or “shape” of the sets. we prove several interesting properties of this operator and compare it with fusion operators. this new operator allows performing mediation between two sets of beliefs, preferences, demands, in an original way, showing an interesting behavior that was not possible to achieve using existing operators.",nan
2006,strengthening admissible coalitions,"we develop a criterion for coalition formation among goal-directed agents, the indecomposable do-ut-des property. the indecomposable do-ut-des property refines the do-ut-des property (literally give to get) by considering the fact that agents prefer to form coalitions whose components cannot be formed independently. a formal description of this property is provided as well as an analysis of algorithms and their complexity.",nan
2006,advanced policy explanations on the web,"the frameworks for protecting security and privacy can be effective only if common users—with no training in computer science or logic—increase their awareness and control over the policy applied by the systems they interact with. towards this end, we introduce a mechanism for answering why, why-not, how-to, and what-if queries on rule-based policies for trust negotiation. our framework is lightweight and scalable but it fulfills the main goals of modern explanation facilities. we adopt a novel tabled explanation structure, that simultaneously shows local and global (intra-proof and inter-proof) information, thereby facilitating navigation. answers are focussed by removing irrelevant parts with suitable heuristics.",nan
2006,prevention of harmful behaviors within cognitive and autonomous agents,being able to ensure that a multiagent system will not generate undesirable behaviors is essential within the context of critical applications (embedded systems or real-time systems). the emergence of behaviors from the agents interaction can generate situations incompatible with the expected system execution. the standard methods to validate a multiagent system do not prevent the occurrence of undesirable behaviors during its execution in real condition. we propose a complementary approach of dynamic self-monitoring and self-regulation allowing the agents to control their own behavior. this paper goes on to present the automatic generation of self-controlled agents. we use the observer approach to verify that the agents behavior respects a set of laws throughout the system execution.,nan
2006,coalition structure generation in task-based settings,"the coalition formation process, in which a number of independent, autonomous agents come together to act as a collective, is an important form of interaction in multi-agent systems. however, one of the main problems that hinders the wide spread adoption of coalition formation technologies is the computational complexity of coalition structure generation. that is, once a group of agents has been identified, how can it be partitioned in order to maximise the social payoff? to date, most work on this problem has concentrated on simple characteristic function games. however, this lacks the notion of tasks which makes it more difficult to apply it in many applications. against this background, this paper studies coalition structure generation in a general task-based setting. specifically, we show that this problem is np-hard and that the minimum number of coalition structures that need to be searched through in order to establish a solution within a bound from the optimal is exponential to the number of agents. we then go onto develop an anytime algorithm that can establish a solution within a bound from the optimal with a minimal search and can reduce the bound further if time permits.",nan
2006,programming agents with emotions,"this paper presents the syntax and semantics of a simplified version of a logic-based agent-oriented programming language to implement agents with emotions. four types of emotions are distinguished: happiness, sadness, anger and fear. these emotions are defined relative to agent's goals and plans. the emotions result from the agent's deliberation process and influence the deliberation process. the semantics of each emotion type is incorporated in the transition semantics of the presented agent-oriented programming language.",nan
2006,goal types in agent programming,"this paper presents three types of declarative goals: perform goals, achieve goals, and maintain goals. the integration of these goal types in a simple but extendable logic-based agent-oriented programming language is discussed and motivated. the computational semantics for each goal type is presented by means of a transition system. it is shown that the presented semantics of the goal types ensure some desirable and expected properties.",nan
2006,alternating-offers bargaining under one-sided uncertainty on deadlines,"alternating-offers is the most prominent negotiation protocol for automatic bilateral bargaining. nevertheless, in most settings it is still not known how two fully rational agents should behave in the protocol. in this paper we study the finite-horizon alternating-offers protocol under one-sided uncertain deadlines. we make a novel use of backward induction in studying bargaining with uncertainty; we employ a “natural” system of beliefs and find, when it exists, the pertinent pure strategy sequential equilibrium. we further show, as an intrinsic limitation of the protocol, that for some parameter values there is no pure strategy sequential equilibrium, whatever system of beliefs is employed.",nan
2006,a logic-based framework to compute pareto agreements in one-shot bilateral negotiation,"we propose a logic-based approach to automated oneshot multi-issue bilateral negotiation. we use logic in two ways: (1) a logic theory to represent relations among issues – e.g., logical implication – in contrast with approaches that describe issues as uncorrelated with each other; (2) utilities over formulas to represent agents having preferences over different bundles of issues. in this case, the utility assigned to a bundle is not necessarily the sum of utilities assigned to single elements in the bundle itself.we illustrate the theoretical framework and the one-shot negotiation protocol, which makes use of a facilitator to compute some particular pareto-efficient outcomes. we prove the computational adequacy of our method by studying the complexity of the problem of finding pareto-efficient solutions in a propositional logic setting.",nan
2006,towards acl semantics based on commitments and penalties,"the importance of defining a standard framework for agent communication languages (acl) with a simple, clear, and a verifiable semantics has been widely recognized. this paper proposes a logic-based semantics which is social in nature. the basic idea is to associate with each speech act a meaning in terms of the commitment induced by that speech act, and the penalty to be paid in case that commitment is violated. a violation criterion based on the existence of arguments is then defined per speech act. moreover, we show that the proposed semantics satisfies some key properties that ensure the approach is well-founded. the logical setting makes the semantics verifiable.",nan
2006,computational opinions,"existing approaches to knowledge representation and reasoning in the context of open systems either deal with “objective” knowledge or with beliefs. in contrast, there has been almost no research on the formal modelling of opinions, i.e., communicatively asserted ostensible beliefs. this is highly surprising, since opinions are in fact the only publicly visible kind of knowledge in open systems, and can neither be reduced to objective knowledge nor to beliefs. in this paper, we propose a formal framework for the representation of dynamic, context-dependent and revisable opinions and ostensible intentions as a sound basis for the external description of agents as obtained from observable communication processes. potential applications include a natural semantics of communicative acts exchanged between truly autonomous agents, and a fine-grained, statement-level concept of trust.",nan
2006,a new semantics for the fipa agent communication language based on social attitudes,"one of the most important aspects of the research on agent interaction is the definition of agent communication languages (acls), and the specification of a proper formal semantics of such languages is a crucial prerequisite for the usefulness and acceptance of artificial agency. nevertheless, those acls which are still mostly used, especially the standard fipa-acl, have a communication act semantics in terms of the participating agents' mental attitudes (viz. beliefs and intentions), which are in general undeterminable from an external point of view due to agent autonomy. in contrast, semantics of acls based on commitments are fully verifiable, but not sufficiently formalized and understood yet. in order to overcome this situation, we propose a fipa-acl semantics which is fully verifiable, fully formalized, lean and easily applicable. it is based on social attitudes represented using a logic of grounding in straightforward extension of the bdi agent model.",nan
2006,contouring of knowledge for intelligent searching for arguments,"a common assumption for logic-based argumentation is that an argument is a pair 〈φ, α〉 where φ is a minimal subset of the knowledgebase such that φ is consistent and φ entails the claim α. different logics are based on different definitions for entailment and consistency, and these give us different options for argumentation. for a variety of logics, in particular for classical logic, there is a need to develop intelligent techniques for generating arguments. since building a constellation of arguments and counterarguments involves repeatedly querying a knowledgebase, we propose a framework based on what we call “contours” for storing information about a knowledgebase that provides boundaries on what is provable in the knowledgebase. using contours allows for more intelligent searching of a knowledgebase for arguments and counterarguments.",nan
2006,on the inability of gathering by asynchronous mobile robots with initial movements,"consider a community of simple autonomous robots freely moving in the plane. the robots are decentralized, asynchronous, deterministic without the common coordination system, identities, direct communication, memory of the past, but with the ability to sense the positions of the other robots. existing results show the inability of gathering in this model and solve the gathering problem in the model extended by the multiplicity detection capability. however, the previous results rely on the fact that the robots are intially not moving. we prove that the gathering problem is unsolvable in the model with initial movement vectors even with the multiplicity detection capability.",nan
2006,testing the limits of emergent behavior in mas using learning of cooperative behavior,"we present a method to test a group of agents for (unwanted) emergent behavior by using techniques from learning of cooperative behavior. the general idea is to mimick users or other systems interacting with the tested agents by a group of tester agents and to evolve the actions of these tester agents. the goal of this evolutionary learning is to get the tested agents to exhibit the (unwanted) emergent behavior. we used our method to test the emergent properties of a team of agents written by students for an assignment in a basic mas class. our method produced tester agents that helped the tested agents to perform at their best and another configuration of our system showed how much the tested agents could hold their own against very competitive agents, which revealed breakdowns in the tested agents' cooperation.",nan
2006,boolean games revisited,"game theory is a widely used formal model for studying strategical interactions between agents. boolean games [8] are two players, zero-sum static games where players' utility functions are binary and described by a single propositional formula, and the strategies available to a player consist of truth assignments to each of a given set of propositional variables (the variables controlled by the player.) we generalize the framework to n-players games which are not necessarily zero-sum. we give simple characterizations of nash equilibria and dominated strategies, and investigate the computational complexity of the related problems.",nan
2006,an automated agent for bilateral negotiation with bounded rational agents with incomplete information,"many day-to-day tasks require negotiation, mostly under conditions of incomplete information. in particular, the opponent's exact tradeoff between different offers is usually unknown. we propose a model of an automated negotiation agent capable of negotiating with a bounded rational agent (and in particular, against humans) under conditions of incomplete information. although we test our agent in one specific domain, the agent's architecture is generic; thus it can be adapted to any domain as long as the negotiators' preferences can be expressed in additive utilities. our results indicate that the agent played significantly better, including reaching a higher proportion of agreements, than human counterparts when playing one of the sides, while when playing the other side there was no significant difference between the results of the agent and the human players.",nan
2006,self-organizing multiagent approach to optimization in positioning problems,"the facility positioningdeployment, location and siting are used as synonyms problem concerns the location of facilities such as bus-stops, fire stations, schools, so as to optimize one or several objectives. this paper contributes to research on location problems by proposing a reactive multiagent approach. particularly, we deal with the p-median problem, where the objective is to minimize the weighted distance between the demand points and the facilities. the proposed model relies on a set of agents (the facilities) situated in a common environment which interact and attempt to reach a global optimization goal: the distance minimization. the interactions between agents and their environment, which is based on the artificial potential fields approach, allow us to locally optimize the agent's location. the optimization of the whole system is then obtained from a self-organization of the agents. the efficiency of the proposed approach is confirmed by computational results based on a set of comparisons with the k-means clustering technique.",nan
2006,arguing with confidential information,"while researchers have looked at many aspects of argumentation, an area often neglected is that of argumentation strategies. that is, given multiple possible arguments that an agent can put forth, which should be selected in what circumstances. in this paper, we propose a heuristic that implements one such strategy. the heuristic assigns a utility cost to revealing information, as well as a utility to winning, drawing and losing an argument. an agent participating in a dialogue then attempts to maximise its utility. we present a formal argumentation framework in which this heuristic may operate, and show how it functions within the framework. finally, we discuss how this heuristic may be extended in future work, and its relevance to argumentation theory in general.",nan
2006,auction mechanisms for efficient advertisement selection on public displays,"public electronic displays can be used as an advertising medium when space is a scarce resource, and it is desirable to expose many adverts to as wide an audience as possible. although the efficiency of such advertising systems can be improved if the display is aware of the identity and interests of the audience, this knowledge is difficult to acquire when users are not actively interacting with the display. to this end, we present bluscreen, an intelligent public display, which selects and displays adverts in response to users detected in the audience. here, users are identified and their advert viewing history tracked, by detecting any bluetooth-enabled devices they are carrying (e.g. phones, pdas, etc.). within bluscreen we have implemented an agent system that utilises an auction-based marketplace to efficiently select adverts for the display, and deployed this within an installation in our department. we demonstrate, by means of an empirical evaluation, that the performance of this auction-based mechanism when used with our proposed bidding strategy, efficiently selects the best adverts in response to the audience presence. we bench-marked our advertising method with two other commonly applied selection methods for displaying adverts on public displays; specifically the round-robin and the random approaches. the results show that our auction-based approach, that utilised the novel use of bluetooth detection, outperforms these two methods by up to 64%.",nan
2006,verifying interlevel relations within multi-agent systems,"an approach to handle the complex dynamics of a multi-agent system is based on distinguishing aggregation levels by structuring the system into parts or components. the behavior of every aggregation level is specified by a set of dynamic properties for components and interactions at that level, expressed in some (temporal) language. the dynamic properties of higher aggregation levels in principle can be logically related to dynamic properties of lower levels. this asks for identification and verification of such interlevel relations. in this article it is shown how this problem can be addressed using model checking techniques.",nan
2006,flexible provisioning of service workflows,"service-oriented computing is a promising paradigm for highly distributed and complex computer systems. in such systems, services are offered by provider agents over a computer network and automatically discovered and provisioned by consumer agents that need particular resources or behaviours for their workflows. however, in open systems where there are significant degrees of uncertainty and dynamism, and where the agents are self-interested, the provisioning of these services needs to be performed in a more flexible way than has hitherto been considered. to this end, we devise a number of heuristics that vary provisioning according to the predicted performance of provider agents. we then empirically benchmark our algorithms and show that they lead to a 350% improvement in average utility, while successfully completing 5–6 times as many workflows as current approaches.",nan
2006,heuristic bidding strategies for multiple heterogeneous auctions,"this paper investigates utility maximising bidding heuristics for agents that participate in multiple heterogeneous auctions, in which the auction format and the starting and closing times can be different. our strategy allows an agent to procure one or more items and to participate in any number of auctions. for this case, forming an optimal bidding strategy by global utility maximisation is computationally intractable, and so we develop two-stage heuristics that first provide reasonable bidding thresholds with simple strategies before deciding which auctions to participate in. the proposed approach leads to an average gain of at least 24% in agent utility over commonly used benchmarks.",nan
2006,are parallel bdi agents really better?,"the traditional bdi agent has 3 basic computational components that generate beliefs, generate intentions and execute intentions. they run in a sequential and cyclic manner. this may introduce several problems. among them, the inability to watch the environment continuously in dynamic environments may be disastrous and makes an agent less rational – the agent may endanger itself. two possible solutions are by parallelism and by controlling and managing the 3 components in suitable ways. we examine a parallel architecture with three parallel running components which are the belief manager, the intention generator and the intention executor. the agent built with this architecture will have the ability of performing several actions at once. to evaluate the parallel bdi agent, we compare the parallel agent against four versions of sequential agents where the 3 components of the bdi agent are controlled and managed in different ways and different time resources are allocated to them. experiments are designed to simulate agents based on the sequential and parallel bdi architectures respectively and the ability of the agents to respond to the same sequences of external events of various priorities are assessed. the comparison results show that the parallel bdi agent has quicker response, react to emergencies immediately and its behaviour is more rational.",nan
2006,dynamic control of intention priorities of human-like agents,"intention scheduling mechanism plays a critical role in the correct behaviour of bdi agents. for human-like agents, the independent intentions should be scheduled based on their priorities, which show the respective importance and urgency of the intentions. we propose to enrich the bdi agent architecture with 2 processing components, a pcf (priority changing function) selector and a priority controller. these enable a bdi agent to assign an initial priority value to an intention and change it with time according to the chosen pcf. the initial priority value reflects its urgency at the time of intention creation. the pcf selected defines how the priority should change with time. as an example, we design a function by simulating human behaviors when dealing with several things at the same time. the priority first increases with time according to the gaussian function to simulate that people are more inclined to do something which has been on their mind for sometime. after a certain time, if the intention still does not get executed because of other higher priority intentions, its priority will decrease according to the ebbinghaus forgetting curve. experiment results show that with this mechanism, the agent can show some human-like characteristics when scheduling intention to execute. this can be used when simulating human-like agents.",nan
2006,knowing minimum/maximum n formulae,"we introduce a logical language with nullary operators min(n), for each non-negative integer n, which mean ‘the reasoner has at least n different beliefs’. the resulting language allows us to express interesting properties of non-monotonic and resource-bounded reasoners. other operators, such as 'the reasoner has at most n different beliefs' and the operator introduced in [1, 4]: 'the reasoner knows at most the formulae &phi;1,…,&phi;n′, are definable using min(n). we introduce several syntactic epistemic logics with min(n) operators, and prove completeness and decidability results for those logics.",nan
2006,modal logics for communicating rule-based agents,"in this paper, we show how to establish correctness and time bounds (e.g., quality of service guarantees) for multi-agent systems composed of communicating rule-based agents. the formal models of multi-agent systems we study are transition systems where each transition corresponds to either a rule firing or an act of communication by an agent. we present a complete and sound modal logic which formalises how the beliefs of communicating rule-based agents change over time. using a simple example, we show how this logic can be used to specify temporal properties of belief change in multi-agent systems in a precise and realistic way, and how existing modal logic techniques such as model-checking can be used to state and verify properties of agents.",nan
2006,causation as production,"recently i suggested that a cause is an event which, in its context of occurrence, is both necessary and sufficient for the effect. however this definition is only appropriate if there is a single potential cause of the effect. consequently i suggest a generalization of the definition and discuss the resulting “production theory”. i suggest that this can be seen as a combination of a regularity theory in the hume tradition and a dependence theory in the lewis tradition, and argue that the production theory inherits the strengths of the component theories while avoiding their weaknesses.",nan
2006,merging possibilistic networks,"this paper deals with merging multiple-source uncertain pieces of information, which are encoded by means of possibilistic networks. we first show that the merging of possibilistic networks having the same graphical structure can be easily achieved in polynomial time. when possibilistic networks have different graphical structures we show that their fusion can also be efficiently done by extending initial possibilistic networks into a same common structure. we then address two important problems: how to deal with cycles, and how to solve the subnormalization problem which reflects conflicts between sources?",nan
2006,compiling possibilistic knowledge bases,"possibilistic knowledge bases gather propositional formulas associated with degrees belonging to a linearly ordered scale. these degrees reflect certainty or priority, depending if the formulas encode pieces of beliefs or goals to be pursued. possibilistic logic provides a simple format that turns to be useful for handling qualitative uncertainty, exceptions or preferences. the main result of the paper provides a way for compiling a possibilistic knowledge base in order to be able to process inference from it in polynomial time. the procedure is based on a symbolic treatment of the degrees under the form of sorted literals and on the idea of forgetting variables. the number of sorted literals that are added corresponds exactly to the number of priority levels existing in the base, and the number of binary clauses added in the compilation is also equal to this number of levels. the resulting extra compilation cost is very low.",nan
2006,improving bound propagation,"this paper extends previously proposed bound propagation algorithm [11] for computing lower and upper bounds on posterior marginals in bayesian networks. we improve the bound propagation scheme by taking advantage of the directionality in bayesian networks and applying the notion of relevant subnetwork. we also propose an approximation scheme for the linear optimization subproblems. we demonstrate empirically that while the resulting bounds loose some precision, we achieve 10-100 times speedup compared to original bound propagation using a simplex solver.",nan
2006,logic programs with multiple chances,"in human-like reasoning it often happens that different conditions, partially alternative and hierarchically structured, are mentally grouped in order to derive some conclusion. the hierarchical nature of such knowledge concerns with the possible failure of a chance of deriving a conclusion and the necessity, instead of blocking the reasoning process, of activating a subordinate chance. traditional logic programming (we refer here to answer set programming) does not allow us to express such situations in a synthetic fashion, since different chances of deriving a conclusion must be distributed over different rules, and conditions enabling the switching among chances must be explicitly represented. we present a new language, relying on answer set programming, which incorporates a new modality able to naturally express the above features. the merits of the proposal about the capability of representing knowledge are shown both by examples and by comparisons with other existing formalisms. a translation to plain asp is finally provided in order to give a practical tool for computing our programs, since a number of optimized asp evaluation systems are nowadays available.",nan
2006,reasoning with inconsistencies in propositional peer-to-peer inference systems,"in a peer-to-peer inference system, there is no centralized control or hierarchical organization: each peer is equivalent in functionality and cooperates with other peers in order to solve a collective reasoning task. since peer theories model possibly different viewpoints, even if each local theory is consistent, the global theory may be inconsistent. we exhibit a distributed algorithm detecting inconsistencies in a fully decentralized setting. we provide a fully distributed reasoning algorithm, which computes only well-founded consequences of a formula, i.e., with a consistent set of support.",nan
2006,conceptual hierarchies matching: an approach based on discovery of implication rules between concepts,"most research works about ontology or schema matching are based on symmetric similarity measures. by transposing the association rules paradigm, we propose to use asymmetric measures in order to enhance matching. we suggest an extensional and asymmetric matching method based on the discovery of significant implications between concepts described in textual documents. we use a probabilistic model of deviation from independence, named implication intensity. our method is divided into two consecutive stages: (1) the extraction in documents of relevant terms for each concept; (2) the discovery of significant implications between the concepts. our method is tested on two benchmarks. the results show that some relevant relations, ignored by a similarity-based matching, can be found thanks to our approach.",nan
2006,"ctl model update: semantics, computations and implementation","minimal change is a fundamental principle for modeling system dynamics. in this paper, we study the issue of minimal change for computational tree logic (ctl) model update. we first propose five primitive operations which capture the basic update of the ctl model, and then define the minimal change criteria for ctl model update based on these primitive operations. we provide essential semantic and computational characterizations for our ctl model update approach. we develop a formal algorithm to implement this update that employs the underlying minimal change principle. we also present a ctl model update example using the well known microwave oven scenario.",nan
2006,resolving conflicts in action descriptions,"we study resolving conflicts between an action description and a set of conditions (possibly obtained from observations), in the context of action languages. in this formal framework, the meaning of an action description can be represented by a transition diagram—a directed graph whose nodes correspond to states and whose edges correspond to transitions describing action occurrences. this allows us to characterize conflicts by means of states and transitions of the given action description that violate some given conditions. we introduce a basic method to resolve such conflicts by modifying the action description, and discuss how the user can be supported in obtaining more preferred solutions. for that, we identify helpful questions the user may ask (e.g., which specific parts of the action description cause a conflict with some given condition), and we provide answers to them using properties of action descriptions and transition diagrams. finally, we discuss the computational complexity of these questions in terms of related decision problems.",nan
2006,possibilistic influence diagrams,"in this article we present the framework of possibilistic influence diagrams (pid), which allow to model in a compact form problems of sequential decision making under uncertainty, when only ordinal data on transitions likelihood or preferences are available. the graphical part of a pid is exactly the same as that of usual influence diagrams, however the semantics differ. transition likelihoods are expressed as possibility distributions and rewards are here considered as satisfaction degrees. expected utility is then replaced by anyone of two possibilistic qualitative utility criteria for evaluating strategies in a pid. we describe a decision tree-based method for evaluating pid and computing optimal strategies. we then study the computational complexity of pid-related problems (computation of the value of a policy, computation of an optimal policy).",nan
2006,solving optimization problems with dll,"propositional satisfiability (sat) is a success story in computer science and artificial intelligence: sat solvers are currently used to solve problems in many different application domains, including planning and formal verification. the main reason for this success is that modern sat solvers can successfully deal with problems having millions of variables. all these solvers are based on the davis-logemann-loveland procedure (dll). dll is a decision procedure: given a formula &phi;, it returns whether &phi; is satisfiable or not. further, dll can be easily modified in order to return an assignment satisfying &phi;, assuming one exists. however, in many cases it is not enough to compute a satisfying assignment: indeed, the returned assignment has also to be “optimal” in some sense, e.g., it has to minimize/maximize a given objective function.in this paper we show that dll can be very easily adapted in order to solve optimization problems like max-sat and min-one. in particular these problems are solved by simply imposing an ordering on a set of literals, to be followed while branching. other popular problems, like distance-sat and weighted-max-sat, can be solved in a similar way. we implemented these ideas in zchaff and the experimental analysis show that the resulting system is competitive with respect to other state-of-the-art systems.",nan
2006,discovering missing background knowledge in ontology matching,"semantic matching determines the mappings between the nodes of two graphs (e.g., ontologies) by computing logical relations (e.g., subsumption) holding among the nodes that correspond semantically to each other. we present an approach to deal with the lack of background knowledge in matching tasks by using semantic matching iteratively. unlike previous approaches, where the missing axioms are manually declared before the matching starts, we propose a fully automated solution. the benefits of our approach are: (i) saving some of the pre-match efforts, (ii) improving the quality of match via iterations, and (iii) enabling the future reuse of the newly discovered knowledge. we evaluate the implemented system on large real-world test cases, thus, proving empirically the benefits of our approach.",nan
2006,extracting muses,"minimally unsatisfiable subformulas (in short, muses) represent the smallest explanations for the inconsistency of sat instances in terms of the number of involved clauses. extracting muses can thus prove valuable because it circumscribes the sources of contradiction in an instance. in this paper, a new heuristic-based approach to approximate or compute muses is presented. it is shown that it often outperforms current competing ones.",nan
2006,on probing and multi-threading in platypus,"the platypus approach offers a generic platform for distributed answer set solving, accommodating a variety of different architectures for distributing the search for answer sets across different processes and different search modes for modifying search behaviour. we describe two major extensions of platypus. first, we present its probing mode which provides a controlled non-linear traversal of the search space. second, we present its new multi-threading architecture allowing for intra-process distribution. both contributions are underpinned by experimental results illustrating their computational impact.",nan
2006,elaborating domain descriptions,"in this work we address the problem of elaborating domain descriptions (alias action theories), in particular those that are expressed in dynamic logic. we define a general method based on contraction of formulas in a version of propositional dynamic logic with a solution to the frame problem. we present the semantics of our theory change and define syntactical operators for contracting a domain description. we establish soundness and completeness of the operators w.r.t. the semantics for descriptions that satisfy a principle of modularity that we have defined in previous work.",nan
2006,on the logic of theory change: relations between incision and selection functions,this work elaborates on the connection between partial meet contractions and kernel contractions in belief change theory. we present a way to define incision functions (used in kernel contractions) from selection functions (used in partial meet contractions) and vice versa. then we make precise under which conditions there are exact correspondences between selection and incision functions so that the same contraction operations can be obtained by using either of them.,nan
2006,representing relative direction as a binary relation of oriented points,"a central issue in robotics is the representation of relative orientation. currently, the standard solution utilizes metrical representations. the main reason for this might be that representing rela- tively fine distinctions is useful in many robotics tasks. if qualitative spatial constraint calculi are to be applied to cognitve robotics, they therefore have to afford relatively fine distinctions. the challenge for us then is to find a calculus which allows these fine distinctions, and yet is still simple enough to provide a provably minimal composition table.in this paper we introduce a new calculus about oriented points which has a scalable granularity. in this calculus, named , simple rules can generate the minimal composition table. furthermore, the algebraic closure for a set of  statements is sufficient to solve knowledge integration tasks in robotics.",nan
2006,modular equivalence for normal logic programs,"a gaifman-shapiro-style architecture of program modules is introduced in the case of normal logic programs under stable model semantics. the composition of program modules is suitably limited by module conditions which ensure the compatibility of the module system with stable models. the resulting module theorem properly strengthens lifschitz and turner's splitting set theorem [17] for normal logic programs. consequently, the respective notion of equivalence between modules, i.e. modular equivalence, proves to be a congruence relation. moreover, it is shown how our translation-based verification method [15] is accommodated to the case of modular equivalence; and how the verification of weak/visible equivalence can be optimized as a sequence of module-level tests.",nan
2006,preference representation with 3-points intervals,"in this article we are interested in the representation of qualitative preferences with the help of 3-points intervals (a vector of three increasingly ordered points). preferences are crucial when an agent has to autonomously make a choice over several possible actions. we provide first of all an axiomatization in order to characterize our representation and then we construct a general framework for the comparison of 3-points intervals. our study shows that from the fifteen possible different ways to compare 3-points intervals, seven different preference structures can be defined, allowing the representation of sophisticated preferences. we show the usefulness of our results in two classical problematics: the comparison of alternatives and the numerical representation of preference structures. concerning the former one, we propose procedures to construct non classical preference relations (intransitive preferences for example) over objects being described by three ordered points. concerning the latter one, assuming that preferences on the pairwise comparisons of objects are known, we show how to associate a 3-points interval to every object, and how to define some comparison rules on these intervals in order to have a compact representation of preferences described with these pairwise comparisons.",nan
2006,reference-dependent qualitative models for decision making under uncertainty,"the aim of this paper is to introduce and investigate a new family of purely qualitative models for decision making under uncertainty. such models do not require any numerical representation and rely only on the definition of a preference relation over consequences and a relative likelihood relation on the set of events. within this family, we focus on decision rules using reference levels in the comparison of acts. we investigate both the descriptive potential of such rules and their axiomatic foundations. we introduce in a savage-like framework, a new axiom requiring that the decision maker's preference between two acts depends on the respective positions of their consequences relatively to reference levels. under this assumption we determine the only possible form of the decision rule and characterize some particular instances of this rule under transitivity constraints.",nan
2006,"decision with uncertainties, feasibilities, and utilities: towards a unified algebraic framework","several formalisms exist to express and solve decision problems. each is designed to capture different kinds of knowledge: utilities expressing preferences, uncertainties on the environment, or feasibility constraints on the decisions, with a possible sequential aspect. despite the fact that every framework relies on specific properties exploited by dedicated algorithms, these formalisms present interesting similarities.in this paper, we show that it is possible to capture these similarities in a generic algebraic framework for sequential decision making with uncertainties, feasibilities, and utilities. this framework subsumes several existing approaches, from constraint satisfaction problems to quantified boolean formulas, bayesian networks or possibilistic markov decision processes. we introduce this framework using a toy example, increasingly sophisticated by uncertainties, feasibilities and possible observations. this leads to a formal definition of the framework together with dedicated queries representing usual decision problems. generic algorithms for solving the considered queries should allow to both factorize existing algorithmic works and allow for cross-fertilization between the subsumed formalisms.",nan
2006,using occlusion calculi to interpret digital images,"this paper reports on an investigation using occlusion calculi to interpret digital images. using a minimal set of digital, region-relation detectors, and assuming a continuous interpretation of physical space, we show how existing calculi can be augmented and embedded in the event calculus to interpolate and recover a larger set of occlusion relations than are otherwise available at the basic detector level.",nan
2006,abductive logic programming in the clinical management of hiv/aids,"this paper presents a new abductive logic programming (alp) approach for assisting clinicians in the selection of antiretroviral drugs for patients infected with human immunodeficiency virus (hiv). the approach is comparable to laboratory genotypic resistance testing in that it aims to determine which viral mutations a patient is carrying and predict which drugs they are most likely resistant to. but, instead of genetically analysing samples of the virus taken from patients – which is not always practicable – our approach infers likely mutations using the patient's full clinical history and a model of drug resistance maintained by a leading hiv research agency. unlike previous applications of abduction, our approach does not attempt to find the “best” explanations, as we can never be absolutely sure which mutations a patient is carrying. rather, the intrinsic uncertainty of this domain means that multiple alternative explanations are inevitable and we must seek ways to extract useful information from them. the computational and pragmatic issues raised by this approach have led us to develop a new alp methodology for handling numerous explanations and for drawing predictions with associated levels of confidence. we present our in-silico sequencing system (is3) for reasoning about hiv drug resistance as a concrete example of this approach.",nan
2006,interleaving belief updating and reasoning in abductive logic programming,"most existing work on knowledge representation and reasoning assumes that the updating of beliefs is performed off-line, and that reasoning from the beliefs is performed either before or after the beliefs are changed. this imposes that, if an update occurs while reasoning is performed, reasoning has to be stopped and re-started anew so that the update is taken into account, with an obvious wastage of reasoning effort. in this paper, we tackle the problem of performing belief updating on-line, while reasoning is taking place by means of an abductive proof procedure.",nan
2006,bridging the gap between informal and formal guideline representations,"clinical guidelines are important means to improve quality of health care while limiting cost and supporting the medical staff. they are written as free text with tables and figures. transforming them into a formal, computer-processable representation is a difficult task requiring both computer scientist skills and medical knowledge.to bridge this gap, we designed an intermediate representation (or ontology) which serves as a mediator between the original text and different formal guideline representations. it is easier to use than the latter, structures the original prose and helps to spot missing information and contradictions.in this paper we describe the representation and a practical evaluation thereof through the modelling of a real-world clinical guideline.",nan
2006,boolean propagation based on literals for quantified boolean formulae,this paper proposes a new set of propagation rules for quantified boolean formulae based on literals and generated automatically thanks to quantified boolean formulae certificates. different decompositions by introduction of existentially quantified variables are discussed in order to construct complete systems. this set of rules is compared with already proposed quantified boolean propagation rule sets and stålmarck's method.,nan
2006,general concept inclusions in fuzzy description logics,"fuzzy description logics (fuzzy dls) have been proposed as a language to describe structured knowledge with vague concepts. a major theoretical and computational limitation so far is the inability to deal with general concept inclusions (gcis), which is an important feature of classical dls. in this paper, we address this issue and develop a calculus for fuzzy dls with gcis.",nan
2006,approximating extended answer sets,"we present an approximation theory for the extended answer set semantics, using the concept of an approximation constraint. intuitively, an approximation constraint, while satisfied by a “perfect” solution, may be left unsatisfied in an approximate extended answer set. approximations improve as the number of unsatisfied constraints decreases. we show how the framework can also capture the classical answer set semantics, thus providing an approximative version of the latter.",nan
2006,an axiomatic approach to qualitative decision theory with binary possibilistic utility,"binary possibilistic utility unifies two previously proposed qualitative decision models: optimistic and pessimistic utilities. all these decision models have been axiomatized in a von neumann-morgenstern setting. these axiomatizations have shown the formal similarity of these qualitative utilities and expected utility. unfortunately in this framework, the representation of uncertainty has to be clearly assumed to be given. in a more general setting, without this restriction, optimistic and pessimistic utilities have been axiomatized à la savage. this paper proposes a study of the axiomatics of binary possibilistic utility in a savagean framework.",nan
2006,an efficient upper approximation for conditional preference,"the fundamental operation of dominance testing, i.e., determining if one alternative is preferred to another, is in general very hard for methods of reasoning with qualitative conditional preferences such as cp-nets and conditional preference theories (cp-theories). it is therefore natural to consider approximations of preference, and upper approximations are of particular interest, since they can be used within a constraint optimisation algorithm to find some of the optimal solutions. upper approximations for preference in cp-theories have previously been suggested, but they require consistency, as well as strong acyclicity conditions on the variables. we define an upper approximation of conditional preference for which dominance checking is efficient, and which can be applied very generally for cp-theories.",nan
2006,a solver for qbfs in nonprenex form,"various problems in artificial intelligence (ai) can be solved by translating them into a quantified boolean formula (qbf) and evaluating the resulting encoding. in this approach, a qbf solver is used as a black box in a rapid implementation of a more general reasoning system. most of the current solvers for qbfs require formulas in prenex conjunctive normal form as input, which makes a further translation necessary, since the encodings are usually not in a specific normal form. this additional step increases the number of variables in the formula or disrupts the formula's structure. moreover, the most important part of this transformation, prenexing, is not deterministic. in this paper, we focus on an alternative way to process qbfs without these drawbacks and describe a solver, qpro, which is able to handle arbitrary formulas. to this end, we extend algorithms for qbfs to the non-normal form case and compare qpro with the leading normal-form provers on problems from the area of ai.",nan
2006,knowledge engineering for bayesian networks: how common are noisy-max distributions in practice?,"one problem faced in knowledge engineering for bayesian networks is the exponential growth of the number of parameters in their conditional probability tables (cpts). the most common practical solution is application of the noisy-or (or their generalization, the noisy-max) gates, which take advantage of independence of causal interactions and provide a logarithmic reduction of the number of parameters required to specify a cpt. in this paper, we propose an algorithm that fits a noisy-max distribution to an existing cpt and we apply it to search for noisy-max gates in three existing practical bayesian networks. we show that noisy-max gate provides a surprisingly good fit for as many as 50% of cpts in these networks. the importance of this finding is that it provides an empirical justification for the use of the noisy-max gate as a powerful knowledge engineering tool.",nan
2006,a unified model for multilabel classification and ranking,"label ranking studies the problem of learning a mapping from instances to rankings over a predefined set of labels. hitherto existing approaches to label ranking implicitly operate on an underlying (utility) scale which is not calibrated in the sense that it lacks a natural zero point. we propose a suitable extension of label ranking that incorporates the calibrated scenario and substantially extends the expressive power of these approaches. in particular, our extension suggests a conceptually novel technique for extending the common learning by pairwise comparison approach to the multilabel scenario, a setting previously not being amenable to the pairwise decomposition technique. we present empirical results in the area of text categorization and gene analysis, underscoring the merits of the calibrated model in comparison to state-of-the-art multilabel learning methods.",nan
2006,learning by automatic option discovery from conditionally terminating sequences,"this paper proposes a novel approach to discover options in the form of conditionally terminating sequences, and shows how they can be integrated into reinforcement learning framework to improve the learning performance. the method utilizes stored histories of possible optimal policies and constructs a specialized tree structure online in order to identify action sequences which are used frequently together with states that are visited during the execution of such sequences. the tree is then used to implicitly run corresponding options. effectiveness of the method is demonstrated empirically.",nan
2006,least squares svm for least squares td learning,we formulate the problem of least squares temporal difference learning (lstd) in the framework of least squares svm (ls-svm). to cope with the large amount (and possible sequential nature) of training data arising in reinforcement learning we employ a subspace based variant of ls-svm that sequentially processes the data and is hence especially suited for online learning. this approach is adapted from the context of gaussian process regression and turns the unwieldy original optimization problem (with computational complexity being cubic in the number of processed data) into a reduced problem (with computional complexity being linear in the number of processed data). we introduce a qr decomposition based approach to solve the resulting generalized normal equations incrementally that is numerically more stable than existing recursive least squares based update algorithms. we also allow a forgetting factor in the updates to track non-stationary target functions (i.e. for the use with optimistic policy iteration). experimental comparison with standard cmac function approximation indicate that ls-svms are well-suited for online rl.,nan
2006,argument based rule learning,"we present a novel approach to machine learning, called abml (argumentation based ml). this approach combines machine learning from examples with concepts from the field of argumentation. the idea is to provide expert's arguments, or reasons, for some of the learning examples. we require that the theory induced from the examples explains the examples in terms of the given reasons. thus arguments constrain the combinatorial search among possible hypotheses, and also direct the search towards hypotheses that are more comprehensible in the light of expert's background knowledge. in this paper we implement abcn2 as an extension of the cn2 rule learning algorithm, and analyze its advantages in comparison with the original cn2 algorithm.",nan
2006,a real generalization of discrete adaboost,"scaling discrete adaboost to handle real-valued weak hypotheses has often been done under the auspices of convex optimization, but little is generally known from the original boosting model standpoint. we introduce a novel generalization of discrete adaboost which departs from this mainstream of algorithms. from the theoretical standpoint, it formally displays the original boosting property; furthermore, it brings interesting computational and numerical improvements that make it significantly easier to handle “as is”. conceptually speaking, it provides a new and appealing scaling to r of some well known facts about discrete (ada)boosting. perhaps the most popular is an iterative weight modification mechanism, according to which examples have their weights decreased iff they receive the right class by the current discrete weak hypothesis. our generalization to real values makes that decreasing weights affect only the examples on which the hypothesis' margin exceeds its average margin. thus, while both properties coincide on the discrete case, examples that receive the right class can still be reweighted higher with real-valued weak hypotheses. from the experimental standpoint, our generalization displays the ability to produce low error formulas with particular cumulative margin distributions, and it provides a nice handling of those noisy domains that represent achilles' heel for common adaptive boosting algorithms.",nan
2006,discovery of entailment relations from event co-occurrences,"lexical entailment is knowledge that may prove very useful for a variety of applications that deal with information implicit in a text. in this paper we address the problem of automatic discovery of pairs of verbs related by entailment. a specific challenge in this task is recognition of the direction of entailment in a pair. we model entailment by a number of linguistic cues as to local coherence between clauses. we first investigate the effect of these cues on the quality of the model and then evaluate it against human judgements. our evaluation reveals that the model is capable of correctly identifying the direction in entailment-related pairs, although their separation from bidirectional pairs proves a more difficult task.",nan
2006,efficient knowledge acquisition for extracting temporal relations,"machine learning approaches in natural language processing often require a large annotated corpus. we present a complementary approach that utilizes expert knowledge to overcome the scarceness of annotated data. in our framework kaftie, the expert could easily create a large number of rules in a systematic manner without the need of a knowledge engineer. using kaftie, a knowledge base was built based on a small data set that outperforms machine learning algorithms trained on a much bigger data set for the task of recognizing temporal relations. furthermore, our knowledge acquisition approach could be used in synergy with machine learning algorithms to both increase the performance of the machine learning algorithms and to reduce the expert's knowledge acquisition effort.",nan
2006,efficient learning from massive spatial-temporal data through selective support vector propagation,"in the proposed approach, learning from large spatial-temporal data streams is addressed using the sequential training of support vector machines (svm) on a series of smaller spatial data subsets collected over shorter periods. a set of representatives are selected from support vectors corresponding to an svm trained with data of a limited spatial-temporal coverage. these representatives are merged with newly arrived data also corresponding to a limited spacetime segment. a new svm is learned using both sources. relying on selected representatives instead of propagating all support vectors to the next iteration allows efficient learning of semi-global svms in a non-stationary series consisting of correlated spatial datasets. the proposed method is evaluated on a challenging geoinformatics problem of aerosol retrieval from terra satellite based multi-angle imaging spectro radiometer instrument. regional features were discovered that allowed spatial partitioning of continental us to several semi-global regions. developed semi-global svm models were reused for efficient estimation of aerosol optical depth from radiances with a high level of accuracy on data cycles spanning several months. the obtained results provide evidence that svms trained as proposed have an extended spatial and temporal range of applicability as compared to svm models trained on samples collected over shorter periods. in addition, the computational cost of training a semi-global svm with selective support vector propagation (ssvp) was much lower than when training a global model using spatial observations from the entire period.",nan
2006,automatic term categorization by extracting knowledge from the web,"this paper addresses the problem of categorizing terms or lexical entities into a predefined set of semantic domains exploiting the knowledge available on-line in the web. the proposed system can be effectively used for the automatic expansion of thesauri, limiting the human effort to the preparation of a small training set of tagged entities. the classification of terms is performed by modeling the contexts in which terms from the same class usually appear. the web is exploited as a significant repository of contexts that are extracted by querying one or more search engines. in particular, it is shown how the required knowledge can be obtained directly from the snippets returned by the search engines without the overhead of document downloads. since the web is continuously updated “world wide”, this approach allows us to face the problem of open-domain term categorization handling both the geographical and temporal variability of term semantics. the performances attained by different text classifiers are compared, showing that the accuracy results are very good independently of the specific model, thus validating the idea of using term contexts extracted from search engine snippets. moreover, the experimental results indicate that only very few training examples are needed to reach the best performance (over 90% for the f1 measure).",nan
2006,strategic foresighted learning in competitive multi-agent games,"we describe a generalized q-learning type algorithm for reinforcement learning in competitive multi-agent games. we make the observation that in a competitive setting with adaptive agents an agent's actions will (likely) result in changes in the opponents policies. in addition to accounting for the estimated policies of the opponents, our algorithm also adjusts these future opponent policies by incorporating estimates of how the opponents change their policy as a reaction to ones own actions. we present results showing that agents that learn with this algorithm can successfully achieve high reward in competitive multi-agent games where myopic self-interested behavior conflicts with the long term individual interests of the players. we show that this approach successfully scales for multi-agent games of various sizes, in particular to the social dilemma type problems: from the small iterated prisoner's dilemma, to larger settings akin to harding's tragedy of the commons. thus, our multi-agent reinforcement algorithm is foresighted enough to correctly anticipate future rewards in the important problem class of social dilemmas, without having to resort to negotiation-like protocols or precoded strategies.",nan
2006,msda: wordsense discrimination using context vectors and attributes,"we present msda (major senses discovery algorithm) – a development over the context vector approach to (noun) sense discrimination [20, 24] that uses attributes and values instead of word features to cluster contexts, and does not require for the number of senses to be fixed beforehand. the algorithm achieves a precision of 89% on a dataset including both ambiguous and non-ambiguous nouns, twice that of previous algorithms.",nan
2006,integrating domain and paradigmatic similarity for unsupervised sense tagging,"an unsupervised methodology for word sense disambiguation, called dynamic domain sense tagging, is presented. it relies on the convergence of two very well known unsupervised approaches (i.e. domain driven disambiguation and conceptual density). for each target word a domain is dynamically modeled by expanding the its topical context, i.e. a set of words evoking the underlying/implict domain where the word is located. the estimation of the paradigmatic similarity within such a specific lexicon is assumed as a disambiguation model. the conceptual density measure is here used to account for paradigmatic associations, and the top scored senses of the target word are selected accordingly. results confirm the impact of domain based representation in capturing useful paradigmatic generalizations, especially when small text fragments are available. in addition, the precision/recall tradeoff of the resulting method can be tuned in a meaningful way, allowing us to achieve impressively high precision scores in a purely unsupervised setting.",nan
2006,disambiguating personal names on the web using automatically extracted key phrases,"when you search for information regarding a particular person on the web, a search engine returns many pages. some of these pages may be for people with the same name. how can we disambiguate these different people with the same name? this paper presents an unsupervised algorithm which produces unique phrases to disambiguate different people with the same name (i.e. namesakes). our algorithm takes in a personal name and outputs multiple sets of phrases which uniquely identify the different namesakes on the web. these phrases could then be added to the query to narrow down the search to a specific namesake. we evaluated the algorithm on a collection of documents retreived from the web. experimental results show a significant improvement over the existing methods proposed for this task.",nan
2006,history-based inside-outside algorithm,"grammar induction is one of the most important research areas of the natural language processing. the lack of a large treebank, which is required in supervised grammar induction, in some natural languages such as persian encouraged us to focus on unsupervised methods. we have found the inside-outside algorithm, introduced by lari and young, as a suitable platform to work on, and augmented io with a history notion. the result is an improved unsupervised grammar induction method called history-based io (hio). applying hio to two very divergent natural languages (i.e., english and persian) indicates that inducing more conditioned grammars improves the quality of the resultant grammar. besides, our experiments on atis and wsj show that hio outperforms most current unsupervised grammar induction methods.",nan
2006,"shallow semantic parsing based on framenet, verbnet and propbank","this article describes a semantic parser based on framenet semantic roles that uses a broad knowledge base created by interconnecting three major resources: framenet, verbnet and propbank. we link the above resources through a mapping between intersective levin classes, which are part of propbank's annotation, and the framenet frames. by using levin classes, we successfully detect framenet semantic roles without relying on the frame information. at the same time, the combined usage of the above resources increases the verb coverage and confers more robustness to our parser. the experiments with support vector machines on automatic levin class detection suggest that (a) tree kernels are well suited for the task and (b) intersective levin classes can be used to improve the accuracy of semantic parsing based on framenet roles.",nan
2006,semantic tree kernels to classify predicate argument structures,"recent work on semantic role labeling (srl) has shown that syntactic information is critical to detect and extract predicate argument structures. as syntax is expressed by means of structured data, i.e. parse trees, its encoding in learning algorithms is rather complex.in this paper, we apply tree kernels to encode the whole predicate argument structure in support vector machines (svms). we extract from the sentence syntactic parse the subtrees that span potential argument structures of the target predicate and classify them in incorrect or correct structures by means of tree kernel based svms. experiments on the propbank collection show that the classification accuracy of correct/incorrect structures is remarkably high and helps to improve the accuracy of the srl task. this is a piece of evidence that tree kernels provide a powerful mechanism to learn the complex relation between syntax and semantics.",nan
2006,a multivalued logic model of planning,"in this work a model for planning with multivalued fluents and graded actions, based on the infinite valued lukasiewicz logic, is introduced. in multivalued planning, fluents can assume truth values in the interval [0, 1] and actions can be executed at different application degrees also varying in [0, 1]. the notions of planning problem and solution plan also reflect a multivalued approach. multivalued fluents and graded actions allow to model many real situations where some features of the world cannot be modeled with boolean values and where actions can be executed with varying strength which produces graded effects as well. even if most existing planning models fail to address this kind of domains, our model is comparable with models allowing flexible actions and soft constraints. a correct/complete algorithm which solves bounded multivalued planning problems based on mip compilation is also described and a prototype implementation is presented.",nan
2006,strong cyclic planning under partial observability,"strong cycling planning aims at generating iterative plans that implement trial-and-error strategies, where loops are allowed only so far as there is a chance to reach the goal. in this paper, we tackle the problem of strong cyclic planning under partial observability, making three main contributions. first, we provide a formal definition of the problem. we point out that several degrees of solution are possible and equally interesting, depending on the admissible delay between achieving the goal and detecting that it has been achieved. second, we present a family of planning algorithms that tackle the different versions of the problem. third, we implement the algorithms using efficient symbolic representation techniques, and experimentally compare their performances.",nan
2006,approximation properties of planning benchmarks,"for many classical planning domains, the computational complexity of non-optimal and optimal planning is known. however, little is known about the area in between the two extremes of finding some plan and finding optimal plans. in this contribution, we provide a complete classification of the propositional domains from the first four international planning competitions with respect to the approximation classes po, ptas, apx, poly-apx, and npo.",nan
2006,approximate linear-programming algorithms for graph-based markov decision processes,"in this article, we consider a form of compact representation of mdp based on graphs, and we propose an approximate solution algorithm derived from this representation. the approach we propose belongs to the family of approximate linear programming methods, but the graph-structure we assume allows it to become particularly efficient. the proposed method complexity is linear in the number of variables in the graph and only exponential in the width of a dependency graph among variables.",nan
2006,mean field approximation of the policy iteration algorithm for graph-based markov decision processes,"in this article, we consider a compact representation of multidimensional markov decision processes based on graphs (gmdp). the states and actions of a gmdp are multidimensional and attached to the vertices of a graph allowing the representation of local dynamics and rewards. this approach is in the line of approaches based on dynamic bayesian networks. for policy optimisation, a direct application of the policy iteration algorithm, of exponential complexity in the number of nodes of the graph, is not possible for such high dimensional problems and we propose an approximate version of this algorithm derived from the gmdp representation. we do not try to approximate directly the value function, as usually done, but we rather propose an approximation of the occupation measure of the model, based on the mean field principle. then, we use it to compute the value function and derive approximate policy evaluation and policy improvement methods. their combination yields an approximate policy iteration algorithm of linear complexity in terms of the number of nodes of the graph. comparisons with the optimal solution, when available, and with a naive short-term policy demonstrate the quality of the proposed procedure.",nan
2006,unified definition of heuristics for classical planning,"in many types of planning algorithms distance heuristics play an important role. most of the earlier works restrict to strips operators, and their application to a more general language with disjunctivity and conditional effects first requires an exponential size reduction to strips operators.i present direct formalizations of a number of distance heuristics for a general operator description language in a uniform way, avoiding the exponentiality inherent in earlier reductive approaches. the formalizations use formulae to represent the conditions under which operators have given effects. the exponentiality shows up in satisfiability tests with these formulae, but would appear to be a minor issue because of the small size of the formulae.",nan
2006,applying trip@dvice recommendation technology to www.visiteurope.com,"implementing decision support technologies in a real commercial tourism destination portal is challenging. first of all, the peculiar problems related to the tourism domain, which have been studied in the recent years in ecommerce and tourism research, must be considered. but, to provide an effective and useful tool, one must tackle additional requirements arising from the technical and operational environment, which influence not only the software development and architectural issues, but also methodological aspects. this paper describes the main choices we taken and the approach we followed to integrate the trip@dvice recommendation technology in www.visiteurope.com, the major european tourism portal launched in march 2006, with the goal of promoting europe as a tourist destination.",nan
2006,natural and intuitive multimodal dialogue for in-car applications: the sammie system,"we present sammie, a laboratory demonstrator of an in-car showcase of a multimodal dialogue system developed in the talk projecttalk (talk and look: tools for ambient linguistic knowledge) www.talk-project.org is funded by the eu as project no. ist-507802 within the 6th framework program. in cooperation between dfki/usaar/bosch/bmw, to show natural, intuitive mixed-initiative interaction, with particular emphasis on multimodal turn-planning and natural language generation. sammie currently supports speech-centered multimodal access for the driver to a mp3-player application including search and browsing, as well as composition and modification of playlists. our approach to dialogue modeling is based on collaborative problem solving integrated with an extended information state update paradigm. a formal usability evaluation of a first baseline system of sammie by naive users in a simulated environment yielded positive results, and the improved final version will be integrated in a bmw research car.",nan
2006,a client/server user-based collaborative filtering algorithm: model and implementation,"this paper describes a new way of implementing an intelligent web caching service, based on an analysis of usage. since the cache size in software is limited, and the search for new information is time-consuming, it becomes interesting to automate the process of selecting the most relevant items for each user. we propose a new generic model based on a client/server collaborative filtering algorithm and a behavior modeling process. in order to highlight the benefits of our solution, we collaborated with a company called astra which is specialized in satellite website broadcasting. astra has finalized a system sponsored by advertisement and supplying to users a high bandwidth access to hundreds of websites for free. our work has been implemented within its software architecture and, in particular, within its recommender system in order to improve the satisfaction of users. our solution is particularly designed to address the issues of data sparsity, privacy and scalability. because of the industrial context, we consider the situation where the set of users is relatively stable, whereas the set of items may vary considerably from an execution to another. in addition to the model and its implementation, we present a performance assessment of our technique in terms of computation time and prediction relevancy.",nan
2006,software companion - the mexar2 support to space mission planners,"this paper describes a fielded ai system in daily use at the european space agency (esa-esoc) since february 2005. the tool, named mexar2, provides continuous support to human mission planners in synthesizing plans for downlinking on-board memory data from the mars express spacecraft to earth.the introduction of the tool in the mission planning workflow significantly decreased the time spent in producing plans. moreover mexar2 improves the quality of the produced plans thus guaranteeing a strong reliability in data return enabling a more intensive science activity on board. the introduction of mexar2 has modified the role of the human mission planners who can now evaluate and compare different solutions rather than dedicating their time exclusively to computing single solutions (a tedious and repetitive task which does not capitalize on the mission planners' decision-making expertise). these characteristics have effectively made mexar2 a fundamental work companion for the human mission planners.",nan
2006,ecue: a spam filter that uses machine learning to track concept drift,"while text classification has been identified for some time as a promising application area for artificial intelligence, so far few deployed applications have been described. in this paper we present a spam filtering system that uses example-based machine learning techniques to train a classifier from examples of spam and legitimate email. this approach has the advantage that it can personalise to the specifics of the user's filtering preferences. this classifier can also automatically adjust over time to account for the changing nature of spam (and indeed changes in the profile of legitimate email). a significant software engineering challenge in developing this system was to ensure that it could interoperate with existing email systems to allow easy managment of the training data over time. this system has been deployed and evaluated over an extended period and the results of this evaluation are presented here.",nan
2006,knowledge-based recommendation: technologies and experiences from projects,"recommender applications are used to support intuitive and personalized product and service selection processes for customers and sales representatives. in this paper we present a knowledge-based recommender environment which assists users in the identification of appropriate solutions from complex product and service assortments. we show how intelligent query relaxation, personalization and intuitive knowledge acquisition support the implementation of customer-friendly sales dialogues and report experiences gained in industrial recommender projects.",nan
2006,diagnosing highly configurable products: troubleshooting support for airbus final assembly line,"in this paper we describe the successful introduction of the model-based technology in a troubleshooting support tool for aircraft wiring under industrial conditions at airbus, where strict cost/benefit analyzes decide over success or failure of a tool.aircraft built at airbus deutschland gmbh are highly customized products: virtually no two aircraft are identical. troubleshooting support cannot be built in the traditional way of compiling static decision trees once and for all aircraft. rather, diagnosis dialogs must be computed on demand for each individual aircraft.our solution is based on dynamically generating the diagnosis dialogs from on-line generated electrical signal nets. generating diagnosis dialogs on demand is a considerable technical challenge. we have solved this with a combination of intelligent document extraction services and model-based diagnosis technologies. however, the paper focuses less on technical issues and more on the non-technological aspects that highlight the challenges of deploying well established intelligent technologies in industry.for example, while existing engineering data is correctly interpretable by “fault tolerant and flexible” humans, it remains a challenging input for algorithms. furthermore, some psychological aspects remain a challenge to the acceptance of the proposed tool.",nan
2006,web-based tools for codification with medical ontologies in switzerland,"this paper presents three aspects through which code retrieval from an medical ontology can be improved. first, the recall of correct codes can be increased by the application of appropriate morphological operations on query terms and the enhancement of concept names with synonymous terms. secondly, the efficiency of a manual selection process from a result set can be improved by structuring the result set correspondingly to the original ontology. finally direct enhancements to the structure and content of the ontologies can be made in order to personalise it. these methods were implemented and evaluated in tools for two ontologies of major importance in the swiss health care system: the icd 10 and the tarmed ontology.",nan
2006,model-based failure analysis with rodon,"the model-based reasoning tool rodon supports engineers in their quest for reliable, well-designed technical products, by providing means to analyze system behavior, especially in case of failure, systematically. based on a quantitative product model, it offers a wide range of analyses, including reliability analyses such as fmea and fta or the generation of diagnostic knowledge such as diagnostic decision trees. an object-oriented modeling language enhances the reusability of models and, together with an integrated design environment and extensive model libraries, considerably reduces the modeling effort. in this paper, we describe the modeling framework and the model analyses supported by rodon (considering as example the model of a comfort seat), and characterize the technology behind them. finally, we discuss the experience gained during the development of rodon.",nan
2006,a learning classifier approach to tomography,"tomography is an important technique for noninvasive imaging: images of the interior of an object are computed from several scanned projections of the object, covering a range of angles. traditionally, tomographic reconstruction algorithms have been based on techniques from analysis and linear algebra. in this paper we describe how a particular version of the tomographic reconstruction problem, known as discrete tomography, can be considered as a classification problem. by making this connection between two seemingly unrelated scientific areas, the full machinery of learning classifier theory can be used to solve tomographic problems.the use of classifiers that can be trained from examples for tomography has two main advantages. first, prior knowledge concerning the images of interest can be learned automatically. second, there are several types of classifiers that can perform the classification task very fast once trained. real-time reconstruction is one of the main goals in tomographic imaging.tomography typically deals with huge amounts of high-dimensional data, which makes the corresponding classification task very hard. we describe several steps to reduce the dimensionality of the input data. for one type of classifier, the multilayer perceptron, we provide experimental evidence showing that it can indeed be used for efficient tomographic imaging.",nan
2006,depth ordering and figure-ground segregation in monocular images derived from illusory contour perception,"one of the elaborated tasks for the low level image processing stage of a vision system is the completion of cluttered or occluded object boundaries and the depth assignment of overlapped boundaries. we describe a novel method for depth ordering and figure-ground segregation from monocular depth cues, namely the arrangement of so-called illusory contours at junctions in the edge map of an image. therefore, a computational approach to the perception of illusory contours, based on the tensor voting technique, is introduced. furthermore, two simple rules for figure-ground segregation are presented that are capable of explaining a wide range of perceptual phenomena like the kanizsa illusion. finally, we present a diffusion approach to derive the depth ordering for the whole contour from the figure-ground segregation at key points.",nan
2006,graph neural networks for object localization,"graph neural networks (gnns) are a recently proposed connectionist model that extends previous neural methods to structured domains. gnns can be applied on datasets that contain very general types of graphs and, under mild hypotheses, they have been proven to be universal approximators on graphical domains. whereas most of the common approaches to graphs processing are based on a preliminary phase that maps each graph onto a simpler data type, like a vector or a sequence of reals, gnns have the ability to directly process input graphs, thus embedding their connectivity into the processing scheme. in this paper, the main theoretical properties of gnns are briefly reviewed and they are proposed as a tool for object localization. an experimentation has been carried out on the task of locating the face of a popular walt disney character in comic covers. in the dataset the character is shown in a number of different poses, often in cluttered backgrounds, and in high variety of colors. the proposed learning framework provides a way to deal with complex data arising from image segmentation process, without exploiting any prior knowledge on the dataset. the results are very encouraging, prove the viability of the method and the effectiveness of the structural representation of images.",nan
2006,situation assessment for sensor-based recovery planning,"we present an approach for recovery from perceptual failures, or more precisely anchoring failures. anchoring is the problem of connecting symbols representing objects to sensor data corresponding to the same objects. the approach is based on using planning, but our focus is not on the plan generation per se. we focus on the very important aspect of situation assessment and how it is carried out for recovering from anchoring failures. the proposed approach uses background knowledge to create hypotheses about world states and handles uncertainty in terms of probabilistic belief states. this work is relevant both from the perspective of developing the anchoring framework, and as a study in plan-based recovery from epistemic failures in mobile robots. experiments on a mobile robot are shown to validate the applicability of the proposed approach.",nan
2006,learning behaviors models for robot execution control,"robust execution of robotic tasks is a difficult problem. in many situations, these tasks involve complex behaviors combining different functionalities (e.g. perception, localization, motion planning and motion execution). these behaviors are often programmed with a strong focus on the robustness of the behavior itself, not on the definition of a “high level” model to be used by a task planner and an execution controller. we propose to learn behaviors models as structured stochastic processes: dynamic bayesian network. indeed, the dbn formalism allows us to learn and control behaviors with controllable parameters. we experimented our approach on a real robot, where we learned over a large number of runs the model of a complex navigation task using a modified version of expectation maximization for dbn. the resulting dbn is then used to control the robot navigation behavior and we show that for some given objectives (e.g. avoid failure, optimize speed), the learned dbn driven controller performs much better than the programmed controller. we also show a way to achieve efficient incremental learning of the dbn. we believe that the proposed approach remains generic and can be used to learn complex behaviors other than navigation and for other autonomous systems.",nan
2006,plan-based configuration of a group of robots,"we consider groups of autonomous robots in which robots can help each other by offering information-producing functionalities. a functional configuration is a way to allocate and connect functionalities among robots. in general, different configurations can be used to solve the same task, depending on the current situation. in this paper, we define the idea of functional configuration, and we propose a plan-based approach to automatically generate a preferred configuration for a given task, environment, and set of resources. to illustrate these ideas, we show a simple experiment in which two robots mutually help each-other to cross a door.",nan
2006,agents with anticipatory behaviors: to be cautious in a risky environment,"this work presents some anticipatory mechanisms in an agent architecture, modeling affective behaviours as effects of surprise. through experiment discussion, the advantages of becoming cautious are presented and is shown how this approach increases not only opportunism and reactivity but also anticipatory capabilities for planning and reasoning. cautious agents outcomes are analyzed in risky surroundings both on the basis of the environment features and of the internal model of caution. expectation-driven and caution enabled agents are designed with the bdi paradigm.",nan
2006,ai and music: toward a taxonomy of problem classes,"the application of artificial intelligence technology to the field of music has always been fascinating, from the first attempts in automating human problem solving behavior till this day. human activities related to music vary in their complexity and in their amenability of becoming automated, and for both musicians and ai researchers various questions arise intuitively, e. g.: what are music-related activities or tasks that can be automated? how are they related to each other? which problem solving methods have proven well? in which places does ai technology contribute?actually, the literature in the intersection of ai and music focuses on single problem classes and particular tasks only, and a comprehensive picture is not drawn. this paper, which outlines key ideas of our research in this field, provides a step toward closing this gap: it proposes a taxonomy of problem classes and tasks related to music, along with methods solving them.",nan
2006,using emotions for behaviour-selection learning,"emotions play a very important role in human behaviour and social interaction. in this paper we present a control architecture which uses emotions in the behaviour selection process of autonomous and social agents. the state of the agent is determined by its internal state, defined by its dominant motivation, and its relation with the external objects including other agents. the behaviour selection is learned by the agent using standard and multiagent q-learning algorithms. the considered emotions are fear, happiness and sadness. the role of these emotions in this architecture is different, while the learning algorithms use happiness/sadness of the agent as positive/negative reinforcement signals, the emotion fear is used to prevent the agent of choosing dangerous actions as well as a motivation.",nan
2006,search better and gain more: investigating new graph structures for multi-agent negotiations,"combined negotiation in multi-agent systems is a hard task since it involves several levels of difficulty. in order to improve their payoff, first agents behave in a strategic manner while bargaining since they need to deal with various types of behaviors. second agents have to react to the proposals of other agents in finding the optimal solutions for their negotiation. this paper tackles the problem of winner determination in combined multi-agent negotiations and addresses two fundamental issues. the first contribution of this work is a winner determination algorithm, which finds an optimal solution, which is a combination of several bids selected from a set of bids at one iteration of a combined negotiation. in addition, the problem of dynamically revising these optimal solutions with regards to changes on bids is considered. these changes occur in multi-agent negotiation processes having several iterations and which use multi-phased protocols. to date no work addresses this problem. the second contribution of this work is an iterative algorithm for updating the optimal solutions to avoid integral and repetitive reapplication of the winner determination algorithm. the results of the experiments carried out using our algorithms have confirmed the importance and the originality of our approach based on the use of shared and unshared item graphs.",nan
2006,an argumentation-based framework for designing dialogue strategies,"a dialogue strategy is the set of rules followed by an agent when choosing a move (act + content) during a dialogue. this paper argues that a strategy is decision problem in which an agent selects i) among the acts allowed by the protocol the best option that, according to some strategic beliefs of the agent will at least satisfy the most important strategic goals of the agent, and ii) among different alternatives (eg. different offers), the best one that according to some basic beliefs of the agent, will satisfy the functional goals of the agent. the paper proposes a formal framework based on argumentation for computing the best move to play at a given step of the dialogue.",nan
2006,a multiagent system for scheduling activities onboard a space system,the possible advantages of employing multiple agents to manage activities on a single space system are largely unexplored. this paper presents the experimental validation of a multiagent scheduler for a low earth orbit satellite.,nan
2006,benefits of combinatorial auctions with transformability relationships,"in this paper we explore whether an auctioneer/buyer may benefit from introducing his transformability relationships (some goods can be transformed into others at a transformation cost) into multi-unit combinatorial reverse auctions. thus, we quantitatively assess the potential savings the auctioneer/buyer may obtain with respect to combinatorial reverse auctions that do not consider tranformability relationships. furthermore, we empirically identify the market conditions under which it is worth for the auctioneer/buyer to exploit transformability relationships.",nan
2006,"count-as conditionals, classification and context","searle represents constitutive norms as count-as conditionals, written as ‘x counts as y in context c’. grossi et al. study a class of these conditionals as ‘in context c, x is classified as y’. in this paper we propose a generalization of this relation among count-as conditionals, classification and context, by defining a class of count-as conditionals as ‘x in context c0 is classified as y in context c’. we show that if context c0 can be different from context c, then we can represent a larger class of examples, and we have a weaker logic of count-as conditionals.",nan
2006,fair distribution of collective obligations,"in social mechanism design, obligation distribution creates individual or contractual obligations that imply a collective obligation. a distinguishing feature from group planning is that also the sanction of the collective obligation has to be distributed, for example by creating sanctions for the individual or contractual obligations. in this paper we address fairness in obligation distribution for more or less powerful agents, in the sense that some agents can perform more or less actions than others. based on this power to perform actions, we characterize a trade-off in negotiation power. on the one hand, more powerful agents may have a disadvantage during the negotiation, as they may be one of the few or even the only agent who can see to some of the actions that have to be performed to fulfill the collective obligation. on the other hand, powerful agents may have an advantage in some negotiation protocols, as they have a larger variety of proposals to choose from. moreover, powerful agents have an advantage because they can choose from a larger set of possible coalitions. we present an ontology and measures to find a fair trade-off between these two forces in social mechanism design.",nan
2006,acyclic argumentation: attack = conflict + preference,"in this paper we study the fragment of dung's argumentation theory in which the strict attack relation is acyclic. we show that every attack relation satisfying a particular property can be represented by a symmetric conflict relation and a transitive preference relation in the following way. we define an instance of dung's abstract argumentation theory, in which 'argument a attacks argument b' is defined as 'argument a conflicts with argument b' and 'argument a is at least as preferred as argument b', where the conflict relation is symmetric and the preference relation is transitive. we show that this new preference-based argumentation theory characterizes the acyclic strict attack relation, in the sense that every attack relation defined as such a combination satisfies the property, and for every attack relation satisfying the property we can find a symmetric conflict relation and a transitive preference relation satisfying the equation.",nan
2006,multi-agent least-squares policy iteration,"least-squares policy iteration [3] is an approximate reinforcement learning technique capable of training policies over large, continuous state spaces. unfortunately, the computational requirements of lspi scale poorly with the number of system agents. work has been done to address this problem, such as the coordinated reinforcement learning (crl) approach of guestrin, et al [1], but this requires that one have prior information about the learning system such as knowing interagent dependencies and the form of the q-function. we demonstrate a hybrid gradient-ascent/lspi approach which is capable of using lspi to efficiently train multi-agent policies. our approach has computational requirements which scale as o(n), where n is the number of system agents, and does not have the prior knowledge requirements of crl. finally, we demonstrate our algorithm on a standard multi-agent network control problem [1].",nan
2006,finding instances of deduction and abduction in clinical experimental transcripts,"this article describes the design and implementation of a prototype that analyzes and classifies transcripts of interviews collected during an experiment that involved lateral-brain damage patients. the patients' utterances are classified as instances of categorization, prediction and explanation (abduction) based on surface linguistic cues. the agreement between our automatic classifier and human annotators is measured. the agreement is statistically significant, thus showing that the classification can be performed in an automatic fashion.",nan
2006,an alternative inference for qualitative choice logic,"qualitative choice logic adds to classical propositional logic a new connective, called ordered disjunction, used to express preferences between alternatives. we present an alternative inference relation for the qcl language that overcomes some qcl limitations.",nan
2006,on the existence of answer sets in normal extended logic programs,"one of the serious problems in answer set programming is that relatively small pieces of information can cause a total absence of answer sets. to cope with this problem, this paper introduces a class of normal extended logic programs which are extended logic programs, whose defeasible rules are comparable to normal defaults in default logic. under suitable program transformations, we show that every normal extended logic program always yields at least one answer set.",nan
2006,smoothed particle filtering for dynamic bayesian networks,"particle filtering (pf) for dynamic bayesian networks (dbns) with discrete-state spaces includes a resampling step which concentrates samples according to their relative weight in regions of interest of the state-space. we propose a more systematic approach than resampling based on regularisation (smoothing) of the empirical distribution associated with the samples, using the kernel method. we show in our experiments that the smoothed particle filtering (spf) leads to more accurate estimates than the pf.",nan
2006,goal revision for a rational agent,we propose a general framework to represent changes in the mental state of a rational agent due to the acquisition of new information and/or to the arising of new desires; fundamental postulates and properties of the function which generates the goal set are also provided.,nan
2006,a redundancy-based method for relation instantiation from the web,"the semantic web requires automatic ontology population methods. we developed an approach, that given existing ontologies, extracts instances of ontology relations, a specific subtask of ontology population. we use generic, domain independent techniques to extract candidate relation instances from the web and exploit the redundancy of information on the web to compensate for loss of precision caused by the use of these generic methods. the candidate relation instances are then ranked based on co-occurrence with a seed set. in an experiment, we extracted instances of the relation between artists and art styles. the results were manually evaluated against selected art resources.",nan
2006,norms with deadlines in dynamic deontic logic,in this paper we extend the logical framework defined by k. segerberg about norms and actions to norms that refer to deadlines. we also characterize the circumstances where these norms are violated.,nan
2006,adaptive multi-agent programming in gtgolog,"we present a novel approach to adaptive multi-agent programming, which is based on an integration of the agent programming language gtgolog with adaptive dynamic programming techniques. gtgolog combines explicit agent programming in golog with game-theoretic multi-agent planning in stochastic games. in gtgolog, the transition probabilities and reward values of the domain must be provided with the model. the adaptive generalization of gtgolog proposed here is directed towards letting the agents themselves explore and adapt these data. we use high-level programs for the generation of both abstract states and optimal policies.",nan
2006,formalizing complex task libraries in golog,"we present an approach to building libraries of tasks in complex action languages, such as golog, for query answering. our formalization is based on a situation calculus framework that allows probabilistic, temporal actions. once a knowledge base is built containing domain knowledge including type information and a library of tasks and the goals they can achieve, we are interested in queries about the achievability of goals. we consider cases where, using domain object type and goal information in the kb, a user is able to get specific answers to a query while leaving some of the details for the system to figure out. in some cases where the specifics are missing from the kb, the user is provided with possible alternative answers that are compatible with the incomplete information in the kb. this approach is being explored in the context of a military operations planning domain for decision support.",nan
2006,automated deduction for logics of default reasoning,"we present a tableau calculus for the rational logic r of default reasoning, introduced by kraus, lehmann and magidor. our calculus is obtained by introducing suitable modalities to interpret conditional assertions, it makes use of labels to represent possible worlds, and it can be used to provide a decision procedure for r.",nan
2006,reasoning about motion patterns,"in order to analyse motion patterns for the purpose of making predictions and to explain the behaviour of systems methods are required for dealing with them. difficulties in verbalising motion patterns for the purpose of communicating spatiotemporal situations, or in order to index spatiotemporal databases, indicate the importance of means which are simple to handle by human users. this paper proposes a set of sixteen atomic motion patterns which form the basis of a relation algebra. the relations are coarse but crisp, they allow imprecise knowledge about motion patterns to be dealt with, and they are easily accessible from the point of view of the user.",nan
2006,variable forgetting in preference relations over propositional domains,"representing (and reasoning about) preference relations over combinatorial domains is computationally expensive. for many problems involving such preferences, it is relevant to simplify them by projecting them on a subset of variables. we investigate several possible definitions, focusing without loss of generality on propositional (binary) variables.",nan
2006,what's a head without a body?,"concepts in answer set programming (asp) are normally defined in terms of atoms.we show that the treatment of atoms and bodies (of rules) as equitable computational objects may yield exponential speed-ups, even for standard asp-solvers such as smodels. to this end, we give simple transformations providing solvers with access to both kinds of objects and show that some families of examples can be solved exponentially faster after they have been transformed. we prove that these transformations may yield exponentially smaller search spaces.",nan
2006,decision making in large-scale domains: a case study,"planning under uncertainty attracted significant attention in ai and in other fields. to overcome computational problems associated with markov decision processes (mdps) in large scale domains researchers often take advantage of structural properties and look for approximately optimal solutions. dtgolog, a decision-theoretic agent programming language based on the situation calculus, was proposed to ease some of the computational difficulties by using natural ordering constraints on execution of actions. using dtgolog, domain specific constraints on the set of available policies can be expressed in a high-level program and this program helps to reduce significantly computation required to find a policy optimal in this set. our paper explores whether the dtgolog framework can be used to evaluate different designs of a decision making agent in a large real-world domain. each design is understood as combination of a template (expressed as a golog program) for available policies and a reward function. to evaluate and compare alternative designs we estimate the probability of goal satisfaction for each design. as a domain, we choose the london ambulance service (las) case study that is well known in software engineering, but remains unknown in ai. in our paper we demonstrate that dtgolog can be applied successfully to quantitative evaluation of alternative designs in terms of their ability to satisfy a system goal with a high probability. we provide a detailed axiomatization of the domain in the temporal situation calculus with stochastic actions. the main advantage of this representation is that neither actions, not states require explicit enumeration. we do an experimental analysis using an on-line implementation of dtgolog coupled with a simulator that models real time actions of many external agents.",nan
2006,towards a logic of agency and actions with duration,"as far as we know, there is no multi-agent system allowing to talk both about choices of agents or groups of agents, strategies, and about sufficiently rich actions. this paper aims at offering a path towards a new more expressive logical framework by mixing a stit-like logic of agency with a pdl-like logic of action. we present the syntax and ontological motivations, and we highlight the expressivity of the resulting framework on an example.",nan
2006,logic profiling for multicriteria rating on web pages,i want to propose a general framework for user-oriented and content-based recommender systems aimed at providing preferential multicriteria rating on web pages and automatic user's profile generation and updating through logic programming techniques.,nan
2006,an empirical analysis of the complexity of model-based diagnosis,"we empirically study the computational complexity of diagnosing systems with real-world structure. we adopt the structure specified by a small-world network, which is a graphical structure that is common to a wide variety of naturally-occurring systems, ranging from biological systems, the www, to human-designed mechanical systems. we randomly generate a suite of digital circuit models with small-world network structure, and show that diagnosing these models is computationally hard.",nan
2006,identifying inter-domain similarities through content-based analysis of hierarchical web-directories,"providing accurate personalized information services to the users requires knowing their interests and needs, as defined by their user models (ums). since the quality of the personalization depends on the richness of the ums, services would benefit from enriching their ums through importing and aggregating partial ums built by other services from relatively similar domains. the obvious question is how to determine the similarity of domains? this paper proposes to compute inter-domain similarities by exploiting well-known information retrieval techniques for comparing textual contents of the web-sites, classified under the domain nodes in web-directories. initial experiments validate feasibility of the proposed approach and raise open research questions.",nan
2006,calibrating probability density forecasts with multi-objective search,"the two objectives when estimating the conditional density function in a regression problem are to maximise sharpness (the density rewarded to the actual observation), while maintaining calibration (the empirical validity of the probability estimates). in this paper we outline a process of optimisation that maximises both these objectives simultaneously to make better probabilistic predictions.",nan
2006,adaptation knowledge discovery from a case base,"in case-based reasoning, the adaptation step depends in general on domain-dependent knowledge, which motivates studies on adaptation knowledge acquisition (aka). cabamaka is an aka system based on principles of knowledge discovery from databases. this system explores the variations within the case base to elicit adaptation knowledge. it has been successfully tested in an application of case-based decision support to breast cancer treatment.",nan
2006,polynomial conditional random fields for signal processing,we describe polynomial conditional random fields for signal processing tasks. it is a hybrid model that combines the ability of polynomial hidden markov models for modeling complex dynamic signals and the discriminant power of conditional random fields. we detail the learning of these models and report experimental results on handwriting recognition.,nan
2006,stream clustering based on kernel density estimation,"we present a novel algorithm for clustering streams of multidimensional points based on kernel density estimates of the data. the algorithm requires only one pass over each data point and a constant amount of space, which depends only on the accuracy of clustering. the algorithm recognizes clusters of nonspherical shapes and handles both inserted and deleted objects in the input stream. querying the membership of a point in a cluster can be answered in constant time.",nan
2006,version space learning for possibilistic hypotheses,"in this paper, we are interested in learning stratified hypotheses from examples and counter-examples associated with weights that express their prototypical importance. it leads to an extension of the well-known version space learning framework. in order to do that, we emphasize that the treatment of positive and negative examples in version space learning is reminding of a bipolar revision process recently studied in the setting of possibilistic information representation. bipolarity appears when the positive and negative sides of information are specified in a distinct way. then, we use the possibilistic bipolar representation setting, which distinguishes between what is guaranteed to be possible, and what is simply not impossible, as a basis for extending version space learning to examples associated with possibility degrees. it allows us to define a formal framework for learning layered hypotheses.",nan
2006,ensembles of grafted trees,"grafted trees are trees that are constructed using two methods. the first method creates an initial tree, while the second method is used to complete the tree.in this work, the first classifier is an unpruned tree from a 10% sample of the training data. grafting is a method for constructing ensembles of decision trees, where each tree is a grafted tree. grafting by itself is better than bagging.moreover, grafted trees can also be used with any other ensemble method. it is clearly beneficial for bagging and random forests. when using grafted trees with boosting, the results depends of the considered variant. the best overall method is grafted random forest.",nan
2006,patch learning for incremental classifier design,we present a learning algorithm for nominal data. it builds a classifier by adding iteratively a simple patch function that modifies the current classifier. its main advantage lies in the possibility to learn every patch function parameters optimally from the bayesian point of view hence avoiding overtraining.,nan
2006,meta-typicalness approach to reliable classification,we propose a meta-typicalness approach to apply the typicalness framework for any type of classifiers. the approach can be used to construct classifiers with specified classification performance. experiments show that the approach results in classifiers that can outperform an existing typicalness-based classifier.,nan
2006,text sampling and re-sampling for imbalanced authorship identification cases,"authorship identification can be seen as a single-label multi-class text categorization problem. very often, there are extremely few training texts at least for some of the candidate authors. in this paper, we present methods to handle imbalanced multi-class textual datasets. the main idea is to segment the training texts into sub-samples according to the size of the class. hence, minority classes can be segmented into many short samples and majority classes into less and longer samples. moreover, we explore text re-sampling in order to construct a training set according to a desirable distribution over the classes. essentially, text re-sampling can be viewed as providing new synthetic data that increase the training size of a class. based on a corpus of newswire stories in english we present authorship identification experiments on various multi-class imbalanced cases.",nan
2006,is web genre identification feasible?,"this paper contributes to a facet from the area of web information retrieval that has recently received much attention: the satisfaction of a user's personal information need with respect to text type, presentation type, or information quality. we imply that such properties can be quantified for all kinds of web documents, and we subsume them under the term “web genre” or “genre”.recent surveys show that there is—to a certain degree—a common understanding of web genre. however, the strictness by which genre and non-genre aspects of a document are experienced is an individual matter. to get a better understanding of the challenges of web genre identification and its possible limits we investigate in this paper a very interesting question, which has not been posed by now:given a categorization c of documents (or bookmarks, links, document identifiers), can we provide a reliable assessment whether c is governed by topic or by genre considerations?",nan
2006,adaptive context-based term (re)weighting - an experiment on single-word question answering,"term weighting is a crucial task in many information retrieval applications. common approaches are based either on statistical or on natural language analysis. in this paper, we present a new algorithm that capitalizes from the advantages of both the strategies. in the proposed method, the weights are computed by a parametric function, called context function, that models the semantic influence exercised amongst the terms. the context function is learned by examples, so that its implementation is mostly automatic. the algorithm was successfully tested on a data set of crossword clues, which represent a case of single-word question answering.",nan
2006,how to analyze free text descriptions for recommending tv programmes?,"this paper presents an approach to exploit free text de- scriptions of tv programmes as available from epg data sets for a recommendation system that takes the content of programmes into accountour research is sponsored by software-offensive bayern. ferdinand herrmann, heike ott, kristina makedonska, and sebastian von mammen provided valuable help implementing major parts of the presented system.. the paper focusses on classifying free text descriptions in relation to natural language user queries.",nan
2006,soft uncoupling of markov chains for permeable language distinction: a new algorithm,"without prior knowledge, distinguishing different languages may be a hard task, especially when their borders are permeable.we develop an extension of spectral clustering — a powerful unsupervised classification toolbox — that is shown to resolve accurately the task of soft language distinction. at the heart of our approach, we replace the usual hard membership assignment of spectral clustering by a soft, probabilistic assignment, which also presents the advantage to bypass a well-known complexity bottleneck of the method.experiments with a readily available system display the potential of the method, which brings a visually appealing soft distinction of languages that may define altogether a whole corpus.",nan
2006,tools for text mining over biomedical literature,"this poster describes ontogene: an environment, based on a deep-linguistic parser, aimed at supporting the process of text mining from biomedical scientific literature. we will illustrate in particular the relation extraction component and the development support facilities.",nan
2006,summar: combining linguistics and statistics for text summarization,we describe a text summarization system that moves beyond standard approaches by using a hybrid approach of linguistic and statistical analysis and by employing text-sort-specific knowledge of document structure and phrases indicating importance. the system is highly modular and entirely xml-based so that different components can be combined easily.,nan
2006,phonetic spelling and heuristic search,"we introduce a new approach to spellchecking for languages with extreme phonetic irregularities. the spelling for such languages can be significantly improved if knowledge about pronunciation and sound becomes the central part of the spelling algorithm. however, given a weak phoneme-grapheme-correspondence the standard spelling algorithms, which are rule-based or edit-distance-based, are severely limited in their phonetic capabilities.a production approach to spelling can overcome the limitations—but suffers from its search space size. we describe in this paper the main building blocks to tackle this problem with heuristic search. our ideas have been operationalized in the smartspell algorithm, with impressive results related to spelling correction and runtime.",nan
2006,cbr-tm: a new case-based reasoning system for help-desk environments,"in this paper, a new cbr system for help-desk environments is presented. this cbr system provides intelligent customer support for multiple domains. it is also portable and flexible. the system is implemented as a module of a complete help-desk application to make it as independent as possible of any change in the help-desk system. each phase of the reasoning cycle is also separated as an independent module, making the cbr system easy to update. the system has been tested in a real call center managed by the spanish company tissat s.a.",nan
2006,verification of medical guidelines using task execution with background knowledge,"the use of a medical guideline can be seen as the execution of computational tasks, sequentially or in parallel, in the face of patient data. it has been shown that many of such guidelines can be represented as a 'network of tasks', i.e., as a number of steps that have a specific function or goal. to investigate the quality of such guidelines we propose a formalization of criteria for good practice medicine a guideline should comply to. we use this theory in conjunction with medical background knowledge to verify the quality of a guideline dealing with diabetes mellitus type 2 using the interactive theorem prover kiv. verification using task execution and background knowledge is a novel approach to quality checking of medical guidelines.",nan
2006,the incompleteness of planning with volatile external information,"in many real-world planning environments, some of the information about the world is both external (the planner must request it from external information sources) and volatile (it changes before the planning process completes). in such environments, a planner faces two challenges: how to generate plans despite changes in the external information during planning, and how to guarantee that a plan returned by the planner will remain valid for some period of time after the planning ends. previous works on planning with volatile information have addressed the first challenge, but not the second one.this paper provides a general model for planning with volatile external information in which the planner offers a guarantee of how long the solution will remain valid after it is returned, and an incompleteness theorem showing that there is no planner that can succeed in solving all solvable planning problems in which there is volatile external information.",nan
2006,cost-optimal symbolic planning with state trajectory and preference constraints,"state trajectory and plan preference constraints are the two language features recently introduced to pddl in the context of the 5th international planning competition. for planning with soft constraints, an objective function monitors their violation. this paper introduces a symbolic approach for finding cost-optimal plans. the set-based branch-and-bound algorithm exploits an efficient symbolic representation of the objective function. state trajectory constraints are compiled into automata, while ordinary preferences are evaluated on-line for the intersection of the search frontier with the goal.",nan
2006,a cooperative distributed problem solving technique for large markov decision processes,"in this paper, we consider the problem of solving a large mdp in a distributed way among several processors. to do that, we propose an approach which decomposes the large mdp into smaller ones each of which is solved on a unique processor. the obtained joint local policies derived from the small mdps (submdps) behave in the same way of the policy of the initial mdp.",nan
2006,integrating off-line and on-line schedulers,"this work pursues an empirical study on the mutual interactions among a set of off-line and on-line constraint-based scheduling approaches. we devise a set of closed loop execution management algorithms, and compare their behavior within an experimental framework which allows to directly assess the consequences of each chosen strategy combination against the injection of a number of disturbing events, through simulated schedule executions.",nan
2006,environment-driven skeletal plan execution for the medical domain,"an important application of both data abstraction and plan execution is the execution of clinical guidelines and protocols (cgp), both to validate them against a large set of test cases and to provide decision support at the point of care. cgps can be represented and executed as a hierarchy of skeletal plans. to bridge the gap between low-level data and high-level concepts in the cgp, intelligent temporal data abstraction must be integrated with plan execution.in this paper we describe a solution to this challenge which was implemented as part of the european project protocure ii to improve the quality of cgps. they are translated to the high-level plan representation language asbru which again is compiled into a network of abstraction modules by the system. then this network performs the content of the plans triggered by the arriving patient data.by this, we seamlessly integrate the synchronisation of guideline execution with observed patient state, complex temporal abstractions and execution of complex plans without requiring the user to handle the low-level details. instead, user-friendly tools are used to create and maintain the guideline.",nan
2006,time constrained vrp: an agent environment-perception model,"in this paper, we present a multiagent model in which agents have a perception upon their shared environment, a measure is associated to the agents' perception field. we apply the model on the vehicle routing problem with time windows (vrptw). the overall process adopts the general schema of parallel insertion methods and it uses the contracting of perception's field of the vehicle agents as a new distance between them. this new measure expresses the feasibility universe of the vehicles and is used as a criterion of choice between candidates vehicles for the insertion of a customer in their plan. our approach provides a new method to tackle the time constrained vrp in which the solving process is focused on the future and constitutes an alternative for handling the dynamic version of the problem.",nan
2006,on packing 2d irregular shapes,"designing and implementing an intelligent system that tackles the problem of placing two-dimensional shapes on a surface, such that no shapes overlap and the uncovered surface area is minimized, is highly important in industrial applications. however, it is also interesting from the scientific perspective, in terms of artificial intelligence, since autonomous systems developed up to now have found it difficult to compete with humans in this task. this paper presents a new algorithm which addresses the on-line packing of two-dimensional irregular shapes, and achieves high quality solutions in short computational times. the key point of this algorithm is the utilization of techniques drawn from computer vision and artificial intelligence.",nan
2006,aliasing maps for robot global localization,in this paper we present a mobile robot localization system that integrates monte-carlo localization with an active action-selection approach based on an aliasing map.the main novelty of the approach is in the off-line evaluation of the perceptual aliasing of the environment and in the use of this knowledge to perform localization processes faster and better. preliminary results show improved performances compared with the classic monte-carlo localization approach.,nan
2006,on interfacing with an ubiquitous robotic system,"the emerging field of ubiquitous robotics presents new challenges for human-robot interface. in this note, we introduce the concept of a common interface point using an expression-based semantics as a way to address some of these challenges. we illustrate this concept in the framework of the peis-ecology approach to ubiquitous robotics.",nan
2006,leaf classification using navigation-based skeletons,"in this paper, we present a leaf classification method based on skeletons produced by a navigation-inspired technique. the classification system comprises three separate stages. first, askeletonisation algorithm is used to gather low level structural and morphological information about the shape. subsequently, the data is converted into a series of attributed graphs. graphs of the same type are then compared using an approximate graph matcher, which identifies a degree of similarity between them. each degree of similarity corresponds to a dimension in a conceptual space, as defined by gã¤rdenfors. we test the performance of our technique on a set of leaves belonging to three different species.",nan
2008,semantic activity recognition,"extracting automatically the semantics from visual data is a real challenge. we describe in this paper how recent work in cognitive vision leads to significative results in activity recognition for visualsurveillance and video monitoring. in particular we present work performed in the domain of video understanding in our pulsar team at inria in sophia antipolis. our main objective is to analyse in real-time video streams captured by static video cameras and to recognize their semantic content. we present a cognitive vision approach mixing 4d computer vision techniques and activity recognition based on a priori knowledge. applications in visualsurveillance and healthcare monitoring are shown. we conclude by current issues in cognitive vision for activity recognition.",nan
2008,bayesian methods for artificial intelligence and machine learning,"bayesian methods provide a framework for representing and manipulating uncertainty, for learning from noisy data, and for making decisions that maximize expected utility----components which are important to both ai and machine learning. however, although bayesian methods have become more popular in recent years, there remains a good degree of skepticism with respect to taking a fully bayesian approach. this talk will introduce fundamental topics in bayesian statistics as they apply to machine learning and ai, and address some misconceptions about bayesian approaches. i will then discuss some current work on non-parametric bayesian machine learning, particularly in the area of unsupervised learning.",nan
2008,the impact of constraint programming,"constraint programming is a success story for artificial intelligence. it quickly moved from research laboratories to industrial applications and is in daily use to solve complex optimization throughout the world. at the same time, constraint programming continued to evolve, addressing new needs and opportunities. this talk reviews some recent progress in constraint programming, including its hybridization with other optimization approaches, the quest for more autonomous search, and its applications in a variety of nontraditional areas.",nan
2008,advanced preprocessing for answer set solving,we introduce the first substantial approach to preprocessing in the context of answer set solving. the idea is to simplify a logic program while identifying equivalences among its relevant constituents. these equivalences are then used for building a compact representation of the program (in terms of boolean constraints). we implemented our approach as well as a sat-based technique to reduce boolean constraints. this allows us to empirically analyze both preprocessing types and to demonstrate their computational impact.,nan
2008,a generic framework for comparing semantic similarities on a subsumption hierarchy,"defining a suitable semantic similarity between concept pairs of a subsumption hierarchy is becoming a generic problem for many applications in knowledge engineering exploiting ontologies. in this paper, we define a generic framework which can guide the proposition of new measures by making explicit the information on the ontology which has not been integrated into existing definitions yet. moreover, this framework allows us to rewrite numerous measures, originally proposed in various contexts, which are in fact closely related to each other. from this observation, we show some metrical and ordinal properties. experimental comparisons on word-net and on collections of human judgments complete the theoretical results and confirm the relevance of our propositions.",nan
2008,complexity of subsumption in the [escr    ][lscr    ] family of description logics: acyclic and cyclic tboxes,"we perform an exhaustive study of the complexity of subsumption in the [escr    ][lscr    ] family of lightweight description logics w.r.t. acyclic and cyclic tboxes. it turns out that there are interesting members of this family for which subsumption w.r.t. cyclic tboxes is tractable, whereas it is exptime-complete w.r.t. general tboxes. for other extensions that are intractable w.r.t. general tboxes, we establish intractability already for acyclic and cyclic tboxes.",nan
2008,reasoning about dynamic depth profiles,"reasoning about perception of depth and about spatial relations between moving physical objects is a challenging problem. we investigate the representation of depth and motion by means of depth profiles whereby each object in the world is represented as a single peak. we propose a logical theory, formulated in the situation calculus (sc), that is used for reasoning about object motion (including motion of the observer). the theory proposed here is comprehensive enough to accommodate reasoning about both sensor data and actions in the world. we show that reasoning about depth profiles is sound and complete with respect to actual motion in the world. this shows that in the conceptual neighbourhood diagram (cnd) of all possible depth perceptions, the transitions between perceptions are logical consequences of the proposed theory of depth and motion.",nan
2008,comparing abductive theories,"this paper introduces two methods for comparing explanation power of different abductive theories. one is comparing for observations, and the other is comparing explanation content for observations. those two measures are represented by generality relations over abductive theories. the generality relations are naturally related to the notion of abductive equivalence introduced by inoue and sakama. we also analyze the computational complexity of these relations.",nan
2008,privacy-preserving query answering in logic-based information systems,"we study privacy guarantees for the owner of an information system who wants to share some of the information in the system with clients while keeping some other information secret. the privacy guarantees ensure that publishing the new information will not compromise the secret one. we present a framework for describing privacy guarantees that generalises existing probabilistic frameworks in relational databases. we also formulate different flavors of privacy-preserving query answering as novel, purely logic-based reasoning problems and establish general connections between these reasoning problems and the probabilistic privacy guarantees.",nan
2008,optimizing causal link based web service composition,"automation of web service composition is one of the most interesting challenges facing the semantic web today. since web services have been enhanced with formal semantic descriptions, it becomes conceivable to exploit causal links i.e., semantic matching between their functional parameters (i.e., outputs and inputs). the semantic quality of causal links involved in a composition can be then used as a innovative and distinguishing criterion to estimate its overall semantic quality. therefore non functional criteria such as quality of service (qos) are no longer considered as the only criteria to rank compositions satisfying the same goal. in this paper we focus on semantic quality of causal link based semantic web service composition. first of all, we present a general and extensible model to evaluate quality of both elementary and composition of causal links. from this, we introduce a global causal link selection based approach to retrieve the optimal composition. this problem is formulated as an optimization problem which is solved using efficient integer linear programming methods. the preliminary evaluation results showed that our global selection based approach is not only more suitable than the local approach but also outperforms the naive approach.",nan
2008,extending the knowledge compilation map: closure principles,"we extend the knowledge compilation map introduced by darwiche and marquis with new propositional fragments obtained by applying closure principles to several fragments studied so far. we investigate two closure principles: disjunction and implicit forgetting (i.e., existential quantification). each introduced fragment is evaluated w.r.t. several criteria, including the complexity of basic queries and transformations, and its spatial efficiency is also analyzed.",nan
2008,semantic modularity and module extraction in description logics,"the aim of this paper is to study semantic notions of modularity in description logic (dl) terminologies and reasoning problems that are relevant for modularity. we define two notions of a module whose independence is formalised in a model-theoretic way. focusing mainly on the dls [escr    ][lscr    ] and [ascr    ][lscr    ][cscr    ], we then develop algorithms for module extraction, for checking whether a part of a terminology is a module, and for a number of related problems. we also analyse the complexity of these problems, which ranges from tractable to undecidable. finally, we provide an experimental evaluation of our module extraction algorithms based on the large-scale terminology snomed ct.",nan
2008,new results for horn cores and envelopes of horn disjunctions,"we provide a characterization of horn cores for formulas in conjunctive normal form (cnf) and, based on it, a novel algorithm for computing horn cores of disjunctions of horn cnfs that has appealing properties (e.g., it is polynomial for a bounded disjunction). furthermore, we show that recognizing the horn envelope of a disjunction of two horn cnfs is intractable, and that computing a compact horn cnf for it (that is irredundant and prime) is not feasible in polynomial total time unless p=np; this answers an open problem.",nan
2008,belief revision with reinforcement learning for interactive object recognition,"from a conceptual point of view, belief revision and learning are quite similar. both methods change the belief state of an intelligent agent by processing incoming information. however, for learning, the focus in on the exploitation of data to extract and assimilate useful knowledge, whereas belief revision is more concerned with the adaption of prior beliefs to new information for the purpose of reasoning. in this paper, we propose a hybrid learning method called sphinx that combines low-level, non-cognitive reinforcement learning with high-level epistemic belief revision, similar to human learning. the former represents knowledge in a sub-symbolic, numerical way, while the latter is based on symbolic, non-monotonic logics and allows reasoning. beyond the theoretical appeal of linking methods of very different disciplines of artificial intelligence, we will illustrate the usefulness of our approach by employing sphinx in the area of computer vision for object recognition tasks. the sphinx agent interacts with its environment by rotating objects depending on past experiences and newly acquired generic knowledge to choose those views which are most advantageous for recognition.",nan
2008,a formal approach for rdf/s ontology evolution,"in this paper, we consider the problem of ontology evolution in the face of a change operation. we devise a general-purpose algorithm for determining the effects and side-effects of a requested elementary or complex change operation. our work is inspired by belief revision principles (i.e., validity, success and minimal change) and allows us to handle any change operation in a provably rational and consistent manner. to the best of our knowledge, this is the first approach overcoming the limitations of existing solutions, which deal with each change operation on a per-case basis. additionally, we rely on our general change handling algorithm to implement specialized versions of it, one per desired change operation, in order to compute the equivalent set of effects and side-effects.this work was partially supported by the eu projects caspar (fp6-2005-ist-033572) and kp-lab (fp6-2004-ist-27490).",nan
2008,modular equivalence in general,"the notion of modular equivalence was recently introduced in the context of a module architecture proposed for logic programs under answer set semantics [12, 6, 13]. in this paper, the module architecture is abstracted for arbitrary knowledge bases, kb-functions for short, giving rise to a universal notion of modular equivalence. a further objective of this paper is to study modular equivalence in the contexts of sat-functions, i.e., propositional theories with a module interface, and their logic programming counterpart, known as lp-functions [6]. as regards sat-functions, we establish the full compositionality of classical semantics. this proves modular equivalence a proper congruence relation for sat-functions. moreover, we address the interoperability of sat-functions and lp-functions in terms of strongly faithful transformations in both directions. these considerations justify the proposed design of kb-functions in general and pave the way for hybrid kb-functions.",nan
2008,description logic rules,"we introduce description logic (dl) rules as a new rule-based formalism for knowledge representation in dls. as a fragment of the semantic web rule language swrl, dl rules allow for a tight integration with dl knowledge bases. in contrast to swrl, however, the combination of dl rules with expressive description logics remains decidable, and we show that the dl [sscr    ][rscr    ][oscr    ][iscr    ][qscr    ] – the basis for the ongoing standardisation of owl 2 – can completely internalise dl rules. on the other hand, dl rules capture many expressive features of [sscr    ][rscr    ][oscr    ][iscr    ][qscr    ] that are not available in simpler dls yet. while reasoning in [sscr    ][rscr    ][oscr    ][iscr    ][qscr    ] is highly intractable, it turns out that dl rules can be introduced to various lightweight dls without increasing their worst-case complexity. in particular, dl rules enable us to significantly extend the tractable dls [escr    ][lscr    ]++ and dlp.",nan
2008,conflicts between relevance-sensitive and iterated belief revision,"the original agm paradigm focuses only on one-step belief revision and leaves open the problem of revising a belief state with whole sequences of evidence. darwiche and pearl later addressed this problem by introducing extra (intuitive) postulates as a supplement to the agm ones. a second shortcoming of the agm paradigm, seemingly unrelated to iterated revision, is that it is too liberal in its treatment of the notion of relevance. once again this problem was addressed with the introduction of an extra (also very intuitive) postulate by parikh. the main result of this paper is that parikh postulate for relevance-sensitive belief revision is inconsistent with each of the darwiche and pearl postulates for iterated belief revision.",nan
2008,conservativity in structured ontologies,"using category theoretic notions, in particular diagrams and their colimits, we provide a common semantic backbone for various notions of modularity in structured ontologies, and outline a general approach for representing (heterogeneous) combinations of ontologies through interfaces of various kinds, based on the theory of institutions. this covers theory interpretations, (definitional) language extensions, symbol identifications, and conservative extensions. in particular, we study the problem of inheriting conservativity between sub-theories in a diagram to its colimit ontology, and apply this to the problem of localisation of reasoning in ‘modular ontology languages’ such as ddls or [escr    ]-connections.",nan
2008,removed sets fusion: performing off the shelf,"merging multiple sources of information is a rising subject in artificial intelligence. most of the proposals are model-based approaches with very high computational complexity, moreover few experimentations are available. this paper proposes a framework for performing removed sets fusion (rsf) of belief bases consisting of prepositional formulae. it then describes the implementation of rsf which stems from answer set programming (asp) and can be performed with any asp solver supporting the minimize statement. it finally presents an experimental study and a comparison.",nan
2008,a coherent well-founded model for hybrid mknf knowledge bases,"with the advent of the semantic web, the question becomes important how to best combine open-world based ontology languages, like owl, with closed-world rules paradigms. one of the most mature proposals for this combination is known as hybrid mknf knowledge bases [11], which is based on an adaptation of the stable model semantics to knowledge bases consisting of ontology axioms and rules. in this paper, we propose a well-founded semantics for such knowledge bases which promises to provide better efficiency of reasoning, which is compatible both with the owl-based semantics and the traditional well-founded semantics for logic programs, and which surpasses previous proposals for such a well-founded semantics by avoiding some issues related to inconsistency handling.",nan
2008,prototype-based domain description,"in this work a novel one-class classifier, namely the prototype-based domain description rule (pdd), is presented. the pdd classifier is equivalent to the nndd rule under the infinity minkowski metric for a suitable choice of the prototype set. the concept of pdd consistent subset is introduced and it is shown that computing a minimum size pdd consistent subset is in general not approximable within any constant factor. a logarithmic approximation factor algorithm, called the cpdd algorithm, for computing a minimum size pdd consistent subset is then introduced. the cpdd algorithm has some parameters which allow to tune the trade off between accuracy and size of the model. experimental results show that the cpdd rule sensibly improves over the cnndd classifier in terms of size of the subset, while guaranteeing a comparable classification quality.",nan
2008,online rule learning via weighted model counting,"online multiplicative weight-update learning algorithms, such as winnow, have proven to behave remarkably for learning simple disjunctions with few relevant attributes. the aim of this paper is to extend the winnow algorithm to more expressive concepts characterized by dnf formulas with few relevant rules. for such problems, the convergence of winnow is still fast, since the number of mistakes increases only linearly with the number of attributes. yet, the learner is confronted with an important computational barrier: during any prediction, it must evaluate the weighted sum of an exponential number of rules. to circumvent this issue, we convert the prediction problem into a weighted model counting problem. the resulting algorithm, sharpnow, is an exact simulation of winnow equipped with backtracking, caching, and decomposition techniques. experiments on static and drifting problems demonstrate the performance of the algorithm in terms of accuracy and speed.",nan
2008,focused ensemble selection: a diversity-based method for greedy ensemble selection,"ensemble selection deals with the reduction of an ensemble of predictive models in order to improve its efficiency and predictive performance. a number of ensemble selection methods that are based on greedy search of the space of all possible ensemble subsets have recently been proposed. this paper contributes a novel method, based on a new diversity measure that takes into account the strength of the decision of the current ensemble. experimental comparison of the proposed method, dubbed focused ensemble selection (fes), against state-of-the-art greedy ensemble selection methods shows that it leads to small ensembles with high predictive performance.",nan
2008,mtforest: ensemble decision trees based on multi-task learning,"many ensemble methods, such as bagging, boosting, random forest, etc, have been proposed and widely used in real world applications. some of them are better than others on noise-free data while some of them are better than others on noisy data. but in reality, ensemble methods that can consistently gain good performance in situations with or without noise are more desirable. in this paper, we propose a new method namely mtforest, to ensemble decision tree learning algorihms by enumerating each input attribute as extra task to introduce different additional inductive bias to generate diverse yet accurate component decision tree learning algorithms in the ensemble. the experimental results show that in situations without classification noise, mtforest is comparable to boosting and random forest and significantly better than bagging, while in situations with classification noise, mtforest is significantly better than boosting and random forest and is slightly better than bagging. so mtforest is a good choice for ensemble decision tree learning algorithms in situations with or without noise. we conduct the experiments on the basis of 36 widely used uci data sets that cover a wide range of domains and data characteristics and run all the algorithms within the weka platform.",nan
2008,many-valued concept lattices for conceptual clustering and information retrieval,"in this paper we present an extension of the galois connection to deal with many-valued formal contexts. we define a many-valued galois connection with respect to similarity between attribute values in a many-valued context. then, we define many-valued formal concepts and many-valued concept lattices. depending on a similarity threshold, many-valued concept lattices may have different levels of precision. this feature makes them very useful for multilevel conceptual clustering. many-valued concept lattices are also used in a new lattice-based information retrieval approach for efficiently answering complex queries.",nan
2008,online optimization for variable selection in data streams,"variable selection for regression is a classical statistical problem, motivated by concerns that too many covariates invite overfitting. existing approaches notably include a class of convex optimisation techniques, such as the lasso algorithm. such techniques are invariably reliant on assumptions that are unrealistic in streaming contexts, namely that the data is available off-line and the correlation structure is static. in this paper, we relax both these constraints, proposing for the first time an online implementation of the lasso algorithm with exponential forgetting. we also optimise the model dimension and the speed of forgetting in an online manner, resulting in a fully automatic scheme. in simulations our scheme improves on recursive least squares in dynamic environments, while also featuring model discovery and changepoint detection capabilities.",nan
2008,sub node extraction with tree based wrappers,"string based as well as tree based methods have been used to learn wrappers for extraction from semi-structured documents (e.g., html documents). previous work has shown that tree based approaches perform better while needing less examples than string based approaches. a disadvantage is that they can only extract complete text nodes, whereas string based approaches can extract within text nodes. this paper proposes a hybrid approach that combines the advantages of both systems and compares it experimentally with a string based approach on some sub node extraction tasks.",nan
2008,automatic recurrent ann development for signal classification: detection of seizures in eegs,"biomedical signal processing is one of the research fields that has received more research in the recent years or decades. inside it, signal classification has shown to be one of the most important aspects. one of the most used tools for doing this analysis are artificial neural networks (anns), which have proven their utility in modeling almost any input/output system. however, their application is not easy, because it involves some design and training stages in which the expert has to do much effort to develop a good network, which is even harder when working with time series, in which recurrent networks are needed. this paper describes a new technique for automatically developing recurrent anns (ranns) for signal processing, in which the expert does not have to take part on their development. these networks are obtained by means of evolutionary computacion (ec) tools, and are applied to the classification of electroencephalogram (eegs) signals in epileptic patients. the objective is to discriminate those eeg signals in which an epileptic patient is having a seizure.",nan
2008,a method for classifying vertices of labeled graphs applied to knowledge discovery from molecules,"the article proposes a generic method to classify vertices or edges of a labeled graph. more precisely the method computes a confidence index for each vertex v or edge e to be a member of a target class by mining the topological environments of v or e. the method contributes to knowledge discovery since it exhibits for each edge or vertex an informative environnement that explains the found confidence. when applied to the problem of discovering strategic bonds in molecules, the method correctly classifies most of the bonds while providing relevant explanations to chemists. the developed algorithm gemsbond outperforms both speed and scalability of the learning method that has previously been applied to the same application while giving similar results.",nan
2008,nonnegative decompositions with resampling for improving gene expression data biclustering stability,"the small sample sizes and high dimensionality of gene expression datasets pose significant problems for unsupervised subgroup discovery. while the stability of unidimensional clustering algorithms has been previously addressed, generalizing existing approaches to biclustering has proved extremely difficult. despite these difficulties, developing a stable biclustering algorithm is essential for analyzing gene expression data, where genes tend to be co-expressed only for subsets of samples, in certain specific biological contexts, so that both gene and sample dimensions have to be taken into account simultaneously.in this paper, we describe an elegant approach for ensuring bicluster stability that combines three ideas. a slight modification of nonnegative matrix factorization that allows intercepts for genes has proved to be superior to other biclustering methods and is used for base-level clustering. a continuous-weight resampling method for samples is employed to generate slight perturbations of the dataset without sacrificing data and a positive tensor factorization is used to extract the biclusters that are common to the various runs. finally, we present an application to a large colon cancer dataset for which we find 5 stable subclasses.",nan
2008,exploiting locality of interactions using a policy-gradient approach in multiagent learning,"in this paper, we propose a policy gradient reinforcement learning algorithm to address transition-independent dec-pomdps. this approach aims at implicitly exploiting the locality of interaction observed in many practical problems. our algorithms can be described by an actor-critic architecture: the actor component combines natural gradient updates with a varying learning rate; the critic uses only local information to maintain a belief over the joint state-space, and evaluates the current policy as a function of this belief using compatible function approximation. in order to speed the convergence of the algorithm, we use an optimistic initialization of the policy that relies on a fully observable, single agent model of the problem. we illustrate our approach in some simple application problems.",nan
2008,a fast method for property prediction in graph-structured data from positive and unlabelled examples,"the analysis of large and complex networks, or graphs, is becoming increasingly important in many scientific areas including machine learning, social network analysis and bioinformatics. one natural type of question that can be asked in network analysis is “given two sets r and t of individuals in a graph with complete and missing knowledge, respectively, about a property of interest, which individuals in t are closest to r with respect to this property?”. to answer this question, we can rank the individuals in t such that the individuals ranked highest are most likely to exhibit the property of interest. several methods based on weighted paths in the graph and markov chain models have been proposed to solve this task. in this paper, we show that we can improve previously published approaches by rephrasing this problem as the task of property prediction in graph-structured data from positive examples, the individuals in r, and unlabelled data, the individuals in t, and applying an inexpensive iterative neighbourhood's majority vote based prediction algorithm (“inmv”) to this task. we evaluate our inmv prediction algorithm and two previously proposed methods using markov chains on three real world graphs in terms of roc auc statistic. inmv obtains rankings that are either significantly better or not significantly worse than the rankings obtained from the more complex markov chain based algorithms, while achieving a reduction in run time of one order of magnitude on large graphs.",nan
2008,vcd bounds for some gp genotypes,"we provide upper bounds for the vapnik-chervonenkis dimension (vcd) of classes of subsets of rn that can be recognized by computer programs represented by expression trees built from arithmetic operations ({+,−,*,/,}), infinitely differentiable algebraic operations (like l-root extraction), conditional instructions and sign tests. our vcd bounds for this genotype are expressed as a polynomial function in the height of the expression trees used to represent the programs. this implies, in particular, that a gp learning machine dealing with a search space containing sequential exponential time computer programs of polynomial parallel complexity needs only a polynomial amount of training examples.",nan
2008,robust division in clustering of streaming time series,"online learning algorithms which address fast data streams should process examples at the rate they arrive, using a single scan of data and fixed memory, maintaining a decision model at any time and being able to adapt the model to the most recent data. these features yield the necessity of using approximate models. one problem that usually arises with approximate models is the definition of a minimum number of observations necessary to assure convergence, which implies a high risk since the system may have to decide based only on a small subset of the entire data. one approach is to apply techniques based on the hoeffding bound to enforce decisions with a confidence level. in divisive clustering of time series, the goal is to find clusters of similar time series over time. in online approaches there are two decisions to make: when to split and how to assign variables to new clusters. we can define a confidence level to both the decision of splitting and the assignment of data variables to new clusters. previous works have already addressed confident decisions on the moment of split. our proposal is to include a confidence level to the assignment process. when a split point is reported, creating two new clusters, we can directly assign points which are confidently closer to one cluster than the other, having a different strategy for those variables which do not satisfy the confidence level. in this paper we propose to assign the unsure variables to a third cluster. experimental evaluation is presented in the context of a recently proposed hierarchical algorithm, assessing the advantages of the proposal, revealing also advantages on memory usage reduction and processing speed. although this proposal is evaluated under the scope of an existent method, it can be generalized to any divisive procedure.",nan
2008,generating diagnoses from conflict sets with continuous attributes,"many techniques in model-based diagnosis and other research fields find the hitting sets of a group of sets. existing techniques apply to sets of finite elements only. this paper addresses the computation of the hitting sets of a group of sets whose elements are convex or non-convex, bounded or unbounded continuous regions. we assume the conflict sets are known and we present a novel procedure, the continuous hitting set algorithm (chs) for transforming conflict sets of continuous elements into minimal hitting sets.",nan
2008,a compositional mathematical model of machines transporting rigid objects,"we present models of various elements of a plant that involves the transportation of lumped material. an application context is provided by a project on diagnosing disturbances in food packaging plants and, more specifically, bottling plants. while there exist models of flow of homogeneous matters, such as liquid material in a hydraulic system, based on simultaneous equations of kirchhoff/ohm type, in our project we need to cope with non-negligible transportation time of objects and capture phenomena like the tailback of units (if transportation is blocked) or the propagation of gaps in the flow of units. because the application context requires compositionality of the model, i.e. local, context-free models of the individual transportation elements, we are also facing the problem that whether or not a single element produces an output flow (or accepts an input flow) cannot be determined solely by the model of this element, but only through modeling the interaction with the subsequent element, which may block the output (or the previous one not providing the input). this issue is addressed by modeling the potential of an existing flow distinctly from the actual occurrence of a flow, an idea which also can enhance models of continuous flow.",nan
2008,model-based diagnosis of discrete event systems with an incomplete system model,"model-based diagnosis of discrete event systems (dess) is more and more active in artificial intelligence. however, there has been always a very restrictive assumption in the previous works that the model of a given des is complete, including all nominal behaviors and all possible failure behaviors of the system. in order to relax this so restrictive assumption, in this paper, model-based diagnosis of a des with an incomplete system model is investigated. a new concept of “p-synchronization product” of finite state automata is proposed, by which the p-diagnosis of the des with an incomplete system model is easily put forward. it is also shown that the traditional synchronization product of finite state automata can be seen as a special situation of p-synchronization product. in addition, an ideal heuristic way from theoretical view to improve the p-synchronization product is discussed as well.",nan
2008,chronicles for on-line diagnosis of distributed systems,"the formalism of chronicles has been proposed to monitor and diagnose dynamic physical systems. even if efficient chronicle recognition algorithms exist, it is now well-known that distributed approaches are better suited to monitor real systems. in this article, we adapt the chronicle-based approach to a distributed context and illustrate this work on the monitoring of software components.",nan
2008,test generation for model-based diagnosis,"this article formalises the dual problem to model-based diagnosis (mbd), i.e., generating tests to isolate multiple simultaneous faults. using a standard propositional mbd framework, we first define a test of minimal size that can isolate multiple simultaneous faults of an arbitrary nature. second, we prove complexity results for multiplefault tests of minimal size in propositional system models, showing such problems have complexity similar to those of mbd problems, i.e., complexity at the second level of the polynomial hierarchy.",nan
2008,observation-subsumption checking in similarity-based diagnosis of discrete-event systems,"in similarity-based diagnosis of discrete-event systems the knowledge generated for solving a previous diagnostic problem can be reused to solve a new one, provided the two problems are similar. problem-similarity requires that the temporal observation relevant to the new problem be subsumed by the temporal observation relevant to the old one. a temporal observation encompasses the (uncertain) events observed over a time interval and their (uncertain) reciprocal temporal order. such an observation has been produced by one out of several distinct certain sequences of observable events, with each of such sequences being a sentence of the regular language of the observation. an observation subsumes another if its regular language contains the regular language of the other. however, checking observation-subsumption by following its formal definition is time consuming. in order to speed up the process, an alternative technique is proposed, which is based on the notion of coverage and exploits a number of necessary conditions, as well as a sufficient condition, for subsumption to hold. such conditions can be directly checked on the properties of the given observations, without any need to appeal to the language theory. experimental evidence confirms the efficiency of subsumption-checking via coverage.",nan
2008,local consistency and junction tree for diagnosis of discrete-event systems,"we extend the decentralised/distributed approach of diagnosis of discrete-event systems modeled using automata. the goal is to avoid computing a global diagnosis, which is expensive, and to perform local diagnoses instead. to still ensure global consistency, we transform the topology of the system into a junction tree where each vertex represents a subsystem. local consistency between the diagnoses of these subsystems ensures global consistency due to the tree structure. this technique will work best for systems whose natural structure is close to a tree structure, as the generated automata will be of reasonable size.",nan
2008,hierarchical explanation of inference in bayesian networks that represent a population of independent agents,"this paper describes a novel method for explaining bayesian network (bn) inference when the network is modeling a population of conditionally independent agents, each of which is modeled as a subnetwork. for example, consider disease-outbreak detection, in which the agents are patients who are modeled as independent, conditioned on the factors that cause disease spread. given evidence about these patients, such as their symptoms, suppose that the bn system infers that a respiratory anthrax outbreak is highly likely. a public-health official who received such a report would generally want to know why anthrax is being given a high posterior probability. this paper describes the design of a system that explains such inferences. the explanation approach is applicable in general to inference in bns that model conditionally independent agents; it complements previous approaches for explaining inference on bns that model a single agent (e.g., explaining the diagnostic inference for a single patient using a bn that models just that patient).",nan
2008,coupling continuous and discrete event system techniques for hybrid system diagnosability analysis,"in this paper we propose a hybrid system modeling framework aimed at analyzing diagnosability. in this framework, the hybrid system is seen as the composition of an underlying discrete event and an underlying continuous systems. diagnosability of these two underlying systems are fully analyzed and new results are provided for the underlying continuous system (called the multimode system). based on these results, a hybrid language that contains ‘natural’ discrete events and discrete events capturing the continuous dynamics, is defined. on the basis of this language the diagnosability definition of hybrid systems is provided. with respect to this definition, we prove that the diagnosability of the underlying continuous or the discrete event system is only a sufficient condition. diagnosability of hybrid systems must be decided by coupling both discrete event and continuous informations. finally, the necessary and sufficient condition of hybrid diagnosability is given.",nan
2008,a probabilistic analysis of diagnosability in discrete event systems,"this paper shows that we can take advantage of information about the probabilities of the occurrences of events, when this information is available, to refine the classical results of diagnosability: instead of giving a binary answer, the approach we propose allows one to quantify, in particular, the degree of non-diagnosability in case of negative answer. the dynamics of the system is modelled by a reducible markov chain. a state of this chain contains information about whether it is faulty (resp. ambiguous) or not. the useful refinements of the decision about diagnosability are then obtained from the asymptotic analysis of this markov chain. this analysis may be very useful in practice since it may lead to take the decision of tolerating some non-diagnosable systems, if their non-diagnosability is not critical, and thus allows one saving the cost of additional sensors necessary to make these systems diagnosable this work is part of diafore project supported by anr under grant anr-05-pdit-016-05..",nan
2008,temporal logic patterns for querying qualitative models of genetic regulatory networks,"formal verification based on model checking provides a powerful technology to query qualitative models of dynamical systems. the application of model-checking approaches is hampered, however, by the difficulty for non-expert users to formulate appropriate questions in temporal logic. in order to deal with this problem, we propose the use of patterns, that is, high-level query templates capturing recurring questions which can be automatically translated to temporal logic. we develop a set of patterns for the analysis of qualitative models of genetic regulatory networks, which are sufficiently generic though to be useful in other application domains. the applicability of the patterns has been investigated by the analysis of a model of the network of global regulators controlling the carbon starvation response in escherichia coli.",nan
2008,fighting knowledge acquisition bottleneck with argument based machine learning,"knowledge elicitation is known to be a difficult task and thus a major bottleneck in building a knowledge base. machine learning has long ago been proposed as a way to alleviate this problem. machine learning usually helps the domain expert to uncover some of the more tacit concepts. however, the learned concepts are often hard to understand and hard to extend. a common view is that a combination of a domain expert and machine learning would yield the best results. recently, argument based machine learning (abml) has been introduced as a combination of argumentation and machine learning. through argumentation, abml enables the expert to articulate his knowledge easily and in a very natural way. abml was shown to significantly improve the comprehensibility and accuracy of the learned concepts. this makes abml a most natural tool for constructing a knowledge base. the present paper shows how this is accomplished through a case study of building a knowledge base of an expert system used in a chess tutoring application.",nan
2008,automatic page turning for musicians via real-time machine listening,"we present a system that automatically turns the pages of the music score for musicians during a performance. it is based on a new algorithm for following an incoming audio stream in real time and aligning it to a music score (in the form of a synthesised audio file). precision and robustness of the algorithm are quantified in systematic experiments, and a demonstration using an actual page turning machine built by an austrian company is described.",nan
2008,cdl: an integrated framework for context specification and recognition,"a framework is introduced that is aimed at integrating ontology and logic approaches for context-awareness, suitable for use in ambient intelligence (ami) scenarios. in particular, the context description language cdl is described, which allows to easily specify patterns of events which occurrences must be monitored by actual systems. as long as systems evolve, symbolic data originating from heterogeneous sources are first aggregated and then classified according to formulas described in cdl. experimental results performed in a smart home environment are presented and discussed.",nan
2008,web page prediction based on conditional random fields,"web page prefetching is used to reduce the access latency of the internet. however, if most prefetched web pages are not visited by the users in their subsequent accesses, the limited network bandwidth and server resources will not be used efficiently and may worsen the access delay problem. therefore, it is critical that we have an accurate prediction method during prefetching. conditional random fields (crfs), which are popular sequential learning models, have already been successfully used for many natural language processing (nlp) tasks such as pos tagging, name entity recognition (ner) and segmentation. in this paper, we propose the use of crfs in the field of web page prediction. we treat the accessing sessions of previous web users as observation sequences and label each element of these observation sequences to get the corresponding label sequences, then based on these observation and label sequences we use crfs to train a prediction model and predict the probable subsequent web pages for the current users. our experimental results show that crfs can produce higher web page prediction accuracy effectively when compared with other popular techniques like plain markov chains and hidden markov models (hmms).",nan
2008,a formal model of emotions: integrating qualitative and quantitative aspects,"when constructing a formal model of emotions for intelligent agents, two types of aspects have to be taken into account. first, qualitative aspects pertain to the conditions that elicit emotions. second, quantitative aspects pertain to the actual experience and intensity of elicited emotions. in this paper, we show how the qualitative aspects of a well-known psychological model of human emotions can be formalized in an agent specification language and how its quantitative aspects can be integrated into this model. furthermore, we discuss several unspecified details and implicit assumptions in the psychological model that are explicated by this effort.",nan
2008,modeling collaborative similarity with the signed resistance distance kernel,"we extend the resistance distance kernel to the domain of signed dissimilarity values, and show how it can be applied to collaborative rating prediction. the resistance distance is a graph kernel inspired by electrical network models where edges of a graph are interpreted as electrical resistances. we model the similarity between users of a large collaborative rating database using this signed resistance distance, generalizing the previously known regular resistance distance kernel which is limited to nonnegative values. we show that the signed resistance distance kernel can be computed effectively using the moore-penrose pseudoinverse of the laplacian matrix of the bipartite rating graph, leading to fast computation based on the eigenvalue decomposition of the laplacian matrix. we apply this technique to collaborative rating prediction on the netflix prize corpus, and show how our new kernel can replace the traditional pearson correlation for rating prediction.",nan
2008,modeling the dynamics of mood and depression,"both for developing human-like virtual agents and for developing intelligent systems that make use of knowledge about the emotional state of the user, it is important to model the mood of a person. in this paper, a model for simulating the dynamics of mood is presented, based on psychological theories about a uni-polar clinical depression. the model was analyzed mathematically and by means of simulations, and it was shown that the model exhibits the most important characteristics of the theories. it shows how stress factors under some conditions can lead to a depression, while it will not lead to a depression under other conditions.",nan
2008,groovy neural networks,"the drum machine has been an important tool in music production for decades. however, its flawless way of playing drum patterns is often perceived as mechanical and rigid, far from the groove provided by a human drummer. this paper presents research towards enhancing the drum machine with learning capabilities. the drum machine learns user-specific variations (i.e. the groove) from human drummers, and stores the groove as attractors in echo state networks (esns). the esns are purely generative (i.e. not driven by an input signal) and the output is used by the drum machine to imitate the playing style of human drummers, making it a cost-effective way of achieving life-like drums.",nan
2008,an efficient student model based on student performance and metadata,this paper describes a new student model technology that combines evidences and knowledge about pedagogical and domain structure. its structure is generated from the metadata available in the content representation of the adaptive web-based learning platform activemath (or other contents). the evidences are processed with item response theory and transferable belief model uncertainty methodologies. we summarize evaluation results for this student model.,nan
2008,reducing bias effects in dop parameter estimation,"data oriented parsing is a natural language processing model that analyses new input based on past experience. the underlying idea is to extract a set of fragment-probability pairs from a given treebank and use these concrete experiences to construct new utterance analyses. initially, probabilities were based on the fragments' relative frequency of occurrence. this estimator, however, was soon shown to be biased towards large corpus trees [8] and inconsistent [10]. to alleviate the effects of bias on performance a set of heuristic constraints was put in force. other estimators addressing these issues have since then been proposed. this paper seeks to show that the most commonly used dop estimators are in fact susceptible to strong size-sensitive bias effects and to present a new estimation algorithm that greatly reduces these effects of bias on performance without complicating the estimation process.",nan
2008,multilingual evidence improves clustering-based taxonomy extraction,"we present a system for taxonomy extraction, aimed at providing a taxonomic backbone in an ontology learning environment. we follow previous research in using hierarchical clustering based on distributional similarity of the terms in texts. we show that basing the clustering on a comparable corpus in four languages gives a considerable improvement in accuracy compared to using only the monolingual english texts. we also show that hierarchical k-means clustering increases the similarity to the original taxonomy, when compared with a bottom-up agglomerative clustering approach.",nan
2008,unsupervised grammar induction using a parent based constituent context model,"grammar induction is one of attractive research areas of natural language processing. since both supervised and to some extent semi-supervised grammar induction methods require large treebanks, and for many languages, such treebanks do not currently exist, we focused our attention on unsupervised approaches. constituent context model (ccm) seems to be the state of the art in unsupervised grammar induction. in this paper, we show that the performance of ccm in free word order languages (fwols) such as persian is inferior to that of fixed order languages such as english. we also introduce a novel approach, called parent-based constituent context model (pccm), and show that by using some history notion of context and constituent information of each span's parent, the performance of ccm, especially in dealing with fwols, can be significantly improved.",nan
2008,word sense induction using graphs of collocations,"word sense induction (wsi) is the task of identifying the different senses (uses) of a target word in a given text. traditional graph-based approaches create and then cluster a graph, in which each vertex corresponds to a word that co-occurs with the target word, and edges between vertices are weighted based on the co-occurrence frequency of their associated words. in contrast, in our approach each vertex corresponds to a collocation that co-occurs with the target word, and edges between vertices are weighted based on the co-occurrence frequency of their associated collocations. a smoothing technique is applied to identify more edges between vertices and the resulting graph is then clustered. our evaluation under the framework of semeval-2007 wsi task shows the following: (a) our approach produces less sense-conflating clusters than those produced by traditional graph-based approaches, (b) our approach outperforms the existing state-of-the-art results.",nan
2008,learning context-free grammars to extract relations from text,"in this paper we propose a novel relation extraction method, based on grammatical inference. following a semisupervised learning approach, the text that connects named entities in an annotated corpus is used to infer a context free grammar. the grammar learning algorithm is able to infer grammars from positive examples only, controlling overgeneralisation through minimum description length. evaluation results show that the proposed approach performs comparable to the state of the art, while exhibiting a bias towards precision, which is a sign of conservative generalisation.",nan
2008,talking points in metaphor: a concise usage-based representation for figurative processing,"an effective speaker can use metaphor to communicate a wealth of propositions and affective attitudes with a single juxtaposition of ideas [12,8,6,10,7,3,15]. but as such, an effective metaphor requires effective communication, which in turn requires that the speaker has a clear idea of the content to be communicated, and an equally clear understanding of which conceptual vehicles best communicate this content. we present here a concise corpusderived meaning representation for metaphor processing that captures the most widely-used talking points that are evoked in everyday metaphors and similes. we illustrate how these talking points can be acquired by harvesting the web, and further show how comparable but discretely different talking points can be reconciled during metaphor processing. finally, by replicating the clustering experiments of [1], we show that talking points yield an especially concise representation of concepts in general.",nan
2008,semantic decomposition for question answering,"in this paper, we develop and evaluate methods for decomposing complex questions for a question answering system to less complex questions. this aims at increasing the number of correct answers, especially in (deep) semantic question answering systems. for example, an event that occurs as a temporal restriction of a question can be queried for its date and the resulting answer can be substituted in the original question leading to a simpler, revised question. we present six decomposition classes, which are employed for annotating the 996 different german qa@clef questions from 2004 till 2008 and trigger different decomposition methods. most methods work on the level of semantic representations, thereby avoiding natural language generation, a second parsing step, and possible errors in these two steps. the decomposition classes are not equally distributed, but three of them occur frequently in the questions. in the evaluation, the precision and recall for automatically classifying questions with respect to the decomposition classes are investigated. then, the impact on a deep question answering system is determined. on the qa@clef questions, which by construction prefer questions that can be answered from single sentences, the performance gain in number of correct answers is not large, but significant. this encourages us to develop and test further decomposition classes and methods as future work.",nan
2008,"finding key bloggers, one post at a time","user generated content in general, and blogs in particular, form an interesting and relatively little explored domain for mining knowledge. we address the task of blog distillation: to find blogs that are principally devoted to a given topic, as opposed to blogs that merely happen to discuss the topic in passing. working in the setting of statistical language modeling, we model the task by aggregating a blogger's blog posts to collect evidence of relevance to the topic and persistence of interest in the topic. this approach achieves state-of-the-art performance. on top of this baseline, we extend our model by incorporating a number of blog-specific features, concerning document structure, social structure, and temporal structure. these blog-specific features yield further improvements.",nan
2008,why is this wrong? – diagnosing erroneous speech recognizer output with a two phase parser,"a major problem of understanding language in spoken dialog systems is to detect recognition errors in the output of a speech recognizer. such a capability is the basis of implementing repair strategies that allow a dialog system to handle communication about misunderstandings similarly to other clarifications. in this paper we present a two-phase approach that combines chunk and dependency parsing and takes the global syntactic structure of recognizer output into account. this enables us to identify dependencies between chunks and detect syntactical errors caused by word confusions in case dependency constraints are violated. finally, we apply these diagnostics to dialog modeling and discuss how the resulting error information can be used by clarification strategies.",nan
2008,task driven coreference resolution for relation extraction,"this paper presents the extension of an existing mimimally supervised rule acquisition method for relation extraction by coreference resolution (cr). to this end, a novel approach to cr was designed and tested. in comparison to state-of-the-art methods for cr, our strategy is driven by the target semantic relation and utilizes domain-specific ontological and lexical knowledge in addition to the learned relation extraction rules. an empirical investigation reveals that newswire texts in our selected domains contain more coreferring noun phrases than prononimal coreferences. this means that existing methods for cr would not suffice and a semantic approach is needed. our experiments show that the utilization of domain knowledge can boost cr. in our approach, the tasks of relation extraction and cr support each other. on the one hand, reference resolution is needed for the detection of arguments of the target relation. on the other hand, domain modelling for the ie task is used for semantic classification of the referring nouns. moreover, the application of the learned relation extraction rules often narrows down the number of candidates for cr.with respect to the minimally supervised learning of relation extraction grammars, we design and evaluate two integration strategies: (i) resolution after the complete pattern acquisition process and (ii) resolution embedded in the iterations of the learning process. the evaluation helps us to gain and substantiate a relevant insight: cr effectively improves recall in both strategies but it can hurt the precision because of its error spreading potential.",nan
2008,www sits the sat: measuring relational similarity on the web,"measuring relational similarity between words is important in numerous natural language processing tasks such as solving analogy questions and classifying noun-modifier relations. we propose a method to measure the similarity between semantic relations that hold between two pairs of words using a web search engine. first, each pair of words is represented by a vector of automatically extracted lexical patterns. then a support vector machine is trained to recognize word pairs with similar semantic relations. we evaluate the proposed method on sat multiple-choice word-analogy questions. the proposed method achieves a score of 40% which is comparable with relational similarity measures which use manually created resources such as wordnet. the proposed method significantly reduces the time taken by previously proposed computationally intensive methods, such as latent relational analysis, to process 374 analogy questions from 8 days to less than 6 hours.",nan
2008,improved statistical machine translation using monolingual paraphrases,"we propose a novel monolingual sentence paraphrasing method for augmenting the training data for statistical machine translation systems “for free” – by creating it from data that is already available rather than having to create more aligned data. starting with a syntactic tree, we recursively generate new sentence variants where noun compounds are paraphrased using suitable prepositions, and vice-versa – preposition-containing noun phrases are turned into noun compounds. the evaluation shows an improvement equivalent to 33%–50% of that of doubling the amount of training data.",nan
2008,orthographic similarity search for dictionary lookup of japanese words,"finding an unknown japanese word in a dictionary is a difficult and slow task when one or more of the word's characters is unknown. for advanced learners, unknown characters evoke the form and meaning of visually similar characters they are familiar with. we propose a range of character distance metrics to allow learners to leverage known characters to search for words containing unknown but visually similar characters. this new form of dictionary search is implemented as an extension to the foks dictionary system.",nan
2008,from belief change to preference change,"various tasks need to consider preferences in a dynamic way. we start by discussing several possible meanings of preference change, and then focus on the one we think is the most natural: preferences evolving after some new fact has been learned. we define a family of such preference change operators, parameterized by a revision function on epistemic states and a semantics for interpreting preferences over formulas. we list some natural properties that this kind of preference change should fulfill and give conditions on the revision function and the semantics of preference for each of these properties to hold.",nan
2008,a general model for epistemic state revision using plausibility measures,"in this paper, we present a general revision model on epistemic states based on plausibility measures proposed by friedman and halpern. we propose our revision strategy and give some desirable properties, e.g., the reversible and commutative properties. moreover, we develop a notion called plausibility kinematics and show that our revision strategy follows plausibility kinematics. furthermore, we prove that the revision following plausibility kinematics satisfies the principle of minimal change based on some distance measures. finally, we discuss a revision operator defined for plausibility functions and its relationship with iterated belief revision proposed by darwiche and pearl. we show that the revision operator satisfies all the dp postulates when it is max-additive.",nan
2008,structure learning of markov logic networks through iterated local search,"many real-world applications of ai require both probability and first-order logic to deal with uncertainty and structural complexity. logical ai has focused mainly on handling complexity, and statistical ai on handling uncertainty. markov logic networks (mlns) are a powerful representation that combine markov networks (mns) and first-order logic by attaching weights to first-order formulas and viewing these as templates for features of mns. state-of-the-art structure learning algorithms of mlns maximize the likelihood of a relational database by performing a greedy search in the space of candidates. this can lead to suboptimal results because of the incapability of these approaches to escape local optima. moreover, due to the combinatorially explosive space of potential candidates these methods are computationally prohibitive. we propose a novel algorithm for learning mlns structure, based on the iterated local search (ils) metaheuristic that explores the space of structures through a biased sampling of the set of local optima. the algorithm focuses the search not on the full space of solutions but on a smaller subspace defined by the solutions that are locally optimal for the optimization engine. we show through experiments in two real-world domains that the proposed approach improves accuracy and learning time over the existing state-of-the-art algorithms.",nan
2008,single-peaked consistency and its complexity,"a common way of dealing with the paradoxes of preference aggregation consists in restricting the domain of admissible preferences. the most well-known such restriction is single-peakedness. in this paper we focus on the problem of determining whether a given profile is single-peaked with respect to some axis, and on the computation of such an axis. this problem has already been considered in [2]; we give here a more efficient algorithm and address some related issues, such as the number of orders that may be compatible with a given profile, or the communication complexity of preference aggregation under the single-peakedness assumption.",nan
2008,belief revision through forgetting conditionals in conditional probabilistic logic programs,"in this paper, we present a revision strategy of revising a conditional probabilistic logic program (plp) when new information is received (which is in the form of probabilistic formulae), through the technique of variable forgetting. we first extend the traditional forgetting method to forget a conditional event in plps. we then propose two revision operators to revise a plp based on our forgetting method. by revision through forgetting, the irrelevant knowledge in the original plp is retained according to the minimal change principle. we prove that our revision operators satisfy most of the postulates for probabilistic belief revision. a main advantage of our revision operators is that a new plp is explicitly obtained after revision, since our revision operator performs forgetting a conditional event at the syntax level.",nan
2008,mastering the processing of preferences by using symbolic priorities in possibilistic logic,"the paper proposes a new approach to the handling of preferences expressed in a compact way under the form of conditional statements. these conditional statements are translated into classical logic formulas associated with symbolic levels. ranking two alternatives then leads to compare their respective amount of violation with respect to the set of formulas expressing the preferences. these symbolic violation amounts, which can be computed in a possibilistic logic manner, can be partially ordered lexicographically once put in a vector form. this approach is compared to the ceteris paribus-based cp-net approach, which is the main existing artificial intelligence approach to the compact processing of preferences. it is shown that the partial order obtained with the cp-net approach fully agrees with the one obtained with the proposed approach, but generally includes further strict preferences between alternatives (considered as being not comparable by the symbolic level logic-based approach). these additional strict preferences are in fact debatable, since they are not the reflection of explicit user's preferences but the result of the application of the ceteris paribus principle that implicitly, and quite arbitrarily, favors father node preferences in the graphical structure associated with conditional preferences. adding constraints between symbolic levels for expressing that the violation of father nodes is less allowed than the one of children nodes, it is shown that it is possible to recover the cp-net-induced partial order. due to existing results in possibilistic logic with symbolic levels, the proposed approach is computationally tractable. key words: preference, priority, partial order, cp-net, possibilistic logic.",nan
2008,interaction-oriented agent simulations: from theory to implementation,"this paper deals with the software architecture for individual-centered simulations, i.e. involving many entities interacting together. many software architectures have been developped in this context, especially many advanced – but domain specific – frameworks. yet those frameworks imply tight software dependencies between agents, behaviors and action selection mechanisms, which leads to many difficulties in modelling and programming. we propose a method and an architecture where interactions are reified regardless of agents, in order to obtain a complete interaction-oriented design process for simulations. then, an agent is only an entity that can perform or undergo a set of interactions, even not specifically developped for it. thus most interactions can be re-used in many contexts. in addition, our method clearly separates knowledge about behaviors from its processing, and thus makes the design of simulations easier. moreover, this new and user-friendly approach helps programmers to build simulations with a large number of different behaviors at the same time, especially in the context of large-scale simulations.",nan
2008,optimal coalition structure generation in partition function games,"the authors are grateful for financial support received from the uk ep-src through the project market-based control of complex computational systems (gr/t10657/01). the authors are also thankful to jennifer mcmanus, school of english, university of liverpool for excellent editorial assistance. in multi-agent systems (mas), coalition formation is typically studied using characteristic function game (cfg) representations, where the performance of any coalition is independent from co-existing coalitions in the system. however, in a number of environments, there are significant externalities from coalition formation where the effectiveness of one coalition may be affected by the formation of other distinct coalitions. in such cases, coalition formation can be modeled using partition function game (pfg) representations. in pfgs, to accurately generate an optimal division of agents into coalitions (so called csg problem), one would have to search through the entire search space of coalition structures since, in a general case, one cannot predict the values of the coalitions affected by the externalities a priori. in this paper we consider four distinct pfg settings and prove that in such environments one can bound the values of every coalition. from this insight, which bridges the gap between pfg and cfg environments, we modify the existing state-of-the-art anytime csg algorithm for the cfg setting and show how this approach can be used to generate the optimal cs in the pfg settings.",nan
2008,coalition structures in weighted voting games,"weighted voting games are a popular model of collaboration in multiagent systems. in such games, each agent has a weight (intuitively corresponding to resources he can contribute), and a coalition of agents wins if its total weight meets or exceeds a given threshold. even though coalitional stability in such games is important, existing research has nonetheless only considered the stability of the grand coalition. in this paper, we introduce a model for weighted voting games with coalition structures. this is a natural extension in the context of multiagent systems, as several groups of agents may be simultaneously at work, each serving a different task. we then proceed to study stability in this context. first, we define the cs-core, a notion of the core for such settings, discuss its non-emptiness, and relate it to the traditional notion of the core in weighted voting games. we then investigate its computational properties. we show that, in contrast with the traditional setting, it is computationally hard to decide whether a game has a non-empty cs-core, or whether a given outcome is in the cs-core. however, we then provide an efficient algorithm that verifies whether an outcome is in the cs-core if all weights are small (polynomially bounded). finally, we also suggest heuristic algorithms for checking the non-emptiness of the cs-core.",nan
2008,agents preferences in decentralized task allocation,"the ability to express preferences for specific tasks in multi-agent auctions is an important element for potential users who are considering to use such auctioning systems. this paper presents an approach to make such preferences explicit and to use these preferences in bids for reverse combinatorial auctions. three different types of preference are considered: (1) preferences for particular durations of tasks, (2) preferences for certain time points, and (3) preferences for specific types of tasks. we study empirically the tradeoffs between the quality of the solutions obtained and the use of preferences in the bidding process, focusing on effects such as increased execution time. we use both synthetic data as well as real data from a logistics company.",nan
2008,game theoretical insights in strategic patrolling: model and algorithm in normal-form,"in artificial intelligence literature there is a rising interest in studying strategic interaction situations. in these situations a number of rational agents act strategically, being in competition, and their analysis is carried out by employing game theoretical tools. one of the most challenging strategic interaction situation is the strategic patrolling: a guard patrols a number of houses in the attempt to catch a rob, which, in its turn, chooses a house to rob in the attempt to be not catched by the guard. our contribution in this paper is twofold. firstly, we provide a critique concerning the models presented in literature and we propose a model that is game theoretical satisfactory. secondly, by exploit the game theoretical analysis to design a solving algorithm more efficient than state-of-the-art's ones.",nan
2008,monitoring the execution of a multi-agent plan: dealing with partial observability,"the paper addresses the task of monitoring and diagnosing the execution of a multi-agent plan (map) which involves actions concurrently executed by a team of cooperating agents. the paper describes a weak commitment strategy to deal with cases where observability is only partial and it is not sufficient for inferring the outcome of all the actions executed so far. the paper discusses the role of target actions in providing sufficient conditions for inferring the pending outcomes in a finite time window. the action outcome provides the basis for computing plan diagnosis and for singling out the goals which will not be achieved because of an action failure.",nan
2008,a hybrid approach to multi-agent decision-making,"in the aftermath of a large-scale disaster, agents' decisions derive from self-interested (e.g. survival), common-good (e.g. victims' rescue) and teamwork (e.g. fire extinction) motivations. however, current decision-theoretic models are either purely individual or purely collective and find it difficult to deal with motivational attitudes; on the other hand, mental-state based models find it difficult to deal with uncertainty. we propose a hybrid, cvi-ji, approach that combines: i) collective ‘versus’ individual (cvi) decisions, founded on the markov decision process (mdp) quantitative evaluation of joint-actions, and ii) joint-intentions (ji) formulation of teamwork, founded on the belief-desire-intention (bdi) architecture of general mental-state based reasoning. the cvi-ji evaluation explores the performance's improvement during the process of learning a coordination policy in a partially observable stochastic domain.",nan
2008,coalition formation strategies for self-interested agents,"coalition formation is a major research issue in multiagent systems in which the agents are self-interested. in these systems, agents have to form groups in order to achieve common goals, which they are not able to achieve individually. a coalition formation mechanism requires two definition levels: firstly agents need a common protocol to reach an agreement and secondly individual strategies are required to make efficient proposals. both issues are addressed in this paper. first, we propose a two-phase decentralized protocol that allows agents to interact directly through message passing. secondly we propose some strategies which allow agents to make clever proposals using the information that has already been collected from other agents. the experimental evaluation shows that the proposed mechanism allows agents to efficiently form coalitions and that the strategies make real improvements for the coalition search process.",nan
2008,of mechanism design multiagent planning,"multiagent planning methods are concerned with planning by and for a group of agents. if the agents are self-interested, they may be tempted to lie in order to obtain an outcome that is more rewarding for them. we therefore study the multiagent planning problem from a mechanism design perspective, showing how to incentivise agents to be truthful. we prove that the well-known truthful vcg mechanism is not always truthful in the context of optimal planning, and present a modification to fix this. finally, we present some (domain-dependent) poly-time planning algorithms using this fix that maintain truthfulness in spite of their non-optimality.",nan
2008,iamwildcat: the winning strategy for the tac market design competition,"in this paper we describe the iamwildcat agent, designed for the tac market design game which is part of the international trading agent competition. the objective of an agent in this competition is to effectively manage and operate a market that attracts traders to compete for resources in it. this market, in turn, competes against markets operated by other competition entrants and the aim is to maximise the market and profit share of the agent, as well as its transaction success rate. to do this, the agent needs to continually monitor and adapt, in response to the competing marketplaces, the rules it uses to accept offers, clear the market, price the transactions and charge the traders. given this context, this paper details iamwildcat's strategic behaviour and describes the wide techniques we developed to operationalise this. finally, we empirically analyse our agent in different environments, including the 2007 competition where it ranked first.",nan
2008,multi-agent reinforcement learning algorithm with variable optimistic-pessimistic criterion,"a reinforcement learning algorithm for multi-agent systems based on variable hurwicz's optimistic-pessimistic criterion is proposed. the formal proof of its convergence is given. hurwicz's criterion allows to embed initial knowledge of how friendly the environment in which the agent is supposed to function will be. thorough testing of the developed algorithm against well-known reinforcement learning algorithms has shown that in many cases its successful performance can be explained by its tendency to force the other agents to follow the policy which is more profitable for it. in addition the variability of hurwicz's criterion allowed it to converge to best-response against opponents with stationary policies.",nan
2008,as safe as it gets: near-optimal learning in multi-stage games with imperfect monitoring,"we introduce the first near-optimal polynomial algorithm for obtaining the mixed safety level value of an initially unknown multi-stage game, played in a hostile environment, under imperfect monitoring. in an imperfect monitoring setting all that an agent can observe is the current state and its own actions and payoffs, but it can not observe other agents' actions. our result holds for any multi-stage generic game with a “reset” action.",nan
2008,a heuristic based seller agent for simultaneous english auctions,"the popularity of online auction emerges due to the flexibility and convenience that it offers to consumers. sellers offer a variety of items for sale with the aims of obtaining more profit. to obtain a reasonable profit, the reserve price for the item must be determined before the item is put up for sale. however, setting the price too high may result in no sale, while setting the price too low may yield a lower profit. in real auction, this is the main selling problem since most sellers fail to place a strategic reserve price for a given item which results in a lower profit. in this work, we develop a seller agent that proposes the item's reserve price based upon several selling constraints that comprise of the number of competitors, the number of bidders, the duration of the auction and the degree of profit that the seller desires when disposing the item. this paper describes the detail implementation and design of our agent's strategy. the seller strategy will be evaluated across a wide context of diverse and varying selling environment using a simulated auction marketplace.",nan
2008,a truthful two-stage mechanism for eliciting probabilistic estimates with unknown costs,"this paper reports on the design of a novel two-stage mechanism, based on strictly proper scoring rules, that motivates selfish rational agents to make a costly probabilistic estimate or forecast of a specified precision and report it truthfully to a centre. our mechanism is applied in a setting where the centre is faced with multiple agents, and has no knowledge about their costs. thus, in the first stage of the mechanism, the centre uses a reverse second price auction to allocate the estimation task to the agent who reveals the lowest cost. while, in the second stage, the centre issues a payment based on a strictly proper scoring rule. when taken together, the two stages motivate agents to reveal their true costs, and then to truthfully reveal their estimate. we prove that this mechanism is incentive compatible and individually rational, and then present empirical results comparing the performance of the well known quadratic, spherical and logarithmic scoring rules. we show that the quadratic and the logarithmic rules result in the centre making the highest and the lowest expected payment to agents respectively. at the same time, however, the payments of the latter rule are unbounded, and thus the spherical rule proves to be the best candidate in this setting.",nan
2008,goal generation and adoption from partially trusted beliefs,"a rational agent adopts (or changes) its goals when new information (beliefs) becomes available or its desires (e.g., tasks it is supposed to carry out) change. in this paper we propose a non-conventional approach for adopting goals which takes the degree of trust in the sources of information into account. beliefs, desires, and goals, as a consequence, are gradual. incoming information may be any propositional formula.two algorithms for updating the mental state of an agent in this new setting are proposed. the first algorithm is relevant to the updating when a new piece of information arrives and the second one is relevant to the updating when a new desire arises.",nan
2008,adaptive play in texas hold'em poker,"we present a texas hold'em poker player for limit headsup games. our bot is designed to adapt automatically to the strategy of the opponent and is not based on nash equilibrium computation. the main idea is to design a bot that builds beliefs on his opponent's hand. a forest of game trees is generated according to those beliefs and the solutions of the trees are combined to make the best decision. the beliefs are updated during the game according to several methods, each of which corresponding to a basic strategy. we then use an exploration-exploitation bandit algorithm, namely the ucb (upper confidence bound), to select a strategy to follow. this results in a global play that takes into account the opponent's strategy, and which turns out to be rather unpredictable. indeed, if a given strategy is exploited by an opponent, the ucb algorithm will detect it using change point detection, and will choose another one.the initial resulting program, called brennus, participated to the aaai'07 computer poker competition in both online and equilibrium competition and ranked eight out of seventeen competitors.",nan
2008,theoretical and computational properties of preference-based argumentation,"during the last years, argumentation has been gaining increasing interest in modeling different reasoning tasks of an agent. many recent works have acknowledged the importance of incorporating preferences or priorities in argumentation. however, relatively little is known about the theoretical and computational implications of preferences in argumentation.in this paper we introduce and study an abstract preference-based argumentation framework that extends dung's formalism by imposing a preference relation over the arguments. under some reasonable assumptions about the preference relation, we show that the new framework enjoys desirable properties, such as coherence. we also present theoretical results that shed some light on the role that preferences play in argumentation. moreover, we show that although some reasoning problems are intractable in the new framework, it appears that the preference relation has a positive impact on the complexity of reasoning.",nan
2008,norm defeasibility in an institutional normative framework,"normative environments have been proposed to regulate agent interaction in open multi-agent systems. however, most approaches rely on pre-imposed regulations that agents are subject to. taking a different stance, we focus on a normative framework that assists agents in establishing by themselves their own commitment norms. with that aim in mind, a model of norm defeasibility is presented that enables exploiting and adapting a normative background to different extents. we formalize the normative state using first-order logic and define rules and norms operating on that state. a suitable semantics regarding the use of norms within a hierarchical context structure is given, based on norm activation conflict and defeasibility.",nan
2008,slide: a useful special case of the cardpath constraint,"we study the cardpath constraint. this ensures a given constraint holds a number of times down a sequence of variables. we show that slide, a special case of cardpath where the slid constraint must hold always, can be used to encode a wide range of sliding sequence constraints including cardpath itself. we consider how to propagate slide and provide a complete propagator for cardpath. since propagation is np-hard in general, we identify special cases where propagation takes polynomial time. our experiments demonstrate that using slide to encode global constraints can be as efficient and effective as specialised propagators.",nan
2008,frontier search for bicriterion shortest path problems,frontier search is a new search technique that achieves important memory savings over previous best-first search algorithms. this paper describes an extension of frontier search to bicriterion graph search problems that achieves also important memory savings. the new algorithm is evaluated using a set of random problems.,nan
2008,heuristics for dynamically adapting propagation,"building adaptive constraint solvers is a major challenge in constraint programming. an important line of research towards this goal is concerned with ways to dynamically adapt the level of local consistency applied during search. a related problem that is receiving a lot of attention is the design of adaptive branching heuristics. the recently proposed adaptive variable ordering heuristics of boussemart et al. use information derived from domain wipeouts to identify highly active constraints and focus search on hard parts of the problem resulting in important saves in search effort. in this paper we show how information about domain wipeouts and value deletions gathered during search can be exploited, not only to perform variable selection, but also to dynamically adapt the level of constraint propagation achieved on the constraints of the problem. first we demonstrate that when an adaptive heuristic is used, value deletions and domain wipeouts caused by individual constraints largely occur in clusters of consecutive or nearby constraint revisions. based on this observation, we develop a number of simple heuristics that allow us to dynamically switch between enforcing a weak, and cheap local consistency, and a strong but more expensive one, depending on the activity of individual constraints. as a case study we experiment with binary problems using ac as the weak consistency and maxrpc as the strong one. results from various domains demonstrate the usefulness of the proposed heuristics.",nan
2008,near admissible algorithms for multiobjective search,"in this paper, we propose near admissible multiobjective search algorithms to approximate, with performance guarantee, the set of pareto optimal solution paths in a state space graph. approximation of pareto optimality relies on the use of an epsilon-dominance relation between vectors, significantly narrowing the set of non-dominated solutions. we establish correctness of the proposed algorithms, and discuss computational complexity issues. we present numerical experimentations, showing that approximation significantly improves resolution times in multiobjective search problems.",nan
2008,compressing pattern databases with learning,a pattern database (pdb) is a heuristic function implemented as a lookup table. it stores the lengths of optimal solutions for instances of subproblems. most previous pdbs had a distinct entry in the table for each subproblem instance. in this paper we apply learning techniques to compress pdbs by using neural networks and decision trees thereby reducing the amount of memory needed. experiments on the sliding tile puzzles and the topspin puzzle show that our compressed pdbs significantly outperforms both uncompressed pdbs as well as previous compressing methods. our full compressing system reduced the size of memory needed by a factor of up to 63 at a cost of no more than a factor of 2 in the search effort.,nan
2008,a decomposition technique for max-csp,"the objective of the maximal constraint satisfaction problem (max-csp) is to find an instantiation which minimizes the number of constraint violations in a constraint network. in this paper, inspired from the concept of inferred disjunctive constraints introduced by freuder and hubbe, we show that it is possible to exploit the arc-inconsistency counts, associated with each value of a network, in order to avoid exploring useless portions of the search space. the principle is to reason from the distance between the two best values in the domain of a variable, according to such counts. from this reasoning, we can build a decomposition technique which can be used throughout search in order to decompose the current problem into easier sub-problems. interestingly, this approach does not depend on the structure of the constraint graph, as it is usually proposed. alternatively, we can dynamically post hard constraints that can be used locally to prune the search space. the practical interest of our approach is illustrated, using this alternative, with an experimentation based on a classical branch and bound algorithm, namely pfc-mrdac.",nan
2008,fast set bounds propagation using bdds,"set bounds propagation is the most popular approach to solving constraint satisfaction problems (csps) involving set variables. the use of reduced ordered binary decision diagrams (bdds) to represent and solve set csps is well understood and brings the advantage that propagators for arbitrary set constraints can be built. this can substantially improve solving. the disadvantages of bdds is that creating and manipulating bdds can be expensive. in this paper we show how we can perform set bounds propagation using bdds in a much more efficient manner by generically creating set constraint predicates, and using a marking approach to propagation. the resulting system can be significantly faster than competing approaches to set bounds propagation.",nan
2008,a new approach for solving satisfiability problems with qualitative preferences,"the problem of expressing and solving satisfiability problems (sat) with qualitative preferences is central in many areas of computer science and artificial intelligence. in previous papers, it has been shown that qualitative preferences on literals allow for capturing qualitative/quantitative preferences on literals/formulas; and that an optimal model for a satisfiability problems with qualitative preferences on literals can be computed via a simple modification of the davis-logemann-loveland procedure (dll): given a sat formula, an optimal solution is computed by simply imposing that dll branches according to the partial order on the preferences. unfortunately, it is well known that introducing an ordering on the branching heuristic of dll may cause an exponential degradation in its performances. the experimental analysis reported in these papers hightlights that such degradation can indeed show up in the presence of a significant number of preferences.in this paper we propose an alternative solution which does not require any modification of the dll heuristic: once a solution is computed, a constraint is added to the input formula imposing that the new solution (if any) has to be better than the last computed. we implemented this idea, and the resulting system can lead to significant improvements wrt the original proposal when dealing with min-one/max-sat problems corresponding to qualitative preferences on structured instances.",nan
2008,combining binary constraint networks in qualitative reasoning,"constraint networks in qualitative spatial and temporal reasoning are always complete graphs. when one adds an extra element to a given network, previously unknown constraints are derived by intersections and compositions of other constraints, and this may introduce inconsistency to the overall network. likewise, when combining two consistent networks that share a common part, the combined network may become inconsistent.in this paper, we analyse the problem of combining these binary constraint networks and develop certain conditions to ensure combining two networks will never introduce an inconsistency for a given spatial or temporal calculus. this enables us to maintain a consistent world-view while acquiring new information in relation with some part of it. in addition, our results enable us to prove other important properties of qualitative spatial and temporal calculi in areas such as representability and complexity.",nan
2008,solving necklace constraint problems,"some constraint problems have a combinatorial structure where the constraints allow the sequence of variables to be rotated (necklaces), if not also the domain values to be permuted (unlabelled necklaces), without getting an essentially different solution. we bring together the fields of combinatorial enumeration, where efficient algorithms have been designed for (special cases of) some of these combinatorial objects, and constraint programming, where the requisite symmetry breaking has at best been done statically so far. we design the first search procedure and identify the first symmetry-breaking constraints for the general case of unlabelled necklaces. further, we compare dynamic and static symmetry breaking on real-life scheduling problems featuring (unlabelled) necklaces.",nan
2008,vivifying propositional clausal formulae,"in this paper, we present a new way to preprocess boolean formulae in conjunctive normal form (cnf). in contrast to most of the current pre-processing techniques, our approach aims at improving the filtering power of the original clauses while producing a small number of additional and relevant clauses. more precisely, an incomplete redundancy check is performed on each original clauses through unit propagation, leading to either a sub-clause or to a new relevant one generated by the clause learning scheme. this preprocessor is empirically compared to the best existing one in terms of size reduction and the ability to improve a state-of-the-art satisfiability solver.",nan
2008,hybrid tractable csps which generalize tree structure,"the constraint satisfaction problem (csp) is a central generic problem in artificial intelligence. considerable progress has been made in identifying properties which ensure tractability in such problems, such as the property of being tree-structured. in this paper we introduce the broken-triangle property, which allows us to define a hybrid tractable class for this problem which significantly generalizes the class of problems with tree structure. we show that the broken-triangle property is conservative (i.e., it is preserved under domain reduction and hence under arc consistency operations) and that there is a polynomial-time algorithm to determine an ordering of the variables for which the broken-triangle property holds (or to determine that no such ordering exists). we also present a non-conservative extension of the broken-triangle property which is also sufficient to ensure tractability and can be detected in polynomial time.",nan
2008,justification-based non-clausal local search for sat,"while stochastic local search (sls) techniques are very efficient in solving hard randomly generated propositional satisfiability (sat) problem instances, a major challenge is to improve sls on structured problems. motivated by heuristics applied in complete circuit-level sat solvers in electronic design automation, we develop novel sls techniques by harnessing the concept of justification frontiers. this leads to sls heuristics which concentrate the search into relevant parts of instances, exploit observability don't cares and allow for an early stopping criterion. experiments with a prototype implementation of the framework presented in this paper show up to a four orders of magnitude decrease in the number of moves on real-world bounded model checking instances when compared to walksat on the standard cnf encodings of the instances.",nan
2008,multi-valued pattern databases,"pattern databases were a major breakthrough in heuristic search by solving hard combinatorial problems various orders of magnitude faster than state-of-the-art techniques at that time. since then, they have received a lot of attention. moreover, pattern databases are also researched in conjunction with other domain-independent techniques for solving planning tasks. however, they are not the only technique for improving heuristic estimates. although more modest, perimeter search can also lead to significant improvements in the number of generated nodes and overall running time. therefore, whether they can be combined or not is a natural and interesting issue. while other researchers have recently proven that a joint application of both ideas (termed as multiple goal) leads to no progress at all, it is shown here that there are other alternatives for putting both techniques together—denoted here as multi-valued. this paper shows that multi-valued pattern databases can still improve the performance of standard (or single-valued) pattern databases in practice. it also examines how to enhance memory usage when comparing multi-valued pattern databases in contraposition to various single-valued standard pattern databases.",nan
2008,using abstraction in two-player games,"for most high-performance two-player game programs, a significant amount of time is devoted to developing the evaluation function. an important issue in this regard is how to take advantage of a large memory. for some two-player games, endgame databases have been an effective way of reducing search effort and introducing accurate values into the search. for some one-player games (puzzles), pattern databases have been effective at improving the quality of the heuristic values used in a search.this paper presents a new approach to using endgame and pattern databases to assist in constructing an evaluation function for two-player games. via abstraction, single-agent pattern databases are applied to two-player games. positions in endgame databases are viewed as an abstraction of more complicated positions; database lookups are used as evaluation function features. these ideas are illustrated using chinese checkers and chess. for each domain, even small databases can be used to produce strong game play. this research has relevance to the recent interest in building general game-playing programs. for two-player applications where pattern and/or endgame databases can be built, abstraction can be used to automatically construct an evaluation function.",nan
2008,a practical temporal constraint management system for real-time applications,"a temporal constraint management system (tcms) is a temporal network together with algorithms for managing the constraints in that network over time. this paper presents a practical tcms, called mysystem, that efficiently handles the propagation of the kinds of temporal constraints commonly found in real-time applications, while providing constant-time access to “all-pairs, shortest-path” information that is extremely useful in many applications. the temporal network in mysystem includes special time-points for dealing with the passage of time and eliminating the need for certain common forms of constraint propagation. the constraint propagation algorithm in mysystem maintains a restricted set of entries in the associated all-pairs, shortest-path matrix by incrementally propagating changes to the network either from adding a new constraint or strengthening, weakening or deleting an existing constraint. the paper presents empirical evidence to support the claim that mysystem is scalable to real-time planning, scheduling and acting applications.",nan
2008,towards efficient belief update for planning-based web service composition,"at the “functional level”, semantic web services (sws) are described akin to planning operators, with preconditions and effects relative to an ontology; the ontology provides the formal vocabulary and an axiomatisation of the underlying domain. composing such sws is similar to planning. a key obstacle in doing so effectively is handling the ontology axioms, which act as state constraints. computing the outcome of an action involves the frame and ramification problems, and corresponds to belief update. the complexity of such updates motivates the search for tractable classes. herein we investigate a class that is of practical relevance because it deals with many commonly used ontology axioms, in particular with attribute cardinality upper bounds which are not handled by other known tractable classes. we present an update computation that is exponential only in a comparatively uncritical parameter; we present an approximate update which is polynomial in that parameter as well.",nan
2008,genetic optimization of the multi-location transshipment problem with limited storage capacity,lateral transshipments afford a valuable mechanism for compensating unmet demands only with on-hand inventory. in this paper we investigate the case where locations have a limited storage capacity. the problem is to determine how much to replenish each period to minimize the expected global cost while satisfying storage capacity constraints. we propose a real-coded genetic algorithm (rcga) with a new crossover operator to approximate the optimal solution. we analyze the impact of different structures of storage capacities on the system behaviour. we find that transshipments are able to correct the discrepancies between the constrained and the unconstrained locations while ensuring low costs and system-wide inventories. our genetic algorithm proves its ability to solve instances of the problem with high accuracy.,nan
2008,regression for classical and nondeterministic planning,"many forms of reasoning about actions and planning can be reduced to regression, the computation of the weakest precondition a state has to satisfy to guarantee the satisfaction of another condition in the successor state. in this work we formalize a general syntactic regression operation for ground pddl operators, show its correctness, and define a composition operation based on regression. as applications we present a very simple yet powerful algorithm for computing invariants, as well as a generalization of the hn heuristic of haslum and geffner to pddl.",nan
2008,combining domain-independent planning and htn planning: the duet planner,"despite the recent advances in planning for classical domains, the question of how to use domain knowledge in planning is yet to be completely and clearly answered. some of the existing planners use domain-independent search heuristics, and some others depend on intensively-engineered domain-specific knowledge to guide the planning process. in this paper, we describe an approach to combine ideas from both of the above schools of thought. we present duet, our planning system that incorporates the ability of using hierarchical domain knowledge in the form of hierarchical task networks (htns) as in shop2 [14] and using domain-independent local search techniques as in lpg [8]. in our experiments, duet was able to solve much larger problems than lpg could solve, with only minimal domain knowledge encoded in htns (much less domain knowledge than shop2 needed to solve those problems by itself).",nan
2008,learning in planning with temporally extended goals and uncontrollable events,"recent contributions to advancing planning from the classical model to more realistic problems include using temporal logic such as ltl to express desired properties of a solution plan. this paper introduces a planning model that combines temporally extended goals and uncontrollable events. the planning task is to reach a state such that all event sequences generated from that state satisfy the problem's temporally extended goal. a real-life application that motivates this work is to use planning to configure a system in such a way that its subsequent, non-deterministic internal evolution (nominal behavior) is guaranteed to satisfy a condition expressed in temporal logic.a solving architecture is presented that combines planning, model checking and learning. an online learning process incrementally discovers information about the problem instance at hand. the learned information is useful both to guide the search in planning and to safely avoid unnecessary calls to the model checking module. a detailed experimental analysis of the approach presented in this paper is included. the new method for online learning is shown to greatly improve the system performance.",nan
2008,a simulation-based approach for solving generalized semi-markov decision processes,"time is a crucial variable in planning and often requires special attention since it introduces a specific structure along with additional complexity, especially in the case of decision under uncertainty. in this paper, after reviewing and comparing mdp frameworks designed to deal with temporal problems, we focus on generalized semi-markov decision processes (gsmdp) with observable time. we highlight the inherent structure and complexity of these problems and present the differences with classical reinforcement learning problems. finally, we introduce a new simulation-based reinforcement learning method for solving gsmdp, bringing together results from simulation-based policy iteration, regression techniques and simulation theory. we illustrate our approach on a subway network control example.",nan
2008,heuristics for planning with action costs revisited,"we introduce a simple variation of the additive heuristic used in the hsp planner that combines the benefits of the original additive heuristic, namely its mathematical formulation and its ability to handle non-uniform action costs, with the benefits of the relaxed planning graph heuristic used in ff, namely its compatibility with the highly effective enforced hill climbing search along with its ability to identify helpful actions. we implement a planner similar to ff except that it uses relaxed plans obtained from the additive heuristic rather than those obtained from the relaxed planning graph. we then evaluate the resulting planner in problems where action costs are not uniform and plans with smaller overall cost (as opposed to length) are preferred, where it is shown to compare well with cost-sensitive planners such as sgplan, sapa, and lpg. we also consider a further variation of the additive heuristic, where symbolic labels representing action sets are propagated rather than numbers, and show that this scheme can be further developed to construct heuristics that can take delete-information into account.",nan
2008,diagnosis of simple temporal networks,"in many domains successful execution of plans requires careful monitoring and repair. diagnosis of plan execution supports this process by identifying causes of plan failure.most plans have to satisfy temporal constraints. an important and common occurring problem during plan execution are violations of temporal plan constraints. this paper addresses diagnosis of such temporal constraint violations by modeling the temporal aspects of a plan as a simple temporal network (stn). we investigate the computational properties of standard diagnostic concepts but we also argue that traditional notions of preferred diagnoses such as minimum diagnosis are not adequate. a new notion of a maximum confirmation diagnosis is introduced.",nan
2008,an attentive machine interface using geo-contextual awareness for mobile vision tasks,"the presented work settles attention in the architecture of ambient intelligence, in particular, for the application of mobile vision tasks in multimodal interfaces. a major issue for the performance of these services is uncertainty in the visual information which roots in the requirement to index into a huge amount of reference images. we propose a system implementation – the attentive machine interface (ami) – that enables contextual processing of multi-sensor information in a probabilistic framework, for example to exploit contextual information from geo-services with the purpose to cut down the visual search space into a subset of relevant object hypotheses.we present a proof-of-concept with results from bottom-up information processing from experimental tracks and image capture in an urban scenario, extracting object hypotheses in the local context from both (i) mobile image based appearance and (ii) gps based positioning, and verify performance in recognition accuracy (> 10%) using bayesian decision fusion. finally, we demonstrate that top-down information processing – geo-information priming the recognition method in feature space – can yield even better results (> 13%) and more economic computing, verifying the advantage of multi-sensor attentive processing in multimodal interfaces.",nan
2008,learning functional object-categories from a relational spatio-temporal representation,"we propose a framework that learns functional object-categories from spatio-temporal data sets such as those abstracted from video. the data is represented as one activity graph that encodes qualitative spatio-temporal patterns of interaction between objects. event classes are induced by statistical generalization, the instances of which encode similar patterns of spatio-temporal relationships between objects. equivalence classes of objects are discovered on the basis of their similar role in multiple event instantiations. objects are represented in a multidimensional space that captures their role in all the events. unsupervised learning in this space results in functional object-categories. experiments in the domain of food preparation suggest that our techniques represent a significant step in unsupervised learning of functional object categories from spatio-temporal patterns of object interaction.",nan
2008,sequential spatial reasoning in images based on pre-attention mechanisms and fuzzy attribute graphs,"spatial relations play a crucial role in model-based image recognition and interpretation due to their stability compared to many other image appearance characteristics, and graphs are well adapted to represent such information. sequential methods for knowledgebased recognition of structures require to define in which order the structures have to be recognized, which can be expressed as the optimization of a path in the representation graph. we propose to integrate pre-attention mechanisms in the optimization criterion, in the form of a saliency map, by reasoning on the saliency of spatial area defined by spatial relations. such mechanisms extract knowledge from an image without object recognition in advance and do not require any a priori knowledge on the image. therefore, pre-attentional mechanisms provide useful knowledge for object segmentation and recognition. the derived algorithms are applied on brain image understanding.",nan
2008,automatic configuration of multi-robot systems: planning for multiple steps,"we consider multi-robot systems where robots need to cooperate tightly by sharing functionalities with each other. there are methods for automatically configuring a multi-robot system for tight cooperation, but they only produce a single configuration. in this paper, we show how methods for automatic configuration can be integrated with methods for task planning in order to produce a complete plan were each step is a configuration. we also consider the issues of monitoring and replanning in this context, and we demonstrate our approach on a real multi-robot system, the peis-ecology.",nan
2008,structure segmentation and recognition in images guided by structural constraint propagation,"in some application domains, such as medical imaging, the objects that compose the scene are known as well as some of their properties and their spatial arrangement. we can take advantage of this knowledge to perform the segmentation and recognition of structures in medical images. we propose here to formalize this problem as a constraint network and we perform the segmentation and recognition by iterative domain reductions, the domains being sets of regions. for computational purposes we represent the domains by their upper and lower bounds and we iteratively reduce the domains by updating their bounds. we show some preliminary results on normal and pathological brain images.",nan
2008,theoretical study of ant-based algorithms for multi-agent patrolling,"this paper addresses the multi-agent patrolling problem, which consists for a set of autonomous agents to visit all the places of an unknown environment as regularly as possible. the proposed approach is based on the ant paradigm. each agent can only mark and move according to its local perception of the environment. we study evaw, a pheromone-based variant of the evap [3] and vaw [12]. the main novelty of the paper is the proof of some emergent spatial properties of the proposed algorithm. in particular we show that obtained cycles are necessarily of same length, which ensures an efficient spatial distribution of the agents. we also report some experimental results and discuss open questions concerning the proposed algorithm.",nan
2008,incremental component-based construction and verification of a robotic system,"autonomous robots are complex systems that require the interaction/cooperation of numerous heterogeneous software components. nowadays, robots are critical systems and must meet safety properties including in particular temporal and real-time constraints. we present a methodology for modeling and analyzing a robotic system using the bip component framework integrated with an existing framework and architecture, the laas architecture for autonomous system, based on genom. the bip componentization approach has been successfully used in other domains. in this study, we show how it can be seamlessly integrated in the preexisting methodology. we present the componentization of the functional level of a robot, the synthesis of an execution controller as well as validation techniques for checking essential “safety” properties.",nan
2008,salience-driven contextual priming of speech recognition for human-robot interaction,"the paper presents an implemented model for priming speech recognition, using contextual information about salient entities. the underlying hypothesis is that, in human-robot interaction, speech recognition performance can be improved by exploiting knowledge about the immediate physical situation and the dialogue history. to this end, visual salience (objects perceived in the physical scene) and linguistic salience (objects, events already mentioned in the dialogue) are integrated into a single cross-modal salience model. the model is dynamically updated as the environment changes. it is used to establish expectations about which words are most likely to be heard in the given context. the update is realised by continuously adapting the word-class probabilities specified in a statistical language model. the paper discusses the motivations behind the approach, and presents the implementation as part of a cognitive architecture for mobile robots. evaluation results on a test suite show a statistically significant improvement of salience-driven priming speech recognition (wer) over a commercial baseline system.",nan
2008,a new cbr approach to the oil spill problem,"oil spills represent one of the most destructing environmental disasters. predicting the possibility of finding oil slicks in a certain area after an oil spill can be crucial in order to reduce the environmental risks. the system presented here forecasts the presence or not of oil slicks in a certain area of the open sea after an oil spill using case-based reasoning methodology. cbr is a computational methodology designed to generate solutions to a certain problem by analysing previous solutions given to previous solved problems. the proposed system wraps other artificial intelligence techniques such as a radial basis function networks, growing cell structures and principal components analysis in order to develop the different phases of the cbr cycle. cbr systems have never been used before to solve oil slicks problems. the proposed system uses information obtained from various satellites such as salinity, temperature, pressure, number and area of the slicks... oscbr system has been able to accurately predict the presence of oil slicks in the north west of the galician coast, using historical data.",nan
2008,questsemantics - intelligent search and retrieval of business knowledge,"keyword-based search engines, though hugely popular, show limitations when trying to answer very specific queries. the processing of search results is performed by users, rather than by software. ontologies provide a means to create formal, machineprocessable descriptions of the knowledge in a certain domain [14], and, by using elements of these descriptions to annotate suitable information sources, they can be analysed and manipulated in an intelligent manner. the questsemantics platform provides automated ontology-based metadata creation and resource annotation, with subsequent ontology-based querying of the annotated resources. the platform has been deployed in two commercial scenarios, providing useful feedback on both the feasibility and effectiveness of applying semantic web technologies to specific business problems.",nan
2008,intelligent adaptive monitoring for cardiac surveillance,"monitoring patients in intensive care units is a critical task. simple condition detection is generally insufficient to diagnose a patient and may generate many false alarms to the clinician operator. deeper knowledge is needed to discriminate among alarms those that necessitate urgent therapeutic action. we propose an intelligent monitoring system that makes use of many artificial intelligence techniques: artificial neural networks for temporal abstraction, temporal reasoning, model based diagnosis, decision rule based system for adaptivity and machine learning for knowledge acquisition. to tackle the difficulty of taking context change into account, we introduce a pilot aiming at adapting the system behavior by reconfiguring or tuning the parameters of the system modules. a prototype has been implemented and is currently experimented and evaluated. some results, showing the benefits of the approach, are given.",nan
2008,a decision support system for breast cancer detection in screening programs,"the goal of breast cancer screening programs is to detect cancers at an early (preclinical) stage, by using periodic mammographic examinations in asymptomatic women. in evaluating cases, mammographers insist on reading multiple images (at least two) of each breast as a cancerous lesion tends to be observed in different breast projections (views). most computer-aided detection (cad) systems, on the other hand, only analyze single views independently, and thus fail to account for the interaction between the views. in this paper, we propose a bayesian framework for exploiting multi-view dependencies between the suspected regions detected by a single-view cad system. the results from experiments with real-life data show that our approach outperforms the singleview cad system in distinguishing between normal and abnormal cases. such a system can support screening radiologists to improve the evaluation of breast cancer cases.",nan
2008,"the design, deployment and evaluation of the animalwatch intelligent tutoring system","europe and the u.s. both face the challenges of urban schools with low-achieving adolescent learners, many of whom are not proficient in the language of instruction. this paper describes the deployment and evaluation of the animalwatch intelligent tutoring system for mathematics in challenging classrooms. previous studies demonstrated that animalwatch benefits 12–14 year-old students in relatively controlled conditions. the current study indicates that the system can help older, very low-achieving students in challenging secondary schools that serve diverse student populations.",nan
2008,ai on the move: exploiting ai techniques for context inference on mobile devices,"context aware computing is a computational paradigm that has faced a rapid growth in the last few years, especially in the field of mobile devices. one of the promises of context-awareness in this field is the possibility of automatically adapting the functioning mode of mobile devices to the environment and the current situation the user is in, with the aim of improving both their efficiency (using the scarce resources in a more efficient way) and effectiveness (providing better services to the user). we propose a novel approach for providing a basic infrastructure for context-aware applications on mobile devices, in which ai techniques (namely a principled combination of rule-based systems, bayesian networks, and ontologies) are applied to context inference. the aim is to devise a general inferential framework to easier the development of context-aware applications by integrating the information coming from physical and logical sensors (e.g., position, agenda) and reasoning about this information in order to infer new and more abstract contexts. in previous contextaware applications, most researches focused almost exclusively on time and/or location and other few data, while the same contexts inference was limited to preconceived values. our approach differs from previous works since we do not focus on particular contextual values, but rather we have developed an architecture where managed contexts can be easily replaced by new contexts, depending on the different needs. moreover, the inferential infrastructure we designed is able to work in a more general way and can be easily adapted to different models of applications distribution. we show some concrete examples of applications built upon the inferential infrastructure and we discuss its strengths and limitations.",nan
2008,two stage knowledge discovery for spatio-temporal radio-emission data,"in this paper, we introduce a method to examine and interpret spatio-temporal radio emission datasets. the goal is to find communication patterns in the data in respect to spatial, temporal, and frequency based attributes. the chosen approach is a combination of two different ai-methods. first a clustering algorithm groups spatially close data points to potential emitters. in a second step a model-based constraint solving technique is applied to find relationships between the identified emitters. the used models describe rules of the communications that are to be found. this guarantees a flexible search for different kinds of communication.",nan
2008,using natural language generation technology to improve information flows in intensive care units,"in the drive to improve patient safety, patients in modern intensive care units are closely monitored with the generation of very large volumes of data. unless the data are further processed, it is difficult for medical and nursing staff to assimilate what is important. it has been demonstrated that data summarization in natural language has the potential to improve clinical decision making; we have implemented and evaluated a prototype system which generates such textual summaries automatically. our evaluation of the computer generated summaries showed that the decisions made by medical and nursing staff after reading the summaries were as good as those made after viewing the currently available graphical presentations with the same information content. since our automatically generated textual summaries can be improved by including additional content and expert knowledge, they promise to enhance information exchange between the medical and nursing staff, particularly when integrated with the currently available graphical presentations. the main feature of this technology is that it brings together a diverse set of techniques such as medical signal analysis, knowledge based reasoning, medical ontology and natural language generation. in this paper we discuss the main components of our approach with a critical analysis of their strengths and limitations and present options for improvement to address these limitations.",nan
2008,application and evaluation of a medical knowledge system in sonography (sonoconsult),"this paper presents the knowledge-system sonoconsult – an intelligent system in the medical domain. we evaluated the accuracy, acceptance and impact of sonoconsult, which has been used in clinical routine since 2002. the system was well accepted and had a significant clinical impact. in contrast to our original expectations, the diagnostic conclusions, although inferred with high accuracy, were less important especially for experienced physicians.",nan
2008,automating accreditation of medical web content,"the increasing amount of freely available health-related web content generates, on one hand, excellent conditions for self-education of patients as well as physicians, but on the other hand entails substantial risks if such information is trusted irrespective of low competence or even bad intentions of its authors. this is why medical web resources accreditation by renowned authorities is of high importance. however, various health web content surveys show that the proportion of accredited web resources is insufficient due to the difficulty of the labeling authorities to cope with the amount and dynamics of the medical web. in this paper, we address the problem of automating the accreditation of medical web content. to this end, we present a system which provides the infrastructure and the means to organize and support various aspects of the daily work of labeling experts, exploiting web content collection and information extraction techniques.",nan
2008,pattern classification techniques for early lung cancer diagnosis using an electronic nose,"we present a method to diagnose lung cancer by the analysis of breath using an electronic nose. this device can react to a gas substance by providing signals that can be analyzed to classify the input. it is composed of a sensor array (6 mos sensors, in our case) and a pattern classification process based on machine learning techniques. during the first phase of our research, we have evaluated the possibility and accuracy of lung cancer diagnosis by classifying the olfactory signal associated to exhalations of subjects. the second part of the research, still in progress, is aimed at assessing the possibility of discriminating also the different types and stages of the disease. at the end of the first phase, results have been very satisfactory and promising: we achieved an average accuracy of 92.6%, sensitivity of 95.3% and specificity of 90.5%. in particular we analyzed the breath of 101 individuals, of which 58 control subjects, and 43 suffer from different types of lung cancer (primary and not) at different stages. in order to find the components able to discriminate between the two classes ‘healthy’ and ‘sick’ at best, and to reduce the dimensionality of the problem, we have extracted the most significant features and projected them into a lower dimensional space using non parametric linear discriminant analysis. finally, we have used these features as input to several supervised pattern classification algorithms, based on different k-nearest neighbors (k-nn) approaches (classic, modified and fuzzy k-nn), linear and quadratic discriminant classifiers and on a feed-forward artificial neural network (ann). the observed results have all been validated using cross-validation. these results pushed us to begin the second phase of the project to investigate the possibility of early lung cancer diagnosis: we are involving a larger number of subjects, partioned in different classes according to the type and stage of the disease. the research demonstrates that the electronic nose is a promising alternative to current lung cancer diagnostic techniques: the obtained predictive errors are lower than those achieved by present diagnostic methods, and the cost of the analysis, both in money, time and resources, is lower. the introduction of this technology will lead to very important social and business effects: its low price and small dimensions allow a large scale distribution, giving the opportunity to perform non invasive, cheap, quick, and massive early diagnosis and screening.",nan
2008,a bdd approach to the feature subscription problem,"modern feature-rich telecommunications services offer significant opportunities to human users. to make these services more usable, facilitating personalisation is very important since it enhances the users' experience considerably. however, regardless how service providers organise their catalogues of features, they cannot achieve complete configurability due to the existence of feature interactions. distributed feature composition (dfc) provides a comprehensive methodology, underpinned by a formal architecture model to address this issue. in this paper we present an approach based on using binary decision diagrams (bdd) to find optimal reconfigurations of features when a user's preferences violate the technical constraints defined by a set of dfc rules. in particular, we propose hybridizing constraint programming and standard bdd compilation techniques in order to scale the construction of a bdd for larger size catalogues. our approach outperforms the standard bdd techniques by reducing the memory requirements by as much as five orders-of-magnitude and compiles the catalogues for which the standard techniques ran out of memory.",nan
2008,continuous plan management support for space missions: the raxem case,"this paper describes raxem, an ai-based system developed to support human mission planners in the daily task to plan uplink commands for an interplanetary spacecraft. the intelligent environment of raxem has been designed to support the users in analyzing the problem and taking planning decisions as a result of an interactive process. the system combines different ingredients like integrating flexible automated algorithms, promoting user active participation during problem solving, and guaranteeing continuity of work practice. the paper touches upon all these aspects and comments on how a key factor for success has been the integration of intelligent technology to continuously support mission plan management.",nan
2008,the i-walker: an intelligent pedestrian mobility aid,"in this paper we focus on the development of an intelligent pedestrian mobility aid that we call i-walker for elders. this target population includes, but is not limited to, persons with low vision, visual field neglect, spasticity, tremors, and cognitive deficits. share-it will provide an agent-based intelligent decision support system to aid the elders.",nan
2008,mixture of gaussians model for robust pedestrian images detection,"automated pedestrian detection is a forward looking challenge for future driver support systems in automotive industry. such system would have to make safety critical decisions based on poor quality images shot in real-time from the unstable moving vehicles. the proposed system offers a simple yet very effective detection methodology based on mixture of gaussians (mog) aided by an expectation-maximisation (em) clustering algorithm. the algorithm operates on a number of features built by aggregation of different variations of the first and second order pixel gradients related to the aggregated templates of pedestrian and non-pedestrian classes. for each class the algorithm fits a fixed number of clusters and using gaussian kernels optimises the parameters of the gaussian mixture model such that the probabilities of belonging to the intraclass clusters is maximised. given a new image the system instantly generates relative features and uses mixture model to build posterior probability densities for all clusters and after aggregation and renormalisation, posterior class probabilities. the system has been fine-tuned against its parameters and feature subsets and tested using almost 10000 real images provided by daimlerchrysler. reaching the testing performance in excess of 95% the model was announced the winner of the nisis competition 2007.",nan
2008,deriving explanations from causal information,"we define an inference system to capture explanations based on causal statements, using an ontology in the form of an is-a hierarchy. we introduce a simple logical language which makes it possible to express that a fact causes another fact and that a fact explains another fact. we present a set of formal inference patterns from causal statements to explanation statements. we introduce an elementary ontology which gives greater expressiveness to the system while staying close to propositional reasoning. we provide an inference system that captures the patterns discussed, in a datalog (limited predicate) framework.",nan
2008,a hybrid tableau algorithm for [ascr    ][lscr    ][cscr    ][qscr    ],"we propose an approach for extending a tableau-based satisfiability algorithm by an arithmetic component. the result is a hybrid concept satisfiability algorithm for the description logic (dl) [ascr    ][lscr    ][cscr    ][qscr    ] which extends [ascr    ][lscr    ][cscr    ] with qualified number restrictions. the hybrid approach ensures a more informed calculus which, on the one hand, adequately handles the interaction between numerical and logical restrictions of descriptions, and on the other hand, when applied is a very promising framework for average case optimizations.",nan
2008,semantic relatedness in semantic networks,"this paper presents a new semantic relatedness measure on semantic networks (sn) that uses both hierarchical and non-hierarchical relations. our approach relies on two assumptions. firstly, in a given sn, only a few numbers of paths can be considered as “semantically correct” and these paths obey to a given set of rules. secondly, following a given edge in a path has a cost (which depends on its type, is-a, part-of, etc.) and its position in the path. we propose an evaluation of our measure on wordnet with two different benchmarks, using the part-of relation. we show that, in this context, our measure does better than the classical semantic measures.",nan
2008,hoopo: a hybrid object-oriented integration of production rules owl ontologies,"we describe a framework for the development of production rule programs on top of owl ontologies, following a hybrid object-oriented (oo) approach. the hybrid nature is realized by separating ontologies and rules, interfacing an external dl reasoner and a production rule engine. the oo nature is realized by mapping owl ontologies into the oo model, in such a way, so to preserve the extensional ontology semantics when the oo ontology constructs are matched in the production rule conditions.",nan
2008,rule-based owl ontology reasoning using dynamic abox entailments,"in the rule-based owl reasoning paradigm, ontologies are mapped into an internal rule engine representation format and rules are applied, such as tbox and abox owl entailment rules, in order to deduce new knowledge. in this paper we briefly introduce the notion of dynamically generating abox entailment rules in order to enhance the abox reasoning performance of a rule engine. the proposed methodology is still based on entailments rules for reasoning, using generic tbox entailments for handling owl semantics about concepts and roles, and dynamic abox entailments for handling ontology instances.",nan
2008,computability and complexity issues of extended rdf,"erdf stable model semantics is a recently proposed semantics for erdf ontologies and a faithful extension of rdfs semantics on rdf graphs. unfortunately, erdf stable model semantics is in general undecidable. in this paper, we elaborate on the computability and complexity issues of the erdf stable model semantics.",nan
2008,propositional merging operators based on set-theoretic closeness,"in the propositional setting, a well-studied family of merging operators are distance-based ones: the models of the merged base are the closest interpretations to the given profile. closeness is, in this context, measured as a number resulting from the aggregation of the distances to each base of the profile. in this work we define a new familly of propositional merging operators, close to such distance-based merging operators, but relying on a set-theoretic definition of closeness, already at work in several revision/update operators from the literature. we study a specific merging operator of this family, obtained by considering set-product as the aggregation function.",nan
2008,partial and informative common subsumers in description logics,"least common subsumers in description logics have shown their usefulness for discovering commonalities among all concepts of a collection. several applications are nevertheless focused on searching for properties shared by significant portions of a collection rather than by the collection as a whole. actually, this is an issue we faced in a real case scenario that provided initial motivation for this study, namely the process of core competence extraction in knowledge intensive companies. the paper defines four reasoning services for the identification of meaningful common subsumers describing partial commonalities in a collection. in particular common sub-sumers adding informative content to the least common subsumer are investigated, with reference to different dls.",nan
2008,approximate structure preserving semantic matching,"typical ontology matching applications, such as ontology integration, focus on the computation of correspondences holding between the nodes of two graph-like structures, e.g., between concepts in two ontologies. however, there are applications, such as web service integration, where we may need to establish whether full graph structures correspond to one another globally, preserving certain structural properties of the graphs being considered. the goal of this paper is to provide a new matching operation, called structure preserving matching. this operation takes two graph-like structures and produces a set of correspondences between those nodes of the graphs that correspond semantically to one another, (i) still preserving a set of structural properties of the graphs being matched, (ii) only in the case if the graphs are globally similar to one another. we present a novel approximate structure preserving matching approach that implements this operation. it is based on a formal theory of abstraction and on a tree edit distance measure. we have evaluated our solution with encouraging results.",nan
2008,discovering temporal knowledge from a crisscross of timed observations,"this paper is concerned with the discovering of temporal knowledge from a sequence of timed observations provided by a system monitoring of dynamic process. the discovering process is based on the stochastic approach framework where a series of timed observations is represented with a markov chain. from this representation, a set of timed sequential binary relations between discrete event classes is discovered with an abductive reasoning and represented as abstract chronicle models. to reduce the search space as close as possible to the potential relations between the process variables, we propose to characterize a set of series of timed observations with a unique measure of the homogeneity of the crisscross of class occurrences and to use this measure to prune abstract chronicle models.",nan
2008,fred meets tweety,"we propose a framework that brings together two major forms of default reasoning in artificial intelligence: applying default property classification rules in static domains, and default persistence of properties in temporal domains. particular attention is paid to the central problem of qualification. we illustrate how previous semantics developed independently for the two separate forms of default reasoning naturally lead to the integration that we propose, and how this gives rise to domains where different types of knowledge interact and qualify each other while preserving elaboration tolerance.",nan
2008,definability in logic and rough set theory,"rough set theory is an effective tool for data mining. according to the theory, a concept is definable if it can be written as a boolean combination of equivalence classes induced from classification attributes. on the other hand, definability in logic has been explicated by beth's theorem. in this paper, we propose two data representation formalisms, called first-order data logic (fodl) and attribute value-sorted logic (avsl), respectively. based on these logics, we explore the relationship between logical definability and rough set definability.",nan
2008,wikitaxonomy: a large scale knowledge resource,"we present a taxonomy automatically generated from the system of categories in wikipedia. categories in the resource are identified as either classes or instances and included in a large subsumption, i.e. isa, hierarchy. the taxonomy is made available in rdfs format to the research community, e.g. for direct use within ai applications or to bootstrap the process of manual ontology creation.",nan
2008,classifier combination using a class-indifferent method,"in this paper we present a novel approach to combining classifiers in the dempster-shafer theory framework. this approach models each output given by classifiers as a list of ranked decisions (classes), which is partitioned into a new evidence structure called a triplet. resulting triplets are then combined by dempster's rule. with a triplet, its first subset contains a decision corresponding to the largest numeric value of classes, the second subset corresponds to the second largest numeric value and the third subset represents uncertainty information in determining the support for the former two decisions. we carry out a comparative analysis with the combination methods of majority voting, stacking and boosting on the uci benchmark data to demonstrate the advantage of our approach.",nan
2008,reinforcement learning with classifier selection for focused crawling,"focused crawlers are programs that wander in the web, using its graph structure, and gather pages that belong to a specific topic. the most critical task in focused crawling is the scoring of the urls as it designates the path that the crawler will follow, and thus its effectiveness. in this paper we propose a novel scheme for assigning scores to the urls, based on the reinforcement learning (rl) framework. the proposed approach learns to select the best classifier for ordering the urls. this formulation reduces the size of the search space for the rl method and makes the problem tractable. we evaluate the proposed approach on-line on a number of topics, which offers a realistic view of its performance, comparing it also with a rl method and a simple but effective classifier-based crawler. the results demonstrate the strength of the proposed approach.",nan
2008,intuitive action set formation in learning classifier systems with memory registers,"an important design goal in learning classifier systems (lcs) is to equally reinforce those classifiers which cause the level of reward supplied by the environment. in this paper, we propose a new method for action set formation in lcs. when applied to a zeroth level classifier system with memory registers (zcsm), our method allows the distribution of rewards among classifiers which result in the same memory state, rather than those encoding the same memory update action.",nan
2008,an ensemble of classifiers for coping with recurring contexts in data streams,"this paper proposes a general framework for classifying data streams by exploiting incremental clustering in order to dynamically build and update an ensemble of incremental classifiers. to achieve this, a transformation function that maps batches of examples into a new conceptual feature space is proposed. the clustering algorithm is then applied in order to group different concepts and identify recurring contexts. the ensemble is produced by maintaining an classifier for every concept discovered in the streamthe full version of this paper as well as the datasets used for evaluation can be found at: http://mlkd.csd.auth.gr/concept_drift.html.",nan
2008,content-based social network analysis,"relationships among actors in traditional social network analysis are modelled as a function of the quantity of relations (co-authorships, business relations, friendship, etc.). in contrast, within a business, social or research community, network analysts are interested in the communicative content exchanged by the community members, not merely in the number of relationships. in order to meet this need, this paper presents a novel social network model, in which the actors are not simply represented through the intensity of their mutual relationships, but also through the analysis and evolution of their shared interests. text mining and clustering techniques are used to capture the content of communication and to identify the most popular topics.",nan
2008,efficient data clustering by local density approximation,"the clustering task is a key part of the data mining process. in today's context of massive data, methods with a computational complexity more than linear are unlikely to be applied practically. in this paper, we begin by a simple assumption: local projections of the data should allow to distinguish local cluster structures. from there, we describe how to obtain “pure” local sub-groupings of points, from projections on randomly chosen lines. the clustering of the data is obtained from the clustering of these sub-groupings. our method has a linear complexity in the dataset size, and requires only one pass on the original dataset. being local in essence, it can handle twisted geometries typical of many high-dimensional datasets. we describe the steps of our method and report encouraging results.",nan
2008,gas turbine fault diagnosis using random forests,"in the present paper, random forests are used in a critical and at the same time non trivial problem concerning the diagnosis of gas turbine blading faults, portraying promising results. random forests-based fault diagnosis is treated as a pattern recognition problem, based on measurements and feature selection. two different types of inserting randomness to the trees are studied, based on different theoretical assumptions. the classifier is compared against other machine learning algorithms such as neural networks, classification and regression trees, naive bayes and k-nearest neighbor. the performance of the prediction model reaches a level of 97% in terms of precision and recall, improving the existing state-of-the-art levels achieved by neural networks by a factor of 1.5%–2%.",nan
2008,how many objects?: determining the number of clusters with a skewed distribution,"we propose a supervised approach to enable accurate determination of the number of clusters in object identification. we use the aggregated attribute values of the data set to be clustered as explanatory variables in the prediction model. attribute aggregation can be done in linear time with respect to the number of data items, so our method can be used to predict the number of clusters with a low computational burden. to deal with skewed target values, we introduce a two-stage method as well as a method using a higher-order combination of explanatory variables. experiments demonstrate our methods enable more accurate prediction than existing methods.",nan
2008,active concept learning for ontology evolution,"this paper proposes an approach that enables agents to teach each other concepts from their ontologies using examples. unlike other concept learning approaches, our approach enables the learner to elicit the most informative examples interactively from the teacher. hence, the learner participates to the learning process actively. we empirically compare the proposed approach with the previous concept learning approaches. our experiments show that using the proposed approach, agents can learn new concepts successfully and with fewer examples.",nan
2008,determining automatically the size of learned ontologies,"determining the size of an ontology that is automatically learned from texts is an open issue. in this paper, we study the similarity between ontology concepts at different levels of a taxonomy, quantifying in a natural manner the quality of the ontology attained. our approach is integrated in a method for language-neutral learning of ontologies from texts, which relies on conditional independence tests over thematic topics that are discovered using lda.",nan
2008,reinforcement learning with the use of costly features,"a common solution approach to reinforcement learning problems with large state spaces (where value functions cannot be represented exactly) is to compute an approximation of the value function in terms of state features. however, little attention has been paid to the cost of computing these state features (e.g., search-based features). to this end, we introduce a cost-sensitive sparse linear-value function approximation algorithm — fovea — and demonstrate its performance on an experimental domain with a range of feature costs.",nan
2008,data-driven induction of functional programs,"we present a new method and system, called igor2, for the induction of recursive functional programs from few nonrecursive, possibly non-ground example equations describing a subset of the input-output behaviour of a function to be implemented.",nan
2008,ctrnn parameter learning using differential evolution,"target behaviours can be achieved by finding suitable parameters for continuous time recurrent neural networks (ctrnns) used as agent control systems. differential evolution (de) has been deployed to search parameter space of ctrnns and overcome granularity, boundedness and blocking limitations. in this paper we provide initial support for de in the context of two sample learning problems.",nan
2008,incremental diagnosis of des by satisfiability,"we propose a sat-based algorithm for incremental diagnosis of discrete-event systems. the monotonicity is ensured by a prediction window that uses the future observations to lead the current diagnosis. experiments stress the impact of parameters tuning on the correctness and the efficiency of the approach.",nan
2008,improving robustness in consistency-based diagnosis using possible conflicts,"behaviour simulation in consistency-based diagnosis requires knowing the initial value. this assumption is not easily fulfilled in real systems, even in the presence of measurements related to state variables due to noise and parameter uncertainties.this work proposes the integration of state observers to estimate initial states for simulation in consistency-based diagnosis with possible conflicts, using the bridge framework, proposing an extension for a class of dynamic systems. suitable state-observer structural models are obtained through same algorithms used to find possible conflicts – minimal subsystems with analytical redundancy–, without additional knowledge in the models.",nan
2008,dependable monitoring of discrete-event systems with uncertain temporal observations,"in discrete-event system monitoring, a set of candidate diagnoses is output at the reception of each observation fragment. however, when the observation is uncertain, this result may be not dependable: the sets of diagnoses, relevant to consecutive observation fragments, may be unrelated to one another, and, even worse, they may be unrelated to the actual diagnosis. to cope with this problem, the notion of monotonic monitoring is introduced, which is supported by specific constraints on the fragmentation of the uncertain observation, leading to the notion of stratification.",nan
2008,compressing binary decision diagrams,"the paper introduces a new technique for compressing binary decision diagrams in those cases where random access is not required. using this technique, compression and decompression can be done in linear time in the size of the bdd and compression will in many cases reduce the size of the bdd to 1-2 bits per node. empirical results for our compression technique are presented, including comparisons with previously introduced techniques, showing that the new technique dominate on all tested instances.",nan
2008,combining abduction with conflict-based diagnosis,"conflict-based diagnosis is a recently proposed probabilistic method for model-based diagnosis, inspired by consistencybased diagnosis, that uses a measure of data conflict, called the diagnostic conflict measure, to rank diagnoses. in this paper, this method is refined using an abductive method that reuses part of the computation of the diagnostic conflict measure.",nan
2008,an activity recognition model for alzheimer's patients: extension of the coach task guidance system,"this paper presents a hybrid plan recognition model, based on probabilistic description logic, which addresses the issue of recognizing the activities and the errors of alzheimer's patients at an early stage of the disease. this model has been implemented to be a new extension of the coach system, an emerging prototype of cognitive device for persons with alzheimer's disease that offers assistance in task completion. we present an initial experimentation done on this new recognition module for coach, which is based on the results of two sets of clinical trials.",nan
2008,not so new: overblown claims for ‘new’ approaches to emotion,"the non-classical thesis of emotion (nce) states that the conceptual resources of classical cognitive science cannot adequately account for certain important features of emotion. it also states that these features can be adequately accounted for by employing the conceptual resources of non-classical forms of cognitive science. there is a general problem with all forms of nce, since they all assume that classical cognitive science is too restrictive, when if anything the reverse is true. in fact, the relationship between classical and non-classical approaches to the study of emotion is much more fuzzy than nce suggests. though the two approaches use different terms of art, this does not grant one group privileged access to cognitive resources inaccessible to the other, but merely directs their attention to different features of the phenomena being studied. thus the real contribution of non-classical models of emotion is to draw our attention to certain key aspects of emotion requiring explanation that had perhaps been somewhat neglected by classical models.",nan
2008,emergence of rules in cell assemblies of flif neurons,"inspired by biological cognition, cabot project explores the ways symbolic processing can emerge in a system of neural cell assemblies (cas). here we show how a stochastic meta–control process can regulate learning of associations between the cas, the neural basis of symbols. an experiment illustrates the learning between cas representing conditions actions pairs, which leads to ca–based representations of ‘if–then’ rules.",nan
2008,ers: evaluating reputations of scientific journals,"current methods for evaluating research are based on counting the number of citations received for publications. thus, the more an article is cited and the more its impact is considered as important. in this article, we propose a new method for assessing the reputation of scientific journals, based on a web application in which are gathered the votes of expert researchers. the voting results indicate degrees of preference for one journal over another. our system uses, in addition, the publications of an expert in order to quantify his expertise in specific fields. these values are coupled with those of votes to determine the relevance, according to the field, of each journal in each topic. an iterative process of transferring values given to journals by experts to values of the experts themselves given their publications has been implemented.",nan
2008,personal experience acquisition support from blogs using event-depicting images,"internet users write blogs related to their personal experience, daily news, and so on. we can obtain blogs about personal experience using search engines on the web. however, the search engines also output blogs about other topics unrelated to personal experience. therefore, it is necessary for us to read all blogs to obtain those about personal experiences. it takes too much time.this paper proposes a support system for obtaining blogs about personal experiences efficiently. the system extracts three keywords that denote place, object, and action from a blog. the three keywords describe an event that leads a person to write a blog about personal experience. the system expresses the event with three pictures depicting the extracted keywords. the pictures help users to judge whether personal experience is written about in the blog. we experimented with the system, and verified that it supports users in obtaining personal experiences efficiently.",nan
2008,object configuration reconstruction from descriptions using relative and intrinsic reference frames,"we provide a technique to reconstruct an object configuration that has been described on site by only using intrinsic and relative frames of reference into an absolute frame of reference, as seen from the survey perspective.",nan
2008,probabilistic reinforcement rules for item-based recommender systems,"the internet is constantly growing, proposing more and more services and sources of information. modeling personal preferences enables recommender systems to identify relevant subsets of items. these systems often rely on filtering techniques based on symbolic or numerical approaches in a stochastic context. in this paper, we focus on item-based collaborative filtering (cf) techniques. we propose a new approach combining a classic cf algorithm with a reinforcement model to get a better accuracy. we deal with this issue by exploiting probabilistic skewnesses in triplets of items.",nan
2008,contextaggregator: a heuristic-based approach for automated feature construction and selection,"our research goal is to work towards a personal contextaware assistance and retrieval of relevant resources to computer users during a certain work task. this paper presents a general-purpose, algorithmic approach for automated context aggregation by heuristic-based feature construction. our implementation of the context reasoning layer combines lower-level context features to new aggregated higher-level context features. our approach allows – in contrast to most other approaches – an automated feature combination to achieve a high prediction accuracy of the user's work task.",nan
2008,a pervasive assistant for nursing and doctoral staff,"the goal of health-care institutions is to provide patient-centric health care services. unfortunately, this goal is frequently undermined due to human-related aspects. the pervasive nursing and doctoral assistant (pinata) provides a patient-centric system powered with ambience intelligence techniques and semantic web technologies. through pinata, the movement of patients and medical staff is tracked via rfid sensors while an automated camera system monitors the interaction of people within their environment. the system reacts to particular situations autonomously by directing medical staff towards emergencies in a timely manner and providing them with just the information they require on their handheld devices. this ensures that patients are given the best care possible on a 24/7 basis especially when the medical staff is not around.",nan
2008,author identification using a tensor space representation,"author identification is a text categorization task with applications in intelligence, criminal law, computer forensics, etc. usually, in such cases there is shortage of training texts. in this paper, we propose the use of second order tensors for representing texts for this problem, in contrast to the traditional vector space model. based on a generalization of the svm algorithm that can handle tensors, we explore various methods for filling the matrix of features taking into account that similar features should be placed in the same neighborhood. to this end, we propose a frequency-based metric. experiments on a corpus controlled for genre and topic and variable amount of training texts show that the proposed approach is more effective than traditional vector-based svm when only limited amount of training texts is used.",nan
2008,answering definition question: ranking for top-k,"as an important form of complex questions, definition question attracts much attention from qa researchers. for many of the definition question answering systems, it is a core step to rank the candidate answer sentences, so that the top-k in the ranked list can be extracted. we integrate these evidences as features into a whole framework, and propose a novel method to learning weights of these features to rank the candidate answer sentences.",nan
2008,ontology-driven human language technology for semantic-based business intelligence,"in this poster submission, we describe the actual state of development of textual analysis and ontology-based information extraction in real world applications, as they are defined in the context of the european r&d project “musing” dealing with business intelligence. we present in some details the actual state of ontology development, including a time and domain ontologies, which are guiding information extraction onto an ontology population task.",nan
2008,evaluation evaluation,"over the last decade there has been increasing concern about the biases embodied in traditional evaluation methods for natural language processing/learning, particularly methods borrowed from information retrieval. without knowledge of the bias and prevalence of the contingency being tested, or equivalently the expectation due to chance, the simple conditional probabilities recall, precision and accuracy are not meaningful as evaluation measures, either individually or in combinations such as f-factor. the existence of bias in nlp measures leads to the ‘improvement’ of systems by increasing their bias, such as the practice of improving tagging and parsing scores by using most common value (e.g. water is always a noun) rather than the attempting to discover the correct one. in this paper, we will analyze both biased and unbiased measures theoretically, characterizing the precise relationship between all these measures.",nan
2008,an analysis of bayesian network model-approximation techniques,"two approaches have been used to perform approximate inference in bayesian networks for which exact inference is infeasible: employing an approximation algorithm, or approximating the structure. in this article we compare two structure-approximation techniques, edge-deletion and approximate structure learning based on sub-sampling, in terms of relative accuracy and computational efficiency. our empirical results indicate that edge-deletion techniques dominate the subsampling/induction strategy, in both accuracy and performance of generating the approximate network. we show, for several large bayesian networks, how edge-deletion can create approximate networks with order-of-magnitude inference speedups and relatively little loss of accuracy.",nan
2008,verifying the conformance of agents with multiparty protocols,"the paper defines a notion of conformance of a set of k agents with a multiparty protocol with k roles, requiring the agents to be interoperable and to produce correct executions of the protocol. conditions are introduced that enable each agent to be independently verified with respect to the protocol.",nan
2008,a default logic based framework for argumentation,we extend the logic-based framework of besnard and hunter for default logic. we present structural sound results that provide a natural extension and introduce new concepts that enable us to characterize an argument based on its use of incomplete information.,nan
2008,an empirical investigation of the adversarial activity model,"multiagent research provides an extensive literature on formal belief-desire-intention (bdi) based models describing the notions of teamwork and cooperation, but adversarial and competitive relationships have received very little formal bdi treatment. moreover, one of the main roles of such models is to serve as design guide-lines for the creation of agents, and while there is work illustrating that role in cooperative interaction, there has been no empirical work done to validate competitive bdi models.in this work we use the adversarial activity model, a bdi-based model for bounded rational agents that are operating in a general zero-sum environment, as an architectural guideline for building bounded rational agents in two adversarial environments: the connect-four game (a bilateral environment) and the risk strategic board game (a multilateral environment). we carry out extensive simulations that illustrate the advantages and limitations of using this model as a design specification.",nan
2008,addressing temporal aspects of privacy-related norms,"agents interacting in open environments such as internet are often in charge of personal information. in order to protect the privacy of human users, such agents have to be aware of the normative context regarding personal data protection (applicable laws and other regulations). these privacy-related norms usually refer to deadlines and durations. to represent these regulations, we introduce the deontic logic for privacy; this logic represents privacy-related obligations while providing the required temporal expressiveness.",nan
2008,evaluation of global system state thanks to local phenomenona,"this paper presents a new approach for the evaluation of a system's global state properties. the approach is intented for the application to reactive multiagent system (rmas) and adresses the evaluation of emergent properties such as global stabilisation. this approach is inspired by statistical physics and thermodynamics, as a way to link the microscopic and a macroscopic points of view. it gives an important role to partition function z as defined in statistical physics. from this mathematical function can be extracted indicators that represent the global evaluation of the system state based on local phenomena. in this paper, the approach is put into practice by considering a classical reactive multiagent system: bird flocks simulation. the methodology was applied to analyze system stability. experimental results obtained with a multiagent simulation platform are presented.",nan
2008,experience and trust — a systems-theoretic approach,"an influential model of agent trust and experience is that of jonker and treur [jonker and treur 99]. in that model an agent uses its experience of the interactions of another agent to assess that agent's trustworthiness. we showed that key properties of that model are subsumed by classical mathematical systems theory. using the latter theory we also clarify the issue of when two experience sequences may be regarded as equivalent. an intuitive feature of the jonker and treur model is that experience sequence orderings are respected by functions that map such sequences to trust orderings. we raise a question about another intuitive property — that of continuity of these functions, viz. that they map experience sequences that resemble each other to trust values that also resemble each other. using fundamental results in the relationship between partial orders and topologies we also showed that these two intutive properties are essentially equivalent.",nan
2008,trust-aided acquisition of unverifiable information,"we propose a mechanism for the acquisition of information from potentially unreliable sources. our mechanism addresses the case where the acquired information cannot be verified. the idea is to intersperse questions (“challenges”) for which the correct answers are known. by evaluating the answers to these challenges, probabilistic conclusions about the correctness of the unverifiable information can be drawn. less challenges need to be used if an information provider has shown to be trustworthy. our approach can resist collusion and shows great promise for various application scenarios such as grid-computing or peer-to-peer networks.",nan
2008,bidflow: a new graph-based bidding language for combinatorial auctions,"in this paper we introduce a new graph based bidding language for combinatorial auctions. in our language, each bidder submits to the arbitrator a generalized flow network (netbid) representing her bids. the interpretation of the winner determination problem as an aggregation of individual preferences represented as flowbids allows building an aggregate netbid for its representation. labelling the nodes with appropriate procedural functions considerably improves upon the expressivity of previous bidding languages.",nan
2008,multi-agent reinforcement learning for intrusion detection: a case study and evaluation,"in this paper we propose a novel approach to train multi-agent reinforcement learning (marl) agents to cooperate to detect intrusions in the form of normal and abnormal states in the network. we present an architecture of distributed sensor and decision agents that learn how to identify normal and abnormal states of the network using reinforcement learning (rl). sensor agents extract network-state information using tile-coding as a function approximation technique and send communication signals in the form of actions to decision agents. by means of an on line process, sensor and decision agents learn the semantics of the communication actions. in this paper we detail the learning process and the operation of the agent architecture. we also present tests and results of our research work in an intrusion detection case study, using a realistic network simulation where sensor and decision agents learn to identify normal and abnormal states of the network.",nan
2008,gr-mas: multi-agent system for geriatric residences,"this paper presents a multiagent architecture (gr-mas) developed for facilitating health care in geriatric residences. gr-mas (geriatric residence multi-agent system) contains different agent types and takes into account the integration within rfid, wi-fi technologies and handheld devices. the core of gr-mas is an autonomous deliberative case-based planner agent called gerag (geriatric agent for monitoring alzheimer patients). this agent, which allows adaptation and learning capabilities, has been designed to plan the nurses' working time dynamically, to maintain the standard working reports about the nurses' activities, and to guarantee that the patients assigned to the nurses are given the right care. a description of gerag, its relationship with the complementary agents, and preliminary results of the multi-agent system prototype in a real environment are presented.",nan
2008,agent-based and population-based simulation of displacement of crime (extended abstract),"within criminology, the process of crime displacement is usually explained by referring to the interaction of three types of agents: criminals, passers-by, and guardians. most existing simulation models of this process are agent-based. however, when the number of agents considered becomes large, population-based simulation has computational advantages over agent-based simulation. this paper presents both an agent-based and a population-based simulation model of crime displacement, and reports a comparative evaluation of the two models. in addition, an approach is put forward to analyse the behaviour of both models by means of formal techniques.",nan
2008,organizing coherent coalitions,"in this paper we provide and discuss a language to talk about coherence, a property of interaction that ensures players' abilities non to contradict one other and the empty coalition not to make active choices. with this property we can model a closed-world interaction, such as those of a coordination game or of a prisoner dilemma, where all the outcomes are determined only by the choices of the agents that are present.",nan
2008,trust aware negotiation dissolution,"in this paper we propose a recommender system that suggests the best moment to end a negotiation. the recommendation is made from a trust evaluation of every agent in the negotiation based on their past negotiation experiences. for this, we introduce the trust aware negotiation dissolution algorithm.",nan
2008,magic agents: using information relevance to control autonomy,"autonomous agents are believed to have control over their internal state and over their behaviour. for that reason, an agent should control how and by whom it is being influenced. we introduce a reasoning component for bdi-agents that deals with the control over external influences, and we propose heuristics using local knowledge to process incoming stimuli. one of those heuristics is based on information relevance with respect to the agent's current plans and goals. we have developed a way to determine the relevance of information in bdi-agents using magic sets from database research as basis. the method presented shows a new application of magic sets by applying the theory in agent systems.",nan
2008,infection-based norm emergence in multi-agent complex networks,"we propose a computational model that facilitates agents in a mas to collaboratively evolve their norms to reach the best norm conventions. our approach borrows from the social contagion phenomenon to exploit the notion of positive infection: agents with good behaviors become infectious to spread their norms in the agent society. by combining infection and innovation, our computational model helps a mas establish better norm conventions even when a sub-optimal one has fully settled in the population.",nan
2008,opponent modelling in texas hold'em poker as the key for success,"over the last few years, research in artificial intelligence has focussed on games with incomplete information and non-deterministic moves. the game of poker is a perfect theme for studying this subject. the best known poker variant is texas hold'em that combines simple rules with a huge amount of possible playing strategies. this paper is focussed on developing algorithms for performing simple online opponent modelling in texas hold'em poker enabling to select the best strategy to play against each given opponent. several autonomous agents were developed in order to simulate typical poker player's behaviour and an observer agent was developed, capable of using simple opponent modelling techniques, in order to select the best playing strategy against each opponent. the results obtained in realistic experiments using eight distinct poker playing agents showed the usefulness of the approach. the observer agent is clearly capable of outperforming all their counterparts in all tests performed.",nan
2008,lrta* works much better with pessimistic heuristics,"recently we showed that under very reasonable conditions, incomplete, real-time search methods like rta* work better with pessimistic heuristic functions than with optimistic, admissible heuristic functions of equal quality. the use of pessimistic heuristic functions results in higher percentage of correct decisions and in shorter solution lengths. we extend this result to learning rta* (lrta*) and demonstrate that the use of pessimistic instead of optimistic (or mixed) heuristic functions of equal quality results in much faster learning process at the cost of just marginally worse quality of converged solutions.",nan
2008,dynamic backtracking for distributed constraint optimization,"we propose a new algorithm for solving distributed constraint optimization problems (dcops). our algorithm, called dybop, is based on branch and bound search with dynamic ordering of agents. a distinctive feature of this algorithm is that it uses the concept of valued nogood. combining lower bounds on inferred valued nogoods computed cooperatively helps pruning dynamically unfeasible sub-problems and speeds up the search. dybop requires a polynomial space at each agent. experiments show that dybop has significantly better performance than other dcop algorithms.",nan
2008,symbolic classification of general multi-player games,"for general two-player turn-taking games, first solvers have been contributed. algorithms for multi-player games like maxn, however, cannot classify general games robustly, and its extension soft-maxn, which can play optimally against unknown and weak opponents, demands large amounts of memory. as ram is a scarce resource, this paper proposes a memory-efficient implementation of the soft-maxn algorithm, by exploiting the functional representation of state and evaluation sets with bdds.",nan
2008,redundancy in csps,"in this paper, we propose a new technique to compute irredundant sub-sets of constraint networks. since, checking redundancy is co-np complete problem, we use different polynomial local consistency entailments for reducing the computational complexity. the obtained constraint network is irredundant modulo a given local consistency. redundant constraints are eliminated from the original instance producing an equivalent one with respect to satisfiability. eliminating redundancy might help the csp solver to direct the search to the most constrained (irredundant) part of the network.",nan
2008,a max-sat algorithm portfolio,"the results of the last maxsat evaluations suggest there is no universal best algorithm for solving maxsat, as the fastest solver often depends on the type of instance. having an oracle able to predict the most suitable maxsat solver for a given instance would result in the most robust solver. inspired by the success of satzilla for sat, this paper describes the first approach for a portfolio of algorithms for maxsat. compared to existing solvers, the resulting portfolio can achieve significant performance improvements on a representative set of instances.",nan
2008,on the practical significance of hypertree vs. treewidth,the recently introduced notion of hypertree width has been shown to provide a broader characterization of tractable constraint and probabilistic networks than the tree width. this paper demonstrates empirically that in practice the bounding power of the tree width is still superior to the hypertree width for many benchmark instances of both probabilistic and deterministic networks.,nan
2008,a new approach to planning in networks,"control of networks like those for transportation, power distribution, communication to name a few, provides challenges to planning and scheduling. many problems can be defined in terms of a basic state space model, but more general problems require an expressive language for talking about the topology and connectivity of the system, which are outside the scope of standard planning languages. in this work we introduce a general framework for defining planning languages for networked systems, with capability to express properties of connectivity and topology of such systems.",nan
2008,detection of unsolvable temporal planning problems through the use of landmarks,"deadline constraints have been recently introduced in pddl3.0. the results obtained in the constraints domains in the last planning competition show that planners are not yet fully competitive. when dealing with deadline constraints the number of feasible solutions for a problem is reduced and thus it is specially relevant the ability to detect unsolvability. in this paper we present a new approach, based on the use of temporal landmarks, for the detection of unsolvable temporal planning problems.",nan
2008,a planning graph heuristic for forward-chaining adversarial planning,"in contrast to classical planning, in adversarial planning, the planning agent has to face an adversary trying to prevent him from reaching his goals. in this paper, we investigate a forwardchaining approach to adversarial planning based on the ao* algorithm. the exploration of the underlying and/or graph is guided by a heuristic evaluation function, inspired by the relaxed planning graph heuristic used in the ff planner. unlike ff, our heuristic uses an adversarial planning graph with distinct proposition and action layers for the protagonist and antagonist. first results suggest that in certain planning domains, our approach yields results competitive with the state of the art.",nan
2008,learning to select object recognition methods for autonomous mobile robots,"selecting which algorithms should be used by a mobile robot computer vision system is a decision that is usually made a priori by the system developer, based on past experience and intuition, not systematically taking into account information that can be found in the images and in the visual process itself to learn which algorithm should be used, in execution time. this paper presents a method that uses reinforcement learning to decide which algorithm should be used to recognize objects seen by a mobile robot in an indoor environment, based on simple attributes extracted on-line from the images, such as mean intensity and intensity deviation. two state-of-the-art object recognition algorithms can be selected: the constellation method proposed by lowe together with its interest point detector and descriptor, the scale-invariant feature transform and a bag of features approach. a set of empirical evaluations was conducted using a household mobile robots image database, and results obtained shows that the approach adopted here is very promising.",nan
2008,automatic animation generation of a teleoperated robot arm,"in this paper we describe the automatic task demonstration generator (atdg), a system implemented into a software prototype for teaching the operation of a robot manipulator deployed on the international space station (iss). the atdg combines the use of path planning and camera planning to take into account the complexity of the manipulator, the limited direct view of the iss exterior, and the unpredictability of lighting conditions in the workspace. the path-planning algorithm not only avoids obstacles in the workspace as is normal for a path-planner, but in addition takes into account the position of corridors for safe operations and the placement of cameras on the iss. the camera planner is then invoked to find the right arrangement of cameras to follow the manipulator on its trajectory. this allows the on-the-fly production of useful and pedagogical task demonstrations to help the student carry out tasks involving the manipulation of the robot on the iss. even if the system has been developed for robotic manipulations, it could be used for any application involving the filming of unpredictable complex scenes.",nan
2010,top-down algorithms for constructing structured dnnf: theoretical and practical implications,"we introduce a top-down compilation algorithm for constructing structured dnnf for any boolean function. with appropriate restrictions, the algorithm can produce various subsets of dnnf such as deterministic dnnf and obdd. we derive a size upper bound for structured dnnf based on this algorithm and use the result to generalize similar upper bounds known for several boolean functions in the case of obdd. we then discuss two realizations of the algorithm that work on cnf formulas. we show that these algorithms have time and space complexities that are exponential in the treewidth and the dual treewidth of the input.",nan
2010,on decomposability and interaction functions,"a formal notion of a boolean-function decomposition was introduced recently and used to provide lower bounds on various representations of boolean functions, which are subsets of decomposable negation normal form (dnnf). this notion has introduced a fundamental optimization problem for dnnf representations, which calls for computing decompositions of minimal size for a given partition of the function variables. we consider the problem of computing optimal decompositions in this paper for general boolean functions and those represented using cnfs. we introduce the notion of an interaction function, which characterizes the relationship between two sets of variables and can form the basis of obtaining such decompositions. we contrast the use of these functions to the current practice of computing decompositions, which is based on heuristic methods that can be viewed as using approximations of interaction functions. we show that current methods can lead to decompositions that are exponentially larger than optimal decompositions, pinpoint the specific reasons for this lack of optimality, and finally present empirical results that illustrate some characteristics of interaction functions in contrast to their approximations.",nan
2010,on computing backbones of propositional theories,"backbones of propositional theories are literals that are true in every model. backbones have been used for characterizing the hardness of decision and optimization problems. moreover, backbones find other applications. for example, backbones are often identified during product configuration. backbones can also improve the efficiency of solving computational problems related with propositional theories. these include model enumeration, minimal model computation and prime implicant computation. this paper investigates algorithms for computing backbones of propositional theories, emphasizing the integration of these algorithms with modern sat solvers. experimental results, obtained on representative problem instances, indicate that the proposed algorithms are effective in practice and can be used for computing the backbones of large propositional theories. in addition, the experimental results indicate that propositional theories can have large backbones, often representing a significant percentage of the total number of variables.",nan
2010,extending clause learning dpll with parity reasoning,"we consider a combined satisfiability problem where an instance is given in two parts: a set of traditional clauses extended with a set of parity (xor) constraints. to solve such problems without translation to cnf, we develop a parity constraint reasoning method that can be integrated to a clause learning solver. the idea is to devise a module that offers a decision procedure and implied literal detection for parity constraints and also provides clausal explanations for implied literals and conflicts. we have implemented the method and integrated it to a state-of-the-art clause learning solver. the resulting system is experimentally evaluated and compared to state-of-the-art solvers.",nan
2010,complexity of axiom pinpointing in the dl-lite family of description logics,"we investigate the complexity of axiom pinpointing for different members of the dl-lite family of description logics. more precisely, we consider the problem of enumerating all minimal subsets of a given dl-lite knowledge base that have a given consequence. we show that for the dl-lite[hscr    ]core, dl-lite[hscr    ]krom and dl-lite[hscr    ][nscr    ]horn fragments such minimal subsets are efficiently enumerable with polynomial delay, but for the dl-litebool fragment they cannot be enumerated in output polynomial time unless p = np. we also show that interestingly, for the dl-lite[hscr    ][nscr    ]horn fragment such minimal sets can be enumerated in reverse lexicographic order with polynomial delay, but it is not possible in the forward lexicographic order since computing the first one is already conp-hard.",nan
2010,tractable reasoning with dl-programs over datalog-rewritable description logics,"the deployment of kr formalisms to the web has created the need for formalisms that combine heterogeneous knowledge bases. nonmonotonic dl-programs provide a loose integration of description logic (dl) ontologies and logic programming (lp) rules with negation, where a rule engine can query an ontology with a native dl reasoner. however, even for tractable dl-programs, the overhead of an external dl reasoner might be considerable. to remedy this, we consider datalog-rewritable dl ontologies, i.e., ones that can be rewritten to datalog programs, such that dl-programs can be reduced to datalog¬, i.e, datalog with negation, under well-founded semantics. to illustrate this framework, we consider several datalog-rewritable dls. besides fragments of the tractable owl 2 profiles, we also present [lscr    ][dscr    ][lscr    ]+ as an interesting dl that is tractable while it has some expressive constructs. our results enable the usage of dblp technology to reason efficiently with dl-programs in presence of negation and recursion, as a basis for advanced applications.",nan
2010,enriching [escr    ][lscr    ]-concepts with greatest fixpoints,"we investigate the expressive power and computational complexity of [escr    ][lscr    ]ν, the extension of the lightweight description logic [escr    ][lscr    ] with concept constructors for greatest fixpoints. it is shown that [escr    ][lscr    ]ν has the same expressive power as [escr    ][lscr    ] extended with simulation quantifiers and that it can be characterized as a largest fragment of monadic second-order logic that is preserved under simulations and has finite minimal models. as in basic [escr    ][lscr    ], all standard reasoning problems for general tboxes can be solved in polynomial time. [escr    ][lscr    ]ν has a range of very desirable properties that [escr    ][lscr    ] itself is lacking. firstly, least common subsumers w.r.t. general tboxes as well as most specific concepts always exist and can be computed in polynomial time. secondly, [escr    ][lscr    ]ν shares with [escr    ][lscr    ] the craig interpolation property and the beth definability property, but in contrast to [escr    ][lscr    ] allows the computation of interpolants and explicit concept definitions in polynomial time.",nan
2010,tableau-based forgetting in [ascr    ][lscr    ][cscr    ] ontologies,"in this paper, we propose two new approaches to forgetting for [ascr    ][lscr    ][cscr    ] based on the well-known tableau algorithm. the first approach computes the result of forgetting by rolling up tableaux, and also provides a decision algorithm for the existence of forgetting in [ascr    ][lscr    ][cscr    ]. when the result of forgetting does not exist, we provide an incremental method for computing approximations of forgetting. this second approach uses variable substitution to refine approximations of forgetting and eventually obtain the result of forgetting. this approach is capable of preserving structural information of the original ontologies enabling readability and comparison. as both approaches are based on the tableau algorithm, their implementations can make use of the mechanisms and optimization techniques of existing description logic reasoners.",nan
2010,verifying properties of infinite sequences of description logic actions,"the verification problem for action logic programs with non-terminating behaviour is in general undecidable. in this paper, we consider a restricted setting in which the problem becomes decidable. on the one hand, we abstract from the actual execution sequences of a non-terminating program by considering infinite sequences of actions defined by a büchi automaton. on the other hand, we assume that the logic underlying our action formalism is a decidable description logic rather than full first-order predicate logic.",nan
2010,a hybrid continuous max-sum algorithm for decentralised coordination,"in this paper we tackle the problem of coordinating multiple decentralised agents with continuous state variables. specifically we propose a hybrid approach, which combines the max-sum algorithm with continuous non-linear optimisation methods. we show that, for problems with acyclic factor graph representations, for suitable parameter choices and sufficiently fine state space discretisations, our proposed algorithm converges to a state with utility close to the global optimum. we empirically evaluate our approach for cyclic constraint graphs in a multi-sensor target classification problem, and compare its performance to the discrete max-sum algorithm, as well as a non-oordinated approach and the distributed stochastic algorithm (dsa). we show that our hybrid max-sum algorithm outperforms the non-coordinated algorithm, dsa and discrete max-sum by up to 40% in this problem domain. furthermore, the improvements in outcome over discrete max-sum come without significant increases in running time nor communication cost.",nan
2010,bnb-adopt+ with several soft arc consistency levels,"distributed constraint optimization problems can be solved by bnb-adopt+, a distributed asynchronous search algorithm. in the centralized case, local consistency techniques applied to constraint optimization have been shown very beneficial to increase performance. in this paper, we combine bnb-adopt+ with different levels of soft arc consistency, propagating unconditional deletions caused by either the enforced local consistency or by distributed search. the new algorithm maintains bnb-adopt+ optimality and termination. in practice, this approach decreases substantially bnb-adopt+ requirements in communication cost and computation effort when solving commonly used benchmarks.",nan
2010,optimal task migration in service-oriented systems: algorithms and mechanisms,"in service-oriented systems, such as grids and clouds, users are able to outsource complex computational tasks by procuring resources on demand from remote service providers. as these providers typically display highly heterogeneous performance characteristics, service procurement can be challenging when the consumer is uncertain about the computational requirements of its task a priori. given this, we here argue that the key to addessing this problem is task migration, where the consumer can move a partially completed task from one provider to another. we show that doing this optimally is np-hard, but we also propose two novel algorithms, based on new and established search techniques, that can be used by an intelligent agent to efficiently find the optimal solution in realistic settings. however, these algorithms require full information about the providers' quality of service and costs over time. critically, as providers are usually self-interested agents, they may lie strategically about these to inflate profits. to address this, we turn to mechanism design and propose a payment scheme that incentivises truthfulness. in empirical experiments, we show that (i) task migration results in an up to 160% improvement in utility, (ii) full information about the providers' costs is necessary to achieve this and (iii) our mechanism requires only a small investment to elicit this information.",nan
2010,modeling the problem of many hands in organisations,"in this paper we provide a formalism to reason about the problem of many hands in organisations. this is a problem that arises whenever the organisation is responsible for some undesirable outcome but none of its members can be held responsible for the outcome. the formalism proposed here is a logic that extends the coalition epistemic dynamic logic by adding a notion of group knowledge and also organisational structures to its semantics. an organisational structure is a set of agents and some relations between them. it defines the power and coordination links between agents, which defines how agents delegate tasks and communicate. we give formal definitions for individual and collective responsibility, as well as for the problem of the many hands.",nan
2010,learning better together,"this article addresses collaborative concept learning in a mas. in a concept learning problem an agent incrementally revises a hypothetical representation of some target concept to keep it consistent with the whole set of examples that it receives from the environment or from other agents. in the program smile, this notion of consistency was extended to a group of agents. a surprising experimental result of that work was that a group of agents learns better the difficult boolean problems, than a unique agent receiving the same examples. the first purpose of the present paper is to propose some explanation about such unexpected superiority of collaborative learning. furthermore, when considering large societies of agents, using pure sequential protocols is unrrealistic. the second and main purpose of this paper is thus to propose and experiment broadcast protocols for collaborative learning.",nan
2010,event model learning from complex videos using ilp,"learning event models from videos has applications ranging from abnormal event detection to content based video retrieval. relational learning techniques such as inductive logic programming (ilp) hold promise for building such models, but have not been successfully applied to the very large datasets which result from video data. in this paper we present a novel supervised learning framework to learn event models from large video datasets (~2.5 million frames) using ilp. efficiency is achieved via the learning from interpretations setting and using a typing system. this allows learning to take place in a reasonable time frame with reduced false positives. the experimental results on video data from an airport apron where events such as loading, unloading, jet-bridge parking etc are learned suggests that the techniques are suitable to real world scenarios.",nan
2010,a decentralised symbolic diagnosis approach,"this paper considers the diagnosis of large discrete-event systems consisting of many components. the problem is to determine, online, all failures and states that explain a given sequence of observations. several model-based diagnosis approaches deal with this problem but they usually have either poor time performance or result in space explosion. recent work has shown that both problems can be tackled when encoding diagnosis approaches symbolically by means of binary decision diagrams. this paper further improves upon these results and presents a decentralised symbolic diagnosis method that computes the diagnosis information for each component off-line and then combines them on-line. experimental results show that our method provides significant improvements over existing approaches.",nan
2010,diagnosability analysis of discrete event systems with autonomous components,"diagnosability is the property of a given partially observable system model to always exhibit unambiguously a failure behavior from its only available observations in finite time after the fault occurrence, which is the basic question that underlies diagnosis taking into account its requirements at design stage. however, for the sake of simplicity, the previous works on diagnosability analysis of discrete event systems (dess) have the same assumption that any observable event can be globally observed, which is at the price of privacy. in this paper, we first briefly describe cooperative diagnosis architecture for dess with autonomous components, where any component can only observe its own observable events and thus keeps its internal structure private. and then a new definition of cooperative diagnosability is consequently proposed. at the same time, we present a formal framework for cooperative diagnosability checking, where global consistency of local diagnosability analysis can be achieved by analyzing communication compatibility between local twin plants without any synchronization. the formal algorithm with its discussion is provided as well.",nan
2010,diagnosing process trajectories under partially known behavior,"diagnosis of process executions is an important task in many application domains, especially in the area of workflow management systems and orchestrated web services. if executions fail because activities of the process do not behave as intended, recovery procedures re-execute some activities to recover from the failure. we present a diagnosis method for identifying incorrect activities in process executions. our method is novel both in that it does not require exact behavioral models for the activities and that its accuracy improves upon dependency-based methods. observations obtained from partial executions and re-executions of a process are exploited. we formally characterize the diagnosis problem and develop a symbolic encoding that can be solved using clp(fd) solvers. our evaluation demonstrates that the framework yields superior accuracy to dependency-based methods on realistically-sized examples.",nan
2010,computation in extended argumentation frameworks,"extended argumentation frameworks (eafs) are a recently proposed formalism that develop abstract argumentation frameworks (afs) by allowing attacks between arguments to be attacked themselves: hence eafs add a relationship [dscr    ] ⊆ [xscr    ] × [ascr    ] to the arguments ([xscr    ]) and attacks ([ascr    ] ⊆ [xscr    ] × [xscr    ]) in an af's basic directed graph structure 〈[xscr    ],[ascr    ]〉. this development provides a natural way to represent and reason about preferences between arguments. studies of eafs have thus far focussed on acceptability semantics, proof-theoretic processes, and applications. however, no detailed treatment of their practicality in computational settings has been undertaken. in this paper we address this lacuna, considering algorithmic and complexity properties specific to eafs. we show that (as for standard afs) the problem of determining if an argument is acceptable w.r.t. a subset of [xscr    ] is polynomial time decidable and, thus, determining the grounded extension and verifying admissibility are efficiently solvable. we, further, consider the status of a number of decision questions specific to the eaf formalism in the sense that these have no counterparts within afs.",nan
2010,an argumentation-based approach to database repair,"the access to high-quality data is essential to make accurate decisions. consequently, when a database becomes inconsistent is crucial to restore its consistency. the main approach for database consistency restoration is based on the notion of repair. in this work, we use argumentation techniques to provide a better understand of the reasoning process behind database reparation. we introduced an extended argumentation framework that provides a comprehensive and alternative way to identify, represent and resolve the conflicts between tuples in an inconsistent database. we studied the complexity of this framework and show that it can be used to check the optimality of a repair based on the extended notions of locally, semi-globally and globally optimal repair with respect to denial constraints and tuple-generating dependencies classes.",nan
2010,a common computational framework for semiring-based argumentation systems,"we suggest semirings as a mean to parametrically represent “weighted” argumentation frameworks (af): different kinds of preference levels related to arguments, e.g. a score representing a “fuzziness”, a “cost” or a probability level of each argument, can be represented by choosing different semirings. the novel idea is to provide a common computational and quantitative framework where attacks (and/or supports) have an associated weight and, consequently, also the computation of the classical dung's semantics has an associated weight representing how much inconsistency we tolerate in the solution. the proposed semiring-based af is then casted into a soft constraint satisfaction problem. adding suitable sets of constraints permits to characterize the solution of the soft csp as the desired dung semantics. this allows for the application of the several solution techniques developed for soft csps to afs.",nan
2010,behavior-oriented commitment-based protocols,"ever since the seminal work of searle, two components of interaction protocols have been identified: constitutive rules, defining the meaning of actions and regulative rules, defining the flow of execution, i.e. the behavior the agent should show. the two parts together define the meaning of the interaction. commitment-based protocols, however, usually do not account for the latter and, when they do it, they do not adopt a decoupled representation of the two parts. a clear distinction in the two representations would, however, bring many advantages, mainly residing in a greater openess of multi-agent systems, an easier re-use of protocols and of action definitions, and a finer specification of protocol properties. in this work we introduce the notion of behavior-oriented commitment-based protocols, which account both for the constitutive and the regulative specifications and that explicitly foresee a representation of the latter based on constraints among commitments. a language, named 2cl, for writing regulative specifications is also given.",nan
2010,using crowdsourcing and active learning to track sentiment in online media,"tracking sentiment in the popular media has long been of interest to media analysts and pundits. with the availability of news content via online syndicated feeds, it is now possible to automate some aspects of this process. there is also great potential to crowdsourcecrowdsourcing is a term, sometimes associated with web 2.0 technologies, that describes outsourcing of tasks to a large often anonymous community. much of the annotation work that is required to train a machine learning system to perform sentiment scoring. we describe such a system for tracking economic sentiment in online media that has been deployed since august 2009. it uses annotations provided by a cohort of non-expert annotators to train a learning system to classify a large body of news items. we report on the design challenges addressed in managing the effort of the annotators and in making annotation an interesting experience.",nan
2010,multiscale adaptive agent-based management of storage-enabled photovoltaic facilities,"we propose a multiscale algorithm for autonomous agents to adaptively manage the operation of storage-enabled photovoltaic (pv) facilities based upon sequential decisions in a partially observable environment. the stochastic environment is learned and modeled by an approach called an ε-machine, which operates on a set of a priori determined temporal scales, to give the agent an additional degree of freedom when optimizing its control decisions. we compare the performance of the proposed scheme with those of (1) control decisions based on a heuristic environment model rather than systematic learning, and (2) decisions made on a pre-determined scale. we argue that the systematic environment learning on multiple temporal scales makes the agent highly adaptable and, as a result, able to demonstrate a superior capability in managing the pv-storage facility according to a predetermined objective. the particular application focused upon in the paper is in managing distributed pv facilities as potential replacement of conventional peaking power plants.",nan
2010,non-intrusive detection of driver distraction using machine learning algorithms,"driver's distraction has become an important and growing safety concern with the increasing use of the so-called in-vehicle information systems (ivis), such as cell-phones, navigation systems, etc. a very promising way to overcome this problem is to detect driver's distraction and thus to adopt in-vehicle systems accordingly, in order to avoid or mitigate the negative effects. the purpose of this paper is to illustrate a method for the non-intrusive detection of visual distraction, based on the vehicle dynamic data; in particular, we present and compare two models, applying artificial neural networks (ann) and support vector machines (svm) which are well-known data-mining methods.despite of what already done in literature, our method does not use eye-tracker data in the final classifier. with respect to other similar works, we regard distraction identification as a classification problem and, moreover, we extend the datasets, both in terms of data-points and of scenarios.data for training the models were collected using a static driving simulator, with real human subjects performing a specific secondary task (surt) while driving. different training methods, model characteristics and features selection criteria have been compared.potential applications of this research include the design of adaptive ivis and of “smarter” partially autonomous driving assistance systems (padas), as well as the evaluation of driver's distraction.",nan
2010,learning and meta-learning for coordination of autonomous unmanned vehicles - a preliminary analysis,"we study models of coordination, negotiation and collaboration in multi-agent systems (mas). more specifically, we investigate scalable models and protocols for various distributed consensus coordination problems in large-scale mas. examples of such problems include conflict avoidance, leader election and coalition formation. we are particularly interested in application domains where robotic or unmanned vehicle agents interact with each other in real-time, as they try to jointly complete various tasks in complex dynamic environments, and where decisions often need to be made “on the fly”. such mas applications, we argue, necessitate a multi-tiered approach to learning how to coordinate effectively. one such collaborative mas application domain is ensembles of autonomous micro unmanned aerial vehicles (micro-uavs). a large ensemble of micro-uavs on a complex, multi-stage mission comprised of many diverse tasks with varying time and other resource requirements provides an excellent framework for studying multi-tiered learning how to better coordinate. a variety of tasks and their resource demands, complexity and unpredictability of the overall environment, types of coordination problems that the uavs may encounter in the course of their mission, multiple time scales at which the overall system can use learning and adaptation in order to perform better in the future, and multiple logical and organizational levels at which large ensembles of micro-uavs can be analyzed and optimized, all suggest the need for a multi-tiered approach to learning. we outline our theoretical and conceptual framework that integrates reinforcement learning and meta-learning, and discuss potential benefits that our framework could provide for enabling autonomous micro-uavs (and other types of autonomous vehicles) to coordinate more effectively.",nan
2010,classification of dreams using machine learning,"we describe a project undertaken by an interdisciplinary team of researchers in sleep and in and machine learning. the goal is sentiment extraction from a corpus containing short textual descriptions of dreams. dreams are categorized in a four-level scale of affections. the approach is based on a novel representation, taking into account the leading themes of the dream and the sequential unfolding of associated affective feelings during the dream. the dream representation is based on three combined parts, two of which are automatically produced from the description of the dream. the first part consists of co-occurrence vectors, which — unlike the standard bag-of-words model — capture non-local relationships between meanings of word in a corpus. the second part introduces the dynamic representation that captures the change in affections throughout the progress of the dream. the third part is the self-reported assessment of the dream by the dreamer according to eight given attributes. the three representations are subject to aggressive feature selection. using an ensemble of classifiers and the combined 3-partite representation, we have achieved 64% accuracy, which is in the range of human experts' consensus in that domain.",nan
2010,deep reasoning in clarification dialogues with mobile robots,"this paper reports our work on qualitative reasoning based clarification dialogues in human-robot interaction on spatial navigation. to interpret humans' route instructions, a qualitative spatial model is introduced which represents the robot's beliefs in the application domain. based on the qualitative spatial model, three tool-supported reasoning strategies are discussed which enable the robot to generate dialogues with differing degrees of clarification if knowledge mismatches or under-specifications in route instructions are detected. the influence of the reasoning strategies on human-robot dialogues is evaluated with an empirical study.",nan
2010,stream-based reasoning support for autonomous systems,"for autonomous systems such as unmanned aerial vehicles to successfully perform complex missions, a great deal of embedded reasoning is required at varying levels of abstraction. to support the integration and use of diverse reasoning modules we have developed dyknow, a stream-based knowledge processing middleware framework. by using streams, dyknow captures the incremental nature of sensor data and supports the continuous reasoning necessary to react to rapid changes in the environment.dyknow has a formal basis and pragmatically deals with many of the architectural issues which arise in autonomous systems. this includes a systematic stream-based method for handling the sense-reasoning gap, caused by the wide difference in abstraction levels between the noisy data generally available from sensors and the symbolic, semantically meaningful information required by many high-level reasoning modules. as concrete examples, stream-based support for anchoring and planning are presented.",nan
2010,variable level-of-detail motion planning in environments with poorly predictable bodies,"motion planning in dynamic environments consists of the generation of a collision-free trajectory from an initial to a goal state. when the environment contains uncertainty, preventing a perfect predictive model of its dynamics, a robot ends up only successfully executing a short part of the plan and then requires replanning, using the latest observed state of the environment. each such replanning step is computationally expensive. furthermore, we note that such sophisticated planning effort is unnecessary as the resulting plans are not likely to ever be fully executed, due to an unpredictable and changing environment. in this paper, we introduce the concept of variable level-of-detail (vlod) planning, that is able to focus its search on obtaining accurate short-term results, while considering the far-future with a different level of detail, selectively ignoring the physical interactions with poorly predictable dynamic objects (e.g., other mobile bodies that are controlled by external entities). unlike finite-horizon planning, which limits the maximum search depth, vlod planning deals with local minima and generates full plans to the goal, while requiring much less computation than traditional planning. we contribute vlod planning on a rich simulated physics-based planner and show results for varying lod thresholds and replanning intervals.",nan
2010,computational aspects of extending the shapley value to coalitional games with externalities,"until recently, computational aspects of the shapley value were only studied under the assumption that there are no externalities from coalition formation, i.e., that the value of any coalition is independent of other coalitions in the system. however, externalities play a key role in many real-life situations and have been extensively studied in the game-theoretic and economic literature. in this paper, we consider the issue of computing extensions of the shapley value to coalitional games with externalities proposed by myerson [21], pham do and norde [23], and mcquillin [17]. to facilitate efficient computation of these extensions, we propose a new representation for coalitional games with externalities, which is based on weighted logical expressions. we demonstrate that this representation is fully expressive and, sometimes, exponentially more concise than the conventional partition function game model. furthermore, it allows us to compute the aforementioned extensions of the shapley value in time linear in the size of the input.",nan
2010,on the stability of an optimal coalition structure,"the two main questions in coalition games are 1) what coalitions should form and 2) how to distribute the value of each coalition between its members. when a game is not superadditive, other coalition structures (css) may be more attractive than the grand coalition. for example, if the agents care about the total payoff generated by the entire society, css that maximize utilitarian social welfare are of interest. the search for such optimal css has been a very active area of research. stability concepts have been defined for games with coalition structure, under the assumption that the agents agree first on a cs, and then the members of each coalition decide on how to share the value of their coalition. an agent can refer to the values of coalitions with agents outside of its current coalition to argue for a larger share of the coalition payoff. to use this approach, one can find the cs s★ with optimal value and use one of these stability concepts for the game with s★. however, it may not be fair for some agents to form s★, e.g., for those that form a singleton coalition and cannot benefit from collaboration with other agents. we explore the possibility of allowing side-payments across coalitions to improve the stability of an optimal cs. we adapt existing stability concepts and prove that some of them are non-empty under our proposed scheme.",nan
2010,ea2: the winning strategy for the inaugural lemonade stand game tournament,"we describe the winning strategy of the inaugural lemonade stand game (lsg) tournament. the lsg is a repeated symmetric 3–player constant–sum finite horizon game, in which a player chooses a location for their lemonade stand on an island with the aim of being as far as possible from its opponents. to receive a high utility in this game, our strategy, ea2, attempts to find a suitable partner with which to coordinate and exploit the third player. to do this, we classify the behaviour of our opponents using the history of joint interactions in order to identify the best player to coordinate with and how this coordination should be established. this approach is designed to be adaptive to various types of opponents such that coordination is almost always achieved, which yields consistently high utilities to our agent, as evidenced by the tournament results and our subsequent experimental analysis. our strategy models behaviours of its opponents, rather than situations of the game (e.g. game theoretic equilibrium or off equilibrium paths), which makes ea2 easy to generalize to many other games.",nan
2010,planning with concurrency under resources and time uncertainty,"planning with actions concurrency under resources and time uncertainty has been recognized as a challenging and interesting problem. most current approaches rely on a discrete model to represent resources and time, which contributes to the combinatorial explosion of the search space when dealing with both actions concurrency and resources and time uncertainty. a recent alternative approach uses continuous random variables to represent the uncertainty on time, thus avoiding the state-space explosion caused by the discretization of timestamps. we generalize this approach to consider uncertainty on both resources and time. our planner is based on a forward chaining search in a state-space where the state representation is characterized by a set of object and numeric state variables. object state variables are associated with random variables tracking the time at which the state variables' current value has been assigned. the search algorithm dynamically generates a bayesian network that models the dependency between time and numeric random variables. the planning algorithm queries the bayesian network to estimate the probability that the resources (numerical state variables) remain in a valid state, the probability of success and the expected cost of the generated plans. experiments were performed on a transport domain in which we introduced uncertainty on the duration of actions and on the fuel consumption of trucks.",nan
2010,brothers in arms? on ai planning and cellular automata,"ai planning is concerned with the selection of actions towards achieving a goal. research on cellular automata (ca) is concerned with the question how global behaviours arise from local updating rules relating a cell to its direct neighbours. while these two areas are disparate at first glance, we herein identify a problem that is interesting to both: how to reach a fixed point in an asynchronous ca where cells are updated one-by-one? considering a particular local updating rule, we encode this problem into pddl and show that the resulting benchmark is an interesting challenge for ai planning. for example, our experiments determine that, very atypically, an optimal sat-based planner outperforms state-of-the-art satisficing heuristic search planners. this points to a severe weakness of current heuristics because, as we prove herein, plans for this problem can always be constructed in time linear in the size of the automaton. our proof of this starts from a high-level argument and then relies on using a planner for flexible case enumeration within localised parts of the ar gument. besides the formal result itself, this establishes a new proof technique for cas and thus demonstrates that the potential benefit of research crossing the two fields is mutual.",nan
2010,landmarks in hierarchical planning,in this paper we introduce a novel landmark technique for hierarchical planning. landmarks are abstract tasks that are mandatory. they have to be performed by any solution plan. our technique relies on a landmark extraction procedure that pre-processes a given planning problem by systematically analyzing the ways in which relevant abstract tasks can be decomposed. we show how the landmark information is used to guide hierarchical planning and present some experimental results that give evidence for the considerable performance increase gained through our technique.,nan
2010,the necessity of bounded treewidth for efficient inference in bayesian networks,"algorithms for probabilistic inference in bayesian networks are known to have running times that are worst-case exponential in the size of the network. for networks with a moralised graph of bounded treewidth, however, these algorithms take a time which is linear in the network's size. in this paper, we show that under the assumption of the exponential time hypothesis (eth), small treewidth of the moralised graph actually is a necessary condition for a bayesian network to render inference efficient by an algorithm accepting arbitrary instances. we thus show that no algorithm can exist that performs inference on arbitrary bayesian networks of unbounded treewidth in polynomial time, unless the eth fails.",nan
2010,context-specific independence in directed relational probabilistic models and its influence on the efficiency of gibbs sampling,"there is currently a large interest in relational probabilistic models. while the concept of context-specific independence (csi) has been well-studied for models such as bayesian networks, this is not the case for relational probabilistic models. in this paper we show that directed relational probabilistic models often exhibit csi by identifying three different sources of csi in such models (two of which are inherent to the relational character of the models).it is known that csi can be used to speed up probabilistic inference. in this paper we show how to do this in a general way for approximate inference based on gibbs sampling. we perform experiments on real-world data to analyze the influence of the three different types of csi. the results show that exploiting csi yields speedups of up to an order of magnitude.",nan
2010,bayesian monte carlo for the global optimization of expensive functions,"in the last decades enormous advances have been made possible for modelling complex (physical) systems by mathematical equations and computer algorithms. to deal with very long running times of such models a promising approach has been to replace them by stochastic approximations based on a few model evaluations. in this paper we focus on the often occuring case that the system modelled has two types of inputs x = (xc, xe) with xc representing control variables and xe representing environmental variables. typically, xc needs to be optimised, whereas xe are uncontrollable but are assumed to adhere to some distribution. in this paper we use a bayesian approach to address this problem: we specify a prior distribution on the underlying function using a gaussian process and use bayesian monte carlo to obtain the objective function by integrating out environmental variables. furthermore, we empirically evaluate several active learning criteria that were developed for the deterministic case (i.e., no environmental variables) and show that the alc criterion appears significantly better than expected improvement and random selection.",nan
2010,an empirical study of the manipulability of single transferable voting,"voting is a simple mechanism to combine together the preferences of multiple agents. agents may try to manipulate the result of voting by mis-reporting their preferences. one barrier that might exist to such manipulation is computational complexity. in particular, it has been shown that it is np-hard to compute how to manipulate a number of different voting rules. however, np-hardness only bounds the worst-case complexity. recent theoretical results suggest that manipulation may often be easy in practice. in this paper, we study empirically the manipulability of single transferable voting (stv) to determine if computational complexity is really a barrier to manipulation. stv was one of the first voting rules shown to be np-hard. it also appears one of the harder voting rules to manipulate. we sample a number of distributions of votes including uniform and real world elections. in almost every election in our experiments, it was easy to compute how a single agent could manipulate the election or to prove that manipulation by a single agent was impossible.",nan
2010,dynamic matching with a fall-back option,"we study dynamic matching without money when one side of the market is dynamic with arrivals and departures and the other is static and agents have strict preferences over agents on the other side of the market. in enabling stability properties, so that no pair of agents can usefully deviate from the match, we consider the use of a fall-back option where the dynamic agents can be matched, if needed, with a limited number of agents from a separate “reserve” pool. we introduce the gsodas mechanism, which is truthful for agents on the static side of the market and stable. in simulations, we establish that gsodas dominates in rank-efficiency a pair of randomized mechanisms that operate without the use of a fall-back option. in addition, we demonstrate good rank-efficiency in comparison to a non-truthful mechanism that employs online stochastic optimization.",nan
2010,learning conditionally lexicographic preference relations,"we consider the problem of learning a user's ordinal preferences on a multiattribute domain, assuming that her preferences are lexicographic. we introduce a general graphical representation called lp-trees which captures various natural classes of such preference relations, depending on whether the importance order between attributes and/or the local preferences on the domain of each attribute is conditional on the values of other attributes. for each class we determine the vapnik-chernovenkis dimension, the communication complexity of preference elicitation, and the complexity of identifying a model in the class consistent with a set of user-provided examples.",nan
2010,identifying necessary reactions in metabolic pathways by minimal model generation,"in systems biology, identifying vital functions like glycolysis from a given metabolic pathway is important to understand living organisms. in this paper, we focus on the problem of finding minimal sub-pathways producing target metabolites from source metabolites. we translate laws of biochemical reactions into propositional formulas and compute its minimal models to solve the problem. an advantage of our method is that it can treat reversible reactions. moreover the translation enables us to obtain solutions for large pathways. we apply our method to a whole escherichia coli metabolic pathway. as a result, we have found the conventional glycolysis sub-pathway described in a biological database ecocyc.",nan
2010,interval forecast of water quality parameters,"the current quality control methodology adopted by the water distribution service provider in the metropolitan region of porto - portugal, is based on simple heuristics and empirical knowledge. based on the domain complexity and data volume, this application is a perfect candidate to apply data mining process. in this paper, we propose a new methodology to predict the range of normality for the values of different water quality parameters. these intervals of normality are of key importance to decide on costly inspection activities. our experimental evaluation confirms that our proposal achieves good results on the task of forecasting the normal distribution of values for the following 30 days. the proposed method can be applied to other domains with similar network monitoring objectives.",nan
2010,data mining for biodiversity prediction in forests,"there is international consensus on the key elements of sustainable forest management. biological diversity has been recognised as one of them. this paper investigates the usefulness of terrestrial laser scanning technology in forest biodiversity assessment. laser scanning is a rapidly emerging technology that captures high-resolution, 3-d structural information about forests and presently has applications in standing timber measurement. forest biodiversity is influenced by structural complexity in the forest although precise repeatable measures are difficult to achieve using traditional methods. the aim of the research presented here is to apply laser scanning technology to the assessment of forest structure and deadwood, and relate this information to the diversity of plants, invertebrates and birds in a range of forest types including native woodlands and commercial plantations. procedures for forest biodiversity assessment are known to be expensive due to their reliance on labour-intensive field visits. we describe our progress on the application of terrestrial laser scanning in an automated approach to biodiversity assessment. we apply regression techniques from the field of data mining to predict several biodiversity measures using physical attributes of the forest with very promising results.",nan
2010,boosting clustering by active constraint selection,"in this paper we address the problem of active query selection for clustering with constraints. the objective is to determine automatically a set of user queries to define a set of must-link or cannot-link constraints. some works on active constraint learning have already been proposed but they are mainly applied to k-means like clustering algorithms which are known to be limited to spherical clusters, while we are interested in clusters of arbitrary sizes and shapes. the novelty of our approach relies on the use of a k-nearest neighbor graph to determine candidate constraints coupled with a new constraint utility function. comparative experiments conducted on real datasets from machine learning repository show that our approach significantly improves the results of constraints based clustering algorithms.",nan
2010,a very fast method for clustering big text datasets,"large-scale text datasets have long eluded a family of particularly elegant and effective clustering methods that exploits the power of pair-wise similarities between data points due to the prohibitive cost, time- and space-wise, in operating on a similarity matrix, where the state-of-the-art is at best quadratic in time and in space.we present an extremely fast and simple method also using the power of all pair-wise similarity between data points, and show through experiments that it does as well as previous methods in clustering accuracy, and it does so with in linear time and space, without sampling data points or sparsifying the similarity matrix.",nan
2010,active testing strategy to predict the best classification algorithm via sampling and metalearning,"currently many classification algorithms exist and there is no algorithm that would outperform all the others in all tasks. therefore it is of interest to determine which classification algorithm is the best one for a given task. although direct comparisons can be made for any given problem using a cross-validation evaluation, it is desirable to avoid this, as the computational costs are significant. we describe a method which relies on relatively fast pairwise comparisons involving two algorithms. this method exploits sampling landmarks, that is information about learning curves besides classical data characteristics. one key feature of this method is an iterative procedure for extending the series of experiments used to gather new information in the form of sampling landmarks. metalearning plays also a vital role. the comparisons between various pairs of algorithm are repeated and the result is represented in the form of a partially ordered ranking. evaluation is done by comparing the partial order of algorithm that has been predicted to the partial order representing the supposedly correct result. the results of our analysis show that the method has good performance and could be of help in practical applications.",nan
2010,improving hierarchical classification with partial labels,"in this paperwe address the problem of semi-supervised hierarchical learning when some cases are fully labeled while other cases are only partially labeled, named hierarchical partial labels. given a label hierarchy, a fully labeled example provides a path from the root node to a leaf node while a partially labeled example only provides a path from the root node to an internal node. we introduce a discriminative learning approach, called partial hsvm, that incorporates partially labeled information into the hierarchical maximum margin-based learning framework. the partially labeled hierarchical learning problem is formulated as a quadratic optimization that minimizes the empirical risk with l2-norm regularization. we also present an efficient algorithm for the hierarchical classification in the presence of partially labeled information. in our experiments with the wipo-alpha patent collection, we compare our proposed algorithm with two other baseline approaches: binary hsvm, a standard approach to hierarchical classification, which builds a binary classifier (svm) at each node in the hierarchy, and pl-svm, a flat multiclass classifier which can take advantages of the partial label information. our empirical results show that partial hsvm outperforms binary hsvm and pl-svm across different performance metrics. the experimental results demonstrate that our proposed algorithm, partial hsvm, combines the strength of both methods, the binary hsvm and pl-svm, since it utilizes both the hierarchical information and the partially labeled examples. in addition, we observe the positive correlation between the labeling effort in obtaining partially labeled data and the improvement in performance.",nan
2010,implicit learning of compiled macro-actions for planning,"we build a comprehensive macro-learning system and contribute in three different dimensions that have previously not been addressed adequately. firstly, we learn macro-sets considering implicitly the interactions between constituent macros. secondly, we effectively learn macros that are not found in given example plans. lastly, we improve or reduce degradation of plan-length when macros are used; note, our main objective is to achieve fast planning. our macro-learning system significantly outperforms a very recent macro-learning method both in solution speed and plan length.",nan
2010,strengthening landmark heuristics via hitting sets,"the landmark cut heuristic is perhaps the strongest known polytime admissible approximation of the optimal delete relaxation heuristic h+. equipped with this heuristic, a best-first search was able to optimally solve 40% more benchmark problems than the winners of the sequential optimization track of ipc 2008. we show that this heuristic can be understood as a simple relaxation of a hitting set problem, and that stronger heuristics can be obtained by considering stronger relaxations. based on these findings, we propose a simple polytime method for obtaining heuristics stronger than landmark cut, and evaluate them over benchmark problems. we also show that hitting sets can be used to characterize h+ and thus provide a fresh and novel insight for better comprehension of the delete relaxation.",nan
2010,sound and complete landmarks for and/or graphs,"landmarks for a planning problem are subgoals that are necessarily made true at some point in the execution of any plan. since verifying that a fact is a landmark is pspace-complete, earlier approaches have focused on finding landmarks for the delete relaxation π+. furthermore, some of these approaches have approximated this set of landmarks, although it has been shown that the complete set of causal delete-relaxation landmarks can be identified in polynomial time by a simple procedure over the relaxed planning graph. here, we give a declarative characterisation of this set of landmarks and show that the procedure computes the landmarks described by our characterisation. building on this, we observe that the procedure can be applied to any delete-relaxation problem and take advantage of a recent compilation of the m-relaxation of a problem into a problem with no delete effects to extract landmarks that take into account delete effects in the original problem. we demonstrate that this approach finds strictly more causal landmarks than previous approaches and discuss the relationship between increased computational effort and experimental performance, using these landmarks in a recently proposed admissible landmark-counting heuristic.",nan
2010,iterative bounding lao*,"iterative bounding lao* is a new algorithm for ε-optimal probabilistic planning problems where an absorbing goal state should be reached at a minimum expected cost from a given ini tial state. the algorithm is based on the lao* algorithm for finding optimal solutions in cyclic and/or graphs. the new algorithm uses two heuristics, one upper bound and one lower bound of the optimal cost. the search is guided by the lower bound as in lao*, while the upper bound is used to prune search branches. the algorithm has a new mechanism for expanding search nodes, and while maintaining the error bounds, it may use weighted heuristics to reduce the size of the explored search space. in empirical tests on benchmark problems, iterative bounding lao* expands fewer search nodes compared to state of the art rtdp variants that also use two-sided bounds.",nan
2010,analysis of inverse reinforcement learning with perturbed demonstrations,"inverse reinforcement learning (irl) addresses the problem of recovering the unknown reward function for a given markov decision problem (mdp) given the corresponding optimal policy or a perturbed version thereof. this paper studies the space of possible solutions to the general irl problem, when the agent is provided with incomplete/imperfect information regarding the optimal policy for the mdp whose reward must be estimated. we focus on scenarios with finite state-action spaces and discuss the constraints imposed on the set of possible solutions when the agent is provided with (i) perturbed policies; (ii) optimal policies; and (iii) incomplete policies. we discuss previous works on irl in light of our analysis and show that, with our characterization of the solution space, it is possible to determine non-trivial closed-form solutions for the irl problem. we also discuss several other interesting aspects of the irl problem that stem from our analysis.",nan
2010,case-based multiagent reinforcement learning: cases as heuristics for selection of actions,"this work presents a new approach that allows the use of cases in a case base as heuristics to speed up multiagent reinforcement learning algorithms, combining case-based reasoning (cbr) and multiagent reinforcement learning (mrl) techniques. this approach, called case-based heuristically accelerated multiagent reinforcement learning (cb-hamrl), builds upon an emerging technique, heuristic accelerated reinforcement learning (harl), in which rl methods are accelerated by making use of heuristic information. cb-hamrl is a subset of mrl that makes use of a heuristic function [hscr    ] derived from a case base, in a case-based reasoning manner. an algorithm that incorporates cbr techniques into the heuristically accelerated minimax–q is also proposed and a set of empirical evaluations were conducted in a simulator for the littman's robot soccer domain, comparing the three solutions for this problem: mrl, hamrl and cb-hamrl. experimental results show that using cb-hamrl, the agents learn faster than using rl or hamrl methods.",nan
2010,uncertainty propagation for efficient exploration in reinforcement learning,"reinforcement learning aims to derive an optimal policy for an often initially unknown environment. in the case of an unknown environment, exploration is used to acquire knowledge about it. in that context the well-known exploration-exploitation dilemma arises—when should one stop to explore and instead exploit the knowledge already gathered? in this paper we propose an uncertainty-based exploration method. we use uncertainty propagation to obtain the q-function's uncertainty and then use the uncertainty in combination with the q-values to guide the exploration to promising states that so far have been insufficiently explored. the uncertainty's weight during action selection can be influenced by a parameter. we evaluate one variant of the algorithm using full covariance matrices and two variants using an approximation and demonstrate their functionality on two benchmark problems.",nan
2010,the dynamics of multi-agent reinforcement learning,"infinite-horizon multi-agent control processes with non-determinism and partial state knowledge have particularly interesting properties with respect to adaptive control, such as the non-existence of nash equilibria (ne) or non-strict ne which are nonetheless points of convergence. the identification of reinforcement learning (rl) algorithms that are robust, accurate and efficient when applied to these general multi-agent domains is an open, challenging problem. this paper uses learning pressure fields as a means for evaluating rl algorithms in the context of multi-agent processes. specifically, we show how to model partially observable infinite-horizon stochastic processes (single-agent) and games (multi-agent) within the finite analytic stochastic process framework. taking long term average expected returns as utility measures, we show the existence of learning pressure fields: vector fields – similar to the dynamics of evolutionary game theory, which indicate medium and long term learning behaviours of agents independently seeking to maximise this utility. we show empirically that these learning pressure fields are followed closely by policy-gradient rl algorithms.",nan
2010,an efficient procedure for collective decision-making with cp-nets,"this paper studies the problem of collective decision-making in the case where the agents' preferences are represented by cp-nets (conditional preference networks). in many real-world decision-making problems, the number of possible outcomes is exponential in the number of domain variables. most related works either do not consider computational concerns, or depend on a strong assumption that all the agents' cp-nets share a common preferential-independence structure. to this end, we introduce a novel procedure for collective decision-making with cp-nets. our proposed approach allows the agents to have different preferential-independence structures and guarantees pareto-optimality. our experimental results demonstrate that our proposed procedure is computationally efficient and produces the results that are close to the fair minimax solution.",nan
2010,modelling multilateral negotiation in linear logic,"we show how to embed a framework for multilateral negotiation, in which a group of agents implement a sequence of deals concerning the exchange of a number of resources, into linear logic. in this model, multisets of goods, allocations of resources, preferences of agents, and deals are all modelled as formulas of linear logic. whether or not a proposed deal is rational, given the preferences of the agents concerned, reduces to a question of provability, as does the question of whether there exists a sequence of deals leading to an allocation with certain desirable properties, such as maximising social welfare. thus, linear logic provides a formal basis for modelling convergence properties in distributed resource allocation.",nan
2010,fair division under ordinal preferences: computing envy-free allocations of indivisible goods,"we study the problem of fairly dividing a set of goods amongst a group of agents, when those agents have preferences that are ordinal relations over alternative bundles of goods (rather than utility functions) and when our knowledge of those preferences is incomplete. the incompleteness of the preferences stems from the fact that each agent reports their preferences by means of an expression of bounded size in a compact preference representation language. specifically, we assume that each agent only provides a ranking of individual goods (rather than of bundles). in this context, we consider the algorithmic problem of deciding whether there exists an allocation that is possibly (or necessarily) envy-free, given the incomplete preference information available, if in addition some mild economic efficiency criteria need to be satisfied. we provide simple characterisations, giving rise to simple algorithms, for some instances of the problem, and computational complexity results, establishing the intractability of the problem, for others.",nan
2010,lp solvable models for multiagent fair allocation problems,"this paper proposes several operational approaches for solving fair allocation problems in the context of multiagent optimization. these problems arise in various contexts such as assigning conference papers to referees or sharing of indivisible goods among agents. we present and discuss various social welfare functions that might be used to maximize the satisfaction of agents while maintaining a notion of fairness in the distribution. all these welfare functions are in fact non-linear, which precludes the use of classical min-cost max-flow algorithms for finding an optimal allocation. for each welfare function considered, we present a mixed integer linear programming formulation of the allocation problem that can be efficiently solved using standard solvers. the results of numerical tests we conducted on realistic cases are given at the end of the paper to confirm the practical feasibility of the proposed approaches.",nan
2010,using bayesian networks in an industrial setting: making printing systems adaptive,"control engineering is a field of major industrial importance as it offers principles for engineering controllable physical devices, such as cell phones, television sets, and printing systems. control engineering techniques assume that a physical system's dynamic behaviour can be completely described by means of a set of equations. however, as modern systems are often of high complexity, drafting such equations has become more and more difficult. moreover, to dynamically adapt the system's behaviour to a changing environment, observations obtained from sensors at runtime need to be taken into account. however, such observations give an incomplete picture of the system's behaviour; when combined with the incompletely understood complexity of the device, control engineering solutions increasingly fall short. probabilistic reasoning would allow one to deal with these sources of incompleteness, yet in the area of control engineering such ai solutions are rare. when using a bayesian network in this context the required model can be learnt, and tuned, from data, uncertainty can be handled, and the model can be subsequently used for stochastic control of the system's behaviour. in this paper we discuss industrial research in which bayesian networks were successfully used to control complex printing systems.",nan
2010,context-aware media agent for public spaces,"this paper presents an agent-based framework for building and operating context-aware media for digital signage in public spaces. the framework is managed in a non-centralized manner by using mobile agent technology and enables user assistant media agents to follow the movement of their users between stationary computers, e.g., public terminals, by using active rfid-tags. when users moves between locations in a public space, their agents provide annotations in personal forms according their current context. to demonstrate the utility and effectiveness of the system, we describe two applications of the frameworks.",nan
2010,an iterative a* algorithm for planning of airport ground movements,"optimization of ground traffic is a major issue of air traffic management: optimal ground circulation could decrease flight delays and consequently decrease costs and increase passenger wellness. this paper proposes a planning algorithm for ground traffic based on contract reservation. this algorithm is iterative: it plans aircraft itinerary one after the other. a first version is described using the classical a* algorithm. then the model is extended to deal with time and speed uncertainty to ensure the feasibility of the planned trajectories while avoiding conflicts between aircrafts. its efficiency is evaluated on toulouse-blagnac airport, regarding quality of the solution and computation times.",nan
2010,a fault-model-based debugging aid for data warehouse applications,"the paper describes a model-based approach to developing a general tool for localizing faults in applications of data warehouse technology. a model of the application is configured from a library of generic models of standard (types of) modules and exploited by a consistency-based diagnosis algorithm, originally used for diagnosing physical devices. observing intermediate results can require high efforts or even be impossible, which limits the discriminability between different faults in a sequence of data processing steps. to compensate for this, fault models are used. this becomes a feasible solution for standard modules of a data warehouse application along with a stratification of the data. fault models capture the potential impact of faults of process steps and data transfer on the data strata as well as on sets of data. reflecting the nature of the initial symptoms and of the potential checks, these descriptions are stated at a qualitative level. the solution has been validated in customer report generation of a provider of mobile phone services.",nan
2010,kernel-based hybrid random fields for nonparametric density estimation,"hybrid random fields are a recently proposed graphical model for pseudo-likelihood estimation in discrete domains. in this paper, we develop a continuous version of the model for nonparametric density estimation. to this aim, nadaraya-watson kernel estimators are used to model the local conditional densities within hybrid random fields. first, we introduce a heuristic algorithm for tuning the kernel bandwidhts in the conditional density estimators. second, we propose a novel method for initializing the structure learning algorithm originally employed for hybrid random fields, which was meant instead for discrete variables. in order to test the accuracy of the proposed technique, we use a number of synthetic pattern classification benchmarks, generated from random distributions featuring nonlinear correlations between the variables. as compared to state-of-the-art nonparametric and semiparametric learning techniques for probabilistic graphical models, kernel-based hybrid random fields regularly outperform each considered alternative in terms of recognition accuracy, while preserving the scalability properties (with respect to the number of variables) that originally motivated their introduction.",nan
2010,multitask kernel-based learning with logic constraints,"this paper presents a general framework to integrate prior knowledge in the form of logic constraints among a set of task functions into kernel machines. the logic propositions provide a partial representation of the environment, in which the learner operates, that is exploited by the learning algorithm together with the information available in the supervised examples. in particular, we consider a multi-task learning scheme, where multiple unary predicates on the feature space are to be learned by kernel machines and a higher level abstract representation consists of logic clauses on these predicates, known to hold for any input. a general approach is presented to convert the logic clauses into a continuous implementation, that processes the outputs computed by the kernel-based predicates. the learning task is formulated as a primal optimization problem of a loss function that combines a term measuring the fitting of the supervised examples, a regularization term, and a penalty term that enforces the constraints on both supervised and unsupervised examples. the proposed semi-supervised learning framework is particularly suited for learning in high dimensionality feature spaces, where the supervised training examples tend to be sparse and generalization difficult. unlike for standard kernel machines, the cost function to optimize is not generally guaranteed to be convex. however, the experimental results show that it is still possible to find good solutions using a two stage learning schema, in which first the supervised examples are learned until convergence and then the logic constraints are forced. some promising experimental results on artificial multi-task learning tasks are reported, showing how the classification accuracy can be effectively improved by exploiting the a priori rules and the unsupervised examples.",nan
2010,kernel methods for revealed preference analysis,"in classical revealed preference analysis we are given a sequence of linear prices (i.e., additive over goods) and an agent's demand at each of the prices. the problem is to determine whether the observed demands are consistent with utility-maximizing behavior, and if so, recover a representation of the agent's utility function. in this work, we consider a setting where an agent responds to non-linear prices and also allow for incomplete price information over the consumption set. we develop two different kernel methods to fit linear and concave utilities to such observations. the methods allow one to incorporate prior information about the utility function into the estimation procedure, and represent semi-parametric alternatives to the classical non-parametric approach. an empirical evaluation exhibits the relative merits of the two methods in terms of generalization ability, solution sparsity, and runtime performance.",nan
2010,regression learning with multiple noisy oracles,"in regression learning, it is often difficult to obtain the true values of the label variables, while multiple sources of noisy estimates of lower quality are readily available. to address this problem, we propose a new bayesian approach that learns a regression model from data with noisy labels provided by multiple oracles. the proposed method provides closed form solution for model parameters and is applicable to both linear and nonlinear regression problems. in our experiments on synthetic and benchmark datasets this new regression model was consistently more accurate than a model trained with averaged estimates from multiple oracles as labels.",nan
2010,constraint based planning with composable substate graphs,"constraint satisfaction techniques provide powerful inference algorithms that can prune choices during search. constraint-based approaches provide a useful complement to heuristic search optimal planners. we develop a constraint-based model for cost-optimal planning that uses global constraints to improve the inference in planning.the key novelty in our approach is in a transformation of the sas+ input that adds a form of macro-action to fully connect chains of composable operators. this translation leads to the development of a natural dominance constraint on the new problem which we add to our constraint model.we provide empirical results to show that our planner, constance, solves more instances than the current best constraint-based planners. we also demonstrate the power of our new dominance constraints in this representation.",nan
2010,knowledge compilation using interval automata and applications to planning,"knowledge compilation [6, 5, 14, 8] consists in transforming a problem offline into a form which is tractable online. in this paper, we introduce new structures, based on the notion of interval automaton (ia), adapted to the compilation of problems involving both discrete and continuous variables, and especially of decision policies and transition tables, in the purpose of controlling autonomous systems.interval automata can be seen as a generalization of binary decision diagrams (bdds) insofar as they are rooted dags with variable-labelled nodes, with the differences that interval automata are non-deterministic structures whose edges are labelled with closed intervals and whose nodes can have a multiplicity greater than two.this paper studies the complexity of the queries and transformations classically considered when examining a new compilation language. we show that a particular subset of interval automata, the focusing ones (fias), have theoretical capabilities very close to those of dnnfs; they notably support in polytime the main operations needed to handle decision policies online. experimental results are presented in order to support these claims.",nan
2010,compiling uncertainty away in non-deterministic conformant planning,"it has been shown recently that deterministic conformant planning problems can be translated into classical problems that can be solved by off-the-shelf classical planners. in this work, we aim to extend this formulation to non-deterministic conformant planning. we start with the well known observation that non-deterministic effects can be eliminated by using hidden conditions that must be introduced afresh each time a non-deterministic action is applied. this observation, however, leads to translations that have to be recomputed as the search for plans proceeds. we then introduce other translations, that while incomplete, appear to be quite effective and result in classical planning problems that need to be solved only once. a number of experimental results over existing and new domains are reported.",nan
2010,analyzing flexible timeline-based plans,timeline-based planners have been shown quite successful in addressing real world problems. nevertheless they are considered as a niche technology in ai p&s research as an application synthesis with such techniques is still considered a sort of “black art”. authors are currently developing a knowledge engineering tool around a timeline-based problem solving environment; in this framework we aim at integrating verification and validation methods. this work presents a verification process suitable for a timeline-based planner. it shows how a problem of flexible temporal plan verification can be cast as model-checking on timed game automata. additionally it provides formal properties and checks the effectiveness of the proposed approach with a detailed experimental analysis.,nan
2010,a unified framework for non-standard reasoning services in description logics,"non-standard reasoning in description logics (dls) comprises computing a least common subsumer (lcs), a concept difference, a concept unifier, or an interpolant concept, to name a few. although some reasoning services have been unified already (e.g., lcs and most specific concept), the definition of non-standard problems and the computation that solve them are very different from each other. we propose to unify the definitions of non-standard services as special second-order sentences in dls; when the solution concepts are optimal with respect to some preferences, a fixpoint replaces the second-order quantification. moreover, we propose to combine the well-known tableaux calculi for dls with rules that compute substitutions of concept variables. we prove soundness and completeness of the combined calculus and we give a sufficient condition for termination, which covers some non-trivial cases.",nan
2010,parallel tbox classification in description logics – first experimental results,"one of the most frequently used inference services of description logic reasoners classifies all named classes of owl ontologies into a subsumption hierarchy. due to emerging owl ontologies from the web community consisting of up to hundreds of thousand of named classes and the increasing availability of multi-processor and multi- or many-core computers, we extend our work on parallel tbox classification and propose a new algorithm that is sound and complete and demonstrates in a first experimental evaluation a low overhead w.r.t. subsumption tests (less than 3%) if compared with sequential classification.",nan
2010,logical ontology validation using an automatic theorem prover,"ontologies are utilized for a wide range of tasks, like information retrieval/extraction or text generation, and in a multitude of domains, such as biology, medicine or business and commerce. to be actually usable in such real-world scenarios, ontologies usually have to encompass a large number of factual statements. however, with increasing size, it becomes very difficult to ensure their complete correctness. this is particularly true in the case when an ontology is not hand-crafted but constructed (semi)automatically through text mining, for example. as a consequence, when inference mechanisms are applied on these ontologies, even minimal inconsistencies oftentimes lead to serious errors and are hard to trace back and find. this paper addresses this issue and describes a method to validate ontologies using an automatic theorem prover and multinet axioms. this logic-based approach allows to detect many inconsistencies, which are difficult or even impossible to identify through statistical methods or by manual investigation in reasonable time. to make this approach accessible for ontology developers, a graphical user interface is provided that highlights erroneous axioms directly in the ontology for quicker fixing.",nan
2010,integrating bipolar fuzzy mathematical morphology in description logics for spatial reasoning,"bipolarity is an important feature of spatial information, involved in the expression of preferences and constraints about spatial positioning or in pairs of opposite spatial relations such as left and right. another important feature is imprecision which has to be taken into account to model vagueness, inherent to many spatial relations (as for instance vague expressions such as close to, to the right of), and to gain in robustness in the representations. in previous works, we have shown that fuzzy sets and fuzzy mathematical morphology are appropriate frameworks, on the one hand, to represent bipolarity and imprecision of spatial relations and, on the other hand, to combine qualitative and quantitative reasoning in description logics extended with fuzzy concrete domains. the purpose of this paper is to integrate the bipolarity feature in the latter logical framework based on bipolar and fuzzy mathematical morphology and description logics with fuzzy concrete domains. two important issues are addressed in this paper: the modeling of the bipolarity of spatial relations at the terminological level and the integration of bipolar notions in fuzzy description logics. at last, we illustrate the potential of the proposed formalism for spatial reasoning on a simple example in brain imaging.",nan
2010,computing the data semantics of wsdl specifications via gradient boosting,"this paper proposes a method for the semi-automatic semantic annotation of wsdl specifications, given ontologies related to the domain of services. the proposed method uses a synthesis of mapping methods to map input/output messages' parameters to ontology classes. exploiting validated results provided by humans, the method learns via the gradient boosting learning algorithm to combine the individual mapping methods towards improving its accuracy. the aim is to mitigate difficulties concerning mappings and address limitations of other approaches, even in challenging cases, so as to assist human annotators to perform their work. the paper presents experimental results of the proposed methods.",nan
2010,opinion question answering: towards a unified approach,"nowadays, the web contains large amounts of heterogeneous (factual and opinionated) data, which is becoming equally important for users to access. the need to efficiently manage this information leads to the necessity of building automatic systems that efficiently process it. in this paper, we propose and evaluate a series of techniques whose aim is to improve the performance of an opinion question answering (oqa) system. we include additional resources and processes with the objective of limiting the sources of errors in the different stages involved - question analysis, answer retrieval and filtering, answer re-ranking. we propose new elements that are significant in these stages and show that their use improves the performance of the system. we conclude that the suggested techniques help to influences the task in a positive manner.",nan
2010,from bursty patterns to bursty facts: the effectiveness of temporal text mining for news,"many document collections are by nature dynamic, evolving as the topics or events they describe change. the goal of temporal text mining is to discover bursty patterns and to identify and highlight these changes to better enable readers to track stories. here, we focus on the news domain, where the changes revolve around novel, previously unpublished, “facts” that have an effect on the story developments. however, despite intense research activities on bursty patterns, a lack of common procedures today makes it impossible to compare methods in a principled way. to close this gap, we (a) investigate how different temporal text mining methods discover novel facts and (b) present an evaluation framework for methods assessment, consisting of a set of procedures and metrics for cross-evaluating models. bursty patterns are transformed into queries for sentence retrieval, either with or without taking into account internal pattern structure, and these sentences are compared with a set of editor-selected ground-truth reference sentences. our experiments on different classes of temporal text mining show that different methods perform at similar levels overall, but provide distinctive advantages in some settings. the experiments also demonstrate the benefits of using patterns' internal structure for query generation.",nan
2010,extraction of places related to flickr tags,"geographic information systems use databases to map keywords to places. these databases are currently most often created by using a top-down approach based on the geographic definitions. however, there is a problem with this approach in that these databases only contain location definitions such as addresses and place names, which does not allow for searches using keywords other than these words. additionally, they do not give any information on the popularity, e.g., which is more popular among the places indexed by the same keyword. a bottom-up approach, based on the actual usage of words, can address these problems. we propose a method to aggregate tagging data and extract places related to a tag using the pair of a tag and a geo-tagged photo. we target the co-occurrence of a tag and the geolocation and represent the places related to a tag as a probability distribution over the longitudes and latitudes. we applied our method to data on the photo sharing service flickr and experimentally confirmed that our method made it possible to highly-accurately extract places related to tags. our direct bottom-up approach enables the extraction of place information that is not obtained by using traditional top-down approaches.",nan
2010,automatic free-text-tagging of online news archives,"in this paper, we shall introduce the problem of free-text-tagging of online news archives. from an application point of view, it has many benefits for online news portals and on the other hand, the task has unique characteristics compared to existing approaches for free-text-tagging. we shall describe our system, which was developed for the archive (consisting of 370 thousand articles) of the most visited hungarian news portal www.origo.hu, along with research questions encountered and solved during our task. as the evaluation of tagging is not straightforward at the end of the project the news company manually investigated the tagging of the automatic system which yielded an f-measure of 71.9.",nan
2010,learning aggregation functions for expert search,"machine learning techniques are increasingly being applied to problems in the domain of information retrieval and text mining. in this paper we present an application of evolutionary computation to the area of expert search. expert search in the context of enterprise information systems deals with the problem of finding and ranking candidate experts given an information need (query). a difficult problem in the area of expert search is finding relevant information given an information need and associating that information with a potential expert.we attempt to improve the effectiveness of a benchmark expert search approach by adopting a learning model (genetic programming) that learns how to aggregate the documents/information associated with each expert. in particular, we perform an analysis of the aggregation of document information and show that different numbers of documents should be aggregated for different queries in order to achieve optimal performance.we then attempt to learn a function that optimises the effectiveness of an expert search system by aggregating different numbers of documents for different queries. furthermore, we also present experiments for an approach that aims to learn the best way to aggregate documents for individual experts. we find that substantial improvements in performance can be achieved, over standard analytical benchmarks, by the latter of these approaches.",nan
2010,parallel model checking for temporal epistemic logic,"we investigate the problem of the verification of multi-agent systems by means of parallel algorithms. we present algorithms for ctlk, a logic combining branching time temporal logic with epistemic modalities. we report on an implementation of these algorithms and present the experimental results obtained. the results point to a significant speed-up in the verification step.",nan
2010,the complexity of epistemic model checking: clock semantics and branching time,"in the clock semantics for epistemic logic, two situations are indistinguishable for an agent when it makes the same observation and the time in the situations is the same. the paper characterizes the complexity of model checking branching time logics of knowledge in finite state systems with respect to the clock semantics.",nan
2010,higher-order coalition logic,"we introduce and study higher-order coalition logic, a multi modal monadic second-order logic with operators [{x}ψ]&phi; expressing that the coalition of all agents satisfying ψ(x) can achieve a state in which &phi; holds. we use neighborhood semantics to model extensive games of perfect information with simultaneous actions and we provide a framework reasoning about agents in the same way as it is reasoning about their abilities. we illustrate higher-order coalition logic to represent and reason about coalition formation and cooperation, we show a more general and expressive way to quantify over coalitions than quantified coalition logic, we give an axiomatization and prove completeness.",nan
2010,mental state ascription using dynamic logic,"in situations where the behavior of a system must be interpreted because its state is not accessible, it is useful to explain observed behavior in mentalistic terms. this paper presents a formalism based on propositional dynamic logic to model ascription of beliefs, goals, or plans on grounds of observed actions. the formalism is used to provide semantics for an existing approach to abducing the mental state of an observed agent; in doing so it is shown how behavior-producing rules can be given different explanatory interpretations.",nan
2010,on the (un-)decidability of model checking resource-bounded agents,"the verification and modelling of multi-agent systems is an important topic that has attracted much attention in recent years. resources, however, have only recently been studied as simple extensions to well-known logics. trying to find a set of useful features while retaining essential properties for practical use, we explore the question: where are the limits of what can be verified about resource-bounded agents? we try to answer this question by considering several natural logic-based settings that may arise and prove that verification is usually undecidable apart from bounded or otherwise restrictive settings. most interestingly, we identify various factors that influence the (un-)decidability and provide grounds for future research on more promising constraints leading to decidable fragments.",nan
2010,an equilibrium analysis of competing double auction marketplaces using fictitious play,"in this paper, we analyse how traders select marketplaces and bid in a setting with multiple competing marketplaces. specifically, we use a fictitious play algorithm to analyse the traders' equilibrium strategies for market selection and bidding when their types are continuous. to achieve this, we first analyse traders' equilibrium bidding strategies in a single marketplace and find that they shade their offers in equilibrium and the degree to which they do this depends on the amount and types of fees that are charged by the marketplace. building on this, we then analyse equilibrium strategies for traders in competing marketplaces in two particular cases. in the first, we assume that traders can only select one marketplace at a time. for this, we show that, in equilibrium, all traders who choose one of the marketplaces eventually converge to the same one. in the second case, we allow buyers to participate in multiple marketplaces at a time, while sellers can only select one marketplace. for this, we show that sellers eventually distribute in different marketplaces in equilibrium and that buyers shade less and sellers shade more in the equilibrium bidding strategy (since sellers have more market power than buyers).",nan
2010,addressing the exposure problem of bidding agents using flexibly priced options,"in this paper we introduce a new option pricing mechanism for reducing the exposure problem encountered by bidding agents with complementary valuations when participating in sequential, second-price auction markets. existing option pricing models have two main drawbacks: they either apply fixed exercise prices, which may deter bidders with low valuations, thereby decreasing allocative efficiency, or options are offered for free, in which case bidders are less likely to exercise them, thereby reducing seller revenues. the proposed mechanism involving flexibly priced options addresses these problems by calculating the exercise price as well as the option price based on the bids received during an auction. for this new model, which extends and encompasses all the previous models examined, we derive the optimal strategies for a bidding agent with complementary preferences. finally, we use these strategies to evaluate the proposed option mechanism through monte-carlo simutions, and compare it to existing mechanisms, both in terms of the seller revenue and the social welfare. we show that our new mechanism achieves higher market efficiency compared to having no options and free options, while achieving higher revenues for the seller than any existing option mechanism.",nan
2010,designing a successful adaptive agent for tac ad auction,"this paper describes the design and evaluation of aston-tac, the runner-up in the ad auction game of 2009 international trading agent competition. in particular, we focus on how aston-tac generates adaptive bid prices according to the market-based value per click and how it selects a set of keyword queries to bid on to maximise the expected profit under limited conversion capacity. through evaluation experiments, we show that astontac performs well and stably not only in the competition but also across a broad range of environments.",nan
2010,propagation of opinions in structural graphs,"trust and reputation measures are crucial in distributed open systems where agents need to decide whom or what to choose. existing work has overlooked the impact of an entity's position in its structural graph and its effect on the propagation of trust in such graphs. this paper presents an algorithm for the propagation of reputation in structural graphs. it allows agents to infer their opinion about unfamiliar entities based on their view of related entities. the proposed mechanism focuses on the “part of” relation to illustrate how reputation may flow (or propagate) from one entity to another. the paper bases its reputation measures on opinions, which it defines as probability distributions over an evaluation space, providing a rich representation of opinions.",nan
2010,social recommendation with interpersonal influence,"social recommendation, that an individual recommends an item to another, has gained popularity and success in web applications such as online sharing and shopping services. it is largely different from a traditional recommendation where an automatic system recommends an item to a user. in a social recommendation, the interpersonal influence plays a critical role but is usually ignored in traditional recommendation systems, which recommend items based on user-item utility. in this paper, we propose an approach to model the utility of a social recommendation through combining three factors, i.e. receiver interests, item qualities and interpersonal influences. in our approach, values of all factors can be learned from user behaviors. experiments are conducted to compare our approach with three conventional methods in social recommendation prediction. empirical results show the effectiveness of our approach, where an increase by 26% in prediction accuracy can be observed.",nan
2010,recommendations over domain specific user graphs,"content providers want to make recommendations across multiple interrelated domains such as music and movies. however, existing collaborative filtering methods fail to accurately identify items that may be interesting to the user but that lie in domains that the user has not accessed before. this is mainly because of the paucity of user transactions across multiple item domains. our method is based on the observation that users who share similar items or who share social connections, can provide recommendation chains (sequences of transitively associated edges) to items in other domains. it first builds domain-specific-usergraphs (dsugs) whose nodes, users, are linked by weighted edges that reflect user similarity. it then connects the dsugs via the users who rated items in several domains or via the users who share social connections, to create a cross-domain-user graph (cdug). it performs random walk with restarts on the cdug to extract user nodes that are related to the starting user node on the cdug even though they are not present in the dsug of the starting user node. it then adds items possessed by those users to the recommendations of the starting node user. furthermore, to extract many more user nodes, we employ a taxonomy-based similarity measure that states that users are similar if they share the same items and/or same classes. thus we can set many suitable routes from the starting user node to other user nodes in the cdug. an evaluation using rating datasets in two interrelated domains and social connection histories of users as extracted from a blog portal, indicates that our method identifies potentially interesting items in other domains with higher accuracy than is possible with existing cf methods.",nan
2010,foundations of tree-like local model updates,"model update is an approach to enhance model checking functions by providing computer aided modifications in system development [2, 9]. it has been observed that one major obstacle restricting the application of this approach, e.g. ctl model update [15], is that the update has to take the entire system model into account, and that is usually not feasible for large scale domains. in this paper, we develop a tree-like local model update approach under the framework of actl – a widely used fragment of ctl in property specification. we define a bisimulation based minimal change principle on tree-like local model update, reveal its relationship to traditional belief update and provide essential semantic characterizations. we also investigate primary semantic and computational properties in relation to tree-like local model update. finally we briefly describe the update system prototype that we have implemented and summarize our experimental results.",nan
2010,the epistemic view of belief merging: can we track the truth?,"belief merging is often described as the process of defining a base which best represents the beliefs of a group of agents (a profile of belief bases). the resulting base can be viewed as a synthesis of the input profile. in this paper another view of what belief merging aims at is considered: the epistemic view. under this view the purpose of belief merging is to best approximate the true state of the world. we point out a generalization of condorcet's jury theorem from the belief merging perspective. roughly, we show that if the beliefs of sufficiently many reliable agents are merged then in the limit the true state of the world is identified. we introduce a new postulate suited to the truth tracking issue. we identify some merging operators from the literature which satisfy it and other operators which do not.",nan
2010,majority merging: from boolean spaces to affine spaces,"this paper is centered on the problem of merging (possibly conflicting) information coming from different sources. though this problem has attracted much attention in propositional settings, propositional languages remain typically not expressive enough for a number of applications, especially when spatial information must be dealt with. in order to fill the gap, we consider a (limited) first-order logical setting, expressive enough for representing and reasoning about information modeled as half-spaces from metric affine spaces. in this setting, we define a family of distance-based majority merging operators which includes the propositional majority operator δdh,σ. we identify a subclass of interpretations of our representation language for which the result of the merging process can be computed and expressed as a formula.",nan
2010,semantics for the jason variant of agentspeak (plan failure and some internal actions),"jason is a platform for agent-based software development that is characterised both by being based on a programming language with formal semantics as well as having many language and platforms features that are very useful for practical programming, but not fully formalised. in this paper, we make significant progress in the direction of formalising the aspects of the variant of agentspeak that is interpreted by jason that were not included in previous work on giving formal semantics to agentspeak. in particular, we give semantics to the plan failure handling mechanism which is unique to jason, and also for some of the predefined internal actions that can alter an agent's mental state. such internal actions are essential for some aspects of bdi-based programming, such as checking or dropping current goals or intentions, and therefore need to be formally defined within the operational semantics of the language.",nan
2010,belief-goal relationships in possibilistic goal generation,"the way in which the relationships between beliefs, goals, and intentions are captured by a formalism can have a significant impact on the design of a rational agent. in particular, what rao and georgeff underline about the relationships between goals and beliefs is that it is reasonable to require a rational agent not to allow goal-belief inconsistency, while goal-belief incompleteness can be allowed.we study a theoretical framework, grounded in possibility theory, which (i) accounts for the aspects involved in representing and changing beliefs and goals, and (ii) obeys rao and georgeff's requirement. we propose a formalization of a possibilistic extension of bratman's asymmetry thesis to hold between goals and beliefs. finally, we show that our formalism avoids the side-effect and the transference problems.",nan
2010,predicting responsiveness of bdi agent,"a performance mark of a bdi agent is how fast it can react to and process incoming event sequences. to the best of our knowledge, few papers have been published about predicting an agent's average response time for an event sequence before the agent is applied in a real project. in this paper, we first introduce a simulation method. in simulation, a sequence of events with attributes, such as priorities and amounts of time needed to process the events, is input to the agent at designed insertion time. the events are processed by the agent according to the attributes. the statistics of processing time can be recorded. then we make some theoretical analysis to estimate the average response time when an agent processes a sequence of events based on probability and queueing theory. comparison experiments show that the results from analysis are quite matching with the results from simulation experiments. the analysis suggests a way to quickly estimate the performance of an agent if the attributes of the incoming event sequence are known in advance. the predicted average response time can help construct efficient bdi agents for various environments.",nan
2010,automating layouts of sewers in subdivisions,"an important part of the creation of a housing subdivision is the design and layout of sewers underneath the road. this is a challenging cost optimization problem in a continuous threedimensional space. in this paper, heuristic-search-based techniques are proposed for tackling this problem. the result is new algorithms that can quickly find near optimal solutions that offer important reductions in the cost of design and construction.",nan
2010,adaptive gaussian process for short-term wind speed forecasting,"we study the problem of short term wind speed prediction, which is a critical factor for effective wind power generation. this is a challenging task due to the complex and stochastic behavior of the wind environment. observing various periods in the wind speed time series present different patterns, we suggest a nonlinear adaptive framework to model various hidden dynamic processes. the model is essentially data driven, which leverages non-parametric heteroscdastic gaussian process to model relevant patterns for short term prediction. we evaluate our model on two different real world wind speed datasets from national data buoy center. we compare our results to state-of-arts algorithms to show improvement in terms of both root mean square error (rmse) and mean absolute percentage error (mape).",nan
2010,introducing personality into team dynamics,"if autonomous agents interact with people, they should achieve the suspension of disbelief in order to offer a good interaction experience to users. to achieve this it is important that the agents' behaviours are consistent with a given personality since people have a tendency to attribute personality to interactive artifacts. the concept of personality is also useful to create diversity in multi-agent simulations even if users do not directly engage in the interactions, for example, to explore different strategies in simulated societies. this paper presents a computational model of personality based on the five factor model of personality for the behaviour of social autonomous agents that interact in teamwork scenarios.",nan
2010,ep for efficient stochastic control with obstacles,"we address the problem of continuous stochastic optimal control in the presence of hard obstacles. due to the non-smooth character of the obstacles, the traditional approach using dynamic programming in combination with function approximation tends to fail. we consider a recently introduced special class of control problems for which the optimal control computation is reformulated in terms of a path integral. the path integral is typically intractable, but amenable to techniques developed for approximate inference. we argue that the variational approach fails in this case due to the non-smooth cost function. sampling techniques are simple to implement and converge to the exact results given enough samples. however, the infinite cost associated with hard obstacles renders the sampling procedures inefficient in practice. we suggest expectation propagation (ep) as a suitable approximation method, and compare the quality and efficiency of the resulting control with an mc sampler on a car steering task and a ball throwing task. we conclude that ep can solve these challenging problems much better than a sampling approach.",nan
2010,constraint-based controller synthesis in non-deterministic and partially observable domains,"controller synthesis consists in automatically building controllers taking as inputs observation data and returning outputs guaranteeing that the controlled system satisfies some desired properties. in system specification, these properties may be safety properties specifying that some conditions must always hold. in planning, they express that the evolution of the controlled system must terminate in a goal state. in this paper, we propose a generic approach able to synthesize memoryless or finite-memory controllers for both safety-oriented and goal-oriented control problems. this approach relaxes some restrictive assumptions made by existing work on controller synthesis with non-determinism and partial observability and is shown to induce potentially significant gains. the proposed “simulate and branch” algorithm consists in exploring the possible evolutions of the controlled system and in adding new control elements when uncovered states are discovered. the approach developed is constraint-based in the sense that control problems are formulated using the flexibility of constraint programming languages and that our implementation uses the gecode constraint programming library.",nan
2010,decision-theoretic optimal sampling in hidden markov random fields,"computation of the most probable explanation (mpe) when probabilistic knowledge is expressed as a factored distribution is a classical ai reasoning problem: complete evidence is available about the values of some of the variables which are observed, and the problem consists in finding the most probable assignment of the remaining variables given the evidence. however, optimising the choice of the variables to observe (the sample) in order to maximise the mpe probability is a less classical and more difficult problem. in this article we tackle this question of optimal sampling in structured problems under limited budget, within the framework of hidden markov random fields (hmrf). the value of a sample (which we seek to optimise) is the expectation, over all possible sample outputs (observations), of the mpe probability. the contributions of this article are: i) an original probabilistic model for optimal sampling in hmrf ii) computational complexity results about this problem, leading in particular to approximability/inapproximability results and iii) an exact solution algorithm and two approximate solution algorithms of decreasing time complexity, which we empirically evaluate on a problem of spatial sampling for occurrence map restoration.",nan
2010,"metric propositional neighborhood logics: expressiveness, decidability, and undecidability","interval temporal logics formalize reasoning about interval structures over (usually) linearly ordered domains, where time intervals are the primitive ontological entities and truth of formulae is defined relative to time intervals, rather than time points. in this paper, we introduce and study metric propositional neighborhood logic (mpnl) over natural numbers. mpnl features two modalities referring, respectively, to an interval that is “met by” the current one and to an interval that “meets” the current one, plus an infinite set of length constraints, regarded as atomic propositions, to constrain the lengths of intervals. we argue that mpnl can be successfully used in different areas of artificial intelligence to combine qualitative and quantitative interval temporal reasoning, thus providing a viable alternative to well-established logical frameworks such as duration calculus. we show that mpnl is decidable in double exponential time and expressively complete with respect to a well-defined subfragment of the two-variable fragment fo2[n, =, <, s] of first-order logic for linear orders with successor function, interpreted over natural numbers. moreover, we show that mpnl can be extended in a natural way to cover full fo2[n, =, <, s], but, unexpectedly, the latter (and hence the former) turns out to be undecidable.",nan
2010,an axiom system for a spatial logic with convexity,"this paper presents a part of work in progress on axiomatizing a spatial logic with convexity and inclusion predicates (hereinafter called convexity logic), with some intended interpretation over the real plane. more formally, let lconv,≤ be a language of first order logic and two non-logical primitives: conv (interpreted as a property of a set of being convex) and ≤ (interpreted as the set inclusion relation). we let variables range over regular open rational polygons in the real plane (denoted roq(r2)). we call the tuple m = 〈roq, conv, ≤〉 — where primitives are defined as indicated above — a standard model. we propose an axiomatization of the theory of m and prove soundness and completeness for this axiomatization.",nan
2010,optimal tableaux for conditional logics with cautious monotonicity,"conditional logics capture default entailment in a modal framework in which non-monotonic implication is a first-class citizen, and in particular can be negated and nested. there is a wide range of axiomatizations of conditionals in the literature, from weak systems such as the basic conditional logic ck, which allows only for equivalent exchange of conditional antecedents, to strong systems such as burgess' system [sscr    ], which imposes the full kraus-lehmann-magidor properties of preferential logic. while tableaux systems implementing the actual complexity of the logic at hand have recently been developed for several weak systems, strong systems including in particular disjunction elimination or cautious monotonicity have so far eluded such efforts; previous results for strong systems are limited to semantics-based decision procedures and completeness proofs for hilbert-style axiomatizations. here, we present tableaux systems of optimal complexity pspace for several strong axiom systems in conditional logic, including system [sscr    ]; the arising decision procedure for system [sscr    ] is implemented in the generic reasoning tool coloss.",nan
2010,linear logic for non-linear storytelling,"whilst narrative representations have played a prominent role in ai research, there has been a renewed interest in the topic with the development of interactive narratives. a typical approach aims at generating narratives from baseline action representations, most often using planning techniques. however, this research has developed empirically, often as an application of planning. in this paper, we explore a more rigorous formalisation of narrative concepts, both at the action level and at the plot level. our aim is to investigate how to bridge the gap between action descriptions and narrative concepts, by considering the latter from the perspective of resource consumption and causality. we propose to use linear logic, often introduced as a logic of resources, for it provides, through linear implication, a better description of causality than in classical and intuitionistic logic. besides advances in the fundamental principles of narrative formalisation, this approach can support the formal validation of scenario description as a preliminary step to their implementation via other computational formalisms.",nan
2010,problog technology for inference in a probabilistic first order logic,"we introduce first order problog, an extension of first order logic with soft constraints where formulas are guarded by probabilistic facts. the paper defines a semantics for foproblog, develops a translation into problog, a system that allows a user to compute the probability of a query in a similar setting restricted to horn clauses, and reports on initial experience with inference.",nan
2010,a note on the complexity of some multiobjective a* search algorithms,"this paper studies the complexity of two different algorithms proposed as extensions of a* for multiobjective search: moa* and namoa*. it is known that, for any given problem, namoa* requires the consideration of no more alternatives than moa* when provided with the same heuristic information.in this paper we show that, in fact, expansions performed by moa* can be many more than those demanded by the problem, and hence than those performed by namoa*. more specifically, we show a sequence of problems whose size grows linearly such that the number of expansions performed by namoa* grows also linearly, but the number of expansions performed by moa* grows exponentially. therefore, there are problems where namoa* performs exponentially better than moa*.",nan
2010,contract search: heuristic search under node expansion constraints,"in this work, we present a heuristic search technique (contract search) which can be automatically adapted for a specified node expansion limitation. we analyze the node expansion properties of best first search and propose a probabilistic model (rank profile) to characterize heuristic search under restricted expansions. we identify the basic properties of the rank profile and establish its relation with the search space configuration. in contract search, we use the rank profile model to formulate an optimal strategy to choose level dependent restriction bounds maximizing the probability of obtaining the goal node under the specified contract. experimental comparison with anytime search techniques like ara* and beam search shows that contract search outperforms these techniques over a range of constraint specifications.",nan
2010,fast local search for fuzzy job shop scheduling,"in the sequel, we propose a new neighbourhood structure for local search for the fuzzy job shop scheduling problem. this is a variant of the well-known job shop problem, with uncertainty in task durations modelled using fuzzy numbers and where the goal is to minimise the expected makespan of the resulting schedule. the new neighbourhood structure is based in changing the relative order of subsequences of tasks within critical blocks. we study its theoretical properties and provide a makespan estimate which allows to select only feasible neighbours while covering a greater portion of the search space than a previous neighbourhood from the literature. despite its larger search domain, experimental results show that this new structure notably reduces the computational load of local search with respect to the previous neighbourhood while maintaining or even improving solution quality.",nan
2010,relative-order abstractions for the pancake problem,"the pancake problem is a famous search problem where the objective is to sort a sequence of objects (pancakes) through a minimal number of prefix reversals (flips). the best approaches for the problem are based on heuristic search with abstraction (pattern database) heuristics. we present a new class of abstractions for the pancake problem called relative-order abstractions. relative-order abstractions have three advantages over the object-location abstractions considered in previous work. first, they are size-independent, i.e., do not need to be tailored to a particular instance size of the pancake problem. second, they are more compact in that they can represent a larger number of pancakes within abstractions of bounded size. finally, they can exploit symmetries in the problem specification to allow multiple heuristic lookups, significantly improving search performance over a single lookup. our experiments show that compared to object-location abstractions, our new techniques lead to an improvement of one order of magnitude in runtime and up to three orders of magnitude in the number of generated states.",nan
2010,isac – instance-specific algorithm configuration,"we present a new method for instance-specific algorithm configuration (isac). it is based on the integration of the algorithm configuration system gga and the recently proposed stochastic offline programming paradigm. isac is provided a solver with categorical, ordinal, and/or continuous parameters, a training benchmark set of input instances for that solver, and an algorithm that computes a feature vector that characterizes any given instance. isac then provides high quality parameter settings for any new input instance. experiments on a variety of different constrained optimization and constraint satisfaction solvers show that automatic algorithm configuration vastly outperforms manual tuning. moreover, we show that instance-specific tuning frequently leads to significant speed-ups over instance-oblivious configurations.",nan
2010,using background knowledge to support coreference resolution,"systems based on statistical and machine learning methods have been shown to be extremely effective and scalable for the analysis of large amount of textual data. however, in the recent years, it becomes evident that one of the most important direction of improvement in natural language processing (nlp) tasks, like word sense disambiguation, coreference resolution, relation extraction, and other tasks related to knowledge extraction, is by exploiting semantics. while in the past, the unavailability of rich and complete semantic descriptions constituted a serious limitation of their applicability, nowadays, the semantic web made available a large amount of logically encoded information (e.g. ontologies, rdf(s)-data, linked data, etc.), which constitute a valuable source of semantics. however, web semantics cannot be easily plugged into machine learning systems. therefore the objective of this paper is to define a reference methodology for combining semantics information available in the web under the form of logical theories, with statistical methods for nlp. the major problems that we have to solve to implement our methodology concern (i) the selection of the correct and minimal knowledge among the large amount available in the web, (ii) the representation of uncertain knowledge, and (iii) the resolution and the encoding of the rules that combine knowledge retrieved from semantic web sources with semantics in the text. in order to evaluate the appropriateness of our approach, we present an application of the methodology to the problem of intra-document coreference resolution, and we show by means of some experiments on the ace 2005 dataset, how the injection of knowledge is correlated to the improvement of the performance of our approach on this tasks.",nan
2010,detecting ironic intent in creative comparisons,"irony is an effective but challenging mode of communication that allows a speaker to express sentiment-rich viewpoints with concision, sharpness and humour. irony is especially common in online documents that express subjective and deeply-felt opinions, and thus represents a significant obstacle to the accurate analysis of sentiment in web texts. in this paper we look at one commonly used framing device for linguistic irony – the simile – to show how irony is often marked in ways that make it computationally feasible to detect. we conduct a very large corpus analysis of web-harvested similes to identify the most interesting characteristics of ironic comparisons, and provide an empirical evaluation of a new algorithm for separating ironic from non-ironic similes.",nan
2010,cross-domain contextualization of sentiment lexicons,"the simplicity of using web 2.0 platforms and services has resulted in an abundance of user-generated content. a significant part of this content contains user opinions with clear economic relevance - customer and travel reviews, for example, or the articles of well-known and respected bloggers who influence purchase decisions. analyzing and acting upon user-generated content is becoming imperative for marketers and social scientists who aim to gather feedback from very large user communities. sentiment detection, as part of opinion mining, supports these efforts by identifying and aggregating polar opinions - i.e., positive or negative statements about facts. for achieving accurate results, sentiment detection requires a correct interpretation of language, which remains a challenging task due to the inherent ambiguities of human languages. particular attention has to be directed to the context of opinionated terms when trying to resolve these ambiguities. contextualized sentiment lexicons address this need by considering the sentiment term's context in their evaluation but are usually limited to one domain, as many contextualizations are not stable across domains. this paper introduces a method which identifies unstable contextualizations and refines the contextualized sentiment dictionaries accordingly, eliminating the need for specific training data for each individual domain. an extensive evaluation compares the accuracy of this approach with results obtained from domain-specific corpora.",nan
2010,learning to author text with textual cbr,"textual reuse is an integral part of textual case-based reasoning (tcbr) which deals with solving new problems by reusing previous similar problem-solving experiences documented as text. we investigate the role of text reuse for text authoring applications that involve feedback or review generation. generally providing feedback in the form of assigning a rating from a likert scale is far easier compared to articulating explanatory feedback as text. when previous feedback generated about the same or similar objects are maintained as cases, there is opportunity for knowledge reuse. in this paper, we show how compositional and transformational adaptation techniques can be applied once sentences in a given case are aligned to relevant structured attribute values. three text reuse algorithms are introduced and evaluated on a dataset gathered from online hotel reviews from tripadvisor. here cases consists of both structured sub-rating attributes together with textual feedback. generally, aligned sentences linked to similar sub-rating values are clustered together and prototypical sentences are then extracted to enable reuse across similar authors. experiments show a close similarity between our proposed texts and actual human edited review text. we also found that problems with variability in vocabulary are best addressed when prototypes are formulated from larger sets of similar sentences in contrast to smaller sets from local neighbourhoods.",nan
2010,classifier ensemble using multiobjective optimization for named entity recognition,"in this paper, we report a multiobjective optimization (moo) based classifier ensemble technique to solve the problem of named entity recognition (ner). our underlying assumption is that rather than searching for the best feature set for a particular classifier, ensembling of several classifiers which are trained using different feature representations could be a more fruitful approach. but, it is very crucial to select the appropriate classifiers that can participate in final ensembling. here, we propose a new technique for classifier ensembling based on moo that can simultaneously optimize several different classification measures. maximum entropy (me) framework is used to generate a number of classifiers by considering the various combinations of the available features. the proposed technique is evaluated for two resource constrained languages, namely bengali and hindi. evaluation results yield the recall, precision and f-measure values of 72.34%, 84.94% and 78.13%, respectively for bengali, and 64.93%, 83.29% and 72.97%, respectively for hindi. experiments also show that the classifier ensemble identified by the proposed multiobjective based approach outperforms all the individual classifiers, two different baseline ensembles and a classifier ensemble identified by the single objective genetic algorithm (ga) based approach.",nan
2010,mining outliers with adaptive cutoff update and space utilization (racas),"recently the efficiency of an outlier detection algorithm orca was improved by rcs (randomization with faster cutoff update and space utilization after pruning), which changes the frequencies of updating the cutoff value and reclaiming the memory space at some pre-specified time. how and when to change the frequencies were only determined empirically. however, the optimal setting may vary for different data sets and computers with different cpu and disk i/o performance. in this paper, we theoretically formulate two methods to further reduce the execution time of rcs by dynamically adapting the frequencies at each step to different data sets and computers with different cpu and disk i/o performance. we conducted experiments on a kdd cup real data set from a network intrusion detection problem under different conditions. the results show that our substantial time-saving from optimized orca is up to five times that of rcs and increases with the relative disk i/o cost, the percentage of outliers to find and the data set size.",nan
2010,feature selection by approximating the markov blanket in a kernel-induced space,"the proposed feature selection method aims to find a minimum subset of the most informative variables for classification/regression by efficiently approximating the markov blanket which is a set of variables that can shield a certain variable from the target. instead of relying on the conditional independence test or network structure learning, the new method uses hilbert-schmidt independence criterion as a measure of dependence among variables in a kernel-induced space. this allows effective approximation of the markov blanket that consists of multiple dependent features rather than being limited to a single feature. in addition, the new method can remove both irrelevant and redundant features at the same time. this method for discovering the markov blanket is applicable to both discrete and continuous variables, whereas previous methods cannot be used directly for continuous features and therefore are not applicable to regression problems. experimental evaluations on synthetic and benchmark classification and regression datasets provide evidence that the new feature selection method can remove useless variables in low and in high dimensional problems more accurately than existing markov blanket based alternatives.",nan
2010,recognising agent behaviour during variable length activities,"in this paper we present a new method for obtaining situation awareness via the automatic recognition of agent behaviours. in contrast to many other approaches, the presented method models different behaviour durations without using a fixed classification window, and does not require a distribution of behaviour durations. we introduce the variable window layered hidden markov model (vw-lhmm) as an extension of the lhmm to specifically address behaviours with irregular duration. we validate our approach by simulating three high-level behaviours within the harbour and coastline security domain. we compare performance against the lhmm and show that our approach provides a 10% improvement in classification accuracy, in addition to earlier classification.",nan
2010,continuous conditional random fields for regression in remote sensing,conditional random fields (crf) are widely used for predicting output variables that have some internal structure. most of the crf research has been done on structured classification where the outputs are discrete. in this study we propose a crf probabilistic model for structured regression that uses multiple non-structured predictors as its features. we construct features as squared prediction errors and show that this results in a gaussian predictor. learning becomes a convex optimization problem leading to a global solution for a set of parameters. inference can be conveniently conducted through matrix computation. experimental results on the remote sensing problem of estimating aerosol optical depth (aod) provide strong evidence that the proposed crf model successfully exploits the inherent spatio-temporal properties of aod data. the experiments revealed that crf are more accurate than the baseline neural network and domain-based predictors.,nan
2010,combining local and global knn with cotraining,"semi-supervised learning is a machine learning paradigm in which the induced hypothesis is improved by taking advantage of unlabeled data. it is particularly useful when labeled data is scarce. cotraining is a widely adopted semi-supervised approach that assumes availability of two views of the training data a restrictive assumption for most real world tasks. in this paper, we propose a one-view cotraining approach that combines two different k-nearest neighbors (knn) strategies referred to as global and local k-nn. in global knn, the nearest neighbors selected to classify a new instance are given by the training examples which include this instance as one of their own k-nearest neighbors. in local knn, on the other hand, the neighborhood considered when classifying a new instance is computed with the traditional knn approach. we carried out experiments showing that a combination of these strategies significantly improves the classification accuracy in cotraining, particularly when one single view of training data is available. we also introduce an optimized algorithm to cope with time complexity of computing the global knn, which enables tackling real classification problems.",nan
2010,multi grain sentiment analysis using collective classification,"multi grain sentiment analysis is the task of simultaneously classifying sentiment expressed at different levels of granularity, as opposed to single level at a time. models built for multi grain sentiment analysis assume fully labeled corpus at fine grained level or coarse grained level or both. huge amount of online reviews are not fully labeled at any of the levels, but are partially labeled at both the levels. we propose a multi grain collective classification framework to not only exploit the information available at all the levels but also use intra dependencies at each level and inter dependencies between the levels. we demonstrate empirically that the proposed framework enables better performance at both the levels compared to baseline approaches.",nan
2010,temporal relations learning with a bootstrapped cross-document classifier,"the ability to accurately classify temporal relation between events is an important task for a large number of natural language processing applications such as question answering (qa), summarization, and information extraction. this paper presents a weakly-supervised machine learning approach for classification of temporal relation between events. in the first stage, the algorithm learns a general classifier from an annotated corpus. then, it applies the hypothesis of “one type of temporal relation per discourse” and expands the scope of “discourse” from a single document to a cluster of topically-related documents. by combining the global information of such a cluster with local decisions of a general classifier, we propose a novel bootstrapping cross-document classifier to extract temporal relations between events. our experiments show that without any additional annotated data, the accuracy of the proposed algorithm is at least 7% higher than that of the pattern based state of the art system.",nan
2010,describing the result of a classifier to the end-user: geometric-based sensitivity,"this paper addresses the issue of supporting the end-user of a classifier, when it is used as a decision support system, to classify new cases. we consider several kinds of classifiers: statistical or machine learning classifiers, which are built on data, but also direct model-based classifiers that are built to solve a particular problem (like in viability or control problems). the end-user relies mainly on global information (like error rates or global sensitivity analysis) to assess the quality of the result given by the system. class membership probability, if available, describes certainly the local statistical viewpoint. but it doesn't take into account other contextual information: cases with high value of class membership probability can also be close to the decision boundary. in the case of numerical state space, we propose to use the decision boundary of the classifier (which always exists, even implicitly), to describe the situation of a particular case: the distance of a case to the decision boundary measures the robustness of the decision to a change in the input data. other geometric concepts, such as the maximal maximal ball, can present a precise picture of the situation to the end-user. we show the interest of such a geometric study on different examples.",nan
2010,soft nearest convex hull classifier,"consider the classification task of assigning a test instance to one of two or more possible classes. an intuitive way to proceed is to assign the instance to that class, to which the distance is minimal. if one considers the distance to the convex hull of a class as a distance measure, then the resulting classification method is the nearest convex hull (nch) classifier. there are two key issues with this method per se that severely restrict its applicability, which we solve in this paper: first, how to handle class overlap, and second, how to provide (nonlinear) solutions with better generalization ability. the first problem is handled via using so-called kernel functions and slack variables. the second problem is dealt with using a penalization term that suppresses too complex solutions. we call the resulting method the soft-nch classifier. in spirit and computationally the method is close to the popular support vector machine (svm) classifier and can be viewed as an instance-based large-margin classification technique. advantages of the soft-nch classifier include its robustness to outliers, good generalization ability and naturally easy handling of multi-class problems. we compare the performance of soft-nch against state-of-art techniques and report promising results.",nan
2010,using domain knowledge to guide lattice-based complex data exploration,"in this paper we propose an approach which combines semantic resources and formal concept analysis to deal with heterogenous data sets represented as many-valued (mv) formal contexts. we define a new galois connection considering the semantic relationships between attribute values in a mv context. the semantic relationships are used to calculate the similarity between attribute values to decide whether an attribute is shared by a set of objects or not. then, based on this galois connection, we define mv formal concepts and mv concept lattices. depending on a chosen similarity threshold, mv concept lattices may have different levels of precision. we take advantage of this feature to browse the content of a biological databases repository in a dynamic and progressive way. the browsing process combines the navigation in several mv concept lattices and allows zooming operations by switching between mv concept lattices with higher or lower precision.",nan
2010,adaptive branching for constraint satisfaction problems,"the two standard branching schemes for csps are d-way and 2-way branching. although it has been shown that in theory the latter can be exponentially more effective than the former, there is a lack of empirical evidence showing such differences. to investigate this, we initially make an experimental comparison of the two branching schemes over a wide range of benchmarks. experimental results verify the theoretical gap between d-way and 2-way branching as we move from a simple variable ordering heuristic like dom to more sophisticated ones like dom/ddeg. however, perhaps surprisingly, experiments also show that when state-of-the-art variable ordering heuristics like dom/wdeg are used then d-way can be clearly more efficient than 2-way branching in many cases. motivated by this observation, we develop two generic heuristics that can be applied at certain points during search to decide whether 2-way branching or a restricted version of 2-way branching, which is close to d-way branching, will be followed. the application of these heuristics results in an adaptive branching scheme. experiments with instantiations of the two generic heuristics confirm that search with adaptive branching outperforms search with a fixed branching scheme on a wide range of problems.",nan
2010,symmetries of symmetry breaking constraints,"symmetry is an important feature of many constraint programs. we show that any problem symmetry acting on a set of symmetry breaking constraints can be used to break symmetry. different symmetries pick out different solutions in each symmetry class. this simple but powerful idea can be used in a number of different ways. we describe one application within model restarts, a search technique designed to reduce the conflict between symmetry breaking and the branching heuristic. in model restarts, we restart search periodically with a random symmetry of the symmetry breaking constraints. experimental results show that this symmetry breaking technique is effective in practice on some standard benchmark problems.",nan
2010,solving pseudo-boolean modularity constraints,"this paper introduces new solving strategies for the resolution of pseudo-boolean modularity (pbmod) constraints. in particular, we deal with modular arithmetic constraints on boolean variables. on the one hand, we analyze translations to pseudo-boolean (pb) constraints and apply pb solvers. we also look at those pb solvers that have shown that a transformation to the sat problem can be an effective solving strategy for pb problems. among the existing translation techniques we focus on the encoding based on a network of sorters. we extend this encoding technique to generate directly a sat formula from the pbmod constraints. we compare our approach to other standard techniques such as satisfiability modulo theories (smt) solvers with support for the quantifier free linear integer arithmetic (qf_lia) theory and the glpk package for mixed integer programming. in order to conduct our experimental investigation we present a generator of random pbmod constraints and study the impact of the several parameters on the hardness of the instances.",nan
2010,learning when to use lazy learning in constraint solving,"learning in the context of constraint solving is a technique by which previously unknown constraints are uncovered during search and used to speed up subsequent search. recently, lazy learning, similar to a successful idea from satisfiability modulo theories solvers, has been shown to be an effective means of incorporating constraint learning into a solver. although a powerful technique to reduce search in some circumstances, lazy learning introduces a substantial overhead, which can outweigh its benefits. hence, it is desirable to know beforehand whether or not it is expected to be useful. we approach this problem using machine learning (ml). we show that, in the context of a large benchmark set, standard ml approaches can be used to learn a simple, cheap classifier which performs well in identifying instances on which lazy learning should or should not be used. furthermore, we demonstrate significant performance improvements of a system using our classifier and the lazy learning and standard constraint solvers over a standard solver. through rigorous cross-validation across the different problem classes in our benchmark set, we show the general applicability of our learned classifier.",nan
2010,ltl goal specifications revisited,"the language of linear temporal logic (ltl) has been proposed as a formalism for specifying temporally extended goals and search control constraints in planning. however, the semantics of ltl is defined wrt. infinite state sequences, while a finite plan generates only a finite trace. this necessitates the use of a finite trace semantics for ltl. a common approach is to evaluate ltl formulae on an infinite extension of the finite trace, obtained by infinitely repeating the last state. we study several aspects of this finite ltl se mantics: we show its satisfiability problem is pspace-complete (same as normal ltl), show that it complies with all equivalence laws that hold under standard (infinite) ltl semantics, and compare it with other finite trace semantics for ltl proposed in planning and in runtime verification. we also examine different mechanisms for determining whether or not a finite trace satisfies or violates an ltl formula, interpreted using the infinite extension semantics.",nan
2010,on the verification of very expressive temporal properties of non-terminating golog programs,"the agent programming language golog and the underlying situation calculus have become popular means for the modelling and control of autonomous agents such as mobile robots. although such agents' tasks are typically open-ended, little attention has been paid so far to the analysis of non-terminating golog control programs. recently we therefore introduced a logic that allows to express properties of golog programs using operators from temporal logics while retaining the full first-order expressiveness of the situation calculus. combining ideas from classical symbolic model checking with first-order theorem proving we presented a verification method for a restricted subclass of temporal properties. in this paper, we extend this work by considering arbitrary temporal formulas. our algorithm is inspired by classical [cscr    ][tscr    ][lscr    ]* model checking, but introduces techniques to cope with arbitrary first-order quantification.",nan
2010,the complexity of handling minimal solutions in logic-based abduction,"logic-based abduction is an important reasoning method with many applications in artificial intelligence including diagnosis, planning, and configuration. the goal of an abduction problem is to find a “solution”, i.e., an explanation for some observed symptoms. usually, many solutions exist, and one is often interested in minimal ones only. previous definitions of “solutions” to an abduction problem tacitly made an open-world assumption. however, as far as minimality is concerned, this assumption may not always lead to the desired behavior. to overcome this problem, we propose a new definition of solutions based on a closed-world approach. moreover, we also introduce a new variant of minimality where only a part of the hypotheses is subject to minimization. a thorough complexity analysis reveals the close relationship between these two new notions as well as the differences compared with previous notions of solutions.",nan
2010,abduction of distributed theories through local interactions,"what happens when distributed sources of information (agents) hold and acquire information locally, and have to communicate with neighbouring agents in order to refine their hypothesis regarding the actual global state of this environment? this question occurs when it is not be possible (e. g. for practical or privacy concerns) to collect observations and knowledge, and centrally compute the resulting theory. in this paper, we assume that agents are equipped with full clausal theories and individually face abductive tasks, in a globally consistent environment. we adopt a learner/critic approach. previous work in this line mostly relied on some assumptions of compositionality (which allow to treat each piece of exchanged information separately). because no shared background knowledge is assumed to start with, this does not hold here. we design a protocol guaranteeing convergence to a situation “sufficiently” satisfying as far as consistency of the system is concerned, and discuss its other properties.",nan
2010,a neat way for evolving echo state networks,"the reinforcement learning (rl) paradigm is an appropriate formulation for agent, goal-directed, sequential decision making. in order though for rl methods to perform well in difficult, complex, real-world tasks, the choice and the architecture of an appropriate function approximator is of crucial importance. this work presents a method of automatically discovering such function approximators, based on a synergy of ideas and techniques that are proven to be working on their own. using echo state networks (esns) as our function approximators of choice, we try to adapt them, by combining evolution and learning, for developing the appropriate ad-hoc architectures to solve the problem at hand. the choice of esns was made for their ability to handle both non-linear and non-markovian tasks, while also being capable of learning online, through simple gradient descent temporal difference learning. for creating networks that enable efficient learning, a neuroevolution procedure was applied. appropriate topologies and weights were acquired by applying the neuroevolution of augmented topologies (neat) method as a meta-search algorithm and by adapting ideas like historical markings, complexification and speciation, to the specifics of esns. our methodology is tested on both supervised and reinforcement learning testbeds with promising results.",nan
2010,unsupervised layer-wise model selection in deep neural networks,"deep neural networks (dnn) propose a new and efficient ml architecture based on the layer-wise building of several representation layers. a critical issue for dnns remains model selection, e.g. selecting the number of neurons in each dnn layer. the hyper-parameter search space exponentially increases with the number of layers, making the popular grid search-based approach used for finding good hyper-parameter values intractable. the question investigated in this paper is whether the unsupervised, layer-wise methodology used to train a dnn can be extended to model selection as well. the proposed approach, considering an unsupervised criterion, empirically examines whether model selection is a modular optimization problem, and can be tackled in a layer-wise manner. preliminary results on the mnist data set suggest the answer is positive. further, some unexpected results regarding the optimal size of layers depending on the training process, are reported and discussed.",nan
2010,acquisition of grammar in autonomous artificial systems,"over the past several decades, psycholinguists have gained countless insights into the process of child language acquisition. can these findings be used for the development of language competence in autonomous artificial systems? this paper reports on our attempt to apply insights from developmental psychology in order to enable artificial systems to acquire language. we consider a comprehensive chain of computational processes, starting from conceptualization and extending through language generation and interpretation, and show how they can be intertwined to allow for acquisition of complex aspects of grammar.",nan
2010,open-ended grounded semantics,"artificial agents trying to achieve communicative goals in situated interactions in the real-world need powerful computational systems for conceptualizing their environment. in order to provide embodied artificial systems with rich semantics reminiscent of human language complexity, agents need ways of both conceptualizing complex compositional semantic structure and actively reconstructing semantic structure, due to uncertainty and ambiguity in transmission. furthermore, the systems must be open-ended and adaptive and allow agents to adjust their semantic inventories in order to reach their goals. this paper presents recent progress in modeling open-ended, grounded semantics through a unified software system that addresses these problems.",nan
2010,adaptive markov logic networks: learning statistical relational models with dynamic parameters,"statistical relational models, such as markov logic networks, seek to compactly describe properties of relational domains by representing general principles about objects belonging to particular classes. models are intended to be independent of the set of objects to which these principles can be applied, and it is assumed that the principles will soundly generalize across arbitrary sets of objects. in this paper, we point out limitations of models that seek to represent the corresponding principles with a fixed set of parameters and discuss the conditions under which the soundness of fixed parameters is indeed questionable. we propose a novel representation formalism called adaptive markov logic networks to allow more flexible representations of relational domains, which involve parameters that are dynamically adjusted to fit the properties of an instantiation by phrasing the model's parameters as functions over attributes of the instantiation at hand. we empirically demonstrate the value of our learning and representation system on a simple but well-motivated example domain.",nan
2010,min-based causal possibilistic networks: handling interventions and analyzing the possibilistic counterpart of jeffrey's rule of conditioning,"this paper deals with two important issues related to the handling of uncertain and causal information in a qualitative (or min-based) possibility theory framework. the first issue addresses encoding interventions using the possibilistic conditioning under uncertain inputs problem. more precisely, we analyze the min-based possibilistic counterpart of jeffrey's rule of conditioning and point out that contrary to the probabilistic setting, this rule does not guarantee the existence of a solution satisfying the kinematics conditions. then we show that this rule can naturally encode the concept of interventions in causal graphical models. surprisingly enough, we show that when dealing with interventions the min-based counterpart of jeffrey's rule provides a unique solution. the second issue deals with the efficient handling of sets of observations and interventions in min-based possibilistic networks, where we propose a solution based on a series of equivalent and efficient transformations on the initial causal graph.",nan
2010,on testing answer-set programs,"answer-set programming (asp) is a well-acknowledged paradigm for declarative problem solving, yet comparably little effort has been spent on the investigation of methods to support the development of answer-set programs. in particular, systematic testing of programs, constituting an integral part of conventional software development, has not been discussed for asp thus far. in this paper, we fill this gap and develop notions enabling the structural testing of answer-set programs, i.e., we address testing based on test cases that are chosen with respect to the internal structure of a given answer-set program. more specifically, we introduce different notions of coverage that measure to what extent a collection of test inputs covers certain important structural components of the program. in particular, we introduce metrics corresponding to path and branch coverage from conventional testing. we also discuss complexity aspects of the considered notions and give strategies how test inputs that yield increasing (up to total) coverage can be automatically generated.",nan
2010,on semantic update operators for answer-set programs,"logic programs under the stable models semantics, or answer-set programs, provide an expressive rule based knowledge representation framework, featuring formal, declarative and well-understood semantics. however, handling the evolution of rule bases is still a largely open problem. the agm framework for belief change was shown to give inappropriate results when directly applied to logic programs under a nonmonotonic semantics such as the stable models. most approaches to address this issue, developed so far, proposed update operators based on syntactic conditions for rule rejection.more recently, agm revision has been successfully applied to a significantly more expressive semantic characterisation of logic programs based on se models. this is an important step, as it changes the focus from the evolution of a syntactic representation of a rule base to the evolution of its semantic content.in this paper, we borrow results from the area of belief update to tackle the problem of updating (instead of revising) logic programs. we prove a representation theorem which makes it possible to constructively define any operator satisfying a set of postulates derived from katsuno and mendelzon's postulates for belief update. we define a specific operator based on this theorem and compare the behaviour of this operator with syntactic update operators defined in the literature. perhaps surprisingly, we uncover a very serious drawback in a large class of semantic update operators to which it belongs.",nan
2010,completeness-preserving pruning for optimal planning,"this paper focuses on efficient methods for pruning the state space in cost-optimal planning. the use of heuristics to guide search and prune irrelevant branches has been widely and successfully explored. however, heuristic computation at every node in the search space is expensive, and even near perfect heuristics still leave a large portion of the search space to be explored [9]. using up-front analysis to reduce the number of nodes to be considered therefore has great potential. our contributions are not concerned with heuristic guidance, rather, with orthogonal completeness-preserving pruning techniques that reduce the number of states a planner must explore to find an optimal solution. we present results showing that our techniques can improve upon state-of-the-art optimal planners, both when using blind search and importantly in conjunction with modern heuristics, specifically hlm-cut [8]. our techniques are not limited to optimal planning and can also be applied in satisfycing planning.",nan
2010,project scheduling as a disjunctive temporal problem,"the main result of the paper is the reduction of the rcpsp/max problem to a disjunctive temporal problem that allows customization of specific properties within a backtracking search procedure for makespan optimization. in addition, a branching strategy is proposed able to deduce new constraints which explicitly represent infeasible or useless search paths. a new variable ordering heuristic (called clustering) is also used which provides a further boosting to the algorithm's effectiveness.",nan
2010,a decision-theoretic approach to cooperative control and adjustable autonomy,"cooperative control can help overcome the limitations of autonomous systems (as) by introducing a supervision unit (su) (human or another system) into the control loop and creating adjustable autonomy. we present a decision-theoretic approach to accomplish this using mixed markov decision processes (mi-mdps). the solution is an optimal plan that tells the as what actions to perform as well as when to request su attention or transfer control to the su. this provides a varying degree of autonomy, particularly suitable for robots exploring a domain with regions that are too complex or risky for autonomous operation, or intelligent vehicles operating in heavy traffic.",nan
2010,learning action effects in partially observable domains,"we investigate the problem of learning action effects in partially observable strips planning domains. our approach is based on a voted kernel perceptron learning model, where action and state information is encoded in a compact vector representation as input to the learning mechanism, and resulting state changes are produced as output. our approach relies on deictic features that assume an attentional mechanism that reduces the size of the representation. we evaluate our approach on a number of partially observable planning domains, and show that it can quickly learn the dynamics of such domains, with low average error rates. we show that our approach handles noisy domains, conditional effects, and that it scales independently of the number of objects in a domain.",nan
2010,a probabilistic approach to appearance-based localization and mapping,"recent approaches to appearance-based robot localization and mapping have relied on a topological representation of the environment and loop closing procedures to build correct maps. because these solutions do not separate the problem of localization in mapped areas from the exploration of new locations, they require that loop-closing detection is performed continuously, inducing an extra computational burden on the robot. in this paper, we introduce a modified version of the markov localization algorithm which avoids this condition by employing procedures that detect when novel areas are being explored or already mapped areas are revisited.",nan
2010,behavior-analysis and -prediction for agents in real-time and dynamic adversarial environments,"we present an approach for recognition and subsequent prediction of spatio-temporal patterns in a physical real-time environment. the motivation is to provide a domain-independent approach for the analysis of agent's behavior in adversarial multi-agent scenarios. the goal is to create an opponent- specific model, which is used for behavior prediction. we develop a framework for representing a set of hierarchically structured facts, events and actions using temporal logic. recognition, learning, and prediction is performed using a probabilistic approach utilizing bayesian networks. the system is applied to the domain of the robocup 3d simulation league and evaluated with regard to the recognition-, prediction- and realtime capabilities.",nan
2010,integrating probability constraints into bayesian nets,"this paper presents a formal convergence proof for eipfp, an algorithm that integrates low dimensional probabilistic constraints into a bayesian network (bn) based on the mathematical procedure ipfp. it also extends e-ipfp to deal with constraints that are inconsistent with each other or with the bn structure.",nan
2010,probabilistic logic with conditional independence formulae,we investigate a probabilistic propositional logic that is an extension of the logic proposed by fagin et al. [4] with conditional independence formulae. we give an axiomatization which we show to be complete for the kind of inferences allowed by bayesian networks.,nan
2010,towards learning to rank in description logics,"in the context of knowledge bases expressed in description logics, a method for learning functions that can predict the ranking of resources encoding some preference criteria implicitly encoded through examples of rated individuals. the method relies on a kernelized version of the perceptron ranking algorithm which is suitable for batch but also online problem settings.",nan
2010,exploiting the accumulated evidence for gene selection in microarray gene expression data,"feature subset selection (fss) methods play an important role for cancer classification using microarray gene expression data. in this scenario, it is extremely important to select genes by taking into account the possible interactions with other gene subsets. this paper shows that, by accumulating the evidence in favour (or against) each gene along a search process, the obtained gene subsets may constitute better solutions, either in terms of size or in predictive accuracy, or in both, at a negligible overhead in computational cost.",nan
2010,diagnosis discrimination for ontology debugging,"debugging is an important prerequisite for the wide-spread application of ontologies, especially in areas that rely upon everyday users to create and maintain knowledge bases, such as the semantic web. recent approaches use diagnosis methods to identify sources of inconsistency. however, in most debugging cases these methods return many alternative diagnoses, thus placing the burden of fault localization on the user. this paper demonstrates how the target diagnosis can be identified by performing a sequence of observations, that is, by querying an oracle about entailments of the target ontology. we exploit probabilities of typical user errors to formulate information theoretic concepts for query selection. our evaluation showed that the suggested method reduces the number of required observations compared to myopic strategies.",nan
2010,a contextual reading of conditional commitments,"this paper puts forward a view on conditional commitments as causal rules, using action language k as the specification framework. the proposal builds upon an operational notion of social context, in such a way that conditional commitments are represented as rules in context. this approach enables the manipulation of conditional commitments in terms of the manipulation of the social interactions which provide their social contexts. moreover, it allows the programmer to exploit the asp metatheory underlying action language k to analyze, simplify and transform commitment-based protocols.",nan
2010,acceptability semantics accounting for strength of attacks in argumentation,"we consider argumentation systems with several attack relations of different strength. we focus on the impact of various strength attacks on the semantics of such systems. first, we refine the classical notion of defence, by comparing the strength of an attack with the strength of a counter-attack. then, we propose different ways to compare defenders, and sets of defenders. that enables us to define admissible sets offering a best defence for their elements.",nan
2010,an argumentation-based dialog for social evaluations exchange,"in open multiagent systems, agents depend on reputation and trust mechanisms to evaluate the behavior of potential partners. often these evaluations (social evaluate) are associated with a measure of reliability that the source agent computes. when considering communicated social evaluations, this may lead to serious problems due to the subjectivity of reputation-related information. in this paper, instead of considering only reliability measures computed from the sources, we provide a mechanism that allows the recipient according to its own knowledge, decide whether the piece of information is reliable. we do this by allowing the agents engage in an argumentation-based dialog",nan
2010,dealing with the dynamics of proof-standard in argumentation-based decision aiding,"usually, in argumentation, the proof-standards that are used are fixed a priori by the procedure. however (multicriteria) decision-aiding is a context where it may be modified dynamically during the process, depending on the responses of the decision-maker. the expert indeed needs to adapt and refine its choice of an appropriate method of aggregating arguments pros and cons, so that it fits the preference model inferred from the interaction. in this short paper we introduce how this aspect can be handled in an argumentation-based decision-aiding framework. the first contribution of the paper is conceptual: the notion of a concept lattice based on simple properties and allowing to navigate among the different proof-standards is put forward. we then show how this can be integrated within the carneades model.",nan
2010,preference-based argumentation framework with varied-preference intensity,"recently, dung's argumentation has been extended in order to consider the strength of the defeat relation, i.e., to quantify the degree to which an argument defeats another one. we construct an argumentation framework with varied-strength defeats from a preference-based argumentation framework with an intensity degree in the preference relation. we also consider the case when the preference over the arguments is constructed from a valued logic.",nan
2010,aba: argumentation based agents,"many works have identified the potential benefits of using argumentation to address a large variety of multiagent problems. in this paper we take this idea one step further and develop the concept of a fully integrated argumentation-based agent architecture that allows us to develop agents that are coherently designed on an underlying argumentation based foundation. under this architecture, an agent is composed of a collection of modules each of which is equipped with a local argumentation theory. similarly, the intra-agent control of the agent is governed by local argumentation theories that are sensitive to the current situation of the agent through dynamically enabled feasibility arguments.",nan
2010,on admissibility in timed abstract argumentation frameworks,"in this work, we define timed abstract argumentation frameworks, a novel argumentation formalism where arguments are only valid for consideration in a given period of time, which is defined for every individual argument. thus, the attainability of attacks and defenses is related to time, and the outcome of the framework may vary accordingly. we define a timed, interval-based notion of grounded extension as a skeptical semantics.",nan
2010,fair mechanisms for recurrent multi unit combinatorial auctions,"auctions have been used to deal with resource allocation in multi-agent systems. in some environments like service-oriented electronic markets, it is advisable to use recurrent auctions since resources are perishable and auctions are repeated over time with the same or a very similar set of agents. recurrent auctions are a sequence of one-shot auctions of any kind. as a drawback some problems do appear that could cause the market to collapse at mid-long term. previous works have dealt with these problems by adding fairness to the auction outcomes but they dealt with multi-unit auctions, in which several units of an item are sold. in this paper, we present a new fair mechanism for multi-unit combinatorial auctions, in which different items, and several units per item are sold in each auction.",nan
2010,balancing optimality and robustness in resource allocation problems,we formally define a flexible robustness mechanism for resource allocation problems encoded as combinatorial auctions based on repairable solutions. our notion of robustness is focused on resources that become unavailable once an allocation has been found. we extend previous works on encoding auctions to weighted max-sat and supermodels for the boolean satisfiability framework (sat) with a mechanism that adds flexibility to the robust solutions.,nan
2010,a knapsack-based approach to bidding in ad auctions,"we model the problem of bidding in ad auctions as a penalized multiple choice knapsack problem (pmckp), a combination of the multiple choice knapsack problem (mckp) and the penalized knapsack problem (pkp) [1]. we present two versions of pmckpglobalpmckp and localpmckp, together with a greedy algorithm that solves the linear relaxation of a globalpmckp optimally. we also develop a greedy heuristic for solving localpmckp. although our heuristic is not optimal, we show that it performs well in tac aa games.",nan
2010,coalition formation strategies for self-interested agents in hedonic games,"in this article, we address the problem of coalition formation in multiagent systems. our work focuses on the class of hedonic games, where the satisfaction of each agent depends on other agents taking part in the coalition. we present in this paper some strategies, which could be used by agents. we describe two types of strategies: proposal acceptance strategies, which allow agents to accept or reject a coalition formation proposal and proposal selection strategies based on the analysis of the history of a negotiation, which allow agents to select interesting coalitions to propose. we underline that a compromise between high and low selectivity allows agents to obtain a higher probability to form coalitions with a satisfying utility. our proposal selection strategies allow agents to reduce the number of proposals to send during the coalition formation process without losing much utility. this speeds up considerably the process.",nan
2010,a network flow approach to coalitional games,"in this paper we propose a novel approach to represent coalitional games, called a coalition-flow network (cf-net), that builds upon a generalization of the network flow literature. specifically, this representation is based on our observation that the coalition formation process can be viewed as the problem of directing the flow through a network where every edge has certain capacity constraints.",nan
2010,taking the final step to a full dichotomy of the possible winner problem in pure scoring rules,"the possible winner problem asks, given an election where the voters' preferences over the candidates are specified only partially, whether a designated candidate can be made win. betzler and dorn [1] proved a result that is only one step away from a full dichotomy of this problem for the important class of pure scoring rules in the case of unweighted voters and an unbounded number of candidates: possible winner is np-complete for all pure scoring rules except plurality, veto, and the scoring rule with vector (2,1,…,1,0), but is solvable in polynomial time for plurality and veto. we take the final step to a full dichotomy by showing that possible winner is np-complete also for the scoring rule with vector (2,1,…,1,0).",nan
2010,complexity of merging and splitting for the probabilistic banzhaf power index in weighted voting games,"the banzhaf power index is a prominent measure of a player's influence for coalition formation in weighted voting games, an important class of simple coalitional games that are fully expressive but compactly representable. for the normalized banzhaf index, aziz and paterson [1] show that it is np-hard to decide whether merging any coalition of players is beneficial, and that in unanimity games, merging is always disadvantageous, whereas splitting is always advantageous. we show that for the probabilistic banzhaf index (which is considered more natural than the normalized banzhaf index), the merging problem is in p for coalitions of size two, and is np-hard for coalitions of size at least three. we also prove a corresponding result for the splitting problem. in unanimity games and for the probabilistic banzhaf index (in strong contrast with the results for the normalized banzhaf index), we show that splitting is always disadvantageous or neutral, whereas merging is neutral for size-two coalitions, yet advantageous for coalitions of size at least three.",nan
2010,alice and bob will fight: the problem of electing a committee in the presence of candidate interdependence,"the problem of electing a committee which satisfies voters is one for which good solutions are scarce. extending single-winner voting rules to the multi-winner case works well only when voters have no preferential dependencies among candidates for the committee. (our motivating example is a voter who believes that alice and bob are the best candidates, but also that the worst possible committee is one with both alice and bob.) in order to tackle the interdependence problem, we propose a voting rule called the goalbase summation rule (gsr), which uses goalbases (sets of weighted propositional formulas) as ballots. using goalbases as ballots lets voters express complex preferences in a compact fashion, while the computational complexity of finding winning committees remains reasonable when the number of seats is fixed. additionally, the gsr is able to simulate and extend many existing voting rules.",nan
2010,"egalitarian utilities divide-and-coordinate: stop arguing about decisions, let's share rewards!","in this paper we formulate a novel divide-and-coordinate (dac) algorithm, the so-called egalitarian utilities divide-and-coordinate (eu-dac) algorithm. the divide-and-coordinate (dac) framework [3] is a family of bounded dcop algorithms that solve dcops by exploiting the concept of agreement. the intuition behind eu-dac is that agents get closer to an agreement, on the optimal solution, when they communicate the local max-marginals utilities for their assignments instead of only their preferred assignments. we provide empirical evidence supporting this hypothesis as well as illustrating the competitiveness of eu-dac.",nan
2010,reasoning about norm compliance with rational agents,"this paper presents a model for rational self-interested agents, which takes into account the possibility of violating norms. the transgressions take place when the expected rewards obtained with the defection from the norms surpass the expected rewards obtained by being norm-compliant. to develop such model, we employ markov decision processes (mdps). our approach consists of representing the reactions for norm violations within the mdps in such a way that the agent is able to reason about how those violations affect her expected utilities and future options.",nan
2010,collective sensor configuration in uncharted environments,"sensor networks (sn) are rapidly becoming the tool of choice for monitoring. their versatility makes them useful in numerous and diverse application domains. however, most sn deployments assume that the area and events to monitor/control are well known/understood at design time. thus, sensors' configurations can be defined prior to their deployment. nevertheless, when the purpose of an sn is to monitor the events of an uncharted environment, where the distribution and nature of events is uncertain, it is rather intricate to configure its sensors at design time. instead, sensors should be able to self-configure at run time. in this paper, we propose a low-cost (in terms of energy and computation) collective approach that allows the sensors in an sn to collaboratively search for their most appropriate configurations only using their local knowledge. we empirically show that our approach can help sensors efficiently monitor environments where various dynamic events exist.",nan
2010,a unified interaction-aware goal framework,"goals are central to the design and implementation of intelligent software agents. much of the literature on goals and reasoning about goals only deals with a limited set of goal types, typically achievement goals, and sometimes maintenance goals; and much of the work on interactions between goals only deals with achievement goals. we aim at extending a previously proposed unifying framework for goals with additional richer goal types, including a combined “achieve and maintain” goal type. we propose to provide an operationalization of these new goal types, proving that the operationalization meets desired properties.",nan
2010,analogical learning using dissimilarity between tree-structures,"in artificial intelligence, analogy is used as a non exact reasoning technique to solve problems, for natural language processing, for learning classification rules, etc. this paper is interested in the analogical proportion, a simple form of the reasoning by analogy, and described its use in artificial learning of the syntactic tree (parsing) of a sentence.",nan
2010,complexity in analogy tasks: an analysis and computational model,"in this article, we introduce a complexity measure for matrix tasks which occur in iq-tests with respect to the kind of functions necessary to solve such tasks. the aim is to capture human reasoning difficulty. we implemented a program which is able to solve matrix tasks and to evaluate their complexity by our measure. the results of the evaluation are compared with the empirical difficulty ranking from cattell's culture fair test.",nan
2010,efficient explanations for inconsistent constraint sets,constraint sets can become inconsistent in different contexts. we are interested in identifying minimal sets of constraints that have to be adapted or deleted in order to restore consistency. in this paper we sketch a highly efficient divide-and-conquer based diagnosis approach which identifies minimal sets of faulty constraints in a given over-constrained problem. this approach is specifically applicable in scenarios where the efficient identification of leading (preferred) diagnoses is crucial.,nan
2010,improving rfid's location based services by means of hidden markov models,"services in ambient intelligence systems must adapt to context information and users. so, the development of these services become in a complex task. in this context, we present amisim: a methodology for the engineering of ami services. using amisim, a lbs based on rfid technology is built. engineering the service is based on hidden markov models (hmms) for locations within an intelligent building. the results show that the resolution of a rfid based lbs can be significantly improved and allows by means of the simulator optimally configure the number and location of rfid antennas.",nan
2010,human activity recognition in intelligent home environments: an evolving approach,"in this paper, we propose an automated approach to track and recognize daily activities. any activity is represented in this research as a sequence of raw sensors data. these sequences are treated using statistical methods in order to discover activity patterns. however, as the way to perform an activity is usually not fixed but it changes and evolves, we propose an activity recognition method based on evolving systems.",nan
2010,joint handling of rational and behavioral reactions in assistant conversational agents,"we describe here a framework dedicated to studies and experimentations upon the nature of the relationships between the rational reasoning process of an artificial agent and its psychological counterpart, namely its behavioral reasoning process. this study is focused on the domain of assistant conversational agents which are software tools providing various kinds of assistance to people of the general public interacting with computer based applications or services. in this context, we show on some examples how the agents must exhibit both rational reasoning about the system functioning and a human-like believable dialogical interaction with the users.",nan
2010,what if it suddenly fails? behavioral aspects of advanced driver assistant systems on the example of local danger alerts,"many researchers argue, in assessing the benefits of advanced driver assistance systems (adas) it has to be taken into account that any gains in terms of security may be again reduced by the fact they affect the drivers' behavior. in this paper, we present results of a driving simulation study in which we compare driving performance as well as driver stress in three conditions: 1) when no assistance is available (noassist) ; 2) when the system is working as it should (work); 3) when the system suddenly fails (fail). results show that the driving performance is severely affected by system failures. the drivers' ability to effectively react to suddenly appearing obstacles fail is significantly lower than in noassist. at the same time, the stress level is significantly higher in fail compared to work.",nan
2010,selecting information based on artificial forms of selective attention,we describe an approach based on artificial forms of selective attention for overcoming the problem of information and interruption overload of intelligent agents so that these can autonomously select relevant information of the environment while ignoring other less relevant information in order to allocate processing resources on it.,nan
2010,antipa: an agent architecture for intelligent information assistance,"human users trying to plan and accomplish information-dependent goals in highly dynamic environments with prevalent uncertainty must consult various types of information sources in their decision-making processes while the information requirements change as they plan and re-plan. when the users must make time-critical decisions in information-intensive tasks they become cognitively overloaded not only by the planning activities but also by the information-gathering activities at various points in the planning process. we have developed the anticipatory information and planning agent (antipa) to manage information adaptively in order to mitigate user cognitive overload. to this end, the agent brings information to the user as a result of user requests but most crucially, it proactively predicts the user's prospective information needs by recognizing the user's plan; pre-fetches information that is likely to be used in the future; and offers the information when it is relevant to the current or future planning decisions. this paper introduces a fully implemented agent of the antipa architecture using a decision-theoretic user model.",nan
2010,nested monte-carlo expression discovery,"nested monte-carlo search is a general algorithm that gives good results in single player games. genetic programming evaluates and combines trees to discover expressions that maximize a given evaluation function. in this paper nested monte-carlo search is used to generate expressions that are evaluated in the same way as in genetic programming. single player nested monte-carlo search is transformed in order to search expression trees rather than lists of moves. the resulting program achieves state of the art results on multiple benchmark problems. the proposed approach is simple to program, does not suffer from expression growth, has a natural restart strategy to avoid local optima and is extremely easy to parallelize.",nan
2010,vectorial pattern databases,"in this work, a new approach for creating pattern databases (pdbs) is suggested that induces non-consistent heuristic functions just by recognizing feasible (yet admissible) heuristic values. this approach serves to generalize even further the bpmx propagation rule, that will work now even in directed graphs. experiments in different state spaces show a noticeable improvement over the scalar pattern databases.",nan
2010,improving the global constraint softprec,"a soft global constraint softprec has been proposed recently for solving optimisation problems involving precedence relations. in this paper we present new pruning rules for this global constraint. we introduce a pruning rule that improves propagation from the objective variable to the decision variables, which is believed to be harder to achieve. we further introduce a pruning rule based on linear programming, and thereby make softprec a hybrid of constraint programming and linear programming. we present results demonstrating the efficiency of the pruning rules.",nan
2010,data-driven detection of recursive program schemes,"we present an extension to a current approach to inductive programming (igor2), that is, learning (recursive) programs from incomplete specifications such as input/outout examples. igor2 uses an analytical, example-driven strategy for generalization. we extend the set of igor2's refinement operators by a further operator – identification of higher-order schemes – and can show that this extension does improve speed as well as scope.",nan
2010,horn belief change: a contraction core,"we show that booth et al.'s horn contraction based on infra-remainder sets corresponds exactly to kernel contraction for belief sets. this result is obtained via a detour through horn contraction for belief bases, which supports the conjecture that horn belief change is best viewed as a “hybrid” version of belief set change and belief base change. moreover, the link with base contraction gives us a more elegant representation result for horn contraction for belief sets in which a version of the core-retainment postulate features.",nan
2010,a motivation-based mechanism to design behaviors,"in human-level simulations, like video games can be, the design of character's behaviors has an important impact on simulation realism. we propose to divide it into a reasoning part, dedicated to a planner, and an individuality part, assigned to an action selection mechanism. applying the separation of declarative and procedural aspects, the principle is to provide every character with the same procedural mechanisms: the planner and the action selection mechanism. declarative knowledge is then used at the agent level to individualize the behavior. the contribution of this paper consists in a motivation-based action selection mechanism that allows individualization in behavior. the modularity provided by the motivations enables a large variety of behaviors for which the designer has to choose parameters. if the simulation of characters are our first motivation, the principles involved in the proposed motivation-based action mechanism are general enough to be used in other contexts.",nan
2010,preferential vs rational description logics: which one for reasoning about typicality?,"extensions of description logics (dls) to reason about typicality and defeasible inheritance have been largely investigated. in this paper, we consider two such extensions, namely (i) the extension of dls with a typicality operator t, having the properties of preferential nonmonotonic entailment p, and (ii) its variant with a typicality operator having the properties of the stronger rational entailment r. the first one has been proposed in [6]. here, we investigate the second one and we show, by a representation theorem, that it is equivalent to the approach to preferential subsumption proposed in [3]. we compare the two extensions, preferential and rational, and argue that the first one is more suitable than the second one to reason about typicality, as the latter leads to very unintuitive inferences.",nan
2010,refining the notion of effort,"the aim of this paper is to incorporate a qualitative approach to measurement into the more general concept of effort. the fundamental idea to achieve this goal is to assign suitable modalities to the methods of measurement being available to an agent. the knowledge of the agent changes, in fact, generally increases by applying such methods. thus, the system should be able to describe this change of knowledge as well. we develop an appropriate logic, show that it satisfies some of the fundamental properties that are desirable at any rate, and discuss possible extensions.",nan
2010,a constructive conditional logic for access control: a preliminary report,"we define an intuitionistic conditional logic for access control called cicl. the logic cicl is based on a conditional language allowing principals to be defined as arbitrary formulas and it includes few uncontroversial axioms of access control logics. we provide an axiomatization and a kripke model semantics for the logic cicl, and we prove that the axiomatization is sound and complete with respect to the semantics.",nan
2010,a logical model of intention and plan dynamics,"we propose a formal semantics of intention and plan dynamics based on the notion of local assignment. the function of a local assignment is to change the truth value of a given proposition at a specific time point along a history. we combine a static modal logic including a temporal modality and modal operators for mental attitudes belief and choice, with three kinds of dynamic modalities and corresponding three kinds of local assignments operating on agent's beliefs, on agent's choices and on the physical world. an agent's intention is defined in our approach as the agent's choice to perform a given action at a certain time point in the future and two operations called intention generation and intention reconsideration are defined as specific kinds of local assignments on choices. in section 1 we introduce a static logic of time, action, and mental attitudes. in section 2 we add the dynamic notion of local assignment to the logic of section 1. in section 3, we focus on two specific kinds of local assignment on choice which allow to model the processes of intention and plan generation and reconsideration.",nan
2010,non-elementary speed up for model checking synchronous perfect recall,"we consider the complexity of the model checking problem for the logic of knowledge and past time in synchronous systems with perfect recall. previously established bounds are k-exponential in the size of the system for specifications with k nested knowledge modalities. we show that the upper bound for positive (respectively, negative) specifications is polynomial (respectively, exponential) in the size of the system irrespective of the nesting depth.",nan
2010,querying in [escr    ][lscr    ]+ with nonmonotonic rules,"a general top-down algorithmization for the well-founded mknf semantics – a semantics for combining rules and ontologies – was recently defined based on an extension of slg resolution for logic programming with an abstract oracle to the parametric ontology language. here we provide a concrete oracle with practical usage, namely for [escr    ][lscr    ]+ which is tractable for reasoning tasks like subsumption. we show that the defined oracle remains tractable (wrt. data complexity) so that the combined (query-driven) approach of non-monotonic rules with that oracle is tractable as well.",nan
2010,implementing simple modular erdf ontologies,"the extended resource description framework has been proposed to equip rdf graphs with weak and strong negation, as well as derivation rules, increasing the expressiveness of ordinary rdf graphs. in parallel, the modular web framework enables collaborative and controlled reasoning in the semantic web. in this paper we show how to use the modular web framework to capture the modular semantics for erdf graphs, supporting local semantics and different points of view, local closed-world and open-world assumptions, and scoped negation-as-failure.",nan
2010,local search algorithms on the stable marriage problem: experimental studies,"the stable marriage problem (sm) has a wide variety of practical applications, ranging from matching resident doctors to hospitals, to matching students to schools, or more generally to any two-sided market. in the classical formulation, n men and n women express their preferences over the members of the other sex. solving an sm means finding a stable marriage: a matching of men to women with no blocking pair. a blocking pair consists of a man and a woman who are not married to each other but both prefer each other to their partners. it is possible to find a male-optimal (resp., female-optimal) stable marriage in polynomial time. however, it is sometimes desirable to find stable marriages without favoring a group at the expenses of the other one. in this paper we present a local search approach to find stable marriages. our experiments show that the number of steps grows as little as o(nlog(n)). we also show empirically that the proposed algorithm samples very well the set of all stable marriages of a given sm, thus providing a fair and efficient approach to generate stable marriages.",nan
2010,knowledge-based adaptive thresholding from shadows,"this paper presents results of a mobile robot qualitative self-localisation experiment using information from cast shadows. we present results of self-localisation using two methods for obtaining the threshold automatically: in one method the images are segmented according to their grey-scale histograms, in the other the threshold is set according to a prediction about the robot's location, given a shadow-based map defined upon a qualitative spatial reasoning theory. to the best of our knowledge this is the first work that uses qualitative spatial representations both to perform egolocation and to calibrate a robot's interpretation of its perceptual input.",nan
2010,a qualitative representation of route networks,route navigation is one of the most widely used everyday application of spatial data. in this paper we investigate how a qualitative representation of route networks can be derived from map data and how this representation can be used to reason about route descriptions. we introduce a concept of route graph that provides an abstract layer on top of metric map data and thus allows for a compact representation of route information. we present selected queries and reasoning tasks that can be expressed in this abstraction layer.,nan
2010,restarts and nogood recording in qualitative constraint-based reasoning,"this paper introduces restart and nogood recording techniques in the domain of qualitative spatial and temporal reasoning. nogoods and restarts can be applied orthogonally to usual methods for solving qualitative constraint satisfaction problems. in particular, we propose a more general definition of nogoods that allows for exploiting information about nogoods and tractable subclasses during backtracking search. first evaluations of the proposed techniques show promising results.",nan
2010,strategic planning in the game of go using coupled non-linear oscillators,"the ancient oriental game of go is the last of the great perfect knowledge games wherein strong human players still outperform ai players, but the gap is narrowing. superior strategic skill is keeping humans several steps ahead even as computers catch up tactically. taking the perspective that large scale topological perception is part of the key to strategic excellence, we present a new method for generating strategic plans in the game of go. treating stones in play as continuous perturbations of a dynamical system composed of coupled non-linear oscillators, we exploit transitory synchrony in the wave patterns thus generated to make topological inferences about possible territories later in the game. our system is shown, using professional game records, to focus search on appropriate areas of the space of continuations. by combining spatial (rather than game-tree) representation with dynamical lookahead, we hope to overcome some of the limitations faced by static spatial mapping and dynamical game-tree based approaches.",nan
2010,semi-automatic revision of formalized knowledge,"as the amount of available ontologies and their size grow, ontology reuse gains in importance. however, the online available formalized knowledge in many cases need a revision which can lead to a high manual effort. in this paper, we propose an approach to support the revision of ontologies. we show that our method reduces the manual effort measured in number of decisions that have to be made by an ontology engineer by up to 83%.",nan
2010,instruction cache prediction using bayesian networks,"storing instructions in caches has led to dramatic increases in the speed at which programs can execute. however, this has also made it harder to reason about the time needed for execution in those domains where temporal behaviour of code is important. this paper presents a novel approach to predicting which instructions will be found in the cache when required using machine learning. more specifically, we demonstrate a method in which a bayesian network is inferred from examples of a program running and is then used to predict the presence of instructions in the cache when the same program is run with unknown inputs.",nan
2010,unsupervised feature generation using knowledge repositories for effective text categorization,"we propose an unsupervised feature generation algorithm using the repositories of human knowledge for effective text categorization. conventional bag of words (bow) depends on the presence / absence of keywords to classify the documents. to understand the actual context behind these keywords, we use knowledge concepts / hyperlinks from external knowledge sources through content and structure mining on wikipedia. then, the features of knowledge concepts are clustered to generate knowledge cluster vectors with which the input text documents are mapped into a high dimensional feature space and the classification is performed. the simulation results show that the proposed approach identifies associated features in the text collection and yields an improved classification accuracy.",nan
2010,discovering an event taxonomy from video using qualitative spatio-temporal graphs,this work proposes a graph mining based approach to mine a taxonomy of events from activities for complex videos which are represented in terms of qualitative spatio-temporal relationships. a hidden markov model to obtain stable qualitative spatial relations from noisy measurements is introduced. the effectiveness of the approach is demonstrated through experimental results for a complex aircraft turnaround apron scenario.,nan
2010,advances in class noise detection,"noise filtering is usually used in data preprocessing to improve the accuracy of induced classifiers. our goal is different: we aim at detecting noisy instances to be inspected by the domain expert in the phase of data understanding. consequently, our noise detection algorithms should have high precision of class noise detection, where the precision-recall trade-off is modeled using the f-measure. new variants of class noise detection algorithms have been developed, including the high agreement random forest filter which ensures very high precision of identified erroneous data instances.",nan
2010,knowledge compilation for itemset mining,"we present a novel approach to itemset mining whereby the set of all itemsets are compiled into a compact form, closely related to binary decision diagrams. while there were previous attempts to utilize decision diagrams for storing the set of frequent itemsets this is the first approach that does not rely on backtrack search to generate such a set. our empirical evaluation demonstrates that our approach is complementary to current approaches.",nan
2010,towards argumentation-based multiagent induction,"in this paper we propose an argumentation-based framework for multiagent induction, where two agents learn separately from individual training sets, and then engage in an argumentation process in order to converge to a common hypothesis about the data. the result is a multiagent induction strategy in which the agents minimize the set of cases that they have to exchange (using argumentation) in order to converge to a shared hypothesis. the proposed strategy works for any induction algorithm which expresses the hypothesis as a collection of rules. we show that the strategy converges to a hypothesis indistinguishable in training set accuracy from that learned by a centralized strategy.",nan
2010,generating time series reference models based on event analysis,"creating a reference model that represents a given set of time series is a relevant problem as it can be applied to a wide range of tasks like diagnosis, decision support, fraud detection, etc. in some domains, like seismography or medicine, the relevant information contained in the time series is concentrated in short periods of time called events. in this paper, we propose a technique for generating time series reference models based on the analysis of the events they contain. the proposed technique has been applied to time series from two medical domains: electroencephalography, a neurological procedure to record the electrical activity produced by the brain and stabilometry, a branch of medicine studying balance-related functions in human beings.",nan
2010,mining physiological data for discovering temporal patterns on disease stages,"analyzing physiological data can be of great importance in unearthing information on the course of a disease. in this paper we propose a data mining approach to analyze these data and acquire knowledge, in the form of temporal patterns, on the physiological events which can frequently trigger particular stages of disease. the application to the sleep sickness scenario is addressed to discover patterns, expressed in terms of breathing and cardiovascular system time-annotated disorders, which may trigger particular sleep stages.",nan
2010,drift severity metric,"concept drift in data is usually considered only as abrupt or gradual thus referring to the speed of change. such simple distinguishing by speed is sufficient for most of the problems, but there might be situations for which a finer representation would be of use.this paper studies further the phenomenon of concept drift and introduces a simple measure which is relevant to the speed and amount of change between different concepts.",nan
2010,prediction of attributes and links in temporal social networks,"the analysis of social networks often assumes the time invariant scenario while in practice node attributes and links in such networks often evolve over time. in this paper, we propose a new method to predict node attributes and links in temporal networks.",nan
2010,reliable predictive intervals for the critical frequency of the f2 ionospheric layer,"this paper addresses the problem of reliably predicting an important hf communication systems parameter, the critical frequency of the f2 ionospheric layer, with the use of a new machine learning technique, called conformal prediction (cp). cp accompanies the predictions of traditional machine learning algorithms with measures of confidence. the proposed approach is based on the wellknown ridge regression technique, but instead of the point predictions produced by the original method, it produces predictive intervals that satisfy a given confidence level. our experimental results on an extended critical frequency dataset show that the obtained intervals are well-calibrated and narrow enough to be useful in practice.",nan
2010,decentralised supply chain formation: a belief propagation-based approach,"decentralised supply chain formation involves determining the set of producers within a network able to supply goods to one or more consumers at the lowest cost. this problem is frequently tackled using auctions and negotiations. in this paper we show how it can be cast as an optimisation of a pairwise cost function. optimising this class of functions is np-hard but good approximations to the global minimum can be obtained using loopy belief propagation (lbp). here we detail a lbp-based approach to the supply chain formation problem, involving decentralised message-passing between potential participants. our approach is evaluated against a well-known double-auction method and an optimal centralised technique, showing several improvements: it obtains better solutions for most networks that admit a competitive equilibriumcompetitive equilibrium as defined in [3] is used as a means of classifying results on certain networks to allow for minor inefficiencies in their auction protocol and agent bidding strategies. while also solving problems where no competitive equilibrium exists, for which the double-auction method frequently produces inefficient solutions.",nan
2010,formal analysis of models for the mammalian vision system,"in this paper we present a rigorous definition of classification in a common family of models for the mammalian vision system, and provide a formal analysis of classification power in this family. we characterize the power of a single unit, and subsequently demonstrate how to reduce a network of arbitrary topology and size to such a single unit. we finally show how such a network, via a reduction to a single unit, is equivalent to a simple 3 layer feed forward network of threshold unitsnumenta inc.(http://www.numenta.com) is a private company, founded by d. dubinsky, d. george and j. hawkins, developing a general purpose computing platform, called nupic, based on the htm model..",nan
2010,implementing an intelligent moving average with a neural network,recent results in hybrid neural networks using extended versions of the core method have shown that we can use background knowledge to guide back-propagation learning. this paper further explores this ideas by adding numeric functions to the encoded knowledge and using the traditional recursive elman neural network model. an illustration of the properties of these neural networks will be used to calculate a simple moving average. simulations on generated data and on the eurostoxx50 financial index will illustrate the potential of such a strategy.,nan
2010,bagged biclustering for microarray data,one of the major tools of transcriptomics is the biclustering that simultaneously constructs a partition of both examples and genes. several methods have been proposed for microarray data analysis that enables to identify groups of genes with similar expression profiles only under a subset of examples. we propose to improve the quality of these biclustering methods by using an ensemble approach. our bagged biclustering method generates a collection of biclusters using the bootstrap samples of the original data and aggregate them into new biclusters. our method improve the performance of biclustering on artificial and real datasets.,nan
2010,automatic creation of a conceptual base for portuguese using clustering techniques,"when a semantic network is based on triples relating terms, ambiguity arises as a problem, so we have used these triples to identify clusters, which can be seen as synsets. we report the results of this approach on a synonymy network extracted from a dictionary and additional tests involving manually created thesaurus. part of the resulting synsets were also evaluated by human subjects.",nan
2010,automatically constructing dictionaries for extracting meaningful crime information from arabic text,"arabic is widely spoken language but very few text mining tools have been developed for arabic language. this paper presents an algorithm for automatically building dictionaries. the target domain is crime profiling. the corpus is mined for crime type, location and nationality. this work is then validated through three experiments, the results of which show that the algorithm developed here is promising.",nan
2010,gnusmail: open framework for on-line email classification,"real-time classification of massive email data is a challenging task that presents its own particular difficulties. since email data presents an important temporal component, several problems arise: emails arrive continuously, and the criteria used to classify those emails can change, so the learning algorithms have to be able to deal with concept drift. our problem is more general than spam detection, which has received much more attention in the literature.in this paper we present gnusmail, an open-source extensible framework for email classification, which structure supports incremental and on-line learning. this framework enables the incorporation of algorithms developed by other researchers, such as those included in weka and moa. we evaluate this framework, characterized by two overlapping phases (pre-processing and learning), using the enron dataset, and we compare the results achieved by weka and moa algorithms.",nan
2010,discovering collaboration opportunities in research-oriented networks,"in this paper we present a method for discovering collaboration opportunities in research-oriented networks based on opportunity networks, modelled as a directed attributed multigraph, an opportunity exploiter, a set of opportunity patterns and a set of opportunity ranking functions. we show how the discovery of joint-research collaboration opportunities in a research-oriented network can be formulated in terms of the proposed model, and we perform an evaluation on a real-world dataset.",nan
2010,high-level perception as focused belief revision,"we present a framework for incorporating perception-induced beliefs into the knowledge base of a rational agent. normally, the agent accepts the propositional content of perception and other propositions that follow from it. given the fallibility of perception, this may result in contradictory beliefs. hence, we model high-level perception as belief revision. we overcome difficulties imposed by the highly idealistic classical belief revision in two ways. first, we adopt a belief revision operator based on relevance logic, thus limiting the derived beliefs to those that relevantly follow from the new percept. second, we focus belief revision on only a subset of the agent's set of beliefsâ€”those that we take to be within the agent's current focus of attention.",nan
2012,probabilistic techniques for mobile robot navigation,"probabilistic approaches have been discovered as one of the most powerful approaches to highly relevant problems in mobile robotics including perception and robot state estimation. major challenges in the context of probabilistic algorithms for mobile robot navigation lie in the questions of how to deal with highly complex state estimation problems and how to control the robot so that it efficiently carries out its task. in this talk, i will present recently developed techniques for efficiently learning a map of an unknown environment with a mobile robot. i will also describe how this state estimation problem can be solved more effectively by actively controlling the robot. for all algorithms i will present experimental results that have been obtained with mobile robots in real-world environments.",nan
2012,generalized decision diagrams: the game is not over yet!,"decision diagrams have played an influential role in computer science and ai over the past few decades, with obdds (ordered binary decision diagrams) as perhaps the most practical and influential example. the practical influence of obdds is typically attributed to their canonicity, their efficient support of boolean combination operations, and the availability of effective heuristics for finding good variable orders (which characterize obdds and their size). over the past few decades, significant efforts have been exerted to generalize obdds, with the goal of defining more succinct representations while retaining the attractive properties of obdds. on the theoretical side, these efforts have yielded a rich set of decision diagram generalizations. practially, however, obdds remain as the single most used decision diagram in applications. in this talk, i will discuss a recent line of research for generalizing obdds based on a new type of boolean-function decompositions (which generalize the shannon decomposition underlying obdds). i will discuss in particular the class of sentential decision diagrams (sdds), which branch on arbitrary sentences instead of variables, and which are characterized by trees instead of total variable orders. sdds retain the main attractive properties of obdds and include obdds as a special case. i will discuss recent theoretical and empirical results, and a soon-to-be-released open source package for supporting sdds, which suggest a potential breakthrough in the quest for producing more practical generalizations of obdds.",nan
2012,never ending learning,"we will never really understand learning or intelligence until we can build machines that learn many different things, over years, and become better learners over time. this talk describes our research to build a never-ending language learner (nell) that runs 24 hours per day, forever, learning to read the web. each day nell extracts (reads) more facts from the web, and integrates these into its growing knowledge base of beliefs. each day nell also learns to read better than yesterday, enabling it to go back to the text it read yesterday, and extract more facts, more accurately. nell has been running 24 hours/day for over two years now. the result so far is a collection of 15 million interconnected beliefs (e.g., servedwtih(coffee, applepie), isa(applepie, bakedgood)), that nell is considering at different levels of confidence, along with hundreds of thousands of learned phrasings, morphoogical features, and web page structures that nell uses to extract beliefs from the web. track nell's progress at http://rtw.ml.cmu.edu.",nan
2012,bad equilibria (and what to do about them),"i begin by arguing that the notion of economic equilibrium is an important analytical tool with which to understand the behaviour of today's networked computer systems. this is because the behaviours that such systems exhibit are in part a function of the preferences and desires of system participants; this gives such systems the flavour of an economic system. in economics, an equilibrium is a steady-state situation, which obtains because no participant has any rational incentive to deviate from it. equilibrium concepts are arguably the most important and widely used analytical weapons in the game theory arsenal. the concept of nash equilibrium in particular has found a huge range of applications, in areas as diverse and seemingly unrelated as evolutionary biology and moral philosophy. however, there remain fundamental problems associated with nash equilibria and their application, which must be considered if we want to apply them to the analysis of computer systems. first, there may be multiple nash equilibria, in which case, how should we choose between them? second, some equilibria may be undesirable, in which case, how can we avoid them? in this essay, i will introduce work that we have done addressing these problems from a computational/ai perspective. assuming no prior knowledge of game theory or economic solution concepts, i will discuss various ways in which we can try to engineer a scenario so that desirable equilibria result, or else engineer out undesirable equilibria.",nan
2012,executable logic for dialogical argumentation,"argumentation between agents through dialogue is an important cognitive activity. there have been a number of proposals for formalizing dialogical argumentation. however, each proposal involves a number of quite complex definitions, and there is significant diversity in the way different proposals define similar features. this complexity and diversity has hindered analysis and comparison of the space of proposals. to address this, we present a general approach to defining a wide variety of systems for dialogical argumentation. our solution is to use an executable logic to specify individual systems for dialogical argumentation. this means we have a common language for specifying a wide range of systems, we can compare systems in terms of a range of standard properties, we can identify interesting classes of system, and we can execute the specification of each system to analyse it empirically.",nan
2012,computational creativity: the final frontier?,"notions relating to computational systems exhibiting creative behaviours have been explored since the very early days of computer science, and the field of computational creativity research has formed in the last dozen years to scientifically explore the potential of such systems. we describe this field via a working definition; a brief history of seminal work; an exploration of the main issues, technologies and ideas; and a look towards future directions. as a society, we are jealous of our creativity: creative people and their contributions to cultural progression are highly valued. moreover, creative behaviour in people draws on a full set of intelligent abilities, so simulating such behaviour represents a serious technical challenge for artificial intelligence research. as such, we believe it is fair to characterise computational creativity as a frontier for ai research beyond all others—maybe, even, the final frontier.",nan
2012,recent advances in imprecise-probabilistic graphical models,"we summarise and provide pointers to recent advances in inference and identification for specific types of probabilistic graphical models using imprecise probabilities. robust inferences can be made in so-called credal networks when the local models attached to their nodes are imprecisely specified as conditional lower previsions, by using exact algorithms whose complexity is comparable to that for the precise-probabilistic counterparts.",nan
2012,lifted probabilistic inference,"many ai problems arising in a wide variety of fields such as machine learning, semantic web, network communication, computer vision, and robotics can elegantly be encoded and solved using probabilistic graphical models. often, however, we are facing inference problems with symmetries and redundancies only implicitly captured in the graph structure and, hence, not exploitable by efficient inference approaches. a prominent example are probabilistic logical models that tackle a long standing goal of ai, namely unifying first-order logic — capturing regularities and symmetries — and probability — capturing uncertainty. although they often encode large, complex models using few rules only and, hence, symmetries and redundancies abound, inference in them was originally still at the propositional representation level and did not exploit symmetries. this paper is intended to give a (not necessarily complete) overview and invitation to the emerging field of lifted probabilistic inference, inference techniques that exploit these symmetries in graphical models in order to speed up inference, ultimately orders of magnitude.",nan
2012,developmental mechanisms for autonomous life-long learning in robots,"developmental robotics studies and experiments mechanisms for autonomous life-long learning of skills in robots and humans. one of the crucial challenges is due to the sharp contrast between the high-dimensionality of their sensorimotor space and the limited number of physical experiments they can make within their life-time. this also includes the capability to adapt skills to changing environments or to novel tasks. to achieve efficient life-long learning in such complex spaces, humans benefit from various interacting developmental mechanisms which generally structure exploration from simple learning situations to more complex ones. i will present recent research in developmental robotics that has proposed several ways to transpose these developmental learning mechanisms to robots. in particular, i will present and discuss computational mechanisms of intrinsically motivated active learning, which automatically select training examples [4] or tasks [2] of increasing complexity, and their interaction with imitation learning [3], as well as maturation and body growth where the number of sensori and motor degrees-of-freedom evolve through phases of freezing and freeing",nan
2012,robot skill learning,"learning robots that can acquire new motor skills and refine existing ones have been a long standing vision of robotics, artificial intelligence, and the cognitive sciences. early steps towards this goal in the 1980s made clear that reasoning and human insights will not suffice. instead, new hope has been offered by the rise of modern machine learning approaches. however, to date, it becomes increasingly clear that off-the-shelf machine learning approaches will not be adequate for robot skill learning as these methods often do not scale into the high-dimensional domains of manipulator and humanoid robotics, nor do they fulfill the real-time requirement of the domain. as an alternative, we propose to divide the generic skill learning problem into parts that can be well-understood from a robotics point of view. after designing appropriate learning approaches for these basic components, these will serve as the ingredients of a general approach to robot skill learning. in this paper, we discuss our recent and current progress in this direction. as such, we present our work on learning to control, learning elementary movements, as well as our steps towards the learning of complex tasks. we show several evaluations using both real robots as well as physically realistic simulations.",nan
2012,conservative social laws,"social laws – sets of constraints imposed on the behaviour of agents within a multi-agent system with the goal of some desirable overall behaviour resulting – are an important mechanism for coordinating multi-agent behaviour. when considering social laws in human environments, the inspiration for social laws in multiagent systems, we argue that a key design principle is least change. that is, social laws are more likely to be accepted and adopted, and hence successful, if they are conservative, in the sense that they represent the smallest change possible from the pre-existing status quo that is required to effect the desired objective. our aim in the present paper is to introduce, formalise, and investigate the notion of a conservative social law for multi-agent systems. to make the idea of a conservative social law precise, we formalise the notion of a distance metric for social laws, and discuss a range of possible properties for such metrics. we then formulate the conservative social law problem, (i.e., the problem of constructing an effective social law that requires the least change according to this metric), discuss some possible interpretations of distance in this context, and discuss some issues surrounding conservative social laws.",nan
2012,finding and proving the optimum: cooperative stochastic and deterministic search,"in this article, we introduce a global cooperative approach between an interval branch and bound algorithm and an evolutionary algorithm, that takes advantage of both methods to optimize a function for which an inclusion function can be expressed. the branch and bound algorithm deletes whole blocks of the search space whereas the evolutionary algorithm looks for the optimum in the remaining space and sends to the ibba the best evaluation found in order to improve its bound. the two algorithms run independently and update common information through shared memory.the cooperative algorithm prevents premature and local convergence of the evolutionary algorithm, while speeding up the convergence of the branch and bound algorithm. moreover, the result found is the proved global optimum.in part 1, a short background is introduced. part 2.1 describes the basic interval branch and bound algorithm and part 2.2 the evolutionary algorithm. part 3 introduces the cooperative algorithm and part 4 gives the results of the algorithms on benchmark functions. the last part concludes and gives suggestions of avenues of further research.",nan
2012,dl-lite with attributes and datatypes,"we extend the dl-lite languages by means of attributes and datatypes. attributes—a notion borrowed from data models— associate concrete values from datatypes to abstract objects and in this way complement roles, which describe relationships between abstract objects. the extended languages remain tractable (with a notable exception) even though they contain both existential and (a limited form of) universal quantification. we present complexity results for two most important reasoning problems in dl-lite: combined complexity of knowledge base satisfiability and data complexity of positive existential query answering.",nan
2012,towards a complete classical music companion,"we present a system that listens to music on-line and almost instantly identifies the piece the performers are playing and the exact position in the musical score. this is achieved via a combination of a state-of-the-art audio-to-note transcription algorithm and a novel symbolic fingerprinting method. the speed and precision of the system are evaluated in systematic experiments with a large corpus of classical music recordings. the results indicate extremely fast and accurate recognition performance — a level of performance, in fact, that even human experts in classical music will find hard to match.",nan
2012,loco — a logic for configuration problems,"loco is a fragment of classical first order logic tailored for expressing configuration problems. the core feature of loco is that the number of components used in configurations does not have to be finitely bounded explicitly, but instead is bounded implicitly through the axioms. computing configurations reduces to model-finding. we present the language, related algorithms and complexity results as well as a prototypical implementation via answer set programming.",nan
2012,using learning to rank approach for parallel corpora based cross language information retrieval,"learning to rank (ltr) refers to machine learning techniques for training a model in a ranking task. ltr has been shown to be useful in many applications in information retrieval (ir). cross language information retrieval (clir) is one of the major ir tasks that can potentially benefit from ltr to improve the ranking accuracy. clir deals with the problem of expressing query in one language and retrieving the related documents in another language. one of the most important issues in clir is how to apply monolingual ir methods in cross lingual environments. in this paper, we propose a new method to exploit ltr for clir in which documents are represented as feature vectors. this method provides a mapping based on ir heuristics to employ monolingual ir features in parallel corpus based clir. these mapped features are considered as training data for ltr. we show that using ltr trained on mapped features can improve clir performance. a comprehensive evaluation on the english-persian clir suggests that our method has significant improvements over parallel corpora based methods and dictionary based methods.",nan
2012,"macros, reactive plans and compact representations","the use and study of compact representations of objects is widespread in computer science. ai planning can be viewed as the problem of finding a path in a graph that is implicitly described by a compact representation in a planning language. however, compact representations of the path itself (the plan) have not received much attention in the literature. although both macro plans and reactive plans can be considered as such compact representations, little emphasis has been placed on this aspect in earlier work. there are also compact plan representations that are defined by their access properties, for instance, that they have efficient random access or efficient sequential access. we formally compare two such concepts with macro plans and reactive plans, viewed as compact representations, and provide a complete map of the relationships between them.",nan
2012,from macro plans to automata plans,"macros have a long-standing role in planning as a tool for representing repeating subsequences of operators. macros are useful both for guiding search towards a solution and for representing plans compactly. in this paper we introduce automata plans which consist of hierarchies of finite state automata. automata plans can be viewed as an extension of macros that enables parametrization and branching. we provide several examples of the utility of automata plans, and prove that automata plans are strictly more expressive than macro plans. we also prove that automata plans admit polynomialtime sequential access of the operators in the underlying “flat” plan, and identify a subset of automata plans that admit polynomial-time random access. finally, we compare automata plans with other representations allowing polynomial-time sequential access.",nan
2012,multirelational consensus clustering with nonnegative decompositions,"unsupervised multirelational learning (clustering) in non-sparse domains such as molecular biology is especially difficult as most clustering algorithms tend to produce distinct clusters in slightly different runs (either with different initializations or with slightly different training data).in this paper we develop a multirelational consensus clustering algorithm based on nonnegative decompositions, which are known to produce sparser and more interpretable clusterings than other data-oriented algorithms.we apply this algorithm to the joint analysis of the largest available gene expression datasets for leukemia and respectively normal hematopoiesis in order to develop a more comprehensive genomic characterization of the heterogeneity of leukemia in terms of 38 normal hematopoietic cell states. surprisingly, we find unusually complex expression programs involving large numbers of transcription factors, whose further in-depth analysis may help develop personalized therapies.",nan
2012,verification of description logic knowledge and action bases,"we introduce description logic (dl) knowledge and action bases (kab), a mechanism that provides both a semantically rich representation of the information on the domain of interest in terms of a dl kb and a set of actions to change such information over time, possibly introducing new objects. we resort to a variant of dl-lite where una is not enforced and where equality between objects may be asserted and inferred. actions are specified as sets of conditional effects, where conditions are based on epistemic queries over the kb (tbox and abox), and effects are expressed in terms of new aboxes. we address the verification of temporal properties expressed in a variant of first-order μ-calculus where a controlled form of quantification across states is allowed. notably, we show decidability of verification, under a suitable restriction inspired by the notion of weak acyclicity in data exchange.",nan
2012,nested monte-carlo tree search for online planning in large mdps,"monte-carlo tree search (mcts) is state of the art for online planning in large mdps. it is a best-first, sample-based search algorithm in which every state in the search tree is evaluated by the average outcome of monte-carlo rollouts from that state. these rollouts are typically random or directed by a simple, domain-dependent heuristic. we propose nested monte-carlo tree search (nmcts), in which mcts itself is recursively used to provide a rollout policy for higher-level searches. in three large-scale mdps, samegame, clickomania and bubble breaker, we show that nmcts is significantly more effective than regular mcts at equal time controls, both using random and heuristic rollouts at the base level. experiments also suggest superior performance to nested monte-carlo search (nmcs) in some domains.",nan
2012,markov constraints for generating lyrics with style,"we address the issue of generating texts in the style of an existing author, that also satisfy structural constraints imposed by the genre of the text. we focus on song lyrics, for which structural constraints are well-defined: rhyme and meter. although markov processes are known to be suitable for representing style, they are difficult to control in order to satisfy non-local properties, such as structural constraints, that require long distance modeling. we show that the framework of constrained markov processes allows us to precisely generate texts that are consistent with a corpus, while being controllable in terms of rhymes and meter, a result that no other technique, to our knowledge, could achieve to date. controlled markov processes consist in reformulating markov processes in the context of constraint satisfaction. we describe how to represent stylistic and structural properties in terms of constraints in this framework and we provide an evaluation of our method by comparing it to both pure markov and pure constraint-based approaches. we show how this approach can be used for the semi-automatic generation of lyrics in the style of a popular author that has the same structure as an existing song.",nan
2012,trust-based solution for robust self-configuration of distributed intrusion detection systems,"protection of network infrastructures against highly sophisticated network attacks requires an intelligent, robust, and adaptive detection system. we present a distributed model for collaboration of multiple heterogeneous intrusion detection sensors. the proposed model assumes that each network can be monitored with multiple detection sensors deployed in various locations. the model optimizes behavior of each sensor with respect to other sensors in highly dynamic network environments by using a game-theoretical approach. we propose a general formalization of the problem of distributed collaboration as a game between defenders and attackers and introduce a trust-based solution concept ε-fire that is suitable for solving this game in highly dynamic environments, preventing any poisoning or manipulation of the cooperative system by intelligent attackers.according to our experimental evaluation on real network traffic, the proposed model of distributed collaboration shows clear improvements in the overall detection capabilities of the system, caused by mutual specialization of individual detection sensors. the concept of opponent aware, self-coordinating and strategically reasoning intrusion detection network represents an effective next-generation solution that may match a market-based collaboration structures of the attackers.",nan
2012,what does it take to enforce an argument? minimal change in abstract argumentation,"argumentation is a dynamic process. the enforcing problem in argumentation, i.e. the question whether it is possible to modify a given argumentation framework (af) in such a way that a desired set of arguments becomes an extension or a subset of an extension, was first studied in [3] and positively answered under certain conditions. in this paper, we take up this research and study the more general problem of minimal change. that is, in brief, i) is it possible to enforce a desired set of arguments, and if so, ii) what is the minimal number of modifications (additions or removals of attacks) to reach such an enforcement, the so-called characteristic. we show for several dung semantics that this problem can be decided by local criteria encoded by the so-called value functions. furthermore, we introduce the corresponding equivalence notions between two afs which guarantee equal minimal efforts needed to enforce certain subsets, namely minimal-e-equivalence and the more general minimal change equivalence. we present characterization theorems for several dung semantics and finally, we show the relations to standard and the recently proposed strong equivalence [9] for a whole range of semantics.",nan
2012,the possible winner problem with uncertain weights,"the original possible winner problem is: given an unweighted election with partial preferences and a distinguished candidate c, can the preferences be extended to total ones such that c wins? we introduce a novel variant of this problem in which not some of the voters' preferences are uncertain but some of their weights. not much has been known previously about the weighted possible winner problem. we present a general framework to study this problem, both for integer and rational weights, with and without upper bounds on the total weight to be distributed, and with and without ranges to choose the weights from. we study the complexity of these problems for important voting systems such as scoring rules, copeland, ranked pairs, plurality with runoff, and (simplified) bucklin and fall-back voting.",nan
2012,representing value functions with recurrent binary decision diagrams,"the agent programming language golog features nondeterministic constructs such as nondeterministic branching. given an optimization theory the nondeterminism can be resolved optimally. there are techniques that allow to derive an abstract first-order description of a value function which is valid across all possible domain instances. the size of the domain may be unknown or even infinite. a finite horizon is assumed, though. that is, although the value function makes no assumptions about the size of the domain, the plans generated on the basis of the value functions are restricted to a certain length. in this paper we present a solution for this dilemma for a specific class of programs. in particular, we present a solution that allows to compute a representation of the value function for non-nested loops without requiring any restrictions on the number of loop iterations. a pleasing side effect is that our new representation of the value function is usually smaller than the representations for fixed horizons.",nan
2012,"an o(nlog n) bound consistency algorithm for the conjunction of an alldifferent and an inequality between a sum of variables and a constant, and its generalization","this paper gives an o(nlog n) bound-consistency filtering algorithm for the conjunction alldifferent(v0,v1,…,vn−1)∧ f(v0)⌖f(v1)⌖…⌖f(vn−1)≤ cst, (v0,v1,…,vn−1,cst∊ +), where (,⌖) is a commutative group, f is a unary function, and both ⌖ and f are monotone increasing. this complexity is equal to the complexity of the bound-consistency algorithm of the alldifferent constraint.",nan
2012,hybrid possibilistic conditioning for revision under weighted inputs,"we propose and investigate new operators in the possibilistic belief revision setting, obtained as different combinations of the conditioning operators on models and countermodels, as well as of how weighted inputs are interpreted. we obtain a family of eight operators that essentially obey the basic postulates of revision, with a few slight differences. these operators show an interesting variety of behaviors, making them suitable to representing changes in the beliefs of an agent in different contexts.",nan
2012,three-valued possibilistic networks,"possibilistic networks are graphical models that compactly encode joint possibility distributions. this paper studies a new form of possibilistic graphical models called three-valued possibilistic networks. contrary to standard belief networks where the beliefs are encoded using belief degrees within the interval [0, 1], three-valued possibilistic networks only allow three values: 0, 1 and 0, 1. the first part of this paper addresses foundational issues of three-valued possibilistic networks. in particular, we show that the semantics that can be associated with a three-valued possibilistic network is a family of compatible boolean networks. the second part of the paper deals with inference issues where we propose an extension to the min-based chain rule for three-valued networks. then, we show that the well-known junction tree algorithm can be directly adapted for the three-valued possibilistic setting.",nan
2012,implementing and evaluating provers for first-order modal logics,"while there is a broad literature on the theory of first-order modal logics, little is known about practical reasoning systems for them. this paper presents several implementations of fully automated theorem provers for first-order modal logics based on different proof calculi. among these calculi are the standard sequent calculus, a prefixed tableau calculus, an embedding into simple type theory, an instance-based method, and a prefixed connection calculus. all implementations are tested and evaluated on the new qmltp problem library for first-order modal logic.",nan
2012,heuristically accelerated reinforcement learning: theoretical and experimental results,"since finding control policies using reinforcement learning (rl) can be very time consuming, in recent years several authors have investigated how to speed up rl algorithms by making improved action selections based on heuristics. in this work we present new theoretical results – convergence and a superior limit for value estimation errors – for the class that encompasses all heuristics-based algorithms, called heuristically accelerated reinforcement learning. we also expand this new class by proposing three new algorithms, the heuristically accelerated q(λ), sarsa(λ) and td(λ), the first algorithms that uses both heuristics and eligibility traces. empirical evaluations were conducted in traditional control problems and results show that using heuristics significantly enhances the performance of the learning process.",nan
2012,learning rules of simplified boardgames by observing,"general game playing (ggp) agents learn strategies to skillfully play a wide variety of games when given only the rules of the game. the rules are provided in a language called game description language (gdl) and specify the initial game setup, what constitutes legal moves and how they update the game state when played, how the game terminates, and what the outcome is. in here we extend this line of research further, that is, we assume that the game-playing agent must learn the rules of a game by observing others play instead of them being provided. our focus here will mainly be on modeling piece movements with less attention placed on the remaining game-rule properties. we define a subset of games, we name simplified boardgames, that despite constituting only a small subset of games expressible in gdl nonetheless encapsulate a large variety of interesting piece movement patterns found in popular boardgames. we provide a well-defined formalism and a practicable algorithm for learning game rules of simplified boardgames. we empirically evaluate the learning algorithm on different boardgames and under different assumptions of availability of observations. furthermore, we show that our formalism offers at least an order of magnitude speedup over state-of-the-art logic-based gdl reasoners for fitting boardgames. the method is thus directly relevant for ggp systems.",nan
2012,a reinforcement-learning algorithm for sampling design in markov random fields,"optimal sampling in spatial random fields is a complex problem, which mobilizes several research fields in spatial statistics and artificial intelligence. in this paper we consider the case where observations are discrete-valued and modelled by a markov random field. then we encode the sampling problem into the markov decision process (mdp) framework. after exploring existing heuristic solutions as well as classical algorithms from the field of reinforcement learning (rl), we design an original algorithm, lsdp (least square dynamic programming), which uses simulated trajectories to solve approximately any finite-horizon mdp problem. based on an empirical study of the behaviour of these different approaches on binary models, we derive the following conclusions: i) a naïve heuristic, consisting in sampling sites where marginals are the most uncertain, is already an efficient sampling approach; ii) lsdp outperforms all the classical rl approaches we have tested; iii) lsdp outperforms the heuristic in cases when reconstruction errors have a high cost, or sampling actions are constrained. in addition, lsdp readily handles action costs in the optimisation problem, as well as cases when some sites of the mrf can not be observed.",nan
2012,a protocol based on a game-theoretic dilemma to prevent malicious coalitions in reputation systems,"in decentralized and open systems, a large number of agents interact and make collective decisions in order to share resources. as those systems are open, the presence of malicious agents needs to be considered. a way to deal with such agents in a decentralized fashion is to use reputation systems. but, as reputation systems are based on the aggregation of local trust between the agents, they are vulnerable to malicious coalitions, particularly to self-promotion based on false identities. in this paper, we propose a game-theoretic approach to prevent such manipulations. its main feature is that honest agents use in turn a false-name manipulation to fool malicious agents and to drive them into a dilemma. we show that the best response to that dilemma in terms of mixed strategy equilibrium leads the malicious agents to give up most of their manipulations.",nan
2012,iterative algorithm for solving two-player zero-sum extensive-form games with imperfect information,"we develop and evaluate a new exact algorithm for finding nash equilibria of two-player zero-sum extensive-form games with imperfect information. our approach is based on the sequence-form representation of the game, and uses an algorithmic framework of double-oracle methods that have been used successfully in other classes of games. the algorithm uses an iterative decomposition, solving restricted games and exploiting fast best-response algorithms to add additional sequences to the game over time. we demonstrate our algorithm on a class of adversarial graph search games motivated by real world border patrolling scenarios. the results indicate that our framework is a promising way to scale up solutions for extensive-form games, reducing both memory and computation time requirements.",nan
2012,interval temporal logics over finite linear orders: the complete picture,"interval temporal logics provide a natural framework for temporal reasoning about interval structures over linearly ordered domains, where intervals are taken as the primitive ontological entities. in this paper, we identify all fragments of halpern and shoham's interval temporal logic hs whose finite satisfiability problem is decidable. we classify them in terms of both relative expressive power and complexity. we show that there are exactly 62 expressively-different decidable fragments, whose complexity ranges from np-complete to non-primitive recursive (all other hs fragments have been already shown to be undecidable).",nan
2012,natural language arguments: a combined approach,"with the growing use of the social web, an increasing number of applications for exchanging opinions with other people are becoming available online. these applications are widely adopted with the consequence that the number of opinions about the debated issues increases. in order to cut in on a debate, the participants need first to evaluate the opinions in favour or against the debated issue. argumentation theory proposes algorithms and semantics to evaluate the set of accepted arguments, given the conflicts among them. the main problem is how to automatically generate the arguments from the natural language formulation of the opinions used in these applications. our paper addresses this problem by proposing and evaluating the use of natural language techniques to generate the arguments. in particular, we adopt the textual entailment approach, a generic framework for applied semantics, where linguistic objects are mapped by means of semantic inferences at a textual level. we couple textual entailment together with a dung-like argumentation system which allows us to identify the arguments that are accepted in the considered online debate. the originality of the proposed framework lies in the following point: natural language debates are analyzed and the arguments are automatically extracted.",nan
2012,preference extraction from negotiation dialogues,"this paper presents an nlp-based approach to extracting preferences from negotiation dialogues. we propose a new annotation scheme to study how preferences are linguistically expressed on two different corpus genres. we then automatically extract preferences in two steps: first, we extract the set of outcomes; then, we identify how these outcomes are ordered. we finally assess the reliability of our method on each corpus genre.",nan
2012,planning as quantified boolean formula,"this paper introduces two techniques for translating bounded propositional reachability problems into quantified boolean formulae (qbf). both exploit the binary-tree structure of the qbf problem to produce encodings logarithmic in the size of the instance and thus exponentially smaller than the corresponding sat encoding with the same bound. the first encoding is based on the iterative squaring formulation of rintanen. the second encoding is a compact tree encoding that is more efficient than the first one, requiring fewer alternations of quantifiers and fewer variables. we present experimental results showing that the approach is feasible, although not yet competitive with current state of the art sat-based solvers.",nan
2012,speeding up 2-way number partitioning,"several algorithms exist for optimally solving 2-way number partitioning. when the cardinality of the multiset to partition is high enough, solving algorithms have to relay on search techniques with low memory complexity. currently, ckk is the reference of such algorithms. here we propose a contribution to speed-up ckk. we detail how to consider terminal nodes with 4, 5, 6 or more numbers left, when the original ckk considers a node terminal when it contains 4 or less numbers left. using this idea, we propose new ckk implementations, which provide savings up to 70% of execution time with respect to the original ckk algorithm. we provide experimental evidence of the benefits of this approach on random number partitioning instances with numbers of 35 bits.",nan
2012,an efficient and adaptive approach to negotiation in complex environments,"this paper studies automated bilateral negotiation among self-interested agents in complex application domains which consist of multiple issues and real-time constraints and where the agents have no prior knowledge about their opponents' preferences and strategies. we describe a novel negotiation approach called omac (standing for “opponent modeling and adaptive concession”) which combines efficient opponent modeling and adaptive concession making. opponent modeling is achieved through standard wavelet decomposition and cubic smoothing spline, and concession adaptivity is achieved through dynamically setting the concession rate on the basis of the expected utilities of forthcoming counter-offers. experimental results are presented which demonstrate the effectiveness of our approach in both discounting and non-discounting domains. specifically, the results show that our approach performs better than the five top agents from the 2011 automated negotiation agents competition (anac).",nan
2012,combining dra and cyc into a network friendly calculus,"qualitative spatial reasoning is usually performed using spatial calculi specially designed to represent certain aspects of spatial knowledge. however most calculi must be adapted for use in applications where additional constraints are at play. this paper combines the dra and cyc algebras into a new calculus for reasoning within network structures. in the process we prove some interesting results about some properties of the standard operations, converse and composition, for versions of cyc and dra.",nan
2012,on exploiting structures of classical planning problems: generalizing entanglements,"much progress has been made in the research and development of automated planning algorithms in recent years. though incremental improvements in algorithm design are still desirable, complementary approaches such as problem reformulation are important in tackling the high computational complexity of planning. while machine learning and adaptive techniques have been usefully applied to automated planning, these advances are often tied to a particular planner or class of planners that are coded to exploit that learned knowledge. a promising research direction is in exploiting knowledge engineering techniques such as reformulating the planning domain and/or the planning problem to make the problem easier to solve for general, state-of-the-art planners. learning (outer) entanglements is one such technique, where relations between planning operators and initial or goal atoms are learned, and used to reformulate a domain by removing unneeded operator instances. here we generalize this approach significantly to cover relations between atoms and pairs of operators themselves, and develop a technique for producing inner entanglements. we present methods for detecting inner entanglements and for using them to do problem reformulation. we provide a theoretical treatment of the area, and an empirical evaluation of the methods using standard planning benchmarks and state-of-the-art planners.",nan
2012,ideal point guided iterative deepening,"many real world search problems involve different objectives, usually in conflict. in these cases the cost of a transition is given by a cost vector. this paper presents ipid, a new exact algorithm based on iterative deepening, that finds the set of all pareto-optimal paths for a search problem in a graph with vectorial costs. formal proofs of the admissibility of ipid are presented, as well as the results of some empirical comparisons between ipid and other approaches based on iterative deepening. empirical results show that ipid is usually faster than those approaches.",nan
2012,opportunistic branched plans to maximise utility in the presence of resource uncertainty,"in many applications, especially autonomous exploration, there is a trade-off between operational safety, forcing conservatism about resource usage; and maximising utility, requiring high resource utilisation. in this paper we consider a method of generating plans that maintain this conservatism whilst allowing exploitation of situations where resource usage is better than pessimistically estimated. we consider planning problems with soft goals, each with a violation cost. the challenge is to maximise utility (minimise the violation cost paid) whilst maintaining confidence that the plan will execute within the specified limits. we first show how forward search planning can be extended to generate such plans. then we extend this to build branched plans: tree structures labelled with conditions on executing branches. lower cost branches can be followed if their conditions are met. we demonstrate that the use of such plans can dramatically increase utility whilst still obeying strict safety constraints.",nan
2012,"a sat-based approach for discovering frequent, closed and maximal patterns in a sequence","in this paper we propose a satisfiability-based approach for enumerating all frequent, closed and maximal patterns with wildcards in a given sequence. in this context, since frequency is the most used criterion, we introduce a new polynomial inductive formulation of the cardinality constraint as a boolean formula. a nogood-based formulation of the anti-monotonicity property is proposed and dynamically used for pruning. this declarative framework allows us to exploit the efficiency of modern sat solvers and particularly their clause learning component. the experimental evaluation on real world data shows the feasibility of our proposed approach in practice.",nan
2012,compression-based aode classifiers,"we propose the comp-aode classifier, which adopts the compression-based approach [1] to average the posterior probabilities computed by different non-naive classifiers (spodes). comp-aode improves classification performance over the well-known aode [10] model. comp-aode assumes a uniform prior over the spodes; we then develop the credal classifier comp-aode*, substituting the uniform prior by a set of priors. comp-aode* returns more classes when the classification is prior-dependent, namely if the most probable class varies with the prior adopted over the spodes. comp-aode* achieves higher classification utility than both comp-aode and aode.",nan
2012,bounded single-peaked width and proportional representation,"this paper is devoted to the proportional representation (pr) problem when the preferences are clustered single-peaked. pr is a “multi-winner” election problem, that we study in chamberlin and courant's scheme [6]. we define clustered single-peakedness as a form of single-peakedness with respect to clusters of candidates, i.e. subsets of candidates that are consecutive (in arbitrary order) in the preferences of all voters. we show that the pr problem becomes polynomial when the size of the largest cluster of candidates (width) is bounded. furthermore, we establish the polynomiality of determining the single-peaked width of a preference profile (minimum width for a partition of candidates into clusters compatible with clustered single-peakedness) when the preferences are narcissistic (i.e., every candidate is the most preferred one for some voter).",nan
2012,when intelligence is just a matter of copying,"the paper investigates a general approach that allows us to solve iq tests based on raven's progressive matrices automatically. the 3 × 3 raven matrices exhibit 8 geometric pictures displayed in 8 cells of the matrix, which have to be logically completed with a 9th picture to be put in the remaining empty matrix cell. in these tests, a set of candidate pictures for the solution is also given. the suggested approach is based on a logical view of analogical proportions (i.e., statements of the form “a is to b as c is to d”). the reading of raven matrices in terms of such proportions can be applied to a feature-based description of the pictures, but also, in a number of cases, to a very low level representation, i.e., the pixel level. it appears that the analogical proportion reading just amounts here to a recopy of patterns of feature values that already appear in the data (after checking that there is no conflicting patterns). implementing this principle, the proposed algorithm computes the 9th picture, without the help of any set of candidate solutions, but only on the basis of the 8 known cells of the raven matrices. a comparison with other approaches is provided, and we emphasize the generality of the approach which is able to provide a simple and uniform mechanism applicable in many situations.",nan
2012,context-aware access control for rdf graph stores,"we present shi3ld, an access control framework for rdf stores. our solution supports access from mobile devices with context-aware policies and is exclusively grounded on standard semantic web languages. designed as a pluggable filter for generic sparql endpoints, the module uses rdf named graphs and sparql to protect triples. evaluation shows faster execution time for low-selective queries and less impact on larger datastores.",nan
2012,decision-making with sugeno integrals: dmu vs. mcdm,"this paper clarifies the connection between multiple criteria decision-making and decision under uncertainty in a qualitative setting relying on a finite value scale. while their mathematical formulations are very similar, the underlying assumptions differ and the latter problem turns out to be a special case of the former. sugeno integrals are very general aggregation operations that can represent preference relations between uncertain acts or between multifactorial alternatives where attributes share the same totally ordered domain. this paper proposes a generalized form of the sugeno integral that can cope with attributes which have distinct domains via the use of qualitative utility functions. in the case of decision under uncertainty, this model corresponds to state-dependent preferences on act consequences. axiomatizations of the corresponding preference functionals are proposed in the cases where uncertainty is represented by possibility measures, by necessity measures, and by general monotonic set-functions, respectively. this is achieved by weakening previously proposed axiom systems for sugeno integrals.",nan
2012,an analysis of chaining in multi-label classification,"the idea of classifier chains has recently been introduced as a promising technique for multi-label classification. however, despite being intuitively appealing and showing strong performance in empirical studies, still very little is known about the main principles underlying this type of method. in this paper, we provide a detailed probabilistic analysis of classifier chains from a risk minimization perspective, thereby helping to gain a better understanding of this approach. as a main result, we clarify that the original chaining method seeks to approximate the joint mode of the conditional distribution of label vectors in a greedy manner. as a result of a theoretical regret analysis, we conclude that this approach can perform quite poorly in terms of subset 0/1 loss. therefore, we present an enhanced inference procedure for which the worst-case regret can be upper-bounded far more tightly. in addition, we show that a probabilistic variant of chaining, which can be utilized for any loss function, becomes tractable by using monte carlo sampling. finally, we present experimental results confirming the validity of our theoretical findings.",nan
2012,a study of local minimum avoidance heuristics for sat,"stochastic local search for satisfiability (sat) has successfully been applied to solve a wide range of problems. however, it still suffers from a major shortcoming, i.e. being trapped in local minima. in this study, we explore different heuristics to avoid local minima. the main idea is to proactively avoid local minima rather than reactively escape from them. this is worthwhile because it is time consuming to successfully escape from a local minimum in a deep and wide valley. in addition, revisiting an encountered local minimum several times makes it worse. our new trap avoidance heuristics that operate in two phases: (i) learning of pseudo-conflict information at each local minimum, and (ii) using this information to avoid revisiting the same local minimum. we present a detailed empirical study of different strategies to collect pseudo-conflict information (using either static or dynamic heuristics) as well as to forget the outdated information (using naive or time window smoothing). we select a bench-mark suite that includes all random and structured instances used in the 2011 sat competition and three sets of hardware and software verification problems. our results show that the new heuristics significantly outperform existing stochastic local search solvers (including sparrow2011 - the best local search solver for random instances in the 2011 sat competition) on all tested benchmarks.",nan
2012,symbolic a* search with pattern databases and the merge-and-shrink abstraction,"the efficiency of heuristic search planning crucially depends on the quality of the search heuristic, while succinct representations of state sets in decision diagrams can save large amounts of memory in the exploration. bdda* – a symbolic version of a* search – combines the two approaches into one algorithm. this paper compares two of the leading heuristics for sequential-optimal planning: the merge-and-shrink and the pattern databases heuristic, both of which can be compiled into a vector of bdds and be used in bdda*. the impact of optimizing the variable ordering is highlighted and experiments on benchmark domains are reported.",nan
2012,preferring properly: increasing coverage while maintaining quality in anytime temporal planning,"temporal fast downward (tfd) is a successful temporal planning system that is capable of dealing with numerical values. rather than decoupling action selection from scheduling, it searches directly in the space of time-stamped states, an approach that has shown to produce plans of high quality at the price of coverage. to increase coverage, tfd incorporates deferred evaluation and preferred operators, two search techniques that usually decrease the number of heuristic calculations by a large amount. however, the current definition of preferred operators offers only limited guidance in problems where heuristic estimates are weak or where subgoals require the execution of mutex operators. in this paper, we present novel methods for refinement of this definition and show how to combine the diverse strengths of different sets of preferred operators using a restarting procedure incorporated into a multi-queue best-first search. these techniques improve tfd's coverage drastically and preserve the average solution quality, leading to a system that solves more problems than each of the competitors of the temporal satisficing track of ipc 2011 and clearly outperforms all of them in terms of ipc score.",nan
2012,weighted manipulation for four-candidate llull is easy,"our main contribution is a surprising polynomial-time algorithm for weighed coalitional manipulation of four-candidate copeland (also known as llull) elections. on the technical side, our algorithm relies on a polynomial-time routine that solves a variant of the partition problem. we also show that there is a pseudopolynomial-time algorithm for weighted coalitional manipulation with a fixed number of candidates under any anonymous rule with a polynomial-time winner-determination procedure.",nan
2012,agent strategies for aba-based information-seeking and inquiry dialogues,"much research has been devoted to the use of argumentation to support inter-agent dialogues. here, we contribute to this line of research by investigating the strategic behaviour of agents in argumentation-based dialogues, using assumption-based-argumentation (aba) as the underlying framework. we will focus on information-seeking and inquiry dialogues, giving formalisations thereof and showing how they can be supported by specific classes of strategy-move functions for agents to select suitable utterances.",nan
2012,"guiding user choice during discussion by silence, examples and justifications","this paper describes an approach for guiding human choice-making by a computerized agent, in a conversational setting, where both user and agent provide meaningful input. in the proposed approach, the agent attempts to convince a person by providing examples for the person to emulate or by providing justifications for the person to internalize and build or change her preferences accordingly. the agent can take into account examples and justifications provided by the person. in a series of experiments where the task was selecting a location for a school, a computer agent interacted with subjects using a textual chat-type interface, with different agent designs being used in different experiments. the results show that the example-providing agent outperformed the justification providing agent and both, surprisingly, outperformed an agent which pre sented the subject with both examples and justifications. in addition, it was demonstrated that in some cases the best strategy for the agent is to keep silent.",nan
2012,combining bootstrapping and feature selection for improving a distributional thesaurus,"work about distributional thesauri has now widely shown that the relations in these thesauri are mainly reliable for high frequency words and for capturing semantic relatedness rather than strict semantic similarity. in this article, we propose a method for improving such a thesaurus through its re-balancing in favor of middle and low frequency words. this method is based on a bootstrapping mechanism: a set of positive and negative examples of semantically related words are selected in a unsupervised way from the results of the initial measure and used for training a supervised classifier. this classifier is then applied for reranking the initial semantic neighbors. we evaluate the interest of this reranking for a large set of english nouns with various frequencies.",nan
2012,argumentation-based reinforcement learning for robocup soccer keepaway,"reinforcement learning (rl) suffers from several difficulties when applied to domains with no obvious goal state defined; this leads to inefficiency in rl algorithms. in this paper we consider a solution within the context of a widely-used testbed for rl, that of robocup keepaway soccer. we introduce argumentation-based rl (abrl), using methods from argumentation theory to integrate domain knowledge, represented by arguments, into the smdp algorithm for rl by using potential-based reward shaping. empirical results show that abrl outperforms the original smdp algorithm, for this game, by improving the optimal performance.",nan
2012,case-based planning for problems with real-valued fluents: kernel functions for effective plan retrieval,"case-based planning (cbp) re-uses existing plans as a starting point to solve new planning problems. in this work, we address cbp for planning in pddl domains with real-valued fluents, that are essential to model real-world problems involving continuous resources. specifically, we propose some new heuristic techniques for retrieving a plan from a library of existing plans that is promising for a new given planning problem, i.e., that can be efficiently adapted to solve the new problem. the effectiveness of these techniques, which derive much of their power from the use of the numerical information in the planning problem specification and in the library plans, is then evaluated by an experimental analysis.",nan
2012,a bayesian multiple kernel learning framework for single and multiple output regression,multiple kernel learning algorithms are proposed to combine kernels in order to obtain a better similarity measure or to integrate feature representations coming from different data sources. most of the previous research on such methods is focused on classification formulations and there are few attempts for regression. we propose a fully conjugate bayesian formulation and derive a deterministic variational approximation for single output regression. we then show that the proposed formulation can be extended to multiple output regression. we illustrate the effectiveness of our approach on a single output benchmark data set. our framework outperforms previously reported results with better generalization performance on two image recognition data sets using both single and multiple output formulations.,nan
2012,approximate tradeoffs on matroids,"we consider problems where a solution is evaluated with a couple. each coordinate of this couple represents an agent's utility. due to the possible conflicts, it is unlikely that one feasible solution is optimal for both agents. then, a natural aim is to find tradeoffs. we investigate tradeoff solutions with guarantees for the agents.the focus is on discrete problems having a matroid structure. we provide polynomial-time deterministic algorithms which achieve several guarantees and we prove that some guarantees are not possible to reach.",nan
2012,process discovery via precedence constraints,"a key task in process mining consists of building a graph of causal dependencies over process activities, which can then be used to derive more expressive models in some high-level modeling language. an approach to accomplish this task is presented where the learning process can exploit the background knowledge that, in many cases, is available to the analysts taking care of the process (re-)design. the method is based on encoding the information gathered from the log and the (possibly) given background knowledge in terms of precedence constraints, i.e., constraints over the topology of the graphs. learning algorithms are eventually formulated in terms of reasoning problems over precedence constraints, and the computational complexity of such problems is thoroughly analyzed by tracing their tractability frontier. the whole approach has been implemented in a prototype system leveraging a solid constraint programming platform, and results of experimental activity are reported.",nan
2012,hard and easy k-typed compact coalitional games: the knowledge of player types marks the boundary,"coalitional games model scenarios where rational agents can form coalitions so as to obtain higher worths than by acting in isolation. once a coalition forms and obtains its worth, the problem of how this worth can be fairly distributed has to be faced. desirable worth distributions are usually referred to as solution concepts. recent research pointed out that, while reasoning problems involving such solution concepts are hard in general for games specified in compact form (e.g., graph games), some of them, in particular the core, become tractable when agents come partitioned into a fixed number k of types, i.e., of classes of strategically equivalent players. the paper continues along this line of research, by firstly showing that two other relevant solution concepts, the kernel and the nucleolus, are tractable in this setting and independently of the specific game encoding, provided worth functions are given as a polynomial-time computable oracles. then, it analyzes a different setting where games are still k-typed but the actual player partitioning is not a-priori known. within this latter setting, the paper addresses the question about how efficiently strategic equivalence between pairs of players can be recognized, and reconsiders the computational complexity of the core, the kernel, and the nucleolus. all such problems and notions emerged to be intractable, thereby evidencing that the knowledge of player types marks the boundary of tractability for reasoning about k-typed coalitional games.",nan
2012,partial cooperation in multi-agent local search,"multi-agent systems usually address one of two forms of interaction. one has completely competitive agents that act selfishly, each maximizing its own gain from the interaction. auctions and voting scenarios usually assume such agents and follow game theoretic results. the other form of interaction has multiple agents that cooperatively search for some global goal, such as an optimal time slot allocation for all landing aircrafts in an airport.the present paper proposes a paradigm for multiple agents solving a distributed problem using local search algorithms and acting in a partially cooperative manner. that is, agents with different preferences search for a minimal cost solution to an asymmetric distributed constraints optimization problem (adcop), while keeping a limited form of self interest.two approaches for using local search in the partial cooperative paradigm are proposed. the first, modifies the anytime mechanism introduced by zivan [19] so that agents can eliminate solutions which do not satisfy their cooperation thresholds. the second proposes a new local search algorithm that explores only valid solutions.the performance of two innovative algorithms implementing these two approaches, are compared with state of the art local search algorithms on three different setups. when personal constraints are strict, the proposed algorithms have a large advantage over existing algorithms. we provide insights to the success of existing algorithms within the anytime framework when constraints are loose.",nan
2012,towards generalizing the success of monte-carlo tree search beyond the game of go,"monte-carlo tree search and specifically the variants of the uct algorithm have been a break-through in ai of the board game go. however, uct has had limited applicability to other domains. we study the limitations of some of the existing variants of uct in a small-scale markov decision process (mdp), and propose new variants that can reduce those limitations. our experiments show great improvements in performance against traditional uct and comparable performance to estabilished reinforcement learning algorithm, thus opening possibilities for applying uct in other problem domains.",nan
2012,complexity of branching temporal description logics,"we study branching-time temporal description logics (tdls) based on the dls alc and el and the temporal logics ctl and ctl*. the main contributions are algorithms for satisfiability that are more direct than existing approaches, and (mostly) tight elementary complexity bounds that range from ptime to 2exptime and 3exptime. a careful use of tree automata techniques allows us to obtain transparent and uniform algorithms, avoiding to deal directly with the intricacies of ctl*.",nan
2012,online voter control in sequential elections,"previous work on voter control, which refers to situations where a chair seeks to change the outcome of an election by deleting, adding, or partitioning voters, takes for granted that the chair knows all the voters' preferences and that all votes are cast simultaneously. however, elections are often held sequentially and the chair thus knows only the previously cast votes and not the future ones, yet needs to decide instantaneously which control action to take. we introduce a framework that models online voter control in sequential elections. we show that the related problems can be much harder than in the standard (non-online) case: for certain election systems, even with efficient winner problems, online control by deleting, adding, or partitioning voters is pspace-complete, even if there are only two candidates. in addition, we obtain completeness for conp in the deleting/adding cases with a bounded deletion/addition limit, and for np in the partition cases with only one candidate. finally, we show that for plurality, online control by deleting or adding voters is in p, and for partitioning voters is conp-hard.",nan
2012,planning with semantic attachments: an object-oriented view,"in recent years, domain-independent planning has been applied to a rising number of real-world applications. usually, the description language of choice is pddl. however, pddl is not suited to model all challenges imposed by real-world applications. dornhege et al. proposed semantic attachments to allow the computation of boolean fluents by external processes called modules during planning. to acquire state information from the planning system a module developer must perform manual requests through a callback interface which is both inefficient and error-prone. in this paper, we present the object-oriented planning language opl, which incorporates the structure and advantages of modern object-oriented programming languages. we demonstrate how a domain-specific module interface that allows to directly access the planner state using object member functions is automatically generated from an opl planning task. the generated domain-specific interface allows for a safe and less error-prone implementation of modules. we show experimentally that this interface is more efficient than the pddl-based module interface of tfd/m.",nan
2012,comparator selection for rpc with many labels,"the ranking by pairwise comparison algorithm (rpc) is a well established label ranking method. however, its complexity is of o(n2)in the number n of labels. we present algorithms for selecting, before model construction, a subset of comparators of size o(n), to reduce computational complexity without loss in accuracy.",nan
2012,an anytime algorithm for finding the ε-core in nontransferable utility coalitional games,"we provide the first anytime algorithm for finding the ε-core in a nontransferable utility coalitional game. for a given set of possible joint actions, our algorithm calculates ε, the maximum utility any agent could gain by deviating from this set of actions. if ε is too high, our algorithm searches for a subset of the joint actions which leads to a smaller ε. simulations show our algorithm is more efficient than an exhaustive search by up to 2 orders of magnitude.",nan
2012,spectrum enhanced dynamic slicing for better fault localization,"debugging consumes a considerable amount of time in software engineering, but it is rarely automated. in this paper, we focus on improving existing fault localization techniques. spectrum-based fault localization (sfl) and slicing-hitting-set-computation (shsc) are two techniques based on program execution traces. both techniques come with small computational overhead and aid programmers to faster identify possible locations of faults. however, they have disadvantages: shsc results in an undesirable high ranking of statements which are executed in many test cases, such as constructors. sfl operates on block level. therefore, it cannot provide fine-grained results. we combine shsc with sfl in order to eliminate these disadvantages. our objective is to improve the ranking of faulty statements so that they allow for better fault localization than when using the previously mentioned methods separately. we show empirically that the resulting approach reduces the number of statements a programmer needs to check manually. in particular, we gain improvements of about 50 % percent for shsc and 25 % for sfl.",nan
2012,sat vs. search for qualitative temporal reasoning,"empirical data from recent work has indicated that sat-based solvers can outperform native search-based solvers on certain classes of problems in qualitative temporal reasoning, particularly over the interval algebra (ia). the present work shows that, for reasoning with ia, sat strictly dominates search in theoretical power: (1) we present a sat encoding of ia that simulates the use of tractable subsets in native solvers. (2) we show that the refutation of any inconsistent ia network can always be done by sat (via our new encoding) as efficiently as by native search. (3) we exhibit a class of ia networks that provably require exponential time to refute by native search, but can be refuted by sat in polynomial time.",nan
2012,symmetries in itemset mining,"in this paper, we describe a new framework for breaking symmetries in itemset mining problems. symmetries are permutations between items that leave invariant the transaction database. such kind of structural knowledge induces a partition of the search space into equivalent classes of symmetrical itemsets. our proposed framework aims to reduce the search space of possible interesting itemsets by detecting and breaking symmetries between items. firstly, we address symmetry discovery in transaction databases. secondly, we propose two different approaches to break symmetries in a preprocessing step by rewriting the transaction database. this approach can be seen as an original extension of the symmetry breaking framework widely used in propositional satisfiability and constraint satisfaction problems. finally, we show that apriori-like algorithms can be enhanced by dynamic symmetry reasoning. our experiments clearly show that several itemset mining instances taken from the available datasets contain such symmetries. we also provide experimental evidence that breaking such symmetries reduces the size of the output on some families of instances.",nan
2012,"concepts, agents, and coalitions in alternating time","we consider a combination of the strategic logic atl with the description logic alco. in order to combine the logics in a flexible way, we assume that every individual can be (potentially) an agent. we also take the novel approach to teams by assuming that a coalition has an identity on its own, and hence its membership can vary. in terms of technical results, we show that the logic does not have the finite model property, though both atl and alco do. we conjecture that the satisfiability problem may be undecidable. on the other hand, model checking of the combined logic is decidable and even tractable. finally, we define a particular variant of realizability that combines satisfiability of alco with model checking of the atl dimension, and we show that this new problem is decidable.",nan
2012,large-scale interactive ontology matching: algorithms and implementation,"in this paper we present the ontology matching system logmap 2, a much improved version of its predecessor logmap. logmap 2 supports user interaction during the matching process, which is essential for use cases requiring very accurate mappings. interactivity, however, imposes very strict scalability requirements; we are able to satisfy these requirements by providing real-time user response even for large-scale ontologies. finally, logmap 2 implements scalable reasoning and diagnosis algorithms, which minimise any logical inconsistencies introduced by the matching process.",nan
2012,characterization of positive and negative information in comparative preference representation,"in the last decade, ai researchers have pointed out the existence of two types of information: positive information and negative information. this distinction has also been asserted in cognitive psychology. distinguishing between these two types of information may be useful in both knowledge and preference representation. in the first case, one distinguishes between situations which are not impossible because they are not ruled out by the available knowledge, and what is possible for sure. in the second case, one distinguishes between what is not rejected and what is really desired. besides it has been shown that possibility theory is a convenient tool to model and distinguish between these two types of information. knowledge/preference representation languages have also been extended to cope with this particular kind of information. nevertheless despite solid theoretical advances in this topic, the crucial question of “which reading (negative or positive) one should have” remains a real bottleneck. in this paper, we focus on comparative statements. we present a set of postulates describing different situations one may encounter. then we provide a representation theorem describing which sets of postulates are satisfied by which kind of information (negative or positive) and conversely. one can then decide which reading to apply depending on which postulates she privileges.",nan
2012,a ranking semantics for first-order conditionals,"usually, default rules in the form of conditional statements are built on propositional logic, representing classes of individuals by propositional variables, as in “birds fly, but penguins don't”. only few approaches have addressed the problem of giving formal semantics to first-order conditionals that allow (nonmonotonic) inferences both for classes and for individuals. in this paper, we present a semantics for first-order conditionals that is based on ordinal conditional (or ranking) functions which are well-known in the area of propositional default reasoning and makes use of representative individuals to establish conditional relationships. we generalize the c-representation approach of [8] for inductive reasoning with first-order conditionals, and evaluate our approach via benchmark examples and a catalogue of general properties.",nan
2012,a new approach to the snake-in-the-box problem,"the “snake-in-the-box” problem, first described more than 50 years ago, is a hard combinatorial search problem whose solutions have many practical applications. until recently, techniques based on evolutionary computation have been considered the state-of-the-art for solving this deterministic maximization problem, and held most significant records. this paper reviews the problem and prior solution techniques, then presents a new technique, based on monte-carlo tree search, which finds significantly better solutions than prior techniques, is considerably faster, and requires no tuning.",nan
2012,delegating decisions in strategic settings,"we formalise and investigate the following problem. a principal must delegate a number of decisions to a collection of agents. once the decisions are delegated, the agents to whom the decisions are delegated will act selfishly, rationally, and independently in pursuit of their own preferences. the principal himself is assumed to be self-interested, and has some goal that he desires to be achieved. the delegation problem is then, given such a setting, is it possible for the principal to delegate decisions in such a way that, if all the agents to whom decisions have been delegated then make decisions rationally, the principal's goal will be achieved in equilibrium. we formalise this problem using boolean games, which provides a very natural framework within which to capture the delegation problem: decisions are directly represented as boolean variables, which the principal assigns to agents. after motivating and formally defining the delegation problem, we investigate the computational complexity of the problem, and some issues surrounding it.",nan
2012,reconciling owl and non-monotonic rules for the semantic web,"we propose a description logic extending sroiq (the description logic underlying owl 2 dl) and at the same time encompassing some of the most prominent monotonic and nonmonotonic rule languages, in particular datalog extended with the answer set semantics. our proposal could be considered a substantial contribution towards fulfilling the quest for a unifying logic for the semantic web. as a case in point, two non-monotonic extensions of description logics considered to be of distinct expressiveness until now are covered in our proposal. in contrast to earlier such proposals, our language has the “look and feel” of a description logic and avoids hybrid or first-order syntaxes.",nan
2012,hybrid regression-classification models for algorithm selection,"many state of the art algorithm selection systems use machine learning to either predict the run time or a similar performance measure of each of a set of algorithms and choose the algorithm with the best predicted performance or predict the best algorithm directly. we present a technique based on the well-established machine learning technique of stacking that combines the two approaches into a new hybrid approach and predicts the best algorithm based on predicted run times. we demonstrate significant performance improvements of up to a factor of six compared to the previous state of the art. our approach is widely applicable and does not place any restrictions on the performance measure used, the way to predict it or the machine learning used to predict the best algorithm. we investigate different ways of deriving new machine learning features from the predicted performance measures and evaluate their effectiveness in increasing performance further. we use five different regression algorithms for performance prediction on five data sets from the literature and present strong empirical evidence that shows the effectiveness of our approach.",nan
2012,justifying dominating options when preferential information is incomplete,"providing convincing explanations to accompany recommendations is a key issue in decision-aiding. in the context of decisions involving multiple criteria, the problem is made very difficult because the decision model itself may involve a complex process. in this paper, we investigate the following issue: when the preferential information provided by the user is incomplete, is there a principled way to define what is a “simple” explanation for a recommended choice? we argue first that explanations may necessitate different levels of detail. next, we show that even when a detailed explanation is necessary, it is possible to distinguish explanations of different levels of complexity. our results rely on an original connection we establish between the “mechanics” required to compute supporting coalitions of criteria and the simplicity of the explanation.",nan
2012,fixed-parameter algorithms for closed world reasoning,"closed world reasoning and circumscription are essential tasks in ai. however, their high computational complexity is a serious obstacle for their practical application. in this work we employ the framework of parameterized complexity theory in order to search for fixed-parameter algorithms. we consider eleven parameters describing different characteristics of the input. for several combinations of these parameters we are able to design efficient fixedparameter tractable algorithms. all our algorithms have a runtime only single-exponential in the parameters and linear in the input size. furthermore, by providing parameterized hardness results we show that we have actually found all tractable fragments involving these eleven parameters. we hereby offer a complete picture of the parameterized complexity of brave closed world reasoning and circumscription.",nan
2012,efficient reasoning in multiagent epistemic logics,"in many applications, agents must reason about what other agents know, whether to coordinate with them or to come out on top in a competitive situation. however in general, reasoning in a multiagent epistemic logic such as kn has high complexity. in this paper, we look at a restricted class of knowledge bases that are sets of modal literals. we call these proper epistemic knowledge bases (pekbs). we show that after a pekb has been put in prime implicate normal form (pinf), an efficient database-like query evaluation procedure can be used to check whether an arbitrary query is entailed by the pekb. the evaluation procedure is always sound and sometimes complete. we also develop a procedure to convert a pekb into pinf. as well, we extend our approach to deal with introspection.",nan
2012,knowledge-based programs as plans - the complexity of plan verification,"knowledge-based programs (kbps) are high-level protocols describing the course of action an agent should perform as a function of its knowledge. the use of kbps for expressing action policies in ai planning has been surprisingly underlooked. given that to each kbp corresponds an equivalent plan and vice versa, kbps are typically more succinct than standard plans, but imply more online computation time. here we compare kbps and standard plans according to succinctness and to the complexity of plan verification.",nan
2012,a path-optimal gac algorithm for table constraints,"filtering by generalized arc consistency (gac) is a fundamental technique in constraint programming. recent advances in gac algorithms for extensional constraints rely on direct manipulation of tables during search. simple tabular reduction (str), which systematically removes invalid tuples from tables, has been shown to be a simple yet efficient approach. str2, a refinement of str, is considered to be among the best filtering algorithms for positive table constraints. in this paper, we introduce a new gac algorithm called str3 that is specifically designed to enforce gac during search. str3 can completely avoid unnecessary traversal of tables, making it optimal along any path of the search tree. our experiments show that str3 is much faster than str2 when the average size of the tables is not reduced drastically during search.",nan
2012,updating inconsistent description logic knowledge bases,"finding an appropriate semantics for task of updating an inconsistent knowledge base is a challenging problem. in this paper, we consider knowledge bases expressed in description logics, and focus on abox inconsistencies, i.e., the case where the tbox is consistent, but the whole knowledge base is not. our first contribution is the definition of a new semantics for updating an inconsistent description logic knowledge base with both the insertion and the deletions of a set of abox assertions. we then concentrate on thedl-lite family of description logics, and present algorithms for updating a possibly inconsistent knowledge base expressed in the most expressive logic of such family. we show that, by virtue of both the characteristics of our semantics, and the limited expressive power of dl-lite, both insertions and deletions can be done in polynomial time with respect of the size of the abox.",nan
2012,almost-truthful mechanisms for fair social choice functions,"this paper deals with the implementation of social choice functions in fair multiagent decision problems. in such problems the determination of the best alternatives often relies on the maximization of a non-utilitarian social welfare function so as to account for equity. however, in such decision processes, agents may have incentive to misreport their preferences to obtain more favorable choices. it is well known that, for social choice functions based on the maximization of an affine aggregator of individual utilities, we can preclude any manipulation by introducing payments (vcg mechanisms). unfortunately such truthful mechanisms do not exist for non-affine maximizers (roberts' theorem). for this reason, we introduce here a notion of “almost-truthfulness” and investigate the existence of payments enabling the elaboration of almost-truthful mechanisms for non-additive social welfare functions such as social gini evaluation functions used in fair optimization.",nan
2012,choosing combinatorial social choice by heuristic search,"this paper studies the problem of computing aggregation rules in combinatorial domains, where the set of possible alternatives is a cartesian product of (finite) domain values for each of a given set of variables, and these variables are usually not preferentially independent. we propose a very general heuristic framework sc* for computing different aggregation rules, including rules for cardinal preference structures and condorcet-consistent rules. sc* highly reduces the search effort and avoid many pairwise comparisons, and thus it significantly reduces the running time. moreover, sc* guarantees to choose the set of winners in aggregation rules for cardinal preferences. with condorcet-consistent rules, sc* chooses the outcomes that are sufficiently close to the winners.",nan
2012,creating features from a learned grammar in a simulated student,"understanding and developing intelligent agents that simulate human learning has been a long-standing goal in both artificial intelligence and cognitive science. although learning agents are able to produce intelligent behavior with less human knowledge engineering than in the past, intelligent agent developers are still required to manually encode much prior domain knowledge. we recently proposed an efficient algorithm that acquires representations of the world using an unsupervised grammar induction algorithm, and integrated this representation learner into a simulated student, simstudent. in this paper, we use the representation learner to automatically generate a set of feature predicates based on the acquired representation, and provide the automatically generated feature predicates to simstudent as prior domain knowledge. we show that with the automatically-generated feature predicates, the learning agent can perform at a level comparable to when it is given manually-constructed feature predicates, but without the effort required to create these feature predicates.",nan
2012,width and serialization of classical planning problems,"we introduce a width parameter that bounds the complexity of classical planning problems and domains, along with a simple but effective blind-search procedure that runs in time that is exponential in the problem width. we show that many benchmark domains have a bounded and small width provided that goals are restricted to single atoms, and hence that such problems are provably solvable in low polynomial time. we then focus on the practical value of these ideas over the existing benchmarks which feature conjunctive goals. we show that the blind-search procedure can be used for both serializing the goal into subgoals and for solving the resulting problems, resulting in a ‘blind’ planner that competes well with a best-first search planner guided by state-of-the-art heuristics. in addition, ideas like helpful actions and landmarks can be integrated as well, producing a planner with state-of-the-art performance.",nan
2012,game-theoretic approach to adversarial plan recognition,"we argue that the problem of adversarial plan recognition, where the observed agent actively tries to avoid detection, should be modeled in the game theoretic framework. we define the problem as an imperfect-information extensive-form game between the observer and the observed agent. we propose a novel algorithm that approximates the optimal solution in the game using monte-carlo sampling. the experimental evaluation is performed on a synthetic domain inspired by a network security problem. the proposed method produces significantly better results than several simple baselines on a practically large domain.",nan
2012,"here, there, but not everywhere: an extended framework for qualitative constraint satisfaction","dealing with spatial and temporal knowledge is an indispensable part of almost all aspects of human activities. the qualitative approach to spatial and temporal reasoning (qstr) provides a promising framework for spatial and temporal knowledge representation and reasoning. qstr typically represents spatial/temporal knowledge in terms of qualitative relations (e.g., to the east of, after), and reasons with the knowledge by solving qualitative constraints. when formulating a qualitative constraint satisfaction problem (csp), it is usually assumed that each variable could be “here, there and everywhere.” practical applications e.g. urban planning, however, often require a variable taking values from a certain finite subset of the universe, i.e. require it to be 'here or there'. this paper extends the classic framework of qualitative constraint satisfaction by allowing variables taking values from finite domains. the computational complexity of this extended consistency problem is examined for five most important qualitative calculi, viz. point algebra, interval algebra, cardinal relation algebra, rcc-5, and rcc-8. we show that the extended consistency problem remains in np, but when only basic constraints are considered, the extended consistency problem for each calculus except point algebra is already np-hard.",nan
2012,inconsistency handling in datalog+/− ontologies,"the advent of the semantic web has made the problem of inconsistency management especially relevant. datalog+/− is a family of ontology languages that is in particular useful for representing and reasoning over lightweight ontologies in the semantic web. in this paper, we study different semantics for query answering in inconsistent datalog+/− ontologies. we develop a general framework for inconsistency management in datalog+/− ontologies based on incision functions from belief revision, in which we can characterize several query answering semantics as special cases: (i) consistent answers, originally developed for relational databases and recently adopted for some classes of description logics (dls), (ii) intersection semantics, a sound approximation of consistent answers, and (iii) lazy consistent answers, a novel alternative semantics that offers a good compromise between quality of answers and computation time. we also provide complexity results for query answering under the different semantics, including data tractability results.",nan
2012,fair division of indivisible goods under risk,"we consider the problem of fairly allocating a set of m indivisible objects to n agents having additive preferences over them. in this paper we propose an extension of this classical problem, where each object can possibly be in bad condition (e.g broken), in which case its actual value is zero. we assume that the central authority in charge of allocating the objects does not know beforehand the objects conditions, but only has probabilistic information. the aim of this work is to propose a formal model of this problem, to adapt some classical fairness criteria to this extended setting, and to introduce several approaches to compute optimal allocations for small instances as well as sub-optimal good allocations for real-world inspired allocation problems of realistic size.",nan
2012,improving local search for random 3-sat using quantitative configuration checking,"configuration checking (cc) was proposed as a new diversification strategy for stochastic local search (sls) algorithm for solving minimum vertex cover, and has been successfully used for solving the boolean satisfiability problems, leading to an sls algorithm called swcc. however, the cc strategy for sat is in the early stage of study, and swcc cannot compete with the best sls solvers for sat in sat competition 2011. this paper presents a new strategy called quantitative configuration checking (qcc), which is a quantitative version of the cc strategy for sat. qcc is based on a new definition of “configuration” and works in a different way from the cc strategy does. specifically, while previous cc strategies work only in the greedy mode, qcc firstly works in the random mode. we use qcc to improve the swcc algorithm, resulting in a new sls algorithm for sat called swqcc. experimental results show that the qcc strategy is more effective than the cc strategy. furthermore, swqcc outperforms the best local search sat solver in sat competition 2011 called sparrow2011 on random 3-sat instances.",nan
2012,efficient norm emergence through experiential dynamic punishment,"peer punishment has been an effective means to ensure that norms are complied with in a population of self-interested agents. however, current approaches to establishing norms have only considered static punishments, which do not vary with the magnitude or frequency of norm violation. such static punishments are difficult to apply because it is difficult to identify an appropriate fixed penalty: one that is not too weak to disincentivise norm violations and not too strong to lead to significant deleterious effects on the system as a whole (such as those incurred by losing the benefits of a member of the population). this paper addresses this concern by developing an adaptive punishment technique that tailors penalty to norm violation. an experimental evaluation of the approach demonstrates its value compared to static punishment. in particular, the results show that our dynamic punishment technique is capable of achieving norm emergence, even when starting with an amount of punishment that is too low to achieve emergence in the traditional static approach.",nan
2012,on computing correct processes and repairs sing partial behavioral models,"diagnosis and repair of failed process executions is an important task for almost any process oriented application. because in practice complete specifications of process activities are not available, diagnosis and repair methods for partial behavior models are of great importance. we show that if the assumption of complete behavioral models is lifted, basic diagnosis and repair problems reside on the second level of the polynomial hierarchy.",nan
2012,routing for continuous monitoring by multiple micro avs in disaster scenarios,"in disaster situation a quickly obtained and regularly updated overview image of an area provides essential information for the rescue mission planning. such an overview image can be composed from the individual pictures taken by a fleet of unmanned aerial vehicles (uavs). however, currently drones are remotely controlled by humans during such missions. to the best of our knowledge, no research has been conducted on the problem of uav routing for such task. therefore, we propose a method based on the wellknown metaheuristic variable neighborhood search. in particular, we developed two new heuristics to construct the initial solution and an additional neighborhood operator. computational experiments indicate that solutions obtained by our metaheuristic do not exceed the optimum by more than 26.9% on small scenarios. for the large instances with hundreds of points (where no optimal solution is known) the proposed method constructs feasible solutions in less than one second.",nan
2012,diagnosing delays in multi-agent plans execution,"the paper introduces the notion of temporal multi-agent plan (tmap) and proposes a methodology, based on simple temporal problems (stp), for detecting and diagnosing action execution delays. actions are characterized by a finite set of behavioral modes, and each behavioral mode is a continuous interval of possible durations of the action. nominal modes represent the expected durations, whereas faulty modes represent delays. solving such diagnostic problems requires to find an assignment of modes to the actions that is consistent with the received observations and maximizes the likelihood of the delayed durations. an implementation of the approach and some preliminary experimental results are also discussed.",nan
2012,improving video activity recognition using object recognition and text mining,"recognizing activities in real-world videos is a challenging ai problem. we present a novel combination of standard activity classification, object recognition, and text mining to learn effective activity recognizers without ever explicitly labeling training videos. we cluster verbs used to describe videos to automatically discover classes of activities and produce a labeled training set. this labeled data is then used to train an activity classifier based on spatio-temporal features. next, text mining is employed to learn the correlations between these verbs and related objects. this knowledge is then used together with the outputs of an off-the-shelf object recognizer and the trained activity classifier to produce an improved activity recognizer. experiments on a corpus of youtube videos demonstrate the effectiveness of the overall approach.",nan
2012,exploiting expert knowledge in factored pomdps,"decision support in real-world applications is often challenging because one has to deal with large and only partially observable domains. in case of full observability, large deterministic domains are successfully tackled by making use of expert knowledge and employing methods like hierarchical task network (htn) planning. in this paper, we present an approach that transfers the advantages of htn planning to partially observable domains. experimental results for two implemented algorithmsucta* search, show that our approach significantly speeds up the generation of high-quality policies: the policies generated by our approach consistently outperform policies generated by symbolic perseus and can be computed in less than 10% of its runtime on average.",nan
2012,combining voting rules together,"we propose a simple method for combining together voting rules that performs a run-off between the different winners of each voting rule. we prove that this combinator has several good properties. for instance, even if just one of the base voting rules has a desirable property like condorcet consistency, the combination inherits this property. on the other hand, some important properties can be lost by the introduction of a run-off, including monotonicity and consistency. in addition, we prove that combining voting rules together in this way can make finding a manipulation more computationally difficult.",nan
2012,expexpexplosion: uniform interpolation in general el terminologies,"although el is a popular logic used in large existing knowledge bases, to the best of our knowledge no procedure has yet been proposed that computes uniform el interpolants of general el terminologies. up to now, also the bounds on the size of uniform el interpolants remain unknown. in this paper, we propose an approach based on proof theory and the theory of formal tree languages to computing a finite uniform interpolant for a general el terminology if it exists. further, we show that, if such a finite uniform el interpolant exists, then there exists one that is at most triple exponential in the size of the original tbox, and that, in the worst-case, no shorter interpolants exist, thereby establishing the triple exponential tight bounds on their size.",nan
2012,tunneling and decomposition-based state reduction for optimal planning,"action pruning is one of the most basic techniques for improving a planner's performance. the challenge of preserving optimality while reducing the state space has been addressed by several methods in recent years. in this paper we describe two optimality preserving pruning methods: the first is a generalization of tunnel macros. the second, the main contribution of this paper, is a novel partition-based pruning method. the latter requires the introduction of new automated domain decomposition techniques which are of independent interest. both methods prune the actions applicable at state s based on the last action leading to s, and both attempt to capture the intuition that, when possible, we should focus on one subgoal at a time. as we demonstrate, neither method dominates the other, and a combination of both allows us to obtain an even stronger pruning rule. we also introduce a few modifications to a* that utilize properties shared by both methods to find an optimal plan. our empirical evaluation compares the pruning power of the two methods and their combination, showing good coverage, reduction in running time, and reduction in the number of expansions.",nan
2012,extending set-based dualization: application to pattern mining,"dualization problems have been intensively studied in combinatorics, ai and pattern mining for years. roughly speaking, for a partial order () and some monotonic predicate q over p, the dualization consists in identifying all maximal elements of p verifying q from all minimal elements of not verifying q, and vice versa. the dualization is equivalent to the enumeration of minimal transversal of hypergraphs whenever () is a boolean lattice. in the setting of interesting pattern mining in databases, p represents a set of patterns and whenever p is isomorphic to a boolean lattice, the pattern mining problem is said to be representable as sets. the class of such problems is denoted by as. in this paper, we introduce a weak representation as sets for pattern mining problems which extends the ras class to a wider and significantly larger class of problems, called wras. we also identify ewras, an efficient subclass of wras for which the dualization problem is still quasi-polynomial. finally, we point out that one representative pattern mining problem known not to be in ras, namely frequent rigid sequences with wildcard, belongs to ewras. these new classes might prove to have large impact in unifying existing pattern mining approaches.",nan
2012,nearness rules and scaled proximity,"an artificial intelligence system that processes geothematic data would profit from a (semi-)formal or controlled natural language interface that incorporates concepts for nearness. though there already exists logical-engineering approaches giving sufficient conditions for nearness relations, we show within a logical analysis that these suffer from some deficiencies. non-engineering approaches to nearness such as the abstract mathematical approach based on proximity spaces do not deal with the implementation aspects but axiomatically formalize intuitions on nearness relations and provide insights on their nature. combining the ideas of the engineering approach with the mathematical approach of proximity spaces, we define and analyze new nearness relations that provide a good compromise between implementation needs and the need for an appropriate approximation of the natural nearness concept.",nan
2012,discovering cross-language links in wikipedia through semantic relatedness,"wikipedia is a large multilingual collection of interlinked articles, used and contributed by millions of users over the internet, that provides editions in up to 283 languages. two articles in different language versions of wikipedia may have information on the exactly the same concept, in which case they are often connected through a cross-language link. however, many cross-language links are either missing or incorrect and this negatively affects both the readers of wikipedia and multilingual information retrieval applications. in this paper, we propose wikicl, an algorithm for discoverinrg cross-language links using the semantic relatedness of two articles derived from the wikipedia graph structure. our evaluation shows that we achieve comparable, and in some cases, better results than previous methods with much less computational time .",nan
2012,optimizations for the boolean approach to computing minimal hitting sets,"the boolean approach to computing minimal hitting sets proposed by lin and jiang is known to offer very attractive general performance, but also has its issues, specifically with a cardinality-restricted search. in this paper we propose optimizations regarding the refinement rules, also offering a revised decision strategy as well as optimized termination criteria that exploit cardinality bounds. our experiments including artificial and real-world samples for the bounded and unbounded case show the potential of our work, where we could achieve speed-ups of up to two orders of magnitude.",nan
2012,hierarchical and overlapping co-clustering of mrna:mirna interactions,"micrornas (mirnas) are an important class of regulatory factors controlling gene expressions at post-transcriptional level. studies on interactions between different mirnas and their target genes are of utmost importance to understand the role of mirnas in the control of biological processes. this paper contributes to these studies by proposing a method for the extraction of co-clusters of mirnas and messenger rnas (mrnas). different from several already available co-clustering algorithms, our approach efficiently extracts a set of possibly overlapping, exhaustive and hierarchically organized co-clusters. the algorithm is well-suited for the task at hand since: i) mrnas and mirnas can be involved in different regulatory networks that may or may not be co-active under some conditions, ii) exhaustive co-clusters guarantee that possible co-regulations are not lost, iii) hierarchical browsing of co-clusters facilitates biologists in the interpretation of results. results on synthetic and on real human mirna:mrna data show the effectiveness of the approach.",nan
2012,a robust approach to addressing human adversaries in security games,"game-theoretic approaches have been proposed for addressing the complex problem of assigning limited security resources to protect a critical set of targets. however, many of the standard assumptions fail to address human adversaries who security forces will likely face. to address this challenge, previous research has attempted to integrate models of human decision-making into the game-theoretic algorithms for security settings. the current leading approach, based on experimental evaluation, is derived from a well-founded solution concept known as quantal response and is known as brqr. one critical difficulty with opponent modeling in general is that, in security domains, information about potential adversaries is often sparse or noisy and furthermore, the games themselves are highly complex and large in scale. thus, we chose to examine a completely new approach to addressing human adversaries that avoids the complex task of modeling human decision-making. we leverage and modify robust optimization techniques to create a new type of optimization where the defender's loss for a potential deviation by the attacker is bounded by the distance of that deviation from the expected-value-maximizing strategy. to demonstrate the advantages of our approach, we introduce a systematic way to generate meaningful reward structures and compare our approach with brqr in the most comprehensive investigation to date involving 104 security settings where previous work has tested only up to 10 security settings. our experimental analysis reveals our approach performing as well as or outperforming brqr in over 90% of the security settings tested and we demonstrate significant runtime benefits. these results are in favor of utilizing an approach based on robust optimization in these complex domains to avoid the difficulties of opponent modeling.",nan
2012,solving raven's iq-tests: an ai and cognitive modeling approach,"human reasoners have an impressive ability to solve analogical reasoning problems and they still outperform computational systems. analogical reasoning is relevant in dealing with intelligence tests. there are two kinds of approaches: to solve iq-test problems in a way similar to humans (i.e., a cognitive approach) or to solve these problems optimally (i.e., the ai approach). most systems can be associated with one of these approaches. detailed systems solving geometrical intelligence tests, explaining cognitive operations based on working memory and producing precise predictions and results such as error rates and response times have not been developed so far. we present a system implemented in the cognitive architecture act-r, able to solve analogously developed problems of raven's standard and advanced progressive matrices. the model solves 66 of the 72 tested problems of both tests. the model's predicted error rates correlate to human performance with r = .8 for the advanced progressive matrices and r = .7 for all problems together.",nan
2012,best reply dynamics for scoring rules,"we consider best-reply dynamics for voting games in which all players are strategic and no coalitions are formed. we study the class of scoring rules, show convergence of a suitably restricted version for the plurality and veto rules, and failure of convergence for other rules including k-approval and borda. in particular, for 3 candidates convergence fails for all rules other than plurality and veto. we give a unified proof for the convergence of these two rules. our proofs in the case of plurality improve the known bound on convergence, and the other convergence results are new.",nan
2012,complexity of conditional planning under partial observability and infinite executions,"the computational properties of many classes of conditional and contingent planning are well known. the main division in the field is between probabilistic planning (typically infinite or unbounded executions, reward rather than goal-based, and focus on expected costs or rewards) and non-probabilistic planning (ignoring probabilities, focus on plans that reach goal states.) in this work, we address the middle ground between these problems: planning with infinite executions and designated goal states. we address worst case rather than expected costs measures for the problem we consider. we analyze the structure of the plans for two possible goal-based specifications such plans may have to satisfy, maintaining a goal property indefinitely as well as visiting a goal state infinitely often, and establish their complexity under different observability assumptions.",nan
2012,engineering efficient planners with sat,"planning with sat has long been viewed as a main approach to ai planning. in comparison to other approaches, its high memory requirements have been considered to be a main obstacle to its scalability to large planning problems. better implementation technology, especially addressing the memory use, together with a shift of understanding about sat-based planning during the past ten years, enables planners that radically differ from those from the late 1990s. we discuss a sat-based planning system that implements modern versions of virtually all components of first planners that used sat, focusing on the new implementation technology for a compact clause representation that is both simpler and more effective than ones proposed earlier. specifically, the decreased memory requirements enable the use of top-level solution strategies that lift the performance of sat-based planning to the same level with other search methods.",nan
2012,coordinated exploration with a shared goal in costly environments,"the paper studies distributed cooperative multi-agent exploration methods in settings where the overall benefit of an opportunity is the minimum of individual findings and the exploration is costly. the primary motivation for the model is the multi-channel cooperative sensing problem which draws from the inter-vehicular cognitive offload paradigm. here, vehicles try to coordinate an offload channel through a dedicated common control channel, and the resulting quality of the channel eventually selected is constrained by the individual qualities. similar settings may arise in other multi-agent settings where the exploration needs to be coordinated. the goal in such problems concerns the optimization of the process as a whole, considering the tradeoff between the quality of the solution obtained for the shared goal and the cost associated with the exploration and coordination process. the methods considered in this paper make use of parallel and sequential exploration. the first approach is more latency-efficient, and the latter is shown to be more cost-effective. the strategy structure in both schemes is threshold-based, and the thresholds which are analytically derived in this paper can be calculated offline, resulting in a very low online computational load. a comparative illustration of the methods' performance is given using a synthetic environment, emphasizing the cost-latency tradeoff.",nan
2012,strategic and epistemic reasoning for the game description language gdl-ii,"the game description language gdl has been developed as a logic-based formalism for representing the rules of arbitrary games in general game playing. a recent language extension called gdl-ii allows to describe nondeterministic games with any number of players who may have incomplete, asymmetric information. in this paper, we show how the well-known alternating-time temporal epistemic logic (atel) can be adapted for strategic and epistemic reasoning about general games described in gdl-ii. we provide a semantic characterisation of gdl-ii descriptions in terms of atel models. we also provide a syntactic translation of gdl-ii descriptions into atel formulas, and we prove that these two characterisations are equivalent. we show that model checking in this setting is decidable by giving an algorithm, and we demonstrate how our results can be used to verify strategic and epistemic properties of games described in gdl-ii.",nan
2012,deciding membership in a class of polyhedra,"parameterized linear systems allow for modelling and reasoning over classes of polyhedra. collections of squares, rectangles, polytopes, and so on can readily be defined by means of linear systems with parameters in constant terms. in this paper, we consider the membership problem of deciding whether a given polyhedron belongs to the class defined by a parameterized linear system. as an example, we are interested in questions such as: “does a given polytope belong to the class of hypercubes?” we show that the membership problem is np-complete, even when restricting to the 2-dimensional plane. despite the negative result, the constructive proof allows us to devise a concise decision procedure using constraint logic programming over the reals, namely clp(r), which searches for a characterization of all instances of a parameterized system that are equivalent to a given polyhedron.",nan
2012,multiple-outcome proof number search,"we present multiple-outcome proof number search (mopns), a proof number based algorithm able to prove positions in games with multiple outcomes. mopns is a direct generalization of proof number search (pns) in the sense that both behave exactly the same way in games with two outcomes. however, mopns targets a wider class of games. when a game features more than two outcomes, pns can be used multiple times with different objectives to finally deduce the value of a position. on the contrary, mopns is called only once to produce the same information. we present experimental results on solving various positions of the games connect four and woodpush showing that in most problems, the total number of node creations of mopns is lower than the cumulative number of node creations of pns, even in the best case where pns does not need to perform a binary search.",nan
2012,institutionalised paxos consensus,"we address the problem of maintaining consistency in systems that are open, decentralised and resource-constrained, where the system components are highly mobile and/or ‘volatile’. an example of these systems is found in vehicular ad hoc networks (vanet). self-organising and norm-governed electronic institutions have been proposed to address these issues, but the problem of maintaining the consistency of conventionally-agreed values (institutional facts) arises due to the fragmentation/aggregation of component clusters, role failure, and revision of agreed values.in this paper, we specify ipcon, an algorithm for institutionalised paxos consensus, which is an extension of the well-known paxos consensus algorithm for maintaining consistency in static distributed databases. the ‘classic’ paxos algorithm is modified to accommodate role-based institutionalised power and extended to allow for dynamic clusters that may change, merge or fragment. we further extend ipcon to allow for the revision of previously agreed parameter values. a proof of correctness for ipcon is given, along with details of an axiomatisation and executable specification of the algorithm in the event calculus. these results show that ipcon is a viable method for coordination, consensus formation and collective-choice in self-organising multi-agent systems using electronic institutions.",nan
2012,introducing datatypes in dl-lite,"in description logics (dls) and in the ontology-based data access (obda) scenario, the use of actual datatypes (such as those adopted in dbmss) has received only limited attention, although datatypes, with their predefined semantics, might have a substantial impact on the computational complexity of inference in ontology systems. in this work we aim at overcoming these limitations, and study the impact of adding datatypes to the obda scenario. to this aim, we introduce a language for datatypes and we define the notion of a datatype hierarchy, constituted by a set of datatypes that depend on each other. we classify hierarchies in three classes according to their distinguishing properties, and we establish a theoretical framework for datatypes in the obda scenario, based on three major components: a dl, a class of datatype hierarchies, and a query language. we establish the computational complexity of query answering for various significant instantiations of this framework, as ranging from fol-rewritable to conp in data complexity.",nan
2012,convex solutions of rcc8 networks,"rcc8 is one of the most widely used calculi for qualitative spatial reasoning. although many applications have been explored where rcc8 relations refer to geographical or physical regions in two- or three-dimensional spaces, their use for conceptual reasoning is still at a rather preliminary stage. one of the core obstacles with using rcc8 to reason about conceptual spaces is that regions are required to be convex in this context. we investigate in this paper how the latter requirement impacts the realizability of rcc8 networks. specifically, we show that consistent rcc8 networks over 2n + 1 variables are guaranteed to have a convex solution in euclidean spaces of n dimensions and higher. we furthermore prove that our bound is optimal for 2- and 3-dimensional spaces, and that for any number of dimensions n ≥ 4, there exists a network of rcc8 relations over 3n variables which is consistent, but does not allow a convex solution in the n-dimensional euclidean space.",nan
2012,propositional planning as optimization,"planning as satisfiability is a most successful approach to optimal propositional planning. although optimality is highly desirable, for large problems it comes at a high, often prohibitive, computational cost. this paper extends planning as propositional satisfiability to planning as pseudo-boolean optimization. the approach has been implemented in a planner called pseudosatplan, that follows the classic solve and expand method of the satplan algorithm, but at each step it seeks to maximize the number of goals that can be achieved. the utilization of the achieved goals at subsequent steps opens up the possibility of implementing various strategies. the method essentially splits a planning problem into smaller subproblems, and employs various techniques for solving them fast. although pseudosatplan cannot guarantee the optimality of the generated plans, it aims at computing solutions of good quality. experimental results show that pseudosatplan can generate parallel plans of high quality for problems that are beyond the reach of the existing implementations of the planning as satisfiability framework.",nan
2012,large-scale parallel stratified defeasible reasoning,"we are recently experiencing an unprecedented explosion of available data coming from the web, sensors readings, scientific databases, government authorities and more. such datasets could benefit from the introduction of rule sets encoding commonly accepted rules or facts, application- or domain-specific rules, commonsense knowledge etc. this raises the question of whether, how, and to what extent knowledge representation methods are capable of handling huge amounts of data for these applications. in this paper, we consider inconsistency-tolerant reasoning in the form of defeasible logic, and analyze how parallelization, using the mapreduce framework, can be used to reason with defeasible rules over huge datasets. we extend previous work by dealing with predicates of arbitrary arity, under the assumption of stratification. moving from unary to multi-arity predicates is a decisive step towards practical applications, e.g. reasoning with linked open (rdf) data. our experimental results demonstrate that defeasible reasoning with millions of data is performant, and has the potential to scale to billions of facts.",nan
2012,path-constrained markov decision processes: bridging the gap between probabilistic model-checking and decision-theoretic planning,"markov decision processes (mdps) are a popular model for planning under probabilistic uncertainties. the solution of an mdp is a policy represented as a controlled markov chain, whose complex properties on execution paths can be automatically validated using stochastic model-checking techniques. in this paper, we propose a new theoretical model, named path-constrained markov decision processes: it allows system designers to directly optimize safe policies in a single design pass, whose possible executions are guaranteed to satisfy some probabilistic constraints on their paths, expressed in probabilistic real time computation tree logic. we mathematically analyze properties of pc-mdps and provide an iterative linear programming algorithm for solving them. we also present experiments that illustrate pc-mdps and highlight their benefits.",nan
2012,a probabilistic semantics for abstract argumentation,"classical semantics for abstract argumentation frameworks are usually defined in terms of extensions or, more recently, labelings. that is, an argument is either regarded as accepted with respect to a labeling or not. in order to reason with a specific semantics one takes either a credulous or skeptical approach, i.e. an argument is ultimately accepted, if it is accepted in one or all labelings, respectively. in this paper, we propose a more general approach for a semantics that allows for a more fine-grained differentiation between those two extreme views on reasoning. in particular, we propose a probabilistic semantics for abstract argumentation that assigns probabilities or degrees of belief to individual arguments. we show that our semantics generalizes the classical notions of semantics and we point out interesting relationships between concepts from argumentation and probabilistic reasoning. we illustrate the usefulness of our semantics on an example from the medical domain.",nan
2012,an empirical study of argumentation schemes for deliberative dialogue,"collaborative decision making among agents in a team is a complex activity, and tasks to achieve individual objectives may conflict in a team context. a number of argumentation-based models have been proposed to address the problem, the rationale being that the revelation of background information and constraints can aid in the discovery and resolution of conflicts. to date, however, no empirical studies have been conducted to substantiate these claims. i this paper, we discuss a model, grounded on argumentation schemes, that captures potential conflicts due to scheduling and causality constraints, and individual goals and norms. we evaluate this model in complex collaborative planning problems and show that such a model facilitates the sharing of relevant information pertaining to plan, goal and normative conflicts. further, we show that this focussed information sharing leads to more effective conflict resolution, particularly in the most challenging problems.",nan
2012,an approach to multi-agent planning with incomplete information,"multi-agent planning (map) approaches have been typically conceived for independent or loosely-coupled problems to enhance the benefits of distributed planning between autonomous agents as solving this type of problems require less coordination between the agents' sub-plans. however, when it comes to tightly-coupled agents' tasks, map has been relegated in favour of centralized approaches and little work has been done in this direction. in this paper, we present a general-purpose map capable to efficiently handle planning problems with any level of coupling between agents. we propose a cooperative refinement planning approach, built upon the partial-order planning paradigm, that allows agents to work with incomplete information and to have incomplete views of the world, i.e. being ignorant of other agents' information, as well as maintaining their own private information. we show various experiments to compare the performance of our system with a distributed csp-based map approach over a suite of problems.",nan
2012,efficient crowdsourcing of unknown experts using multi-armed bandits,"we address the expert crowdsourcing problem, in which an employer wishes to assign tasks to a set of available workers with heterogeneous working costs. critically, as workers produce results of varying quality, the utility of each assigned task is unknown and can vary both between workers and individual tasks. furthermore, in realistic settings, workers are likely to have limits on the number of tasks they can perform and the employer will have a fixed budget to spend on hiring workers. given these constraints, the objective of the employer is to assign tasks to workers in order to maximise the overall utility achieved. to achieve this, we introduce a novel multi–armed bandit (mab) model, the bounded mab, that naturally captures the problem of expert crowdsourcing. we also propose an algorithm to solve it efficiently, called bounded ε–first, which uses the first εb of its total budget b to derive estimates of the workers' quality characteristics (exploration), while the remaining (1−ε) b is used to maximise the total utility based on those estimates (exploitation). we show that using this technique allows us to derive an o(b2/3) upper bound on our algorithm's performance regret (i.e. the expected difference in utility between the moptimal and our algorithm). in addition, we demonstrate that our algorithm outperforms existing crowdsourcing methods by up to 155% in experiments based on real–world data from a prominent crowdsourcing site, while achieving up to 75% of a hypothetical optimal with full information.",nan
2012,logic-based benders decomposition for alternative resource scheduling with sequence dependent setups,we study an unrelated parallel machines scheduling problem with sequence and machine dependent setup times. a logic-based benders decomposition approach is proposed to minimize the makespan. this approach is a hybrid model that makes use of a mixed integer programming master problem and a specialized solver for travelling salesman subproblems. the master problem is used to assign jobs to machines while the subproblems obtain optimal schedules on each machine given the master problem assignments. computational results show that the benders model is able to find optimal solutions up to six orders of magnitude faster as well as solving problems six times the size previously possible with a mixed integer programming model in the literature and twice the size that a branchand-bound algorithm can solve for similar problems. we further relax the benders decomposition to accept suboptimal schedules and demonstrate the ability to parameterize solution quality while out-performing state-of-the-art metaheuristics both in terms of solution quality and mean run-time.,nan
2012,relation mining in the biomedical domain using entity-level semantics,"this work explores the use of semantic information from background knowledge sources for the task of relation mining between medical entities such as diseases, drugs, and their functional effects/actions. we hypothesize that the semantics of medical entities, and the information about them in different knowledge sources play an important role in determining their interactions and can thus be exploited to infer relations between these entities. we capture entities' semantics using a number of resources such as wikipedia, umls semantic network, medcin, mesh and snomed. de-pending on coverage and specificity of the resources, and features of interest, different classifiers are learnt. an ensemble based approach is then used to fuse together individual predictions. using a human-curated ontology as the gold standard, the proposed approach has been used to recognize ten medical relations of interest. we show that the proposed approach achieves substantial improvements in both coverage and performance over a distant supervision based baseline that uses sentence-level information. finally, we also show that even a simple ensemble approach that combines all the semantic information is able to get the best coverage and performance.",nan
2012,arvandherd: parallel planning with a portfolio,"arvandherd is a parallel planner that won the multicore sequential satisficing track of the 2011 international planning competition (ipc 2011). it assigns processors to run different members of an algorithm portfolio which contains several configurations of each of two different planners: lama-2008 and arvand. in this paper, we demonstrate that simple techniques for using different planner configurations can significantly improve the coverage of both of these planners. we then show that these two planners, when using multiple configurations, can be combined to construct a high performance parallel planner. in particular, we will show that arvandherd can solve more ipc benchmark problems than even a perfect parallelization of lama-2011, which won the satisficing track at ipc 2011. we will also show that the coverage of arvandherd can be further improved if lama-2008 is replaced by lama-2011 in the portfolio.",nan
2012,joint assessment and restoration of power systems,"this paper studies the joint damage assessment and recovery of the power infrastructure after a natural disaster has occurred. earlier work in this area proposed an optimization algorithm for the recovery phase, assuming that the infrastructure damage was known precisely. this paper lifts this assumption which does not always hold in practice. it proposes three approaches to this problem: an online stochastic optimization algorithm, a 2-stage algorithm that first evaluates the damage and then perform restoration, and a hybridization of both approaches. each of these approaches use information produced by weather and fragility simulation tools that provide potential damage scenarios for the disaster. experimental results on natural disaster scenarios for the us infrastructure indicate that online stochastic combinatorial optimization provides high-quality solutions to the joint damage assessment and recovery problem.",nan
2012,self-assessing agents for explaining language change: a case study in german,"language change is increasingly recognized as one of the most crucial sources of evidence for understanding human cognition. unfortunately, despite sophisticated methods for documenting which changes have taken place, the question of why languages evolve over time remains open for speculation. this paper presents a novel research method that addresses this issue by combining agent-based experiments with deep language processing, and demonstrates the approach through a case study on german definite articles. more specifically, two populations of autonomous agents are equipped with a model of old high german (500–1100 ad) and modern high german definite articles respectively, and a set of self-assessment criteria for evaluating their own linguistic performances. the experiments show that inefficiencies detected in the grammar by the old high german agents correspond to grammatical forms that have actually undergone the most important changes in the german language. the results thus suggest that the question of language change can be reformulated as an optimization problem in which language users try to achieve their communicative goals while allocating their cognitive resources as efficiently as possible.",nan
2012,detecting human patterns in laser range data,"in this paper we present a novel method for detecting humans from laser range scans, where the core idea is to treat neither individual frames, which hold so little information that the task is impossible, nor motion patterns, as is the case with tracking methods. rather, we map short time series of planar scans to 3d objects with time as the depth dimension; we then cluster and classify these 3d objects using unsupervised and off-line training, circumventing the need for predefining and parametrizing motion models.",nan
2012,maxi-consistent operators in argumentation,"this paper studies an instantiation of dung-style argumentation system with classical propositional logic. our goal is to explore the link between the result obtained by using argumentation to deal with an inconsistent knowledge base and the result obtained by using maximal consistent subsets of the same knowledge base. namely, for a given attack relation and semantics, we study the question: does every extension of the argumentation system correspond to exactly one maximal consistent subset of the knowledge base? we study the class of attack relations which satisfy that condition. we show that such a relation must be conflict-dependent, must not be valid, must not be conflict-complete, must not be symmetric etc. then, we show that some attack relations serve as lower or upper bounds with respect to the condition we study (e.g. we show that if an attack relation contains “canonical undercut” then it does not satisfy this condition). by using our results, we show for each attack relation and each semantics whether or not they satisfy the condition. finally, we interpret our results and discuss more general questions, like does (and when) this link is a desirable property. this work will help us obtain our long-term goal, which is to better understand the role of argumentation and, more particularly, the expressivity of logic-based instantiations of dung-style argumentation frameworks.",nan
2012,multi-unit auctions with a stochastic number of asymmetric bidders,"existing work on auctions assumes that bidders are symmetric in their types — they have the same risk attitude and their valuations are drawn from the same distribution. this is unrealistic in many real-world applications, where highly heterogeneous bidders with different risk attitudes and widely varying valuation distributions commonly compete with each other. using computational service auctions that are emerging in cloud and grid settings as a motivating example, we examine how an intelligent agent should bid in such multi-unit auctions with asymmetric bidders. specifically, we describe the equilibrium bidding strategies in three different settings that are distinguished by the levels of uncertainty about the types of other agents. first, we consider a setting with full knowledge about all agents' types, then we consider the case where the types are uncertain, but the number of bidders is known. finally, we consider the case where both the number of bidders and their types are uncertain. our experiments show that using the equilibrium strategies derived from our full analysis leads to increased utility (typically 20–25%) for the participants compared to previous state-of-the-art strategies.",nan
2012,synonymy extraction from semantic networks using string and graph kernel methods,"synonyms are a highly relevant information source for natural language processing. automatic synonym extraction methods have in common that they are either applied on the surface representation of the text or on a syntactical structure derived from it. in this paper, however, we present a semantic synonym extraction approach that operates directly on semantic networks (sns), which were derived from text by a deep syntactico-semantic analysis.synonymy hypotheses are extracted from the sns by graph matching. these hypotheses are then validated by a support vector machine (svm) employing a combined graph and string kernel. our method was compared to several other approaches and the evaluation has shown that our results are considerably superior.",nan
2012,ordinal decision models for markov decision processes,"setting the values of rewards in markov decision processes (mdp) may be a difficult task. in this paper, we consider two ordinal decision models for mdps where only an order is known over rewards. the first one, which has been proposed recently in mdps [23], defines preferences with respect to a reference point. the second model, which can been viewed as the dual approach of the first one, is based on quantiles. based on the first decision model, we give a new interpretation of rewards in standard mdps, which sheds some interesting light on the preference system used in standard mdps. the second model based on quantile optimization is a new approach in mdps with ordinal rewards. although quantile-based optimality is state-dependent, we prove that an optimal stationary deterministic policy exists for a given initial state. finally, we propose solution methods based on linear programming for optimizing quantiles.",nan
2012,"negotiating concurrently with unknown opponents in omplex, real-time domains","we propose a novel strategy to enable autonomous agents to negotiate concurrently with multiple, unknown opponents in real-ime, over complex multi-issue domains. we formalise our strategy as an optimisation problem, in which decisions are based on probabilistic information about the opponents' strategies acquired during negotiation. in doing so, we develop the first principled approach that enables the coordination of multiple, concurrent negotiation threads for practical negotiation settings. furthermore, we validate our strategy using the agents and domains developed for the international automated negotiating agents competition (anac), and we benchmark our strategy against the state-of-the-art. we find that our approach significantly outperforms existing approaches, and this difference improves even further as the number of available negotiation opponents and the complexity of the negotiation domain increases.",nan
2012,improving local decisions in adversarial search,"until recently, game-tree pathology (in which a deeper game-tree search results in worse play) has been thought to be quite rare. we provide an analysis that shows that every game should have some sections that are locally pathological, assuming that both players can potentially win the game.we also modify the minimax algorithm to recognize local pathologies in arbitrary games, and cut off search accordingly (shallower search is more effective than deeper search when local pathologies occur). we show experimentally that our modified search procedure avoids local pathologies and consequently provides improved performance, in terms of decision accuracy, when compared with the ordinary minimax algorithm.",nan
2012,enhancing predictability of schedules by task grouping,"an important problem in scheduling is ensuring predictability of solutions in case of execution delays. we propose a new method, task grouping, and apply it in combination with a precedence constraint posting algorithm to solve the resource-constrained project scheduling problem. using this method tasks that must be executed sequentially can be grouped, but their definitive order is determined at execution time such that delays can sometimes be mitigated. as a consequence, our method generates a set of execution options for a schedule. using the well-known psplib instances, we show that our method can reduce the impact of delays on the predictability of schedule execution.",nan
2012,importance-based semantics of polynomial comparative peference inference,"a basic task in preference reasoning is inferring a preference between a pair of outcomes (alternatives) from an input set of preference statements. this preference inference task for comparative preferences has been shown to be computationally very hard for the standard kind of inference. recently, a new kind of preference inference has been developed, which is polynomial for relatively expressive preference languages, and has the additional property of being much less conservative; this can be a major advantage, since it will tend to make the number of undominated outcomes smaller. it derives from a semantics where models are weak orders that are generated by objects called cp-trees, which represent a kind of conditional lexicographic order. we show that there are simple conditions, based on the notion of importance, that determine whether a weak order can be generated by a cp-tree of the given form. this enables a simple characterisation of the less conservative preference inference. we go on to study the importance properties satisfied by a simple kind of cp-tree, leading to another characterisation of the corresponding preference inference.",nan
2012,sample-based policy iteration for constrained dec-pomdps,"we introduce constrained dec-pomdps — an extension of the standard dec-pomdps that includes constraints on the optimality of the overall team rewards. constrained dec-pomdps present a natural framework for modeling cooperative multi-agent problems with limited resources. to solve such dec-pomdps, we propose a novel sample-based policy iteration algorithm. the algorithm builds on multi-agent dynamic programming and benefits from several recent advances in dec-pomdp algorithms such as mb-dp [12] and tbdp [13]. specifically, it improves the joint policy by solving a series of standard nonlinear programs (nlps), thereby building on recent advances in nlp solvers. our experimental results confirm the algorithm can efficiently solve constrained dec-pomdps that cause general dec-pomdp algorithms to fail.",nan
2012,inconsistency measurement based on variables in minimal unsatisfiable subsets,"measuring inconsistency degrees of knowledge bases (kbs) provides important context information for facilitating inconsistency handling. several semantic and syntax based measures have been proposed separately.in this paper, we propose a new way to define inconsistency measurements by combining semantic and syntax based approaches. it is based on counting the variables of minimal unsatisfiable subsets (muses) and minimal correction subsets (mcses), which leads to two equivalent inconsistency degrees, named idmus and idmcs. we give the theoretical and experimental comparisons between them and two purely semantic-based inconsistency degrees: 4-valued and the quasi classical semantics based inconsistency degrees. moreover, the computational complexities related to our new inconsistency measurements are studied. as it turns out that computing the exact inconsistency degrees is intractable in general, we then propose and evaluate an anytime algorithm to make idmus and idmcs usable in knowledge management applications. in particular, as most of syntax based measures tend to be difficult to compute in reality due to the exponential number of muses, our new inconsistency measures are practical because the numbers of variables in muses are often limited or easily to be approximated.we evaluate our approach on the dc benchmark. our encouraging experimental results show that these new inconsistency measurements or their approximations are efficient to handle large knowledge bases and to better distinguish inconsistent knowledge bases.",nan
2012,adversarial label flips attack on support vector machines,"to develop a robust classification algorithm in the adversarial setting, it is important to understand the adversary's strategy. we address the problem of label flips attack where an adversary contaminates the training set through flipping labels. by analyzing the objective of the adversary, we formulate an optimization framework for finding the label flips that maximize the classification error. an algorithm for attacking support vector machines is derived. experiments demonstrate that the accuracy of classifiers is significantly degraded under the attack.",nan
2012,disambiguating road names in text route descriptions using exact-all-hop shortest path algorithm,"automatic extraction and understanding of human-generated route descriptions have been critical to research aiming at understanding human cognition of geospatial information. among all research issues involved, road name disambiguation is the most important, because one road name can refer to more than one road. compared with traditional toponym (place name) disambiguation, the challenges of disambiguating road names in human-generated route description are three-fold: (1) the authors may use a wrong or obsolete road name and the gazetteer may have incomplete or out-of-date information; (2) geographic ontologies often used to disambiguate cities or counties do not exist for roads, due to their linear nature and large spatial extent; (3) knowledge of the co-occurrence of road names and other toponyms are difficult to learn due to the difficulty in automatic processing of natural language and lack of external information source of road entities. in this paper, we solve the problem of road name disambiguation in human-generated route descriptions with noise, i.e. in the presence of wrong names and incomplete gazetteer. we model the problem as an exact-all-hop shortest path problem on a semi-complete directed k-partite graph, and design an efficient algorithm to solve it. our disambiguation algorithm successfully handles the noisy data and does not require any extra information sources other than the gazetteer. we compared our algorithm with an existing map-based method. experiment results show that our algorithm significantly outperforms the existing method.",nan
2012,multi-unit double auction under group buying,"group buying is a business model in which a number of buyers join together to make an order of a product in a certain quantity in order to gain a desirable discounted price. such a business model has recently received significant attention from researchers in economics and computer science, mostly due to its successful application in online businesses, such as groupon. this paper deals with the market situation when multiple sellers sell a product to a number of buyers with discount for group buying. we model this problem as a multi-unit double auction. we first examine two deterministic mechanisms that are budget balanced, individually rational and only one-sided truthful, i.e. it is truthful for either buyers or sellers. then we find that, although there exists a “trivial” (non-deterministic) mechanism that is (weakly) budget balanced, individually rational and truthful for both buyers and sellers, such a mechanism is not achievable if we further require that both the trading size and the payment are neither seller-independent nor buyer-independent. in addition, we show that there is no budget balanced, individually rational and truthful mechanism that can also guarantee a reasonable trading size.",nan
2012,a stubborn set algorithm for optimal planning,"we adapt a partial order reduction technique based on stubborn sets, originally proposed for detecting dead ends in petri nets, to the setting of optimal planning. we demonstrate that stubborn sets can provide significant state space reductions on standard planning benchmarks, outperforming the expansion core method.",nan
2012,preemption operators,"we introduce a family of operators for belief change that aim at making a new piece of information to be preemptive so that any former belief subsuming it is given up. that is, the current belief base is to be altered even in the case that it is logically consistent with the new piece of information. existing operators for belief revision are inadequate for this purpose because they amount to set-theoretic union in a contradiction-free case. we propose a series of postulates for such preemption operators. we show that a preemption operator can be defined as a multiple contraction followed by an expansion, drawing on operators from belief revision.",nan
2012,reasoning for agreement technologies,"we present a logical viewpoint on agreement technologies by combining reasoning methods for aggregation, norms, dependence, argumentation, and trust. starting from the agreement technologies tower, we introduce an architecture for the agreement process with interacting reasoning processes. we discuss the input/output perspective on reasoning for agreement technologies, the combination of reasoning methods, the role of abstraction, and game theoretic foundations.",nan
2012,an adaptive clustering model that integrates expert rules and n-gram statistics for coreference resolution,we present an adaptive clustering model for coreference resolution in which the expert rules of a state of the art deterministic system are used as features over pairs of clusters. a significant advantage of the new approach is that new features can be easily added to the system. we demonstrate this advantage by incorporating semantic compatibility features for neutral pronouns computed from web n-gram statistics. experimental results show that the combination of the new features with the expert rules in the adaptive clustering approach results in an overall performance improvement and a substantial 5% improvement in f1 measure for the target pronouns.,nan
2012,mining extremes: severe rainfall and climate change,"theoretical developments for the analysis and modeling of extreme value data have tended to focus on limiting cases and assumptions of independence. however, massive datasets from models and sensors, space-time dimensionality, comp dependence structures, long-memory, long-range and low frequency processes all motivate the need for sophisticated methods for correlated and finite data that follow complex processes. the importance of extremes has been rapidly growing in areas ranging from climate change and critical infrastructures to insurance and financial markets. here we briefly discuss the state-of-the-art and key gaps, through the case of rainfall extremes under climate change. preliminary analysis suggests new directions and points to research areas that deserve further attention.",nan
2012,cakes: cross-lingual wikipedia knowledge enrichment and summarization,"wikipedia is a huge source of multilingual knowledge curated by human contributors. wiki articles are independently written in the various languages and may cover different perspectives about a given subject. the aim of this paper is to exploit wikipedia multilingual information for knowledge enrichment and summarization. investigating the link structure of a wiki article in a source language and comparing it with the structure of articles about the same subject written in other languages gives insights about the body of knowledge shared among languages. this investigation is also useful to identify knowledge perspectives not covered in the source language but covered in other languages. we implemented these ideas in cakes, which: i) exploits wikipedia information on the fly without requiring any data preprocessing; ii) enables to specify the set of languages to be considered and; iii) ranks subjects interesting for a given article on the basis of their popularity among languages.",nan
2012,"a novel way to connect bnb-adopt+, with soft ac","combining bnb-adopt+, with ac and fdac levels of soft arc consistency (sac) improves efficiency for optimal dcop solving. however, it seems difficult in distributed context to achieve the higher consistency level edac, especially considering privacy. as alternative, we propose dac by token passing. agents receiving a token ask neighbors for cost extensions. when deletions or c&phi; increments occur, the token is passed to neighbor agents. this strategy turns out to be more efficient than fdac when combined with bnb-adopt+, improving communication and specially computation.",nan
2012,controlling candidate-sequential elections,"all previous work on “candidate-control” manipulation of elections has been in the model of full-information, simultaneous voting. this is a problem, since in quite a few real-world settings— from tv singing/dancing talent shows to university faculty-hiring processes—candidates are introduced, and appraised by the voters, in sequence. we provide a natural model for sequential candidate evaluation, a framework for evaluating the computational complexity of controlling the outcome within that framework, and some initial results on the range such complexity can take on. we hope our work will lead to further examination of temporally involved candidate control.",nan
2012,implementation of critical path heuristics for sat,"recent work has shown that sat can be theoretically more powerful than heuristic search provided the heuristic used by search is implemented as a set of clauses on which unit propagation simulates the evaluation of the heuristic. the hmax heuristic has been shown to be implemented trivially by the empty set of clauses. this paper presents an implementation of hm, a generalization of hmax.",nan
2012,evolutionary clustering on cuda,"unsupervised clustering of large data sets is a complicated task. due to its complexity, various meta-heuristic machine learning algorithms have been used to automate the clustering process. genetic and evolutionary algorithms have been deployed to find clusters in data sets with success. the gpu computing is a recent programming paradigm introducing high performance parallel computing to general audience. this work presents an acceleration of a genetic algorithm for density based clustering on the gpu using the nvidia compute unified device architecture (cuda).",nan
2012,practical reformulations with table constraints,"the constraint programming practioners are often facing the same modeling questions. in this paper, we start with several such recurring questions on the use of table constraints, and we present efficient reformulations.",nan
2012,on partitioning for maximum satisfiability,"partitioning formulas is motivated by the expectation to identify easy to solve subformulas, even though at the cost of having more formulas to solve. in this paper we suggest to apply partitioning to maximum satisfiability (maxsat), the optimization version of the well-known satisfiability (sat) problem. the use of partitions can be naturally combined with unsatisfiability-based algorithms for maxsat that are built upon successive calls to a sat solver, where each call identifies an unsatisfiable subformula. one of the drawbacks of these algorithms results from the sat solver returning large unsatisfiable subformulas. however, when using partitions, the solver is more likely to identify smaller unsatisfiable subformulas. experimental results show that the use of partitions in maxsat significantly improves the performance of unsatisfiability-based algorithms.",nan
2012,ontologising semantic relations into a relationless thesaurus,"this paper proposes several algorithms for moving from term-based semantic relations to relations between the synsets of a thesaurus. the thesaurus does not encompass semantic relations, and the extraction context is not exploited for this task. the algorithms were compared after evaluation against a gold reference with manual attachments of portuguese relations into a thesaurus.",nan
2012,advances in distributed branch and bound,"we describe a distributed version of an advanced branch and bound algorithm over graphical models. the crucial issue of load balancing is addressed by estimating subproblem complexity through learning, yielding impressive speedups on various hard problems using hundreds of parallel cpus.",nan
2012,intermediary local consistencies,"we propose a new definition for characterizing levels of consistency. a perspective is to provide new tools for classifying filtering algorithms, including incomplete algorithms based on the semantics of constraints.",nan
2012,the consistency of majority rule,"we propose an analysis of the impossibility results in judgement aggregation by means of a proof-theoretical approach to collective rationality. in particular, we use linear logic in order to analyse the group inconsistencies and to show possible ways to circumvent them.",nan
2012,probabilistic path-disruption games,"path-disruption games, recently introduced by bachrach and porat [1], are coalitional games played on graphs where one or multiple adversaries each seek to reach a given target vertex from a given source vertex and a coalition of agents seeks to prevent that from happening by blocking every path from the source to the target, for each adversary. we expand their model by allowing uncertainty about the targets. in probabilistic path-disruption games, we assign to each vertex the probability that an adversary wants to reach it. we study the complexity of various problems related to such games.",nan
2012,towards a declarative spatial reasoning system,"we present early results on the development of a declarative spatial reasoning system within the context of the constraint logic programming (clp) framework. the system is capable of modelling and reasoning about qualitative spatial relations pertaining to multiple spatial domains, i.e., one or more aspects of space such as topology, and intrinsic and extrinsic orientation. it provides a seamless mechanism for combining formal qualitative spatial calculi within one framework, and provides a prolog-based declarative interface for ai applications to abstract and reason about quantitative, geometric information in a qualitative manner. based on previous work concerning the formalisation of the framework [2], we present ongoing work to develop the theoretical result into a comprehensive reasoning system (and prolog-based library) which may be used independently, or as a logic-based module within hybrid intelligent systems.",nan
2012,an alternative eager encoding of the all-different constraint over bit-vectors,a novel eager encoding of the alldifferent constraint over bit-vectors is presented in this short paper. it is based on 1-to-1 mapping of the input bit-vectors to a linearly ordered set of auxiliary bit-vectors. experiments with four sat solvers showed that the new encoding can be solved order of magnitudes faster than the standard encoding in a hard unsatisfiable case.,nan
2012,voi-aware mcts,"uct, a state-of-the art algorithm for monte carlo tree search (mcts) in games and markov decision processes, is based on ucb1, a sampling policy for the multi-armed bandit problem (mab) that minimizes the cumulative regret. however, search differs from mab in that in mcts it is usually only the final “arm pull” (the actual move selection) that collects a reward, rather than all “arm pulls”. in this paper, an mcts sampling policy based on value of information (voi) estimates of rollouts is suggested. empirical evaluation of the policycomparison to ucb1uct is performed on random mab instances as well as on computer go.",nan
2012,approximation of steiner minimum trees in euclidean planar graphs using euclidian steiner minimum trees,"exact solutions for steiner tree problems in large graphs with large terminal sets cannot be calculated efficiently at the moment. for approximating steiner minimum trees in large euclidean planar graphs, we propose an algorithm, which uses a solution to the problem in the euclidian plane for initialisation. this is further optimized using stochastic hillclimbing. the algorithm is empirically evaluated with respect to approximation ratio, running time and memory consumption on street networks and compared to an implementation of the dreyfus wagner algorithm. the results show, that a smt can be efficiently approximated in our scenario with an observed average approximation ratio of 1.065 or 1.034 respectively by also using means of local search.",nan
2012,reasoning with fuzzy-el+ ontologies using mapreduce,"fuzzy extension of description logics (dls) allows the formal representation and handling of fuzzy knowledge. in this paper, we consider fuzzy-el+, which is a fuzzy extension of el+. we first present revised completion rules for fuzzy-el+ that can be handled by mapreduce programs. we then propose an algorithm for scale reasoning with fuzzy-el+ ontologies based on mapreduce.",nan
2012,context-based search in software development,"software developers usually spend a large amount of time navigating their own source code or searching for a specific source code artifact they need to work with. we propose a context-based search approach that focuses in the source code artifacts that exist in the workspace of the developer. these artifacts are stored in a knowledge base and represented using ontologies. the contextual information of the developer is used to rank the search results according to their relevance for the developer. the results of an experiment performed with two groups of developers show that our approach has a positive impact in retrieving relevant artifacts for the developer, helping them find what they need more quickly and easily.",nan
2012,event processing for intelligent resource management,"the need for intelligent resource management (irm) spans across a multitude of applications. to address this requirement, we present ep-irm, an event processing system recognising composite events given multiple sources of information in order to support irm. ep-irm has been deployed in two real-world applications. moreover, with a small effort it may be used in a wide range of applications requiring irm. we present an evaluation of the system, and discuss the lessons learnt during its development and deployment.",nan
2012,partially observable markov decision process for closed-loop anesthesia control,"recently, researchers have favored computer-automated drug delivery system to reduce the risks of intraoperative awareness and postoperative morbidity, and their use is associated with a number of favorable patient outcomes. for example, proportional-integral-derivative, maximum a posteriori (map) bayesian approaches, fuzzy logic, and reinforcement learning, have been developed and applied successfully in simulated patients or volunteers. despite the successes, variations of errors in the observed data are known to affect the performances of the controllers, especially when a patient state estimation is required. to have a better controller, we apply partially observable markov decision process (pomdp) to achieve better drug delivery policy, even when there is incomplete information about patients' current states during operations. in this paper, a pomdp model for closed-loop anesthesia control is introduced. then, a state-of-the-art pomdp solver is used to compute a good control policy, in other words, propofol rates to administer to a patient, in efforts to reduce the risk of intraoperative awareness and postoperative side effects in patients.",nan
2012,pomdp-based online target detection and recognition for autonomous uavs,"this paper presents a target detection and recognition mission by an autonomous unmanned aerial vehicule (uav) modeled as a partially observable markov decision process (pomdp). the pomdp model deals in a single framework with both perception actions (controlling the camera's view angle), and mission actions (moving between zones and flight levels, landing) needed to achieve the goal of the mission, i.e. landing in a zone containing a car whose model is recognized as a desired target model with sufficient belief. we explain how we automatically learned the probabilistic observation pomdp model from statistical analysis of the image processing algorithm used on-board the uav to analyze objects in the scene. we also present our “optimize-while-execute” framework, which drives a pomdp sub-planner to optimize and execute the pomdp policy in parallel under action duration constraints, reasoning about the future possible execution states of the robotic system. finally, we present experimental results, which demonstrate that artificial intelligence techniques like pomdp planning can be successfully applied in order to automatically control perception and mission actions hand-in-hand for complex time-constrained uav missions.",nan
2012,a multi-objective approach to balance buildings construction cost and energy efficiency,"the issue of energy efficiency of buildings must be taken into account as early as possible in the life cycle of buildings, i.e., during the architectural phase. the orientation of the building, its structure, the choice of materials, windows and other openings, all these aspects participate to the future energy consumption profile in a complex and highly non-linear way. furthermore, even though sustainability is today a major objective, the cost of the construction remains another decision factor that cannot be underestimated – and energy efficiency and cost are alas contradictory objectives. thus the problem of designing efficient buildings is a multi-objective problem. this work tackles this problem using a state-of-the-art evolutionary multi-objective algorithm, hype, tightly linked to an energy con-sumption simulation program, energyplus. several parameters defining the design are considered, namely the orientation angle, the materials used for the thermal insulation of the walls, and the size of the windows in order to explore daylighting. in the end, a diverse set of pareto optimal solutions (i.e., solutions offering optimal trade-offs between both objectives) are proposed to the decision maker. the approach is validated on five buildings of different categories, where energy savings of up to 20% compared to the original design are automatically obtained.",nan
2012,lsa for mining hidden information in action game semantics,"this paper describes the application of latent semantic analysis to the term-document matrices that result from modeling an action game. innovative solutions to address challenges like the definition of “words” and “documents” in the dynamic and complex domain of action games are proposed, and interesting, previously unknown semantic information is extracted.",nan
2012,cooperatives for demand side management,"we propose a new scheme for efficient demand side management for the smart grid. specifically, we envisage and promote the formation of cooperatives of medium-large consumers and equip them (via our proposed mechanisms) with the capability of regularly participating in the existing electricity markets by providing electricity demand reduction services to the grid. based on mechanism design principles, we develop a model for such cooperatives by designing methods for estimating suitable reduction amounts, placing bids in the market and redistributing the obtained revenue amongst the member agents. our mechanism is such that the member agents have no incentive to show artificial reductions with the aim of increasing their revenues.",nan
2012,wind speed forecasting using spatio-temporal indicators,"from small farms to electricity markets the interest and importance of wind power production is continuously increasing. this interest is mainly caused by the fact that wind is a continuous resource of clean energy. to take full advantage of the potential of wind power production it is crucial to have tools that accurately forecast the expected wind speed. however, forecasting the wind speed is not a trivial task. wind speed is characterised by a random behaviour as well as several other intermittent characteristics. this paper proposes a new approach to the task of wind speed forecasting. the main distinguishing feature of this proposal is its reliance on both temporal and spatial characteristics to produce a forecast of the future wind speed. we have experimentally tested the proposed method with historical data concerning wind speed on the eastern region of the us. nevertheless, the methodology that is described in the paper can be seen as a general approach to spatio-temporal prediction. we have compared our proposal to other standard approaches in the task of forecasting 2 hours ahead wind speed. our extensive experiments show that our proposal has clear advantages in most setups.",nan
2012,predicting the power output of distributed renewable energy resources within a broad geographical region,"in recent years, estimating the power output of inherently intermittent and potentially distributed renewable energy sources has become a major scientific and societal concern. in this paper, we provide an algorithmic framework, along with an interactive web-based tool, to enable short-to-middle term forecasts of photovoltaic (pv) systems and wind generators output. importantly, we propose a generic pv output estimation method, the backbone of which is a solar irradiance approximation model that incorporates free-to-use, readily available meteorological data coming from online weather stations. the model utilizes non-linear approximation components for turning cloud-coverage into radiation forecasts, such as an mlp neural network with one hidden layer. we present a thorough evaluation of the proposed techniques, and show that they can be successfully employed within a broad geographical region (the mediterranean belt) and come with specific performance guarantees. crucially, our methods do not rely on complex and expensive weather models and data, and our web-based tool can be of immediate use to the community as a simulation data acquisition platform.",nan
2012,a reinforcement learning approach to optimize the longitudinal behavior of a partial autonomous driving assistance system,"the partially autonomous driving assistance system (padas) is an artificial intelligent co-driver, able to act in critical situations, whose objective is to assist people in driving safely, by providing pertinent and accurate information in real-time about the external situation. such a system intervenes continuously from warnings to automatic intervention in the whole longitudinal control of the vehicle. this paper illustrates the optimization process of the padas, following a statistical machine learning methods - reinforcement learning - where the action selection is derived from a set of recorded interactions with human drivers. experimental results on a driving simulator prove this method achieves a significant reduction in the risk of collision.",nan
2012,wemit: web-mining for translation,"the quality of machine translation is often dependent on the quality of lexical transfer from a source language to a target language. in this work we present an automatic method to translate specialized terms. the proposed approach is based on two steps: (1) extraction of candidates for translation into web pages, (2) identification of the most relevant candidates by using web-mining techniques.",nan
2012,master orientation tool,"this paper describes our ongoing work on developing a master orientation tool for university college maastricht (ucm). ucm bachelor students use the tool to discover master programs that fit their academic profiles. the tool includes a memory-based collaborative recommender system. the system memory contains data on academic profiles of ucm alumni students, labeled by the master programs they have chosen. the tool operates as a collaborative system: given the academic profile of a bachelor student, it recommends master programs for that student based on the proximity of her profile to the profiles of the alumni. the master orientation tool allows students to modify their own profiles and thus to explore alternatives in their study and how they influence their master program possibilities. the tool is operational at ucm since september 2011 and is popular among the students.",nan
2012,an infrastructure for human inclusion in mas,"the field of multi-agent systems (mas) focuses on the design and development of systems composed of autonomous entities (i.e. agents) which interact in order to perform specific activities. in general, direct human participation in these systems is not considered. we advocate that 3d virtual worlds technology can be used to facilitate human inclusion in a mas system. in this demo we present an infrastructure, the virtual institutions execution environment (vixee), which allows direct human participation in mas providing an immersive experience.",nan
2012,training crisis managers with pandora,"this short paper introduces a novel use of timeline-based planning as the core element of a dynamic training environment for crisis managers called pandora. a trainer is provided with a combination of planning and execution functions that allow him/her to maintain and adapt a “lesson plan” as the basis for the interaction between the trainer and a class of trainees. the training session is based on the concept of scenario, that is a set of events and alternatives deployed on a timeline-based system, that shapes an abstract plan proposed to trainees. throughout a training session a continuous planning, execution, re-planning loop takes place, based around trainer observation of trainees and self-reporting by trainees, which provides analysis of both their behavioral and psychological changes. these, combined with the trainee decisions about what actions to take to manage the crisis, are used to maintain an updated model of each user. in addition the trainer has the ability to directly intervene in a training session to, for example, interject new scenario events. the training session is therefore managed through a combination of automated analysis of trainee behaviour and decisions, coupled with trainer input and direction.",nan
2012,flowopt: bridging the gap between optimization technology and manufacturing planners,"flowopt is an integrated collection of tools for workflow optimization in production environments. it was developed as a demonstration of advancements in the areas of modeling and optimization with the focus on simplifying the usage of the technology for end customers. the system consists of several interconnected modules. first, the user visually models a workflow describing the production of some item. then the user specifies which items and how many of them should be produced (order management) and the system automatically generates a production schedule. this schedule is then visualized in the form of a gantt chart where the user can arbitrarily modify the schedule. finally, the system can analyze the schedule and suggest some improvements such as buying a new machine. constraint satisfaction technology is the solving engine behind these modules.",nan
2012,wanteat: interacting with social networks of smart objects for sharing cultural heritage and supporting sustainability,"wanteat is about interacting with everyday objects that become intelligent hubs for accessing and sharing the cultural heritage of a territory. objects are smart in the sense that they share knowledge with users, interact with them in a personalized way and maintain social relationships with users and other objects. when interacting with an object, a user is also introduced to the social network of its friends; the user can explore this network to discover new interesting information, services, objects, people. the objects we consider belong to the realm of gastronomy, including food items, shops, restaurants, cooks, recipes, etc. on the one hand, this allows people to get deeply in touch with the culture of a territory, making people aware of its traditions and supporting a sustainable gastronomy; on the other hand, the approach supports networking and the promotion of local quality productions and economy.",nan
2012,mo.di.bot – mobile diagnostic robot,"the purpose of this paper is the description of mo.di.bot system and demonstration. mo.di.bot is a mobile robot with manipulation and measurement capabilities conceived for advanced diagnosis and quality control in industrial environments. mo.di.bot is composed of a mobile platform, a robotic arm and a high dof end-effector, and it can autonomously move and interact with the surrounding environment. the safety of the system is guaranteed by a set of time of flight sensors used to model and monitor the working area where also human operators can be present. in addition, the robot is equipped with measurement instruments specific for the diagnostic inspection of the products under test. washing machines life-test laboratories have been chosen in order to validate mo.di.bot capabilities in a real-world industrial application scenario, but the results achieved can be easily extended to many industrial sectors, goods and electromechanical components where high levels of flexibility and autonomy are needed.",nan
2012,metaheuristic aided software features assembly,one significant task addressed during software development project is to determine which features should be covered by the application that is being developed. this problem is known as the next release problem (nrp) and has been solved using metaheuristic search techniques. we show how to apply these techniques by its embedding into a requirement management tool as an assistant functionality. we have called this new utility masa (metaheuristic aided software features assembly).,nan
2012,designing kdd-workflows via htn-planning,"knowledge discovery in databases (kdd) has evolved a lot during the last years and reached a mature stage offering plenty of operators to solve complex data analysis tasks. however, the user support for building workflows has not progressed accordingly. the large number of operators currently available in kdd systems makes it difficult for users to successfully analyze data. in addition, the correctness of workflows is not checked before execution. this demo presents our tools, eproplan and eida, which solve the above problems by supporting the whole cycle of (semi-) automatic workflow generation. our modeling tool eproplan, allows to describe operators and build a task/method decomposition grammar to specify the desired workflows. additionally, our intelligent discovery assistant, eida, allows to place workflows into data mining (dm) suites or workflow engines for execution.",nan
2012,confidence: ubiquitous care system to support independent living,"the confidence system aims at helping the elderly stay independent longer by detecting falls and unusual movement which may indicate a health problem. the system uses location sensors and wearable tags to determine the coordinates of the user's body parts, and an accelerometer to detect fall impact and movement. machine learning is combined with domain knowledge in the form of rules to recognize the user's activity. the fall detection employs a similar combination of machine learning and domain knowledge. it was tested on five atypical falls and events that can be easily mistaken for a fall. we show in the paper and demo that neither sensor type can correctly recognize all of these events on its own, but the combination of both sensor types yields highly accurate fall detection. in addition, the detection of unusual movement can observe both the user's micro-movement and macro-movement. this makes it possible for the confidence system to detect most types of threats to the user's health and well-being manifesting in his/her movement.",nan
2012,"autonomous construction with a mobile robot in a resource-limited environment: a demonstration of the integration of perception, planning and action","this demo presents a miniature mobile robot performing autonomous construction in an environment where resources are limited. after an exploration phase, the robot builds a structure at a designated location according to an order from a human. since local resources are scarce, the robot must change its environment to get access to enough resources to complete the construction. this process involves perceiving the environment, reasoning on possible courses of action using a task planner, and implementing these actions to successfully build the requested structure.",nan
2012,wisski: a virtual research environment for cultural heritage,"in this paper we present the wisski system, an open source virtual research environment and content management system for cultural heritage. it promotes semantic enrichment of data on the basis of owl / rdf using the ontology cidoc crm / iso 21127. the data is rendered in a wikipedia-like fashion, combining textual, visual and structured information (info boxes) for a documented object on one page. likewise, data can be acquired using field–based forms and semi–automatically annotated free text, resembling the most common traditional modes of documentation in the cultural heritage domain. this retains a user–friendly visualisation while at the same time providing detailed rdf triple data for automatic processing and data exchange.",nan
2014,breaking conditional symmetry in automated constraint modelling with conjure,"many constraint problems contain symmetry, which can lead to redundant search. if a partial assignment is shown to be invalid, we are wasting time if we ever consider a symmetric equivalent of it. a particularly important class of symmetries are those introduced by the constraint modelling process: model symmetries. we present a systematic method by which the automated constraint modelling tool conjure can break conditional symmetry as it enters a model during refinement. our method extends, and is compatible with, our previous work on automated symmetry breaking in conjure. the result is the automatic and complete removal of model symmetries for the entire problem class represented by the input specification. this applies to arbitrarily nested conditional symmetries and represents a significant step forward for automated constraint modelling.",nan
2014,decidable model-checking for a resource logic with production of resources,"several logics for expressing coalitional ability under resource bounds have been proposed and studied in the literature. previous work has shown that if only consumption of resources is considered or the total amount of resources produced or consumed on any path in the system is bounded, then the model-checking problem for several standard logics, such as resource-bounded coalition logic (rb-cl) and resource-bounded alternating-time temporal logic (rb-atl) is decidable. however, for coalition logics with unbounded resource production and consumption, only some undecidability results are known. in this paper, we show that the model-checking problem for rb-atl with unbounded production and consumption of resources is decidable.",nan
2014,data interlinking through robust linkkey extraction,"links are important for the publication of rdf data on the web. yet, establishing links between data sets is not an easy task. we develop an approach for that purpose which extracts weak linkkeys. linkkeys extend the notion of a key to the case of different data sets. they are made of a set of pairs of properties belonging to two different classes. a weak linkkey holds between two classes if any resources having common values for all of these properties are the same resources. an algorithm is proposed to generate a small set of candidate linkkeys. depending on whether some of the, valid or invalid, links are known, we define supervised and non supervised measures for selecting the appropriate linkkeys. the supervised measures approximate precision and recall, while the non supervised measures are the ratio of pairs of entities a linkkey covers (coverage), and the ratio of entities from the same data set it identifies (discrimination). we have experimented these techniques on two data sets, showing the accuracy and robustness of both approaches.",nan
2014,knowledge and gossip,"a well-studied phenomenon in network theory are optimal schedules to distribute information by one-to-one communication between nodes. one can take these communicative actions to be ‘telephone calls’, and this process of spreading information is known as gossiping [4]. it is typical to assume a global scheduler who simply executes a possibly non-deterministic protocol. such a protocol can be seen as consisting of a sequence of instructions “first, agent a calls b, then c, next, d calls b ...”. we investigate epistemic gossip protocols, where an agent a will call another agent not because it is so instructed but based on its knowledge or ignorance of the factual information that is distributed over the network. such protocols therefore don't need a central schedular, but they come at a cost: they may take longer to terminate than non-epistemic, globally scheduled, protocols. we describe various epistemic protocols, we give their logical properties, and we model them in a number of ways.",nan
2014,"the significance of bidding, accepting and opponent modeling in automated negotiation","given the growing interest in automated negotiation, the search for effective strategies has produced a variety of different negotiation agents. despite their diversity, there is a common structure to their design. a negotiation agent comprises three key components: the bidding strategy, the opponent model and the acceptance criteria. we show that this three-component view of a negotiating architecture not only provides a useful basis for developing such agents but also provides a useful analytical tool. by combining these components in varying ways, we are able to demonstrate the contribution of each component to the overall negotiation result, and thus determine the key contributing components. moreover, we study the interaction between components and present detailed interaction effects. furthermore, we find that the bidding strategy in particular is of critical importance to the negotiator's success and far exceeds the importance of opponent preference modeling techniques. our results contribute to the shaping of a research agenda for negotiating agent design by providing guidelines on how agent developers can spend their time most effectively.",nan
2014,parameterising the complexity of planning by the number of paths in the domain-transition graphs,"we apply the theory of parameterised complexity to planning, using the concept of fixed-parameter tractability (fpt) which is more relaxed than the usual tractability concept. the parameter we focus on is the maximal number of paths in the domain-transition graphs, and we show that for this parameter, optimal planning is fpt for planning instances with polytree causal graphs and acyclic domain-transition graphs. if this parameter is combined with the additional parameters of domain size for the variables and the treewidth of the causal graph, then planning is fpt also for instances with arbitrary causal graphs. furthermore, all these parameters are fpt to test in advance. these results also imply that delete-relaxed planning is fpt, even in its recent generalisation to non-binary variables.",nan
2014,extending acyclicity notions for existential rules,"existential rules have been proposed for representing ontological knowledge, specifically in the context of ontology-based query answering. entailment with existential rules is undecidable. we focus in this paper on conditions that ensure the termination of a breadth-first forward chaining algorithm known as the chase. first, we propose a new tool that allows to extend existing acyclicity conditions ensuring chase termination, while keeping good complexity properties. second, we consider the extension to existential rules with nonmonotonic negation under stable model semantics and further extend acyclicity results obtained in the positive case.",nan
2014,communicating with unknown teammates,"past research has investigated a number of methods for coordinating teams of agents, but with the growing number of sources of agents, it is likely that agents will encounter teammates that do not share their coordination methods. therefore, it is desirable for agents to adapt to these teammates, forming an effective ad hoc team. past ad hoc teamwork research has focused on cases where the agents do not directly communicate. however when teammates do communicate, it can provide a valuable channel for coordination. therefore, this paper tackles the problem of communication in ad hoc teams, introducing a minimal version of the multiagent, multiarmed bandit problem with limited communication between the agents. the theoretical results in this paper prove that this problem setting can be solved in polynomial time when the agent knows the set of possible teammates. furthermore, the empirical results show that an agent can cooperate with a variety of teammates following unknown behaviors even when its models of these teammates are imperfect.",nan
2014,symmetry-driven decision diagrams for knowledge compilation,"in this paper, symmetries are exploited for achieving significant space savings in a knowledge compilation perspective. more precisely, the languages fbdd and ddg of decision diagrams are extended to the languages sym-fbddx,y and sym-ddgx,y of symmetry-driven decision diagrams, where x is a set of “symmetry-free” variables and y is a set of “top” variables. both the time efficiency and the space efficiency of sym-fbddx,y and sym-ddgx,y are analyzed, in order to put those languages in the knowledge compilation map for propositional representations. it turns out that each of sym-fbddx,y and sym-ddgx,y satisfies ct (the model counting query). we prove that no propositional language over a set x∪y of variables, satisfying both co (the consistency query) and cd (the conditioning transformation), is at least as succinct as any of sym-fbddx,y and sym-ddgx,y unless the polynomial hierarchy collapses. the price to be paid is that only a restricted form of conditioning and a restricted form of forgetting are offered by sym-fbddx,y and sym-ddgx,y . nevertheless, this proves sufficient for a number of applications, including configuration and planning. we describe a compiler targeting sym-fbddx,y and sym-ddgx,y and give some experimental results on planning domains, highlighting the practical significance of these languages.",nan
2014,effective and robust natural language understanding for human-robot interaction,"robots are slowly becoming part of everyday life, as they are being marketed for commercial applications (viz. telepresence, cleaning or entertainment). thus, the ability to interact with non-expert users is becoming a key requirement. even if user utterances can be efficiently recognized and transcribed by automatic speech recognition systems, several issues arise in translating them into suitable robotic actions. in this paper, we will discuss both approaches providing two existing natural language understanding workflows for human robot interaction. first, we discuss a grammar based approach: it is based on grammars thus recognizing a restricted set of commands. then, a data driven approach, based on a free-from speech recognizer and a statistical semantic parser, is discussed. the main advantages of both approaches are discussed, also from an engineering perspective, i.e. considering the effort of realizing hri systems, as well as their reusability and robustness. an empirical evaluation of the proposed approaches is carried out on several datasets, in order to understand performances and identify possible improvements towards the design of nlp components in hri.",nan
2014,context-free and context-sensitive kernels: update and deletion equivalence in abstract argumentation,"notions of equivalence which guarantee intersubstitutability w.r.t. further modifications have received considerable interest in nonmonotonic reasoning. this paper is within the context of abstract argumentation and we focus on the most general form of a dynamic scenarios, so-called updates as well as certain sub-classes, namely local, normal and arbitrary deletions. we provide characterization theorems for the corresponding equivalence notions and draw the relations to the recently proposed kinds of expansion equivalence [15, 3]. many of the results rely on abstract concepts like context-free kernels or semantics satisfying isolate-inclusion. therefore, the results may apply to future semantics as well as further equivalence notions.",nan
2014,compact argumentation frameworks,"abstract argumentation frameworks (afs) are one of the most studied formalisms in ai. in this work, we introduce a certain subclass of afs which we call compact. given an extension-based semantics, the corresponding compact afs are characterized by the feature that each argument of the af occurs in at least one extension. this not only guarantees a certain notion of fairness; compact afs are thus also minimal in the sense that no argument can be removed without changing the outcome. we address the following questions in the paper: (1) how are the classes of compact afs related for different semantics? (2) under which circumstances can afs be transformed into equivalent compact ones? (3) finally, we show that compact afs are indeed a non-trivial subclass, since the verification problem remains conp-hard for certain semantics.",nan
2014,scoring rules for the allocation of indivisible goods,"we define a family of rules for dividing m indivisible goods among agents, parameterized by a scoring vector and a social welfare aggregation function. we assume that agents' preferences over sets of goods are additive, but that the input is ordinal: each agent simply ranks single goods. similarly to (positional) scoring rules in voting, a scoring vector s = (s1,...,sm) consists of m nonincreasing nonnegative weights, where si is the score of a good assigned to an agent who ranks it in position i. the global score of an allocation for an agent is the sum of the scores of the goods assigned to her. the social welfare of an allocation is the aggregation of the scores of all agents, for some aggregation function ★ such as, typically, + or min. the rule associated with s and ★ maps a profile to (one of) the allocation(s) maximizing social welfare. after defining this family of rules, and focusing on some key examples, we investigate some of the social-choice-theoretic properties of this family of rules, such as various kinds of monotonicity, separability, envy-freeness, and pareto efficiency.",nan
2014,model checking auctions as artifact systems: decidability via finite abstraction,"the formal verification of auctions has recently received considerable attention in the ai and logic community. we tackle this problem by adopting methodologies and techniques originally developed for artifact systems, a novel paradigm in service oriented computing. specifically, we introduce a typed version of artifactcentric multi-agent systems (ac-mas), a multi-agent setting for artifact systems, and consider the model checking problem against typed first-order temporal epistemic specifications. notably, this formal framework is expressive enough to capture a relevant class of auctions: parallel english (ascending bid) auctions. we prove decidability of the model checking problem for ac-mas via finite abstraction. in particular, we put forward a methodology to formally verify interesting properties of auctions.",nan
2014,incremental elicitation of choquet capacities for multicriteria decision making,"the choquet integral is one of the most sophisticated and expressive preference models used in decision theory for multicriteria decision making. it performs a weighted aggregation of criterion values using a capacity function assigning a weight to any coalition of criteria, thus enabling positive and/or negative interactions among criteria and covering an important range of possible decision behaviors. however, the specification of the capacity involves many parameters which raises challenging questions, both in terms of elicitation burden and guarantee on the quality of the final recommendation. in this paper, we investigate the incremental elicitation of the capacity through a sequence of preference queries selected one-by-one using a minimax regret strategy so as to progressively reduce the set of possible capacities until a decision can be made. we propose a new approach designed to efficiently compute minimax regret for the choquet model. numerical experiments are provided to demonstrate the practical efficiency of our approach.",nan
2014,automating gödel's ontological proof of god's existence with higher-order automated theorem provers,"kurt gödel's ontological argument for god's existence has been formalized and automated on a computer with higher-order automated theorem provers. from gödel's premises, the computer proved: necessarily, there exists god. on the other hand, the theorem provers have also confirmed prominent criticism on gödel's ontological argument, and they found some new results about it.the background theory of the work presented here offers a novel perspective towards a computational theoretical philosophy.",nan
2014,boosting constraint acquisition via generalization queries,"constraint acquisition assists a non-expert user in modeling her problem as a constraint network. in existing constraint acquisition systems the user is only asked to answer very basic questions. the drawback is that when no background knowledge is provided, the user may need to answer a great number of such questions to learn all the constraints. in this paper, we introduce the concept of generalization query based on an aggregation of variables into types. we present a constraint generalization algorithm that can be plugged into any constraint acquisition system. we propose several strategies to make our approach more efficient in terms of number of queries. finally we experimentally compare the recent quacq system to an extended version boosted by the use of our generalization functionality. the results show that the extended version dramatically improves the basic quacq.",nan
2014,influencing social networks: an optimal control study,"we study the evolution of cooperation in social networks, aiming in particular at ways of influencing the behavior in such networks using methods and techniques from optimal control theory. this is of importance to many scenarios where politicians or policy makers strive to push consensus on some topic that may seem suboptimal from individuals' perspectives. to this end, we employ the continuous action iterated prisoner's dilemma (caipd) as model for the interactions in a social network. this model describes how neighboring nodes influence each other, and in effect determines how different strategies may spread through the network. we extend this model, incorporating a mechanism for external influence on the behavior of individual nodes. next we prove reachability of an arbitrary network-wide agreement using the lyapunov's direct method. based on the theory of linear-quadratic trackers we propose a step-wise iterative control algorithm, and show the effectiveness of the proposed controller in various small world and scale free social networks.",nan
2014,inference in the fo(c) modelling language,"recently, fo(c), the integration of c-log with classical logic, was introduced as a knowledge representation language. up to this point, no systems exist that perform inference on fo(c), and very little is known about properties of inference in fo(c). in this paper, we study both of the above problems. we define normal forms for fo(c), one of which corresponds to fo(id). we define transformations between these normal forms, and show that, using these transformations, several inference tasks for fo(c) can be reduced to inference tasks for fo(id), for which solvers exist. we implemented this transformation and hence, created the first system that performs inference in fo(c). we also provide results about the complexity of reasoning in fo(c).",nan
2014,abduction and dialogical proof in argumentation and logic programming,"we develop a model of abduction in abstract argumentation, where changes to an argumentation framework act as hypotheses to explain the support of an observation. we present dialogical proof theories for the main decision problems (i.e., finding hypotheses that explain skeptical/credulous support) and we show that our model can be instantiated on the basis of abductive logic programs.",nan
2014,credibility-limited improvement operators,"in this paper we introduce and study credibility-limited improvement operators. the idea is to accept the new piece of information if this information is judged credible by the agent, so in this case a revision is performed. when the new piece of information is not credible then it is not accepted (no revision is performed), but its plausibility is still improved in the epistemic state of the agent, similarly to what is done by improvement operators. we use a generalized definition of darwiche and pearl epistemic states, where to each epistemic state can be associated, in addition to the set of accepted formulas (beliefs), a set of credible formulas. we provide a syntactic and semantic characterization of these operators.",nan
2014,‘being a manifold’ as the topological primitive of mereotopology,"mereotopology is an approach to modeling space that allows to formalize human spatial intuition without reference to points. a predominant formal theory in this area is the regions connection calculus (rcc) introduced in 1992. rcc has an original fault: it relies on the notion of (euclidean) point for the interpretation of its primitive, i.e., the connection relation c. in this paper we show that in the natural structures for mereotopology, rcc is a theory of manifolds in disguise. it follows that rcc can be reformulated without reference to any notion of point both at the syntactic and at the semantic levels.",nan
2014,analogical classification: a new way to deal with examples,"introduced a few years ago, analogy-based classification methods are a noticeable addition to the set of lazy learning techniques. they provide amazing results (in terms of accuracy) on many classical datasets. they look for all triples of examples in the training set that are in analogical proportion with the item to be classified on a maximal number of attributes and for which the corresponding analogical proportion equation on the class has a solution. in this paper when classifying a new item, we demonstrate a new approach where we focus on a small part of the triples available. to restrict the scope of the search, we first look for examples that are as similar as possible to the new item to be classified. we then only consider the pairs of examples presenting the same dissimilarity as between the new item and one of its closest neighbors. thus we implicitly build triples that are in analogical proportion on all attributes with the new item. then the classification is made on the basis of a majority vote on the pairs leading to a solvable class equation. this new algorithm provides results as good as other analogical classifiers with a lower average complexity.",nan
2014,manipulating picking sequences,"picking sequences are a natural way of allocating indivisible items to agents in a decentralized manner: at each stage, a designated agent chooses an item among those that remain available. we address the computational issues of the manipulation of picking sequences by an agent or a coalition of agents. we show that a single agent can compute an optimal manipulation in polynomial time. then we consider several notions of coalitional manipulation; for one of these notions, we show that computing an optimal manipulation is easy. we temper these results by giving a nontrivial upper bound on the impact of manipulation on the loss of social welfare.",nan
2014,on the properties of belief tracking for online contingent planning using regression,"planning under partial observability typically requires some representation of the agent's belief state – either online to determine which actions are valid, or offline for planning. due to its potential exponential size, efficient maintenance of a belief state is, thus, a key research challenge in this area. the state-of-the-art factored belief tracking (fbt) method addresses this problem by maintaining multiple smaller projected belief states, each involving only a subset of the variable set. its complexity is exponential in the size of these subsets, as opposed to the entire variable set, without jeopardizing completeness. in this paper we develop the theory of regression to serve as an alternative tool for belief-state maintenance. regression is a well known technique enjoying similar, and potentially even better worst-case complexity, as its complexity depends on the actions and observations that actually took place, rather than all actions and potential observations, as in the fbt method. on the other hand, fbt is likely to have better amortized complexity if the number of queries to the belief state is very large. an empirical comparison of regression with fbt-based belief maintenance is carried out, showing that the two perform similarly.",nan
2014,grappa: a semantical framework for graph-based argument processing,"graphical models are widely used in argumentation to visualize relationships among propositions or arguments. the intuitive meaning of the links in the graphs is typically expressed using labels of various kinds. in this paper we introduce a general semantical framework for assigning a precise meaning to labelled argument graphs which makes them suitable for automatic evaluation. our approach rests on the notion of explicit acceptance conditions, as first studied in abstract dialectical frameworks (adfs). the acceptance conditions used here are functions from multisets of labels to truth values. we define various dung style semantics for argument graphs. we also introduce a pattern language for specifying acceptance functions. moreover, we show how argument graphs can be compiled to adfs, thus providing an automatic evaluation tool via existing adf implementations. finally, we also discuss complexity issues.",nan
2014,multi-context systems for reactive reasoning in dynamic environments,"we show in this paper how managed multi-context systems (mmcs) can be turned into a reactive formalism suitable for continuous reasoning in dynamic environments. we extend mmcs with (abstract) sensors and define the notion of a run of the extended systems. we then show how typical problems arising in online reasoning can be addressed: handling potentially inconsistent sensor input, modeling intelligent forms of forgetting, and controlling the reasoning effort spent by contexts. we also investigate the complexity of some important related decision problems.",nan
2014,abstract disjunctive answer set solvers,"a fundamental task in answer set programming is to compute answer sets of logic programs. answer set solvers are the programs that perform this task. the problem of deciding whether a disjunctive program has an answer set is σp2-complete. the high complexity of reasoning within disjunctive logic programming is responsible for few solvers capable of dealing with such programs, namely dlv, gnt, cmodels and clasp. we show that transition systems introduced by nieuwenhuis, oliveras, and tinelli to model and analyze satisfiability solvers can be adapted for disjunctive answer set solvers. in particular, we present transition systems for cmodels (without backjumping and learning), gnt and dlv (without backjumping). the unifying perspective of transition systems on satisfiability and non-disjunctive answer set solvers proved to be an effective tool for analyzing, comparing, proving correctness of each underlying search algorithm as well as bootstrapping new algorithms. given this, we believe that this work will bring clarity and inspire new ideas in design of more disjunctive answer set solvers.",nan
2014,planning and execution of robot tasks based on a platform-independent model of robot capabilities,"the diversity of robotic architectures is a major factor in developing platform-independent algorithms. there is a need for a widely usable model of robot capabilities which can help to describe and reason about the diversity of robotic systems. we propose such a model and present an integrated framework for task planning and task execution using this model. existing planning techniques need to be extended to support this model, as it requires 1) generating new objects during planning time; and, 2) establishing concurrency based on data flow within the robotic system. we present results on planning and execution of an object transportation task in simulation.",nan
2014,atl* with truly perfect recall: expressivity and validities,"in alternating-time temporal logic atl*, agents with perfect recall assign choices to sequences of states, i.e., to possible finite histories of the game. however, when a nested strategic modality is interpreted, the new strategy does not take into account the previous sequence of events. it is as if agents collect their observations in the nested game again from scratch, thus effectively forgetting what they observed before. intuitively, it does not fit the assumption of agents having perfect recall of the past.recently we have proposed a new semantics for atl* where the past is not forgotten in nested games [8]. in this paper we give a formal treatment and show that the standard semantics of atl* coincides with our new semantics in case of agents with perfect information. on the other hand, both kinds of semantics differ if agents have imperfect information about the state of the game. we compare the expressivity of the logics and their sets of validities. the latter characterize general properties of the underlying class of games.",nan
2014,eliciting a suitable voting rule via examples,"we address the problem of specifying a voting rule by means of a series of examples. each example consists of the answer to a simple question: how should the rule rank two alternatives, given the positions at which each voter ranks the two alternatives? to be able to formalise this elicitation problem, we develop a novel variant of classical social choice theory in terms of associations of alternatives with vectors of ranks rather than the common associations of voters with preference orders. we then define and study a class of voting rules suited for elicitation using such answers. finally, we propose and experimentally evaluate several elicitation strategies for arriving at a good approximation of the target rule with a reasonable number of queries.",nan
2014,a gpu implementation of large neighborhood search for solving constraint optimization problems,"constraint programming has gained prominence as an effective and declarative paradigm for modeling and solving complex combinatorial problems. techniques based on local search have proved practical to solve real-world problems, providing a good compromise between optimality and efficiency. in spite of the natural presence of concurrency, there has been relatively limited effort to use novel massively parallel architectures, such as those found in modern graphical processing units (gpus), to speedup local search techniques in constraint programming. this paper describes a novel framework which exploits parallelism from a popular local search method (the large neighborhood search method), using gpus.",nan
2014,a systematic solution to the (de-)composition problem in general game playing,"general game players can drastically reduce the cost of search if they are able to solve smaller subproblems individually and synthesise the resulting solutions. to provide a systematic solution to this (de-)composition problem, we start off with generalising the standard decomposition problem in planning by allowing the composition of individual solutions to be further constrained by domain-dependent requirements of the global planning problem. we solve this generalised problem based on a systematic analysis of composition operators for transition systems, and we demonstrate how this solution can be further generalised to general game playing.",nan
2014,practical performance of refinements of nash equilibria in extensive-form zero-sum games,"nash equilibrium (ne) is the best known solution concept used in game theory. it is known that ne is particularly weak even in zero-sum extensive-form games since it can prescribe irrational actions to play that do not exploit mistakes made by an imperfect opponent. these issues are addressed by a number of refinements of ne that strengthen the requirements for equilibrium strategies. however, a thorough experimental analysis of practical performance of the nash equilibria refinement strategies is, to the best of our knowledge, missing. this paper aims to fill this void and provides the first broader experimental comparison of the quality of refined nash strategies in zero-sum extensive-form games. the experimental results suggest that (1) there is a significant difference between the best and the worst ne strategy against imperfect opponents, (2) the existing refinements outperform the worst ne strategy, (3) they typically perform close to the best possible ne strategy, and (4) the difference in performance of all compared refinements is very small.",nan
2014,"formal arguments, preferences, and natural language interfaces to humans: an empirical evaluation","it has been claimed that computational models of argumentation provide support for complex decision making activities in part due to the close alignment between their semantics and human intuition. in this paper we assess this claim by means of an experiment: people's evaluation of formal arguments — presented in plain english — is compared to the conclusions obtained from argumentation semantics. our results show a correspondence between the acceptability of arguments by human subjects and the justification status prescribed by the formal theory in the majority of the cases. however, post-hoc analyses show that there are some significant deviations, which appear to arise from implicit knowledge regarding the domains in which evaluation took place. we argue that in order to create argumentation systems, designers must take implicit domain specific knowledge into account.",nan
2014,lattice-based biclustering using partition pattern structures,"in this work we present a novel technique for exhaustive bicluster enumeration using formal concept analysis (fca). particularly, we use pattern structures (an extension of fca dealing with complex data) to mine similar row/column biclusters, a specialization of biclustering when attribute values have coherent variations. we show how biclustering can benefit from the fca framework through its robust theoretical description and efficient algorithms. finally, we evaluate our bicluster mining approach w.r.t. a standard biclustering technique showing very good results in terms of bicluster quality and performance.",nan
2014,reducing global consistency to local consistency in ontology-based data access,"ontology-based data access (obda) is a new paradigm aiming at accessing and managing data by means of an ontology, i.e., a conceptual representation of the domain of interest in the underlying information system. in the last years, this new paradigm has been used for providing users with suitable mechanisms for querying the data residing at the information system sources. most of the research has been concentrating on making query answering efficient. howeverquery answering is not the only service that an obda system must provide. another crucial service is consistency checking. current approaches to this problem involves executing expensive queries at run-time. in this paper we address a fundamental problem for obda system: given an obda specification, can we avoid the consistency check on the whole obda system (global consistency check), and rely instead on the constraint checking carried out by the dbms on the data source (local consistency checking)? we present algorithms and complexity analysis for this problem, showing that it can be solved efficiently for a class of obda systems that is very relevant in practice.",nan
2014,on the efficient implementation of social abstract argumentation,"in this paper we present a novel iterative algorithm – the iterative successive substitution (iss) – to efficiently approximate the models of debates structured according to social abstract argumentation [10]. classical iterative algorithms such as the iterative newton-raphson (inr) and the iterative fixed-point (ifp) don't always converge and, when they do, usually take too long to be effective. we analytically prove convergence of iss, and empirically show that, even when inr and ifp converge, iss always outperforms them, often by several orders of magnitude. the iss is able to approximate the models of complex debates with thousands of arguments in well under a second, often in under one tenth of a second, making it comfortably suitable for its purpose. additionally, we present a small modification to iss that, with a negligible overhead, takes advantage of the topological structure of certain debates to significantly increase convergence times.",nan
2014,belief merging within fragments of propositional logic,"recently, belief change within the framework of fragments of propositional logic has gained increasing attention. previous works focused on belief contraction and belief revision on the horn fragment. however, the problem of belief merging within fragments of propositional logic has been neglected so far. this paper presents a general approach to define new merging operators derived from existing ones such that the result of merging remains in the fragment under consideration. our approach is not limited to the case of horn fragment but applicable to any fragment of propositional logic characterized by a closure property on the sets of models of its formulæ. we study the logical properties of the proposed operators in terms of satisfaction of merging postulates, considering in particular distance-based merging operators for horn and krom fragments.",nan
2014,a single-agent approach to multiagent planning,"in this paper we present a novel approach to multiagent planning in domains with concurrent actions and associated concurrent action constraints. in these domains, we associate the actions of individual agents with subsets of objects, which allows for a transformation of the problems into single-agent planning problems that are considerably easier to solve. the transformation forces agents to select joint actions associated with a single subset of objects at a time, and ensures that the concurrency constraints on this subset are satisfied. joint actions are serialised such that each agent performs their part of the action separately. the number of actions in the resulting single-agent planning problem turns out to be manageable in many real-world domains, thus allowing the problem to be solved efficiently using a standard single-agent planner. we also describe a cost-optimal algorithm for compressing the resulting plan, i.e. merging individual actions in order to reduce the total number of joint actions. results show that our approach can handle large problems that are impossible to solve for most multiagent planners.",nan
2014,characterising semantic relatedness using interpretable directions in conceptual spaces,"various applications, such as critique-based recommendation systems and analogical classifiers, rely on knowledge of how different entities relate. in this paper, we present a methodology for identifying such semantic relationships, by interpreting them as qualitative spatial relations in a conceptual space. in particular, we use multi-dimensional scaling to induce a conceptual space from a relevant text corpus and then identify directions that correspond to relative properties such as “more violent than” in an entirely unsupervised way. we also show how a variant of foil is able to learn natural categories from such qualitative representations, by simulating a fortiori inference, an important pattern of commonsense reasoning.",nan
2014,concept dissimilarity based on tree edit distances and morphological dilations,"a number of similarity measures for comparing description logic concepts have been proposed. criteria have been developed to evaluate a measure's fitness for an application. these criteria include on the one hand those that ensure compatibility with the semantics, such as equivalence soundness, and on the other hand the properties of a metric, such as the triangle inequality. in this work we present two classes of dissimilarity measures that are at the same time equivalence sound and satisfy the triangle inequality: a simple dissimilarity measure, based on description trees for the lightweight description logic el; and an instantiation of a general framework, presented in our previous work, using dilation operators from mathematical morphology, and which exploits the link between hausdorff distance and dilations using balls of the ground distance as structuring elements.",nan
2014,nonparametric bayesian multi-task large-margin classification,"in this paper, we present a nonparametric bayesian multi-task large-margin classification model which can cluster tasks into the most appropriate number of groups and induce flexible model sharing within each task group simultaneously. specifically, we first show a very simple method to integrate large margin learning with hierarchical bayesian models by employing an important variant of the standard svmi.e.proximal svm (psvm)whose loss function is used to define a novel likelihood function. and then we assume that the model parameter of each task consists of two parts: one is shared within each task group (group-level parameter) while the other is specific to each distinct task (task rescaling parameter). a dirichlet process prior is imposed on the group-level parameter while the task rescaling parameter is assigned a one-mean laplace prior. finally the parameter of a task is the corresponding group parameter times its specific rescaling parameter. we give efficient markov chain monte calo (mcmc) algorithm to conduct model inference. experiments on the landmine detection data and the uci yeast data demonstrate the effectiveness of our method.",nan
2014,reasoning about uncertainty and explicit ignorance in generalized possibilistic logic,"generalized possibilistic logic (gpl) is a logic for reasoning about the revealed beliefs of another agent. it is a two-tier propositional logic, in which propositional formulas are encapsulated by modal operators that are interpreted in terms of uncertainty measures from possibility theory. models of a gpl theory represent weighted epistemic states and are encoded as possibility distributions. one of the main features of gpl is that it allows us to explicitly reason about the ignorance of another agent. in this paper, we study two types of approaches for reasoning about ignorance in gpl, based on the idea of minimal specificity and on the notion of guaranteed possibility, respectively. we show how these approaches naturally lead to different flavours of the language of gpl and a number of decision problems, whose complexity ranges from the first to the third level of the polynomial hierarchy.",nan
2014,an axiomatic analysis of structured argumentation for prioritized default reasoning,"several systems of argument-based and non-argument-based semantics have been proposed for prioritized default reasoning. as the proposed semantics often sanction contradictory conclusions (even for skeptical reasoners), there is a fundamental need for guidelines for understanding and evaluating them, especially their conceptual foundations and relationships. in this paper, we introduce several natural axioms for structural argumentation with preferences that capture both the consistency and closure postulates. we show that aspic+ semantics do not satisfy key axioms including the consistency postulate and propose a simple one satisfying all axioms. we show that the prescriptive non-argument-based approach to prioritized default reasoning is sound (and complete for a relevant class of knowledge bases) wrt our proposed simple semantics.",nan
2014,a profit-aware negotiation mechanism for on-demand transport services,"as new markets for transportion arise, on-demand transport services are set to grow as more passengers seek affordable personalized journeys. to reduce passenger prices and increase provider revenue, these journeys will often be shared with other passengers. as such, new negotiation mechanisms between passengers and the service provider are required to plan and price journeys. in this paper, we propose a novel profit-aware negotiation mechanism: a multiagent approach that accounts for both passenger and service provider preferences. our negotiation mechanism prices each passenger's journey, in addition to providing vehicle routing and scheduling. we prove a stability property of our negotiation mechanism using a connection to hedonic games. this connection yields new insights into the link between vehicle routing and passenger pricing. we also show via simulations the dependence of the service provider profit and passenger prices on the number of passengers as well as passenger demographics. in particular, our key observation is that increasing the number of passengers has the effect of increasing passenger diversity, which in turn increases the service provider's profit.",nan
2014,mining heterogeneous multidimensional sequential patterns,"all domains of science and technology produce large and heterogeneous data. although much work has been done in this area, mining such data is still a challenge. no previous research targets the mining of heterogeneous multidimensional sequential data. in this work, we present a new approach to extract heterogeneous multidimensional sequential patterns with different levels of granularity by relying on external taxonomies. we show the efficiency and interest of our approach with the analysis of trajectories of care for colorectal cancer using data from the french casemix information system.",nan
2014,towards practical deletion repair of inconsistent dl-programs,"nonmonotonic description logic (dl-) programs couple nonmonotonic logic programs with dl-ontologies through queries in a loose way which may lead to inconsistency, i.e., lack of an answer set. recently defined repair answer sets remedy this but a straightforward computation method lacks practicality. we present a novel evaluation algorithm for deletion repair answer sets based on support sets, which reduces evaluation of dl-litea ontology queries to constraint matching. this leads to significant performance gains towards inconsistency management in practice.",nan
2014,collective rationality in graph aggregation,suppose a number of agents each provide us with a directed graph over a common set of vertices. graph aggregation is the problem of computing a single “collective” graph that best represents the information inherent in this profile of individual graphs. we consider this aggregation problem from the point of view of social choice theory and ask what properties shared by the individual graphs will transfer to the graph computed by a given aggregation procedure. our main result is a general impossibility theorem that applies to a wide range of graph properties.,nan
2014,self-decomposable global constraints,"scalability becomes more and more critical to decision support technologies. in order to address this issue in constraint programming, we introduce the family of self-decomposable constraints. these constraints can be satisfied by applying their own filtering algorithms on variable subsets only. we introduce a generic framework which dynamically decompose propagation, by filtering over variable subsets. our experiments over the cumulative constraint illustrate the practical relevance of self-decomposition.",nan
2014,solving maximum weight clique using maximum satisfiability reasoning,"satisfiability (sat) and maximum satisfiability (maxsat) techniques are proved to be powerful in solving combinatorial optimization problems. in this paper, we encode the maximum weight clique (mwc) problem into weighted partial maxsat and use maxsat techniques to solve it. concretely, we propose a new algorithm based on maxsat reasoning called top-k failed literal detection to improve the upper bound for mwc, and implement an exact branch-and-bound solver for the mwc problem called maxwclq based on the top-k failed literal detection algorithm. to our best knowledge, this is the first time that maxsat techniques are integrated to solve the mwc problem. experimental evaluations on the broadly used dimacs benchmark, bhoslib benchmark and random graphs show that maxwclq outperforms state-of-the-art exact algorithms on the vast majority of instances. in particular, our algorithm is surprisingly powerful for dense and hard graphs.",nan
2014,multilateral bargaining for resource division,"we address the problem of how a group of agents can decide to share a resource, represented as a unit-sized pie. we investigate a finite horizon non-cooperative bargaining game, in which the players take it in turns to make proposals on how the resource should be allocated, and the other players vote on whether or not to accept the allocation. voting is modelled as a bayesian weighted voting game with uncertainty about the players' weights. the agenda, (i.e., the order in which the players are called to make offers), is defined exogenously. we focus on impatient players with heterogeneous discount factors. in the case of a conflict, (i.e., no agreement by the deadline), all the players get nothing. we provide a bayesian subgame perfect equilibrium for the bargaining game and conduct an ex-ante analysis of the resulting outcome. we show that, the equilibrium is unique, computable in polynomial time, results in an instant pareto optimal agreement, and, under certain conditions provides a foundation for the core of the bayesian voting game. our analysis also leads to insights on how an individual's bargained share is influenced by his position on the agenda. finally, we show that, if the conflict point of the bargaining game changes, then the problem of determining a non-cooperative equilibrium becomes np-hard even under the perfect information assumption.",nan
2014,bargaining for coalition structure formation,"many multiagent settings require a collection of agents to partition themselves into coalitions. in such cases, the agents may have conflicting preferences over the possible coalition structures that may form. we investigate a noncooperative bargaining game to allow the agents to resolve such conflicts and partition themselves into non-overlapping coalitions. the game has a finite horizon and is played over discrete time periods. the bargaining agenda is defined exogenously. an important element of the game is a parameter 0≤δ≤1 that represents the probability that bargaining ends in a given round. thus, δ is a measure of the degree of democracy (ranging from democracy for δ=0, through increasing levels of authoritarianism as δ approaches 1, to dictatorship for δ=1). for this game, we focus on the question of how a player's position on the agenda affects his power. we also analyse the relation between the distribution of the power of individual players, the level of democracy, and the welfare efficiency of the game. surprisingly, we find that purely democratic games are welfare inefficient due to an uneven distribution of power among the individual players. interestingly, introducing a degree of authoritarianism into the game makes the distribution of power more equitable and maximizes welfare.",nan
2014,monte-carlo tree search: to mc or to dp?,"state-of-the-art monte-carlo tree search algorithms can be parametrized with any of the two information updating procedures: mc-backup and dp-backup. the dynamics of these two procedures is very different, and so far, their relative pros and cons have been poorly understood. formally analyzing the dependency of mc- and dp-backups on various mdp parameters, we reveal numerous important issues that get hidden by the worst-case bounds on the algorithm performance, and reconfirm these findings by a systematic experimental test.",nan
2014,improving pattern discovery relevancy by deriving constraints from expert models,"to support knowledge discovery from data, many pattern mining techniques have been proposed. one of the bottlenecks for their dissemination is the number of computed patterns that appear to be either trivial or uninteresting with respect to available knowledge. integration of domain knowledge in constraint-based data mining is limited. relevant patterns still miss because methods partly fail in assessing their subjective interestingness. however, in practice, we often have in the literature mathematical models defined by experts based on their domain knowledge. we propose here to exploit such models to derive constraints that can be used during the data mining phase to improve both pattern relevancy and computational efficiency. even though the approach is generic, it is illustrated on pattern set discovery from real data for studying soil erosion.",nan
2014,argumentation accelerated reinforcement learning for cooperative multi-agent systems,"multi-agent learning is a complex problem, especially in real-time systems. we address this problem by introducing argumentation accelerated reinforcement learning (aarl), which provides a methodology for defining heuristics, represented by arguments, and incorporates these heuristics into reinforcement learning (rl) by using reward shaping. we define aarl via argumentation and prove that it can coordinate independent cooperative agents that have a shared goal but need to perform different actions. we test aarl empirically in a popular rl testbed, robocup takeaway, and show that it significantly improves upon standard rl.",nan
2014,detecting the reputation polarity of microblog posts,"we address the task of detecting the reputation polarity of social media updates, that is, deciding whether the content of an update has positive or negative implications for the reputation of a given entity. typical approaches to this task include sentiment lexicons and linguistic features. however, they fall short in the social media domain because of its unedited and noisy nature, and, more importantly, because reputation polarity is not only encoded in sentiment-bearing words but it is also embedded in other word usage. to this end, automatic methods for extracting discriminative features for reputation polarity detection can play a role. we propose a data-driven, supervised approach for extracting textual features, which we use to train a reputation polarity classifier. experiments on the replab 2013 collection show that our model outperforms the state-of-the-art method based on sentiment analysis by 20% accuracy.",nan
2014,lower and upper approximations for depleting modules of description logic ontologies,"it is known that no algorithm can extract the minimal depleting σ-module from ontologies in expressive description logics (dls). thus research has focused on algorithms that approximate minimal depleting modules ‘from above’ by computing a depleting module that is not necessarily minimal. the first contribution of this paper is an implementation (amex) of such a depleting module extraction algorithm for expressive acyclic dl ontologies that uses a qbf solver for checking conservative extensions relativised to singleton interpretations. to evaluate amex and other module extraction algorithms we propose an algorithm approximating minimal depleting modules ‘from below’ (which also uses a qbf solver). we present experiments based on nci (the national cancer institute thesaurus) that indicate that our lower approximation often coincides with (or is very close to) the upper approximation computed by amex, thus proving for the first time that an approximation algorithm for minimal depleting modules can be almost optimal on a large ontology. we use the same technique to evaluate locality-based module extraction and a hybrid approach on nci.",nan
2014,answer set programming as sat modulo acyclicity,"answer set programming (asp) is a declarative programming paradigm for solving search problems arising in knowledge-intensive domains. one viable way to implement the computation of answer sets corresponding to problem solutions is to recast a logic program as a boolean satisfiability (sat) problem and to use existing sat solver technology for the actual search. such mappings can be obtained by augmenting clark's completion with constraints guaranteeing the strong justifiability of answer sets. to this end, we consider an extension of sat by graphs subject to an acyclicity constraint, called sat modulo acyclicity. we devise a linear embedding of logic programs and study the performance of answer set computation with sat modulo acyclicity solvers.",nan
2014,"past, present, and future: an optimal online algorithm for single-player gdl-ii games","in general game playing, a player receives the rules of an unknown game and attempts to maximize his expected reward. since 2011, the gdl-ii rule language extension allows the formulation of nondeterministic and partially observable games. in this paper, we present an algorithm for such games, with a focus on the single-player case. conceptually, at each stage, the proposed norns algorithm distinguishes between the past, present and future steps of the game. more specifically, a belief state tree is used to simulate a potential past that leads to a present that is consistent with received observations. unlike other related methods, our method is asymptotically optimal. moreover, augmenting the belief state tree with iteratively improved probabilities speeds up the process over time significantly.as this allows a true picture of the present, we additionally present an optimal version of the well-known uct algorithm for partially observable single-player games. instead of performing hindsight optimization on a simplified, fully observable tree, the true future is simulated on an action-observation tree that takes partial observability into account. the expected reward estimates of applicable actions converge towards the true expected rewards even for moves that are only used to gather information. we prove that our algorithm is asymptotically optimal for single-player games and pomdps and support our claim with an empirical evaluation.",nan
2014,nested dichotomies with probability sets for multi-class classification,"binary decomposition techniques transform a multi-class problem into several simpler binary problems. in such techniques, a classical issue is to ensure the consistency between the binary assessments of conditional probabilities. nested dichotomies, which consider tree-shaped decomposition, do not suffer from this issue. yet, a wrong probability estimate in the tree can strongly biase the results and provide wrong predictions. to overcome this issue, we consider in this paper imprecise nested dichotomies, in which binary probabilities become imprecise. we show in experiments that the approach has many advantages: it provides cautious inferences when only little information is available, and allows to make efficient computations with imprecise probabilities even when considering generic cost functions.",nan
2014,ltl verification of online executions with sensing in bounded situation calculus,"we look at agents reasoning about actions from a first-person perspective. the agent has a representation of world as situation calculus action theory. it can perform sensing actions to acquire information. the agent acts “online”, i.e., it performs an action only if it is certain that the action can be executed, and collects sensing results from the actual world. when the agent reasons about its future actions, it indeed considers that it is acting online; however only possible sensing values are available. the kind of reasoning about actions we consider for the agent is verifying a first-order (fo) variant (without quantification across situations) of linear time temporal logic (ltl) . we mainly focus on bounded action theories, where the number of facts that are true in any situation is bounded. the main results of this paper are: (i) possible sensing values can be based on consistency if the initial situation description is fo; (ii) for bounded action theories, progression over histories that include sensing results is always fo; (iii) for bounded theories, verifying our fo ltl against online executions with sensing is decidable.",nan
2014,evolving multi-context systems,"managed multi-context systems (mmcss) provide a general framework for integrating knowledge represented in heterogeneous kr formalisms. however, mmcss are essentially static as they were not designed to run in a dynamic scenario. in this paper, we introduce evolving multi-context systems (emcss), a general and flexible framework which inherits from mmcss the ability to integrate knowledge represented in heterogeneous kr formalisms, and at the same time is able to both react to, and reason in the presence of commonly temporary dynamic observations, and evolve by incorporating new knowledge. we show that emcss are indeed very general and expressive enough to capture several existing kr approaches that model dynamics of knowledge.",nan
2014,embedding heterogeneous data by preserving multiple kernels,"heterogeneous data may arise in many real-life applications under different scenarios. in this paper, we formulate a general framework to address the problem of modeling heterogeneous data. our main contribution is a novel embedding method, called multiple kernel preserving embedding (mkpe), which projects heterogeneous data into a unified embedding space by preserving crossdomain interactions and within-domain similarities simultaneously. these interactions and similarities between data points are approximated with gaussian kernels to transfer local neighborhood information to the projected subspace. we also extend our method for out-of-sample embedding using a parametric formulation in the projection step. the performance of mkpe is illustrated on two tasks: (i) modeling biological interaction networks and (ii) cross-domain information retrieval. empirical results of these two tasks validate the predictive performance of our algorithm.",nan
2014,bayesian multiview dimensionality reduction for learning predictive subspaces,"multiview learning basically tries to exploit different feature representations to obtain better learners. for example, in video and image recognition problems, there are many possible feature representations such as color- and texture-based features. there are two common ways of exploiting multiple views: forcing similarity (i) in predictions and (ii) in latent subspace. in this paper, we introduce a novel bayesian multiview dimensionality reduction method coupled with supervised learning to find predictive subspaces and its inference details. experiments show that our proposed method obtains very good results on image recognition tasks in terms of classification and retrieval performances.",nan
2014,near fairness in matroids,"this article deals with the fair allocation of indivisible goods and its generalization to matroids. the notions of fairness under consideration are equitability, proportionality and envy-freeness. it is long known that some instances fail to admit a fair allocation. however, an almost fair solution may exist if an appropriate relaxation of the fairness condition is adopted. this article deals with a matroid problem which comprises the allocation of indivisible goods as a special case. it is to find a base of a matroid and to allocate it to a pool of agents. we first adapt the aforementioned fairness concepts to matroids. next we propose a relaxed notion of fairness said to be near to fairness. near fairness respects the fairness up to one element. we show that a nearly fair solution always exists and it can be constructed in polynomial time in the general context of matroids.",nan
2014,strategic argumentation is np-complete,we study the complexity of the strategic argumentation problem for 2-player dialogue games where a player should decide what move (set of rules) to play at each turn in order to prove (disprove) a given thesis. we show that this is an np-complete problem.,nan
2014,diagnosis of hybrid systems with smt: opportunities and challenges,"we propose a new approach to diagnosis of hybrid systems. in this approach, questions about the behavior of the system are asked and translated into satisfiability modulo theory (smt) problems, which are then solved by an smt solver. we show the reduction to smt. we also discuss the benefits and the drawbacks of this approach and conclude with a number of research directions that will make this approach applicable to large systems.",nan
2014,controlling two-stage voting rules,"we study the computational complexity of control problems for two-stage voting rules. an example of a two-stage voting rule is the black's procedure. the first stage of the black's procedure selects the condorcet winner if one exists; otherwise, in the second stage the borda winner is selected. the computational complexity of the manipulation problem of two-stage voting rules has recently been studied by narodytska and walsh [20] and fitzsimmons et al. [14]. extending their work, we consider the control problems for similar scenarios, focusing on constructive control by adding or deleting votes, denoted as ccav and ccdv, respectively.let x be the voting rule applied in the first stage and y the one in the second stage. as for the manipulation problem shown in [20, 14], we prove that there is basically no connection between the complexity of ccav and ccdv for x or y and the complexity of ccav and ccdv for the two-stage election x then y: ccav and ccdv for x then y could be np-hard, while both problems are polynomial-time solvable for x and y . on the other hand, combining two rules x and y, both with np-hard ccav and ccdv, could lead to a two-stage election, where both ccav and ccdv become polynomial-time solvable. hereby, we also achieve some complexity results for the special case x then x. in addition, we show that, compared to the manipulation problem, the control problems for two-stage elections admit more diverse behaviors concerning their complexity. for example, there exist rules x and y, for each of which ccav and ccdv have the same complexity, but ccav and ccdv behave differently for x then y.",nan
2014,human-computer negotiation in three-player market settings,"this paper studies commitment strategies in three-player negotiation settings comprising human players and computer agents. we defined a new game called the contract game which is analogous to real-world market settings in which participants need to reach agreement over contracts in order to succeed. the game comprises three players, two service providers and one customer. the service providers compete to make repeated contract offers to the customer consisting of resource exchanges in the game. we formally analyzed the game and defined sub-game perfect equilibrium strategies for the customer and service providers that involve commitments. we conducted extensive empirical studies of these strategies in three different countries, the u.s., israel and china. we ran several configurations in which two human participants played a single agent using the equilibrium strategies in various role configurations in the game (both customer and service providers). our results showed that the computer agent using equilibrium strategies for the customer role was able to outperform people playing the same role in all three countries. in contrast, the computer agent playing the role of the service provider was not able to outperform people. analysis reveals this difference in performance is due to the contracts proposed in equilibrium being significantly beneficial to the customer players, as well as irrational behavior taken by human customer players in the game.",nan
2014,measuring diversity of preferences in a group,"we introduce a general framework for measuring the degree of diversity in the preferences held by the members of a group. we formalise and investigate three specific approaches within that framework: diversity as the range of distinct views held, diversity as aggregate distance between individual views, and diversity as distance of the group's views to a single compromise view. while similarly attractive from an intuitive point of view, the three approaches display significant differences when analysed using both the axiomatic method and empirical studies.",nan
2014,spatio-temporal stream reasoning with incomplete spatial information,"reasoning about time and space is essential for many applications, especially for robots and other autonomous systems that act in the real world and need to reason about it. in this paper we present a pragmatic approach to spatio-temporal stream reasoning integrated in the robot operating system through the dyknow framework. the temporal reasoning is done in metric temporal logic and the spatial reasoning in the region connection calculus rcc-8. progression is used to evaluate spatio-temporal formulas over incrementally available streams of states. to handle incomplete information the underlying first-order logic is extended to a three-valued logic. when incomplete spatial information is received, the algebraic closure of the known information is computed. since the algebraic closure might have to be re-computed every time step, we separate the spatial variables into static and dynamic variables and reuse the algebraic closure of the static variables, which reduces the time to compute the full algebraic closure. the end result is an efficient and useful approach to spatio-temporal reasoning over streaming information with incomplete spatial information.",nan
2014,on the revision of planning tasks,"when a planning task cannot be solved then it can often be made solvable by modifying it a bit: one may change either the set of actions, or the initial state, or the goal description. we show that modification of actions can be reduced to initial state modification. we then apply katsuno and mendelzon's distinction between update and revision and show that the modification of the initial state is an update and the modification of the goal description is a revision. we consider variants of forbus's update and dalal's revision operation and argue that existing belief change operations do not apply as they stand because their inputs are boolean formulas, while plan task modification involves counterfactual statements. we show that they can be captured in dynamic logic of propositional assignments dl-pa.",nan
2014,“distance”? who cares? tailoring merge-and-shrink heuristics to detect unsolvability,"research on heuristic functions is all about estimating the length (or cost) of solution paths. but what if there is no such path? many known heuristics have the ability to detect (some) unsolvable states, but that ability has always been treated as a by-product. no attempt has been made to design heuristics specifically for that purpose, where there is no need to preserve distances. as a case study towards leveraging that advantage, we investigate merge-and-shrink abstractions in classical planning. we identify safe abstraction steps (no information loss regarding solvability) that would not be safe for traditional heuristics. we design practical algorithm configurations, and run extensive experiments showing that our heuristics outperform the state of the art for proving planning tasks unsolvable.",nan
2014,language classification of hierarchical planning problems,"theoretical results on htn planning are mostly related to the plan existence problem. in this paper, we study the structure of the generated plans in terms of the language they produce. we show that such languages are always context-sensitive. furthermore we identify certain subclasses of htn planning problems which generate either regular or context-free languages. most importantly we have discovered that htn planning problems, where preconditions and effects are omitted, constitute a new class of languages that lies strictly between the context-free and context-sensitive languages.",nan
2014,progression in maximum satisfiability,"maximum satisfiability (maxsat) is a well-known optimization version of propositional satisfiability (sat), that finds a wide range of relevant practical applications. despite the significant progress made in maxsat solving in recent years, many practically relevant problem instances require prohibitively large run times, and many cannot simply be solved with existing algorithms. one approach for solving maxsat is based on iterative sat solving, which may optionally be guided by unsatisfiable cores. a difficulty with this class of algorithms is the possibly large number of times a sat solver is called, e.g. for instances with very large clause weights. this paper proposes the use of geometric progressions to tackle this issue, thus allowing, for the vast majority of problem instances, to reduce the number of calls to the sat solver. the new approach is also shown to be applicable to core-guided maxsat algorithms. experimental results, obtained on a large number of problem instances, show gains when compared to state-of-the-art implementations of maxsat algorithms.",nan
2014,"a practical, integer-linear programming model for the delete-relaxation in cost-optimal planning","we propose a new integer-linear programming model for the delete relaxation in cost-optimal planning. while a naive formulation of the delete relaxation as ip is impractical, our model incorporates landmarks and relevance-based constraints, resulting in an ip that can be used to directly solve the delete relaxation. we show that our ip model outperforms the previous state-of-the-art solver for delete-free problems.we then use lp relaxation of the ip as a heuristics for a forward search planner, and show that our lp-based solver is competitive with the state-of-the-art for cost-optimal planning.",nan
2014,"combining restarts, nogoods and decompositions for solving csps","from a theoretical viewpoint, the (tree-)decomposition methods offer a good approach when the (tree)-width of constraint networks (csps) is small. in this case, they have often shown their practical interest. however, sometimes, a bad choice for the root cluster (a tree-decomposition is a tree of clusters) may drastically degrade the performance of the solving.in this paper, we highlight an explanation of this degradation and we propose a solution based on restart techniques. then, we present a new version of the btd algorithm (for backtracking with tree-decomposition [8]) integrating restart techniques. from a theoretical viewpoint, we prove that reduced nld-nogood can be safely recorded during the search and that their size is smaller than ones recorded by mac+rst+ng [9].we also show how structural (no)goods may be exploited when the search restarts from a new root cluster. finally, from a practical viewpoint, we show experimentally the benefits of using restart techniques for solving csps by decomposition methods.",nan
2014,conflict resolution in partially ordered owl dl ontologies,"inconsistency handling in owl dl ontologies is an important problem because an ontology can easily be inconsistent when it is generated or modified. current approaches to dealing with inconsistent ontologies often assume that there exists a total order over axioms and use such an order to select axioms to remove. however, in some cases, such as ontology merging, a total order may not be available and we only have a partial order over axioms. in this paper, we consider a general notion of logical inconsistency and define the notion of conflict of an inconsistent ontology. we then propose a general approach to resolving inconsistency of a partially ordered ontology. we instantiate this approach by proposing two algorithms to calculate prioritized hitting sets for a set of conflicts. we implement the algorithms and provide evaluation results on the efficiency and effectiveness by considering both artificial and real-life data sets.",nan
2014,gosu: computing goal support with commitments in multiagent systems,"goal-based agent architectures have been one of the most effective architectures for designing agents. in such architectures, the state of the agent as well as its goal set are represented explicitly. the agent then uses its set of actions to reach the goals in its goal set. however, in multiagent systems, most of the time, an agent cannot reach a goal only using its own actions but needs other agents to act as well. commitments have been successfully used to regulate those interactions between agents. this paper proposes a framework and an environment for agents to manage the relations between their commitments and goals. more specifically, we provide an algorithm called gosu to compute if a given set of commitments can be used to achieve a particular goal. we describe how gosu can be implemented using the reactive event calculus and demonstrate its capabilities over a case study.",nan
2014,learning pruning rules for heuristic search planning,"when it comes to learning control knowledge for planning, most works focus on “how to do it” knowledge which is then used to make decisions regarding which actions should be applied in which state. we pursue the opposite approach of learning “how to not do it” knowledge, used to make decisions regarding which actions should not be applied in which state. our intuition is that “bad actions” are often easier to characterize than “good” ones. an obvious application, which has not been considered by the few prior works on learning bad actions, is to use such learned knowledge as action pruning rules in heuristic search planning. fixing a canonical rule language and an off-the-shelf learning tool, we explore a novel method for generating training data, and implement rule evaluators in state-of-the-art planners. the experiments show that the learned rules can yield dramatic savings, even when the native pruning rules of these planners, i.e., preferred operators, are already switched on.",nan
2014,knowledge-based bias correction – a case study in veterinary decision support,"in collaboration with experts from veterinary research institutes throughout europe, we developed a decision-support system for the early detection of classical swine fever in pigs. for evaluating our system's diagnostic performance, practitioners and researchers collected data from the real-world field and from laboratory experiments. originating from different sources, these data could not be viewed as constituting an unbiased sample from a single probability distribution. in this paper, we present a knowledge-based method for correcting the biases in estimates from such divergent data. we demonstrate the use of our method for estimating the sensitivity and specificity characteristics of our veterinary decision-support system.",nan
2014,qualitative spatial and temporal reasoning with and/or linear programming,"this paper explores the use of generalized linear programming techniques to tackle two long-standing problems in qualitative spatio-temporal reasoning: using lp as a unifying basis for reasoning, one can jointly reason about relations from different qualitative calculi. also, concrete entities (fixed points, regions fixed in shape and/or position, etc.) can be mixed with free variables. both features are important for applications but cannot be handled by existing techniques. in this paper we discuss properties of encoding constraint problems involving spatial and temporal relations. we advocate the use of and/or graphs to facilitate efficient reasoning and we show feasibility of our approach.",nan
2014,how hard is it to compute majority-preserving judgment aggregation rules?,"several recent articles have studied judgment aggregation rules under the point of view of the normative properties they satisfy. however, a further criterion to choose between rules is their computational complexity. here we review a few rules already proposed and studied in the literature, and identify the complexity of computing the outcome.",nan
2014,the complexity of reasoning with relative directions,"whether reasoning with relative directions can be performed in np has been an open problem in qualitative spatial reasoning. efficient reasoning with relative directions is essential, for example, in rule-compliant agent navigation. in this paper, we prove that reasoning with relative directions is ∃r-complete. as a consequence, reasoning with relative directions is not in np, unless np=∃r.",nan
2014,ltlf satisfiability checking,"we consider here linear temporal logic (ltl) formulas interpreted over finite traces. we denote this logic by ltlf. the existing approach for ltlf satisfiability checking is based on a reduction to standard ltl satisfiability checking. we describe here a novel direct approach to ltlf satisfiability checking, where we take advantage of the difference in the semantics between ltl and ltlf. while ltl satisfiability checking requires finding a fair cycle in an appropriate transition system, here we need to search only for a finite trace. this enables us to introduce specialized heuristics, where we also exploit recent progress in boolean sat solving. we have implemented our approach in a prototype tool and experiments show that our approach outperforms existing approaches.",nan
2014,constrained latent dirichlet allocation for subgroup discovery with topic rules,"subgroup discovery is the task of identifying subgroups that show the most unusual statistical (distributional) characteristics with respect to a given target variable, at the intersection of predictive and descriptive induction. redundancy and lack of rule interpretability constitute the major challenges in subgroup discovery today. we address these two issues by constrained latent dirichlet allocation (lda) to identify co-occurring feature values (descriptions) for subgroup rule search, obtaining a less redundant and more diverse rule set. latent dirichlet allocation, as a topic modeling approach, is able to identify diverse topics, from which the rules can be derived. the resulting rules are less redundant and can also be interpreted by the corresponding topic. experimental results on six benchmark datasets show that the presented approach provides rule sets with better rule redundancy and diversity compared to those of four existing algorithms. one unique and interesting advantage of the proposed method is that it can categorize rules by topics as well as the assignment of a probability to each feature value of a discovered rule, which can be used in the interpretation of the results.",nan
2014,bias reformulation for one-shot function induction,"in recent years predicate invention has been underexplored as a bias reformulation mechanism within inductive logic programming due to difficulties in formulating efficient search mechanisms. however, recent papers on a new approach called meta-interpretive learning have demonstrated that both predicate invention and learning recursive predicates can be efficiently implemented for various fragments of definite clause logic using a form of abduction within a meta-interpreter. this paper explores the effect of bias reformulation produced by meta-interpretive learning on a series of program induction tasks involving string transformations. these tasks have real-world applications in the use of spreadsheet technology. the existing implementation of program induction in microsoft's flashfill (part of excel 2013) already has strong performance on this problem, and performs one-shot learning, in which a simple transformation program is generated from a single example instance and applied to the remainder of the column in a spreadsheet. however, no existing technique has been demonstrated to improve learning performance over a series of tasks in the way humans do. in this paper we show how a functional variant of the recently developed metagold system can be applied to this task. in experiments we study a regime of layered bias reformulation in which size-bounds of hypotheses are successively relaxed in each layer and learned programs re-use invented predicates from previous layers. results indicate that this approach leads to consistent speed increases in learning, more compact definitions and consistently higher predictive accuracy over successive layers. comparison to both flashfill and human performance indicates that the new system, metagoldf, has performance approaching the skill level of both an existing commercial system and that of humans on one-shot learning over the same tasks. the induced programs are relatively easily read and understood by a human programmer.",nan
2014,uncorrelated multilinear nearest feature line analysis,"in this paper, we propose a new subspace learning method, called uncorrelated multilinear nearest feature line analysis (umnfla), for the recognition of multidimensional objects, known as tensor objects. motivated by the fact that existing nearest feature line (nfl) can effectively characterize the geometrical information of limited samples, and uncorrelated features are desirable for many pattern analysis applications since they contain minimum redundancy and ensure independence of features, we propose using the nfl metric to seek a feature subspace such that the within-class feature line (fl) distances are minimized and between-class fl distances are maximized simultaneously in the reduced subspace, and impose an uncorrelated constraint to extract statistically uncorrelated features directly from tensorial data. umnfla seeks a tensor-to-vector projection (tvp) that captures most of the variation in the original tensorial input, and employs sequential iterative steps based on the alternating projection method. experimental results on the task of single trial electroencephalography (eeg) recognition suggest that umnfla is particularly effective in determining the low-dimensional projection space needed in such recognition tasks.",nan
2014,common spatial-spectral boosting pattern for brain-computer interface,"classification of multichannel electroencephalogram (eeg) recordings during motor imagination has been exploited successfully for brain-computer interfaces (bci). frequency bands and channels configuration that relate to brain activities associated with bci tasks are often pre-decided as default in eeg analysis without deliberations. however, a steady configuration usually loses effects due to individual variability across different subjects in practical applications. in this paper, we propose an adaptive boosting algorithm in a unifying theoretical framework to model the usually predetermined spatial-spectral configurations into variable preconditions, and further introduce a novel heuristic of stochastic gradient boost for training base learners under these preconditions. we evaluate the effectiveness and robustness of our proposed algorithm based on two data sets recorded from diverse populations including the healthy people and stroke patients. the results demonstrate its superior performance.",nan
2014,decidability of model checking multi-agent systems against a class of ehs specifications,we define and illustrate the expressiveness of the  fragment of the epistemic halpern–shoham logic as a specification language for multi-agent systems. we consider the model checking problem for systems against specifications given in the logic. we show its decidability by means of a novel technique that may be reused in other contexts for showing decidability of other logics based on intervals.,nan
2014,trust-based belief change,we propose a modal logic that supports reasoning about trust-based belief change. the term trust-based belief change refers to belief change that depends on the degree of trust the receiver has in the source of information.,nan
2014,the logical difference for [escr    ][lscr    ][hscr    ]r-terminologies using hypergraphs,"we propose a novel approach for detecting semantic differences between ontologies. in this paper we investigate the logical difference for [escr    ][lscr    ]-terminologies extended with role inclusions, and domain & range restrictions of roles. three types of queries are covered: concept subsumption, instance and conjunctive queries. using a hypergraph representation of such ontologies, we show that logical differences can be detected by checking for the existence of simulations between the corresponding hypergraphs. a minor adaptation of the simulation notions allows us to capture different types of queries. we also evaluate our hypergraph approach by applying a prototype implementation on large ontologies.",nan
2014,probabilistic preference logic networks,"reasoning about an entity's preferences (be it a user of an application, an individual targeted for marketing, or a group of people whose choices are of interest) has a long history in different areas of study. in this paper, we adopt the point of view that grows out of the intersection of databases and knowledge representation, where preferences are usually represented as strict partial orders over the set of tuples in a database or the consequences of a knowledge base. we introduce probabilistic preference logic networks (pplns), which flexibly combine such preferences with probabilistic uncertainty. their applications are clear in domains such as the social semantic web, where users often express preferences in an incomplete manner and through different means, many times in contradiction with each other. we show that the basic problems associated with reasoning with pplns (computing the probability of a world or a given query) are #p-hard, and then explore ways to make these computations tractable by: (i) leveraging results from order theory to obtain a polynomial-time randomized approximation scheme (fpras) under fixed-parameter assumptions; and (ii) studying a fragment of the language of pplns for which exact computations can be performed in fixed-parameter polynomial time.",nan
2014,an efficient bayesian network structure learning algorithm in the presence of deterministic relations,"faithfulness is one of the main hypotheses on which rely most bayesian network (bn) structure learning algorithms. when some random variables are deterministically determined by others, faithfulness is ruled out and classical learning algorithms fail to discover many dependences between variables, hence producing incorrect bns. even state-of-the-art algorithms dedicated to learning with deterministic variables prove to be inefficient to discover many dependences/independences. for critical applications, e.g., in nuclear safety, such failure is a serious issue. this paper introduces a new hybrid algorithm, combining a constraint-based approach with a greedy search, that includes specific rules dedicated to deterministic nodes that significantly reduce the incorrect learning. experiments show that our method significantly outperforms state-of-the-art algorithms.",nan
2014,an evolutionary spatial game-based approach for the self-regulation of social exchanges in mas,"an open problem in social simulation and mas applications is the self-regulation of social exchange processes, aiming at the achievement/maintenance of equilibrated exchanges by the agents themselves, providing the continuation of the interactions in time. this paper faces this problem through an approach based on the proposed spatial and evolutionary game of self-regulation of social exchange processes. the agents, adopting different social exchange strategies, which take into account both the short and long-term aspects of interactions, evolve such strategies by themselves in time, in order to maximize their respective strategy-based fitness functions. in consequence, the agents happen to perform more equilibrated and fair interactions, increasing the number of successful exchanges.",nan
2014,how hard is control in single-crossing elections?,"election control problems model situations where some entity (traditionally called the election chair) wants to ensure some agent's victory by either adding or deleting candidates or voters. the complexity of deciding if such control actions can be successful is well-studied for many typical voting rules and, usually, such control problems are np-complete. however, faliszewski et al. [16] have shown that many control problems become polynomial-time solvable when we consider single-peaked elections. in this paper we show that a similar phenomenon applies to the case of single-crossing elections. specifically, we consider the complexity of control by adding/deleting candidates/voters under plurality and condorcet voting. for each of these control types and each of the rules, we show that if the control type is np-complete in general, it becomes polynomial-time solvable for single-crossing elections.",nan
2014,comparing defeasible logics,"in this paper we seek to formally establish the similarities and differences between two formalizations of defeasible reasoning: the defeasible logics of nute and maier, and defeasible logics in the framework of antoniou et al. both families of logics have developed from earlier logics of nute, but their development has followed different paths and they are formulated very differently. we examine these logics from the standpoint of relative inference strength – how much the logics can infer from a given theory – and relative expressiveness – how well one logic can simulate another. we identify similarities between logics in the two families and pinpoint aspects that distinguish them.",nan
2014,information-based incentivisation when rewards are inadequate,"in many cases, intermediaries play a major role in linking between service providers and their target users. yet, attracting intermediaries at a marketplace to promote a service to their existing customers can be very challenging, since they are usually very busy and would incur additional cost as a result of such promotion. in response, this paper presents an information-based incentivisation framework, which combines financial rewards with other motivating information, in order to incentivise intermediaries at a marketplace to undertake service promotion. specifically, the intermediaries are associated with a group of incentivising agents, capable of learning the individual motivational needs of these intermediaries, and accordingly target them with the most effective incentives. the incentivising agents collaborate with each other to gather motivational information, by sharing their observations on intermediaries. the proposed incentivisation approach is evaluated through a corresponding agent-based simulation, and the experimental results obtained demonstrate its effectiveness.",nan
2014,privacy preserving landmark detection,"in many cases several entities, such as commercial companies, need to work together towards the achievement of joint goals, while hiding certain private information. multi-agent strips (ma-strips) is a new and attractive model for describing collaborative multi-agent privacy preserving planning, which is appropriate for such problems. in single agent classical planning, landmarks are key to constructing strong heuristics for state space search. in this paper we propose a method for identifying landmarks in ma-strips in a privacy preserving distributed setting. the agents collaborate to find sound landmarks without revealing their private actions or goals. in addition, we also propose a novel ma-strips planner that uses these landmarks. we empirically show that our detected landmarks improve the performance of previous approaches, and that our new planner is faster than all existing planners for multi-agent problems.",nan
2014,efficient autarkies,"autarkies are partial truth assignments that satisfy all clauses having literals in the assigned variables. autarkies provide important information in the analysis of unsatisfiable formulas. indeed, clauses satisfied by autarkies cannot be included in minimal explanations or in minimal corrections of unsatisfiability. computing the maximum autarky allows identifying all such clauses. in recent years, a number of alternative approaches have been proposed for computing a maximum autarky. this paper develops new models for representing autarkies, and proposes new algorithms for computing the maximum autarky. experimental results, obtained on a large number of problem instances, show orders of magnitude performance improvements over existing approaches, and solving instances that could not otherwise be solved.",nan
2014,some elements for a prehistory of artificial intelligence in the last four centuries,"artificial intelligence (ai) was not born ex nihilo in the mid-fifties of the xxth century. beyond its immediate roots in cybernetics and in computer science that started about two decades before, its emergence is the result of a long and slow process in the history of humanity. this can be articulated around two main questions: the formalization of reasoning and the design of machines having autonomous capabilities in terms of computation and action. the aim of this paper is to gather some insufficiently known elements about the prehistory of ai in the last 350 years that precede the official birth of ai, a time period where only a few very well-known names, such as thomas bayes and georges boole, are usually mentioned in relation with ai.",nan
2014,random forests of very fast decision trees on gpu for mining evolving big data streams,"random forest is a classical ensemble method used to improve the performance of single tree classifiers. it is able to obtain superior performance by increasing the diversity of the single classifiers. however, in the more challenging context of evolving data streams, the classifier has also to be adaptive and work under very strict constraints of space and time. furthermore, the computational load of using a large number of classifiers can make its application extremely expensive.in this work, we present a method for building random forests that use very fast decision trees for data streams on gpus. we show how this method can benefit from the massive parallel architecture of gpus, which are becoming an efficient hardware alternative to large clusters of computers. moreover, our algorithm minimizes the communication between cpu and gpu by building the trees directly inside the gpu. we run an empirical evaluation and compare our method to two well know machine learning frameworks, vfml and moa. random forests on the gpu are at least 300x faster while maintaining a similar accuracy.",nan
2014,imprecise probabilistic horn clause logic,"approaches for extending logic to deal with uncertainty immanent to many real-world problems are often on the one side purely qualitative, such as modal logics, or on the other side quantitative, such as probabilistic logics. research on combinations of qualitative and quantitative extensions to logic which put qualitative constraints on probability distributions, has mainly remained theoretical until now. in this paper, we propose a practically useful logic, which supports qualitative as well as quantitative uncertainty and can be extended with modalities with varying level of quantitative precision. this language has a solid semantic foundation based on imprecise probability theory. while in general imprecise probabilistic inference is much harder than the precise case, this is the first expressive imprecise probabilistic formalism for which probabilistic inference is shown to be as hard as corresponding precise probabilistic problems. a second contribution of this paper is an inference algorithm for this language based on the translation to a weighted model counting (wmc) problem, an approach also taken by state-of-the-art probabilistic inference methods for precise problems.",nan
2014,from analogical proportions in lattices to proportional analogies in formal concepts,"the paper provides an attempt at bridging formal concept analysis and the modeling of analogical proportions (i.e., statements of the form “a is to b as c is to d”). a suitable definition for analogical proportions in non distributive lattices is proposed and then applied to concept lattices. this enables us to compute what we call proportional analogies that establish analogies on a proportional basis between pairs (a, b) and (c, d) when a and c belong to a domain and b and d to another domain (as in “moby dick is to herman melville as alice in wonderland is to lewis carroll”).",nan
2014,landmarks in oversubscription planning,"in the basic setup of oversubscription planning (osp), the objective is to achieve an as valuable as possible subset of goals within a fixed allowance of the total action cost [32]. continuing from the recent successes in exploiting logical goal-reachability landmarks in classical planning, we develop a framework for exploiting such landmarks in heuristic-search osp. we show how standard landmarks of certain classical planning tasks can be compiled into the osp task of interest, resulting in an equivalent osp task with a lower budget, and thus with a smaller search space. we then show how such landmark-based task enrichment can be combined in a mutually stratifying way with the bfbb search used for osp planning. our empirical evaluation confirms the effectiveness of the proposed landmark-based budget reduction scheme.",nan
2014,a cluster-based approach to improve similarity-based retrieval for process-oriented case-based reasoning,"in case-based reasoning, improving the performance of the retrieval phase is still an important research issue for complex case representations and computationally expensive similarity measures. this holds particularly for the of retrieval workflows, which is a recent topic in process-oriented case-based reasoning. while most index-based retrieval methods are restricted to attribute-value representations, the application of a mac/fac retrieval approach introduces significant additional domain-specific development effort due to design the mac phase. in this paper, we present a new index-based retrieval algorithm, which is applicable beyond attribute-value representations without introducing additional domain-specific development effort. it consists of a new clustering algorithm that constructs a cluster-based index structure based on case similarity, which helps finding the most similar cases more efficiently. the approach is developed and analyzed for the retrieval of semantic workflows. it significantly improves the retrieval time compared to a linear retriever, while maintaining a high retrieval quality. further, it achieves a similar performance than the mac/fac retriever if the case base has a cluster structure, i.e., if it contains groups of similar cases.",nan
2014,a finite-valued solver for disjunctive fuzzy answer set programs,"fuzzy answer set programming (fasp) is a declarative programming paradigm which extends the flexibility and expressiveness of classical answer set programming (asp), with the aim of modeling continuous application domains. in contrast to the availability of efficient asp solvers, there have been few attempts at implementing fasp solvers. in this paper, we propose an implementation of fasp based on a reduction to classical asp. we also develop a prototype implementation of this method. to the best of our knowledge, this is the first solver for disjunctive fasp programs. moreover, we experimentally show that our solver performs well in comparison to an existing solver (under reasonable assumptions) for the more restrictive class of normal fasp programs.",nan
2014,a shapley value-based approach to determine gatekeepers in social networks with applications,"inspired by emerging applications of social networks, we introduce in this paper a new centrality measure termed gate-keeper centrality. the new centrality is based on the well-known game-theoretic concept of shapley value and, as we demonstrate, possesses unique qualities compared to the existing metrics. furthermore, we present a dedicated approximate algorithm, based on the monte carlo sampling method, to compute the gatekeeper centrality. we also consider two well known applications in social network analysis, namely community detection and limiting the spread of mis-information; and show the merit of using the proposed framework to solve these two problems in comparison with the respective benchmark algorithms.",nan
2014,the computational impact of partial votes on strategic voting,"in many real world elections, agents are not required to rank all candidates. we study three of the most common methods used to modify voting rules to deal with such partial votes. these methods modify scoring rules (like the borda count), elimination style rules (like single transferable vote) and rules based on the tournament graph (like copeland) respectively. we argue that with an elimination style voting rule like single transferable vote, partial voting does not change the situations where strategic voting is possible. however, with scoring rules and rules based on the tournament graph, partial voting can increase the situations where strategic voting is possible. as a consequence, the computational complexity of computing a strategic vote can change. for example, with borda count, the complexity of computing a strategic vote can decrease or stay the same depending on how we score partial votes.",nan
2014,how much trust is enough to trust? a market-adaptive trust threshold setting for e-marketplaces,"the inherent uncertainties of open marketplaces motivate the design of reputation systems to facilitate buyers in finding honest feedback from other buyers (advisers). defining the threshold for an acceptable level of honesty of advisers is very important, since inappropriately set thresholds would filter away possibly good advice, or the opposite – allow malicious buyers to badmouth good services. however, currently, there is no systematic approach for setting the honesty threshold. we propose a self-adaptive honesty threshold management mechanism based on pid feedback controller. experimental results show that adaptively tuning the honesty threshold to the market performance enables honest buyers to obtain higher quality of services in comparison with static threshold values defined by intuition and used in previous work.",nan
2014,pattern-based explanation for automated decisions,"explanations play an essential role in decision support and recommender systems as they are directly associated with the acceptance of those systems and the choices they make. although approaches have been proposed to explain automated decisions based on multi-attribute decision models, there is a lack of evidence that they produce the explanations users need. in response, in this paper we propose an explanation generation technique, which follows user-derived explanation patterns. it receives as input a multi-attribute decision model, which is used together with user-centric principles to make a decision to which an explanation is generated. the technique includes algorithms that select relevant attributes and produce an explanation that justifies an automated choice. an evaluation with a user study demonstrates the effectiveness of our approach.",nan
2014,cv-width: a new complexity parameter for cnfs,"we present new complexity results on the compilation of cnfs into dnnfs and obdds. in particular, we introduce a new notion of width, called cv-width, which is specific to cnfs and that dominates the treewidth of the cnf incidence graph. we then show that cnfs can be compiled into structured dnnfs in time and space that are exponential only in cv-width. not only does cv-width dominate the incidence graph treewidth, but the former width can be bounded when the latter is unbounded. we also introduce a restricted version of cv-width, called linear cv-width, and show that it dominates both pathwidth and cutwidth, which have been used to bound the complexity of obdds. we show that cnfs can be compiled into obdds in time and space that are exponential only in linear cv-width. we also show that linear cv-width can be bounded when pathwidth and cutwidth are unbounded. the new notion of width significantly improves existing upper bounds on both structured dnnfs and obdds, and is motived by a new decomposition technique that combines variable splitting with clause splitting.",nan
2014,integrating bdi agents into a matsim simulation,"matsim is a mature and powerful traffic simulator, used for large scale traffic simulations, primarily to assess likely results of various infrastructure or road network changes. more recently there has been work to extend matsim to allow its use in applications requiring what has been referred to as “within day replanning”. in the work described here we have coupled matsim with a bdi (belief desire intention) system to allow both more extensive modelling of the agent's decision making, as well as reactivity to environmental situations. the approach used allows for all agents to be “intelligent” or for some to be “intelligent”/reactive, while others operate according to plans that are static within a single day. the former is appropriate for simulations such as a bushfire evacuation, where all agents will be reacting to the changing environment. the latter is suited to introducing agents such as taxis into a standard matsim simulation, as they cannot realistically have a predetermined plan, but must constantly respond to the current situation. we have prototype applications for both bushfire evacuation and taxis. by extending the capabilities of matsim to allow agents to respond intelligently to changes in the environment, we facilitate the use of matsim in a wide range of simulation applications. the work also opens the way for matsim to be used alongside other simulation components, in a simulation integrating multiple components.",nan
2014,schedule generation schemes for job shop problems with fuzziness,"we consider the job shop scheduling problem with fuzzy durations and expected makespan minimisation. we formally define the space of semi-active and active fuzzy schedules and propose and analyse different schedule-generation schemes (sgss) in this fuzzy framework. in particular, we study dominance properties of the set of schedules obtained with each sgs. finally, a computational study illustrates the great difference between the spaces of active and the semi-active fuzzy schedules, an analogous behaviour to that of the deterministic job shop.",nan
2014,the persistence of most probable explanations in bayesian networks,"monitoring applications of bayesian networks require computing a sequence of most probable explanations for the observations from a monitored entity at consecutive time steps. such applications rapidly become impracticable, especially when computations are performed in real time. in this paper, we argue that a sequence of explanations can often be feasibly computed if consecutive time steps share large numbers of observed features. we show more specifically that we can conclude persistence of an explanation at an early stage of propagation. we present an algorithm that exploits this result to forestall unnecessary re-computation of explanations.",nan
2014,dito: a csp-based diagnostic engine,we present a new generic diagnostic engine to diagnose systems that are modelled with first order logic. the originality of this engine is that it takes benefit of recent advances in constraint programming to perform satisfiability tests by the use of an off-the-shelf csp solver. the second contribution of this paper is the definition of an incremental search strategy that deals with the intrinsic complexity of the problem to look for minimal diagnoses in complex digital systems. the use of the dito engine and its strategy is fully illustrated on the c6288 circuit that is part of the classical iscas85 benchmark.,nan
2014,quality-based rewards for monte-carlo tree search simulations,"monte-carlo tree search is a best-first search technique based on simulations to sample the state space of a decision-making problem. in games, positions are evaluated based on estimates obtained from rewards of numerous randomized play-outs. generally, rewards from play-outs are discrete values representing the outcome of the game (loss, draw, or win), e.g., r∊{−1,0,1}, which are backpropagated from expanded leaf nodes to the root node. however, a play-out may provide additional information. in this paper, we introduce new measures for assessing the a posteriori quality of a simulation. we show that altering the rewards of play-outs based on their assessed quality improves results in six distinct two-player games and in the general game playing agent cadiaplayer. we propose two specific enhancements, the relative bonus and qualitative bonus. both are used as control variates, a variance reduction method for statistical simulation. relative bonus is based on the number of moves made during a simulation and qualitative bonus relies on a domain-dependent assessment of the game's terminal state. we show that the proposed enhancements, both separate and combined, lead to significant performance increases in the domains discussed.",nan
2014,syntactic possibilistic goal generation,we propose syntactic deliberation and goal election algorithms for possibilistic agents which are able to deal with incomplete and imprecise information in a dynamic world. we show that the proposed algorithms are equivalent to their semantic counterparts already presented in the literature. we show that they lead to an efficient implementation of a possibilistic bdi model of agency which integrates goal generation.,nan
2014,multi-objective learning of hybrid classifiers,"we propose a multi-objective machine learning approach guaranteed to find the pareto optimal set of hybrid classification models consisting of comprehensible and incomprehensible submodels. the algorithm run-times are below 1 s for typical applications despite the exponential worst-case time complexity. the user chooses the model with the best comprehensibility-accuracy trade-off from the pareto front which enables a well informed decision or repeats finding new pareto fronts with modified seeds. for a classification trees as the comprehensible seed, the hybrids include single black-box model, invoked in hybrid leaves. the comprehensibility of such hybrid classifiers is measured with the proportion of examples classified by the regular leaves. we propose one simple and one computationally efficient algorithm for finding the pareto optimal hybrid trees, starting from an initial classification tree and a black-box classifier. we evaluate the proposed algorithms empirically, comparing them to the baseline solution set, showing that they often provide valuable improvements. furthermore, we show that the efficient algorithm outperforms the nsga-ii algorithm in terms of quality of the result set and efficiency (for this optimisation problem). finally we show that the algorithm returns hybrid classifiers that reflect the expert's knowledge on activity recognition problem well.",nan
2014,a resource-sensitive logic of agency,"we study a fragment of intuitionistic linear logic combined with non-normal modal operators. focusing on the minimal modal logic, we provide a gentzen-style sequent calculus as well as a semantics in terms of kripke resource models. we show that the proof theory is sound and complete with respect to the class of minimal kripke resource models. we also show that the sequent calculus allows cut elimination. we put the logical framework to use by instantiating it as a logic of agency. in particular, we apply it to reason about the resource-sensitive use of artefacts.",nan
2014,consolidation of probabilistic knowledge bases by inconsistency minimization,"consolidation describes the operation of restoring consistency in an inconsistent knowledge base. here we consider this problem in the context of probabilistic conditional logic, a language that focuses on probabilistic conditionals (if-then rules). if a knowledge base, i. e., a set of probabilistic conditionals, is inconsistent traditional model-based inference techniques are not applicable. in this paper, we develop an approach to repair such knowledge bases that relies on a generalized notion of a model of a knowledge base that extends to classically inconsistent knowledge bases. we define a generalized approach to reasoning under maximum entropy on these generalized models and use it to repair the knowledge base. this approach is founded on previous work on inconsistency measures and we show that it is well-defined, provides a unique solution, and satisfies other desirable properties.",nan
2014,on support relations in abstract argumentation as abstractions of inferential relations,"arguably the significance of an abstract model of argumentation depends on the range of realistic instantiations it allows. this paper therefore investigates for three frameworks for abstract argumentation with support relations whether they can be instantiated with the aspic+ framework for structured argumentation. both evidential argumentation systems and a simple extension of dung's abstract frameworks with support relations proposed by dung & thang (2014) are shown to allow such an instantiation. however, for bipolar argumentation frameworks a positive result is only obtained for variants with only direct and secondary attacks; counterexamples are provided for variants with supported attacks, even for the special case of deductive support.",nan
2014,symmetry breaking for exact solutions in adjustable robust optimisation,"one of the key unresolved challenges in adjustable robust optimisation is how to deal with large discrete uncertainty sets. in this paper we present a technique for handling such sets based on symmetry breaking ideas from constraint programming. in earlier work we applied the technique to a pre-disaster planning problem modelled as a two-stage stochastic program, and we were able to solve exactly instances that were previously considered intractable and only had approximate solutions. in this paper we show that the technique can also be applied to an adjustable robust formulation that scales up to larger instances than the stochastic formulation. we also describe a new fast symmetry breaking heuristic that gives improved results.",nan
2014,an agent-based model of procrastination,"procrastination is a widespread type of self-regulation failure that can have serious negative effects on people's health (e.g., because people delay or omit important health behaviors) and well-being. new “e-coaching” technologies make it possible in principle to offer tailored support to individuals in their efforts to change their self-undermining behavior. in practice, however, such automated support is currently unfeasible because the causal mechanisms behind procrastination are complex and poorly understood. this paper presents a new agent-based model of procrastination that integrates insights from economic models about the dynamics of procrastination with psychological concepts that can help explain the behavior on an individual level. the model is validated by using 5-fold cross validation with simulated annealing to fit and test the parameters on an existing dataset on academic procrastination (n=293). results show that the agent displays realistic behavior and that the model with the fitted parameters performs significantly better (p<0.01) than the model with randomly selected parameters.",nan
2014,an argumentation system for reasoning with lpm,"inconsistent knowledge-bases can entail useful conclusions when using the three-valued semantics of the paraconsistent logic lp. however, the set of conclusions entailed by a consistent knowledge-base under the three-valued semantics is smaller than set of conclusions entailed by the knowledge-base under a two-valued semantics. preferring conflict-minimal interpretations of the logic lp; i.e., lpm, reduces the gap between these two sets of conclusions.preferring conflict-minimal interpretations introduces non-monotonicity. to handle the non-monotonicity, this paper proposes an assumption-based argumentation system. assumptions needed to close branches of a semantic tableaux form the arguments. stable extensions of the set of derived arguments correspond to conflict minimal interpretations and conclusions entailed by all conflict-minimal interpretations are supported by arguments in all stable extensions.",nan
2014,provisional propagation for verifying monotonicity of bayesian networks,"many real-world bayesian networks are expected to exhibit commonly known properties of monotonicity. since monotonicity violations may be introduced despite careful engineering efforts, these properties need be verified before using a network in practice. we will show that the problem of verifying monotonicity in general has a prohibitively high computational complexity. we will argue however, that the runtime complexity involved can be substantially reduced by using a tailored algorithm which we coined provisional propagation. by means of this algorithm in fact, verifying monotonicity may become feasible for a range of real-world networks.",nan
2014,optimal planning in the presence of conditional effects: extending lm-cut with context splitting,"the lm-cut heuristic is currently the most successful heuristic in optimal strips planning but it cannot be applied in the presence of conditional effects. keyder, hoffmann and haslum recently showed that the obvious extensions to such effects ruin the nice theoretical properties of lm-cut. we propose a new method based on context splitting that preserves these properties.",nan
2014,the semantics of behavior,"the bdi architecture is one of the most popular architectures for agents with symbolic reasoning capabilities. to formally define the notions of beliefs, desires and intentions, different formal logics have been proposed in the literature. although these proposals often refer to the work of bratman [2], none, however, correctly capture the form of practical reasoning that bratman describes. what is lacking, is a proper characterization of the agent's behavior. the formal logics proposed so far, do not allow for an adequate characterization of the refinement of behaviors that bratman describes.this paper focuses on describing an agent's behavior. the proposed behavioral descriptions allow for the specification of abstract behaviors, which can subsequently be refined. the approach enables a more accurate characterization of bratman's bdi model.",nan
2014,statistical constraints,"we introduce statistical constraints, a declarative modelling tool that links statistics and constraint programming. we discuss two statistical constraints and some associated filtering algorithms. finally, we illustrate applications to standard problems encountered in statistics and to a novel inspection scheduling problem in which the aim is to find inspection plans with desirable statistical properties.",nan
2014,proactive and reactive reconfiguration for the robust execution of multi modality plans,"the paper addresses the problem of executing a plan in a dynamic environment for tasks involving constraints on consumable resources modeled as numeric fluents. in particular, the paper proposes a novel monitoring and adaptation strategy joining reactivity and proactivity in a unified framework. by exploiting the flexibility of a multi modality plan (where each action can be executed in different modalities), reactivity and proactivity are guaranteed by means of a reconfiguration step. the reconfiguration is performed (i) when the plan is no more valid to recovery from the impasse (reactively), or (ii) under the lead of a kernel based strategy to enforce the tolerance to unexpected situations (proactivity). both mechanisms have been integrated into a continual planning system and experimentally evaluated over three numeric domains, extensions of planning competition domains. results show that the approach is able to increase the percentage of cases successfully solved while preserving efficiency in most situations.",nan
2014,normalized relevance distance – a stable metric for computing semantic relatedness over reference corpora,"we propose the normalized relevance distance (nrd): a robust metric for computing semantic relatedness between terms. nrd makes use of a controlled reference corpus for a statistical analysis. the analysis is based on the relevance scores and joint occurrence of terms in documents. on the basis of established reference datasets, we demonstrate that nrd does not require sophisticated data tuning and is less dependent on the choice of the reference corpus than comparable approaches.",nan
2014,declarative spatial reasoning with boolean combinations of axis-aligned rectangular polytopes,"we present a formal framework and implementation for declarative spatial representation and reasoning about the topological relationships between boolean combinations of regions (i.e., union, intersection, difference, xor). regions of space here correspond to arbitrary axis aligned n-polytope objects, with geometric parameters either fully grounded, partially grounded, or completely unspecified. the framework is implemented in the context of clp([qscr    ][sscr    ])clp([qscr    ][sscr    ]): a declarative spatial reasoning system. www.spatial-reasoning.com, a constraint logic programming based declarative spatial reasoning system providing support for geometric and qualitative spatial abstraction and inference capabilities.we demonstrate that our method can solve packing, contact, containment, and constructive proof problems that are unsolvable using standard relational algebraic approaches for qualitative spatial reasoning (qsr). our approach is driven by general accessibility of spatial reasoning via kr languages for their application in domains such as design, geography, robotics, and cognitive vision.",nan
2014,a semantic account of iterated belief revision in the situation calculus,"recently shapiro et al. explored the notion of iterated belief revision within reiter's version of the situation calculus. in particular, they consider a notion of belief defined as truth in the most plausible situations. to specify what an agent is willing to believe at different levels of plausibility they make use of so-called belief conditionals, which themselves neither refer to situations or plausibilities explicitly. reasoning about such belief conditionals turns out to be complex because there may be too many models satisfying them and negative belief conditionals are also needed to obtain the desired conclusions. in this paper we show that, by adopting a notion of only-believing, these problems can be overcome. the work is carried out within a modal variant of the situation calculus with a possible-world semantics which features levels of plausibility. among other things, we show that only-believing a knowledge base together with belief conditionals always leads to a unique model, which allows characterizing the beliefs of an agent, after any number of revisions, in terms of entailments within the logic.",nan
2014,heterogeneous facility location without money on the line,"the study of facility location in the presence of self-interested agents has recently emerged as the benchmark problem in the research on mechanism design without money. here we study the related problem of heterogeneous 2-facility location, that features more realistic assumptions such as: (i) multiple heterogeneous facilities have to be located, (ii) agents' locations are common knowledge and (iii) agents bid for the set of facilities they are interested in. we study the approximation ratio of both deterministic and randomized truthful algorithms when the underlying network is a line. we devise an (n−1)-approximate deterministic truthful mechanism and prove a constant approximation lower bound. furthermore, we devise an optimal and truthful (in expectation) randomized algorithm.",nan
2014,sequential diagnosis of high cardinality faults in knowledge-bases by direct diagnosis generation,"sequential diagnosis methods compute a series of queries for discriminating between diagnoses. queries are answered by probing such that eventually the set of faults is identified. the computation of queries is based on the generation of a set of most probable diagnoses. however, in diagnosis problem instances where the number of minimal diagnoses and their cardinality is high, even the generation of a set of minimum cardinality diagnoses is unfeasible with the standard conflict-based approach. in this paper we propose to base sequential diagnosis on the computation of some set of minimal diagnoses using the direct diagnosis method, which requires less consistency checks to find a minimal diagnosis than the standard approach. we study the application of this direct method to high cardinality faults in knowledge-bases. in particular, our evaluation shows that the direct method results in almost the same number of queries for cases when the standard approach is applicable. however, for the cases when the standard approach is not applicable, sequential diagnosis based on the direct method is able to locate the faults correctly.",nan
2014,unleashing dec-mdps in security games: enabling effective defender teamwork,"multiagent teamwork and defender-attacker security games are two areas that are currently receiving significant attention within multiagent systems research. unfortunately, despite the need for effective teamwork among multiple defenders, little has been done to harness the teamwork research in security games. this paper is the first to remedy this situation by integrating the powerful teamwork mechanisms offered by dec-mdps into security games. we offer the following novel contributions in this paper: (i) new models of security games where a defender team's pure strategy is defined as a dec-mdp policy for addressing coordination under uncertainty; (ii) new algorithms based on column generation that enable efficient generation of mixed strategies given this new model; (iii) handling global events during defender execution for effective teamwork; (iv) exploration of the robustness of randomized pure strategies. the paper opens the door to a potentially new area combining computational game theory and multiagent teamwork.",nan
2014,on combinatorial actions and cmabs with linear side information,"online planning algorithms are typically a tool of choice for dealing with sequential decision problems in combinatorial search spaces. many such problems, however, also exhibit combinatorial actions, yet standard planning algorithms do not cope well with this type of “the curse of dimensionality”. following a recently opened line of related work on combinatorial multi-armed bandit (cmab) problems, we propose a novel cmab planning scheme, as well as two specific instances of this scheme, dedicated to exploiting what is called linear side information. using a representative strategy game as a benchmark, we show that the resulting algorithms very favorably compete with the state-of-the-art.",nan
2014,optimisation for the ride-sharing problem: a complexity-based approach,"the dial-a-ride problem is a classic challenge in transportation and continues to be relevant across a large spectrum of applications, e.g. door-to-door transportation services, patient transportation, etc. recently a new variant of the dial-a-ride problem, called ride-sharing, has received attention due to emergence of the use of smartphone-based applications that support location-aware transportation services. the general dial-a-ride problem involves complex constraints on a time-dependent network. in ride-sharing riders (resp. drivers) specify transportation requests (resp. offers) between journey origins and destinations. the two sets of participants, namely riders and drivers, have different constraints; the riders have time windows for starting and finishing the journey, while drivers have a starting time window, a destination, and a vehicle capacity. the challenge is to maximise the overall utility of the participants in the system which can be defined in a variety of ways. in this paper we study variations of the ride-sharing problem, under different notions of utility, from a computational complexity perspective, and identify a number of tractable and intractable cases. these results provide a basis for the development of efficient methods and heuristics for solving problems of real-world scale.",nan
2014,opensim: a framework for integrating agent-based models and simulation components,"the growing use of agent-based modelling and simulation for complex systems analysis has led to the availability of numerous published models. however, reuse of existing models in new simulations, for studying new problems, is largely not attempted. this is mainly because there is no systematic way of integrating agent-based models, that deals with the nuances of complex interactions and overlaps in concepts between components, in the shared environment. in this paper we present an open source framework, called opensim, that allows such integrated simulations to be built in a modular way, by linking together agent-based and other models. opensim is designed to be easy to use, and we give examples of the kinds of simulations we have built with this framework.",nan
2014,set-theoretic duality: a fundamental feature of combinatorial optimisation,"the duality between conflicts and diagnoses in the field of diagnosis, or between plans and landmarks in the field of planning, or between unsatisfiable cores and minimal co-satisfiable sets in sat or csp solving, has been known for many years. recent work in these communities (davies and bacchus, cp 2011, bonet and helmert, ecai 2010, haslum et al., icaps 2012, stern et al., aaai 2012) has brought it to the fore as a topic of current interest. the present paper lays out the set-theoretic basis of the concept, and introduces a generic implementation of an algorithm based on it. this algorithm provides a method for converting decision procedures into optimisation ones across a wide range of applications without the need to rewrite the decision procedure implementations. initial experimental validation shows good performance on a number of benchmark problems from ai planning.",nan
2014,graph abstraction for closed pattern mining in attributed networks,"we address the problem of finding patterns in an attributed graph. our approach consists in extending the standard methodology of frequent closed pattern mining to the case in which the set of objects, in which are found the pattern supports, is the set of vertices of a graph, typically representing a social network. the core idea is then to define graph abstractions as subsets of the vertices satisfying some connectivity property within the corresponding induced subgraphs. preliminary experiments illustrate the reduction in closed patterns we obtain as well as what kind of abstract knowledge is found via abstract implications rules.",nan
2014,hybrid query answering over owl ontologies,"query answering over owl 2 dl ontologies is an important reasoning task for many modern applications. unfortunately, due to its high computational complexity, owl 2 dl systems are still not able to cope with datasets containing billions of data. consequently, application developers often employ provably scalable systems which only support a fragment of owl 2 dl and which are, hence, most likely incomplete for the given input. however, this notion of completeness is too coarse since it implies that there exists some query and some dataset for which these systems would miss answers. nevertheless, there might still be a large number of user queries for which they can compute all the right answers even over owl 2 dl ontologies. in the current paper, we investigate whether, given a query [qscr    ] with only distinguished variables over an owl 2 dl ontology [tscr    ] and a system ans, it is possible to identify in an efficient way if ans is complete for [qscr    ], [tscr    ] and every dataset. we give sufficient conditions for (in)completeness and present a hybrid query answering algorithm which uses ans when it is complete, otherwise it falls back to a fully-fledged owl 2 dl reasoner. however, even in the latter case, our algorithm still exploits ans as much as possible in order to reduce the search space of the owl 2 dl reasoner. finally, we have implemented our approach using a concrete system ans and owl 2 dl reasoner obtaining encouraging results.",nan
2014,verifying the precision of diagnostic algorithms,"diagnosis of discrete event systems requires to decide whether the system model allows for certain types of executions to take place. because this problem is hard, incomplete yet faster algorithms may be needed. this however can lead to a loss of precision. this paper presents a method to decide whether precision is maintained by such incomplete algorithms. to this end we define the simulation, which is a modification of the model that simulates how the algorithm works. we then use the twin plant method to decide whether diagnosability is maintained despite the imprecision of the diagnostic algorithm. we illustrate the benefits of this approach on two diagnostic algorithms, namely independent-windows algorithms and chronicle-based diagnosis.",nan
2014,a centrality measure for networks with community structure based on a generalization of the owen value,"there is currently much interest in the problem of measuring the centrality of nodes in networks/graphs; such measures have a range of applications, from social network analysis, to chemistry and biology. in this paper we propose the first measure of node centrality that takes into account the community structure of the underlying network. our measure builds upon the recent literature on game-theoretic centralities, where solution concepts from cooperative game theory are used to reason about importance of nodes in the network. to allow for flexible modelling of community structures, we propose a generalization of the owen value—a well-known solution concept from cooperative game theory to study games with a priori-given unions of players. as a result we obtain the first measure of centrality that accounts for both the value of an individual node's relationships within the network and the quality of the community this node belongs to.",nan
2014,knowledge-based specification of robot motions,"in many cases, the success of a manipulation action performed by a robot is determined by how it is executed and by how the robot moves during the action. examples are tasks such as unscrewing a bolt, pouring liquids and flipping a pancake. this aspect is often abstracted away in ai planning and action languages that assume that an action is successful as long as all preconditions are fulfilled. in this paper we investigate how constraint-based motion representations used in robot control can be combined with a semantic knowledge base in order to let a robot reason about movements and to automatically generate executable motion descriptions that can be adapted to different robots, objects and tools.",nan
2014,quantifying the completeness of goals in bdi agent systems,"given the current set of intentions an autonomous agent may have, intention selection is the agent's decision which intention it should focus on next. often, in the presence of conflicts, the agent has to choose between multiple intentions. one factor that may play a role in this deliberation is the level of completeness of the intentions. to that end, this paper provides pragmatic but principled mechanisms for quantifying the level of completeness of goals in a bdi-style agent. our approach leverages previous work on resource and effects summarization but we go beyond by accommodating both dynamic resource summaries and goal effects, while also allowing a non-binary quantification of goal completeness. we demonstrate the computational approach on an autonomous robot case study.",nan
2014,modelling the dynamics of emotional awareness,"in this paper, based on literature from cognitive and affective neuroscience, a computational agent model is introduced incorporating the role of emotional awareness states in the dynamics of action generation. more specifically, it covers both automatic, unconscious (bottom-up) and more cognitive and conscious (top-down) emotion generation processes, and their mutual interaction. the model was formalised in a dynamical system format. in different scenarios the model shows simulation results that are in line with patterns reported in literature.",nan
2014,coherence and compatibility of markov logic networks,"markov logic is a robust approach for probabilistic relational knowledge representation that uses a log-linear model of weighted first-order formulas for probabilistic reasoning. this log-linear model always exists but may not represent the knowledge engineer's intentions adequately. in this paper, we develop a general framework for measuring this coherence of markov logic networks by comparing the resulting probabilities in the model with the weights given to the formulas. our measure takes the interdependence of different formulas into account and analyzes the degree of impact they have on the probabilities of other formulas. this approach can be used by the knowledge engineer in constructing a well-formed markov logic network if data for learning is not available. we also apply our approach to the problem of assessing the compatibility of multiple markov logic networks, i. e., to measure to what extent the merging of these networks results in a change of probabilities.",nan
2014,mixing materialization and query rewriting for existential rules,"ontology-based data access (obda) is a recent paradigm aiming at enhancing data access by taking ontological knowledge into account. when using existential rules as ontological language, query answering is an undecidable problem, whence numerous decidable classes of ontologies have been defined, ranging from classes with very good computational complexities (ac0 in data complexity) to classes with much larger expressivity. however, actually implementable algorithms have been proposed only for very restricted classes (typically those coinciding with lightweight description logics). the aim of this paper is to show how to deal with more expressive ontologies by proposing an algorithm that performs both materialization and rewriting and is applicable for a significant generalization of lightweight description logics. to this end, we first modify an existing algorithm previously proposed for a very generic class of rules, namely greedy bounded treewidth sets of rules. we then exhibit a special case, called pattern oblivious rule sets, which significantly generalizes the [escr    ][lscr    ][hscr    ]dr description logic, which underlies the owl 2 el ontology standard, while keeping the beneficial worst-case computational complexity. we last define a subclass of pattern oblivious rules that is recognizable in polynomial time.",nan
2014,computing skypattern cubes,we introduce skypattern cubes and propose an efficient bottom-up approach to compute them. our approach relies on derivation rules collecting skypatterns of a parent node from its child nodes without any dominance test. non-derivable skypatterns are computed on the fly thanks to dynamic csp. the bottom-up principle enables to provide a concise representation of the cube based on skypattern equivalence classes without any supplementary effort. experiments show the effectiveness of our proposal.,nan
2014,the peerrank method for peer assessment,"we propose the peerrank method for peer assessment. this constructs a grade for an agent based on the grades proposed by the agents evaluating the agent. since the grade of an agent is a measure of their ability to grade correctly, the peerrank method weights grades by the grades of the grading agent. the peerrank method also provides an incentive for agents to grade correctly. as the grades of an agent depend on the grades of the grading agents, and as these grades themselves depend on the grades of other agents, we define the peerrank method by a fixed point equation similar to the pagerank method for ranking web-pages. we identify some formal properties of the peerrank method (for example, it satisfies axioms of unanimity, no dummy, no discrimination and symmetry), discuss some examples, compare with related work and evaluate the performance on some synthetic data. our results show considerable promise, reducing the error in grade predictions by a factor of 2 or more in many cases over the natural baseline of averaging peer grades.",nan
2014,qualitative simulation with answer set programming,"qualitative simulation (qsim) reasons about the behaviour of dynamic physical systems as they evolve over time. the system is represented by a coarse qualitative model rather than precise numerical models. however, for large complex domains, such as robotics for urban search and rescue, existing qsim implementations are inefficient. aspqsim is a novel formulation of the qsim algorithm in answer set programming that takes advantage of the similarities between qualitative simulation and constraint satisfaction problems. aspqsim is compared against an existing qsim implementation on a variety of domains that demonstrate aspqsim provides a significant improvement in efficiency especially on complex domains, and producing simulations in domains that are not solvable by the procedural implementation.",nan
2014,preference inference based on lexicographic models,"with personalisation becoming more prevalent, it can often be useful to be able to infer additional preferences from input user preferences. preference inference techniques assume a set of possible user preference models, and derive inferences that hold in all models satisfying the inputs; the more restrictive one makes the set of possible user preference models, the more inferences one gets. sometimes it can be useful to have an adventurous form of preference inference when the input information is relatively weak, for example, in a conversational recommender system context, to give some justification for showing some options before others. this paper considers an adventurous inference based on assuming that the user preferences are lexicographic, and also an inference based on an even more restrictive preference model. we show how preference inference can be efficiently computed for these cases, based on a relatively general language of preference inputs.",nan
2014,learning domain-specific sentiment lexicon with supervised sentiment-aware lda,"analyzing and understanding people's sentiments towards different topics has become an interesting task due to the explosion of opinion-rich resources. in most sentiment analysis applications, sentiment lexicons play a crucial role, to be used as metadata of sentiment polarity. however, most previous works focus on discovering general-purpose sentiment lexicons. they cannot capture domain-specific sentiment words, or implicit and connotative sentiment words that are seemingly objective. in this paper, we propose a supervised sentiment-aware lda model (sslda). the model uses a minimal set of domain-independent seed words and document labels to discover a domain-specific lexicon, learning a lexicon much richer and adaptive to the sentiment of specific document. experiments on two publicly-available datasets (movie reviews and obama-mccain debate dataset) show that our model is effective in constructing a comprehensive and high-quality domain-specific sentiment lexicon. furthermore, the resulting lexicon significantly improves the performance of sentiment classification tasks.",nan
2014,novel architecture of a digital neuron for ffnn employing special multiplication,"this paper presents the design of a new architecture of digital neurons for use in the feed-forward neural networks (ffnn) and their subsequent implementation on a chip. the proposed neuron uses a special type of multiplication realized by and gate. comparison of usual ways of implementing digital feed-forward neural networks using fixed/floating point numbers to the novel architecture using the special multiplication was performed. consequently, the investigated ffnn architectures were implemented into fpga and asic, where the chip area was the main concern. chip area and other features of both the new neural network architecture and standard nn architectures we compared and evaluated.",nan
2014,verifying ctl* properties of golog programs over local-effect actions,"golog is a high-level action programming language for controlling autonomous agents such as mobile robots. it is defined on top of a logic-based action theory expressed in the situation calculus. before a program is deployed onto an actual robot and executed in the physical world, it is desirable, if not crucial, to verify that it meets certain requirements (typically expressed through temporal formulas) and thus indeed exhibits the desired behaviour. however, due to the high (first-order) expressiveness of the language, the corresponding verification problem is in general undecidable. in this paper, we extend earlier results to identify a large, non-trivial fragment of the formalism where verification is decidable. in particular, we consider properties expressed in a first-order variant of the branching-time temporal logic ctl*. decidability is obtained by (1) resorting to the decidable first-order fragment c2 as underlying base logic, (2) using a fragment of golog with ground actions only, and (3) requiring the action theory to only admit local effects.",nan
2014,false-name-proof combinatorial auction design via single-minded decomposition,"this paper proposes a new approach to building false-name-proof (fnp) combinatorial auctions from those that are fnp only with single-minded bidders, each of whom requires only one particular bundle. under this approach, a general bidder is decomposed into a set of single-minded bidders, and after the decomposition the price and the allocation are determined by the fnp auctions for single-minded bidders. we first show that the auctions we get with the single-minded decomposition are fnp if those for single-minded bidders satisfy a condition called pia. we then show that another condition, weaker than pia, is necessary for the decomposition to build fnp auctions. to close the gap between the two conditions, we have found another sufficient condition weaker than pia for the decomposition to produce strategy-proof mechanisms. furthermore, we demonstrate that once we have pia, the mechanisms created by the decomposition actually satisfy a stronger version of false-name-proofness, called false-name-proofness with withdrawal.",nan
2014,checking the correctness of agent designs against model-based requirements,"agent systems are used for a wide range of applications, and techniques to detect and avoid defects in such systems are valuable. in particular, it is desirable to detect issues as early as possible in the software development lifecycle. we describe a technique for checking the plan structures of a bdi agent design against the requirements models, specified in terms of scenarios and goals. this approach is applicable at design time, not requiring source code. a lightweight evaluation demonstrates that a range of defects can be found using this technique.",nan
2014,dl-lite and interval temporal logics: a marriage proposal,"description logics of the dl-lite family are widely used in knowledge representation because of their low computational complexity and rather good expressivity sufficient to capture important conceptual modelling constructs and the owl2 ql profile of the ontology web language (owl). recently, various point-based temporal extensions of dl-lite have been investigated. here, we propose to extend dl-lite with fragments of halpern and shoham's interval logic of allen's relations ([hscr    ][sscr    ]). we formally define such extensions and show how they can be successfully used in knowledge representation. in the quest for a decidable logic, we discuss the challanges in combining decidable fragments of [hscr    ][sscr    ] with dl-lite.",nan
2014,social computing in jacamo,"social computing (sc) requires agents to reason seamlessly both on their social relationships and on their goals, beliefs. we claim the need to explicitly represent the social state and social relationships as resources, available to agents. we built a framework, based on jacamo, where this vision is realized and sc is implemented through social commitments and commitment protocols.",nan
2014,analysis of interval-based possibilistic networks,this paper proposes interval-based possibilistic networks. this extension allows to compactly encode and reason with epistemic uncertainty and imprecise beliefs as well as with multiple expert knowledge. we propose a natural semantics based on compatible possibilistic networks. the paper shows finally that computing uncertainty bounds of an event can be done in interval-based networks without extra computational cost.,nan
2014,post-processing a classifier's predictions: strategies and empirical evaluation,"in this paper, we propose an approach allowing to revise the outputs of a classifier in order to take into account the available domain knowledge. this approach can be applied for any classifier be it probabilistic or not. we propose post-processing criteria and methods to encode and exploit different kinds of domain knowledge. finally, we provide experimental studies on a set of benchmarks.",nan
2014,assertional-based prioritized removed sets revision of dl-liter knowledge bases,"the paper proposes an extension of “prioritized removed sets revision” (prsr) to dl-liter stratified knowledge bases. the revision strategy is based on inconsistency minimization and consists in determining smallest subsets of assertions to be dropped from the current dl-liter knowledge base, taking the stratification into account, in order to restore consistency and accept the input. we consider different forms of input: membership assertion, positive inclusion axiom or negative inclusion axiom. we show that according to the form of input and under some conditions prsr can be achieved in polynomial time.",nan
2014,a first comparison of abstract argumentation reasoning-tools,"we compare three different implementations of reasoning tools dedicated to abstract argumentation frameworks. these systems are aspartix, conarg2, and dung-o-matic. they have been tested over three different random graph-models, corresponding to the erdös-rényi model, kleinberg small-world model, and scale-free barabasi model.",nan
2014,mining balanced sequential patterns in rts games,"the video game industry has grown enormously over the last twenty years, bringing new challenges to the artificial intelligence and data analysis communities. we tackle here the problem of automatic discovery of strategies in real-time strategy games through pattern mining. such patterns are the basic units for many tasks such as automated agent design, but also to build tools for the professionally played video games in the electronic sports scene. our formalization relies on a sequential pattern mining approach and a novel measure, the balance measure, telling how a strategy is likely to win. we experiment our methodology on a real-time strategy game that is professionally played in the electronic sport community.",nan
2014,from formal requirements on technical systems to complete designs – a holistic approach,"the design processes of todays more and more complex automation systems require computer-based support to maintain their manageability. as a base for that, the authors introduce a holistic design approach for these systems. requirements on the system to be designed are represented by an extended feature model which serves as consistent requirements model during the entire design process. a grammar-based synthesis applies formalised expert knowledge to generate solutions to these requirements. the paper's main contribution is to combine formalisms from overlapping areas of artificial intelligence and software engineering to obtain a holistic design process for industrial automation systems.",nan
2014,combining reasoning on semantic web metadata,"as the amount of available linked data expand and the number of related applications increases, the management of aspects such as provenance and access control of such data begin to become an issue. current approaches do not provide sufficient support for automatic reasoning over different metadata types and their possible interdependencies. metareasons is a framework that supports representation and automated reasoning over metadata in a single logical formalism. different types of metadata, like data-provenance and accessibility-restrictions, are represented as distinct meta-theories and dependencies between metadata types are represented by rules between different meta-theories. in this paper we present the definition of the metareasons framework and two examples meta-theories for provenance and access control. moreover, we propose a materialization calculus for forward reasoning on the two aspects.",nan
2014,using ensemble techniques and multi-objectivization to solve reinforcement learning problems,"recent work on multi-objectivization has shown how a single-objective reinforcement learning problem can be turned into a multi-objective problem with correlated objectives, by providing multiple reward shaping functions. the information contained in these correlated objectives can be exploited to solve the base, single-objective problem faster and better, given techniques specifically aimed at handling such correlated objectives. in this paper, we identify ensemble techniques as a set of methods that is suitable to solve multi-objectivized reinforcement learning problems. we empirically demonstrate their use on the pursuit domain.",nan
2014,spatial evolutionary game-theoretic perspective on agent-based complex negotiations,"the complexity of automated negotiation in a multi-issue, incomplete-information and continuous-time environment poses severe challenges, and in recent years many strategies have been proposed in response to this challenge. for the traditional evolution, strategies are studied in games assuming that “globally” negotiates with all other participates. this evaluation, however, is not suited for negotiation settings that are primarily characterized by “local” interactions among the participating agents, that is, settings in which each of possibly many participating agents negotiates only with its local neighbors rather than all other agents. a new class of negotiation games is therefore introduced that take negotiation locality (hence spatial information about the agents) into consideration. it is shown how spatial evolutionary game theory can be used to interpret bilateral negotiation results among state-of-the-art strategies.",nan
2014,predicting agents' behavior by measuring their social preferences,"there are many situations in which two or more agents (e.g., human or computer decision makers) interact with each other repeatedly in settings that can be modeled as repeated stochastic games. in such situations, each agent's performance may depend greatly on how well it can predict the other agents' preferences and behavior. for use in making such predictions, we adapt and extend the social value orientation (svo) model from social psychology, which provides a way to measure an agent's preferences for both its own payoffs and those of the other agents.the original svo model was limited to one-shot games, and assumed that each individual's behavioral preferences remain constant over time—an assumption that is inadequate for repeated-game settings, where an agent's future behavior may depend not only on its svo but also on its observations of the other agents' behavior. we extend the svo model to take this into account. our experimental evaluation, on several dozen agents that were written by students in classroom projects, show that our extended model works quite well.",nan
2014,a deductive approach to the identification and description of clusters in linked open data,"we propose an approach for inferring clusters in collections of rdf resources based on the features shared by their descriptions. the approach grounds on an algorithm for computing common subsumers in rdf proposed in a previous research work. the clustering service introduced here returns not only a possible partition of resources in a collection, but also a description of the knowledge shared within each cluster, in terms of (generalized) rdf triples.",nan
2014,adaptive active learning as a multi-armed bandit problem,"in this paper, we present a new active learning strategy whose main focus is to have the ability to adapt to the unknown (or changing) learning scenario. we introduce the learners' ensemble based approach and model it as the multi-armed bandit problem. presented application of simple exploration-exploitation trade-off algorithms from the ucb and exp3 families show an improvement over using the classical strategies. evaluation on data from uci database compare three different selection algorithms. in our tests, presented method shows promising results.",nan
2014,inconsistency resolution and global conflicts,"over the years, inconsistency management has caught the attention of researchers of different areas. inconsistency is a problem that arises in many different scenarios, for instance, ontology development or knowledge integration. in such settings, it is important to have adequate automatic tools for handling conflicts that may appear in a knowledge base. we introduce an approach to consolidation of belief bases based on a refinement of kernel contraction that accounts for the relation among kernels using clusters instead. we define cluster contraction-based consolidation operators contraction by falsum on a belief base using cluster incision functions, a refinement of kernel incision functions.",nan
2014,actions with durations and failures in bdi languages,"bdi programming languages provide a well developed route to implementing intelligent agents. however, as such agents are increasingly being used in physical environments their treatment of external actions needs to be improved. in this paper we outline a mechanism for handling actions which have durations and failures.",nan
2014,a logic of part and whole for buffered geometries,"we propose a new qualitative spatial logic for reasoning about part-whole relations between geometries (sets of points) represented in different geospatial datasets, in particular crowd-sourced datasets. since geometries in crowd-sourced data can be less inaccurate or precise, we buffer geometries by a margin of error or level of tolerance σ, and define part-whole relation for buffered geometries. the relations between geometries considered in the logic are: buffered part of (bpt), near and far. we provide a sound and complete axiomatisation of the logic with respect to metric models, and show that its satisfiability problem is np-complete.",nan
2014,learning non-cooperative behaviour for dialogue agents,"non-cooperative dialogue behaviour for artificial agents (e.g. deception and information hiding) has been identified as important in a variety of application areas, including education and health-care, but it has not yet been addressed using modern statistical approaches to dialogue agents. deception has also been argued to be a requirement for high-order intentionality in ai. we develop and evaluate a statistical dialogue agent using reinforcement learning which learns to perform non-cooperative dialogue moves in order to complete its own objectives in a stochastic trading game with imperfect information. we show that, when given the ability to perform both cooperative and non-cooperative dialogue moves, such an agent can learn to bluff and to lie so as to win more games. for example, we show that a non-cooperative dialogue agent learns to win 10.5% more games than a strong rule-based adversary, when compared to an optimised agent which cannot perform non-cooperative moves. this work is the first to show how agents can learn to use dialogue in a non-cooperative way to meet their own goals.",nan
2014,propositional merging and judgment aggregation: two compatible approaches?,there are two theories of aggregation of logical formulae: merging and judgment aggregation. in this work we investigate the relationships between these theories; one of our objectives is to point out some correspondences/discrepancies between the associated rationality properties.,nan
2014,analyzing the tradeoff between efficiency and cost of norm enforcement in stochastic environments,"in multiagent systems, agents might interfere with each other as a side-effect of their activities. one approach to coordinating these agents is to restrict their activities by means of social norms whose violation results in sanctions to violating agents. we formalize a normative system within a stochastic environment and norm enforcement follows a stochastic model in which stricter enforcement entails higher cost. within this type of system, we provide an approach to analize the tradeoff between norm enforcement efficiency and its cost considering a population of norm-aware selfish agents.",nan
2014,on computing explanations in abstract argumentation,"argumentation can be viewed as a process of generating explanations. we propose a new argumentation semantics, related admissibility, for closely capturing explanations in abstract argumentation, and distinguish between compact and verbose explanations. we show that dispute forests, composed of dispute trees, can be used to correctly compute these explanations.",nan
2014,planning with ensembles of classifiers,"learning search control for forward state planning has been previously addressed as a relational classification task, where predictions are used to generate action policies. in this paper, we describe a new bagging approach to learn and apply ensembles of relational decision trees to generate more robust policies for planning. preliminary experimental results demonstrate that new policies produce on average plans of better quality.",nan
2014,social network data analysis for event detection,"cities concentrate enough social network (sn) activity to empower rich models. we present an approach to event discovery based on the information provided by three sn, minimizing the data properties used to maximize the total amount of usable data. we build a model of the normal city behavior which we use to detect abnormal situations (events). after collecting half a year of data we show examples of the events detected and introduce some applications.",nan
2014,conditioned belief propagation revisited,"belief propagation (bp) applied to cyclic problems is a well known approximate inference scheme for probabilistic graphical models. to improve the accuracy of bp, a divide-and-conquer approach termed conditioned belief propagation (cbp) has been proposed in the literature. it recursively splits a problem by conditioning on variables, applies bp to subproblems, and merges the results to produce an answer to the original problem. in this essay, we propose a reformulated version of cbp that exhibits anytime behavior, and allows for more specific tuning by formalizing a further decision point that decides which subproblem is to be decomposed next. we propose some simple and easy to compute heuristics, and demonstrate their performance using an empirical evaluation on randomly generated problems.",nan
2014,utility-based htn planning,"we propose the use of htn planning for risk-sensitive planning domains. we suggest utility functions that reflect the risk attitude of compound tasks, and adapt a best-first search algorithm to take such utilities into account.",nan
2014,using multiple contexts to distinguish standing from sitting with a single accelerometer,"activity recognition with a single accelerometer placed on the torso is fairly common task, but distinguishing standing from sitting in this way is very difficult because the torso is oriented the same way during both activities, and the transition between the two is very hard to classify into going down or up. we propose a novel approach based on the multiple contexts ensemble (mce) algorithm which classifies the activity with an ensemble of classifiers, each of which considers the problem in the context of a single feature. the improvement stems from using multiple viewpoints, based on accelerometer data only, designed specifically to distinguish standing from sitting. this approach improves the accuracy on the two activities by 24 percentage points compared to regular machine learning.",nan
2014,enforcing solutions in constraint networks,"a method is proposed to enforce specific solutions in constraint networks. contrary to previous approaches, it yields a set of constraints to be dropped whose cardinality is minimal.",nan
2014,negotiation to execute continuous long-term tasks,"recently, research has focused on processing tasks that require continuous execution to produce data in a real-time manner. such tasks often also need to be executed for long periods of time such as years, requiring large amounts of resources (e.g. cpus) that can be found in a grid. however, a grid may be unwilling or unable to allocate resources for continuous usage far in advance, because of high fluctuations in resource availability and/or resource demand. therefore, a client must relax its requirements in terms of long-term execution, and negotiate a shorter period of execution time; when this period ends, the client must negotiate again to continue task's execution. we propose a negotiation strategy, contask, which helps to increase the periods of execution time, and reduce the length of interruptions between them.",nan
2014,off-policy shaping ensembles in reinforcement learning,in this work we propose learning an ensemble of policies related through potential-based shaping rewards via the off-policy horde framework.,nan
2014,ads2 : anytime distributed supervision of distributed systems that face unreliable or costly communication,"nowadays industrial process are mainly distributed, and their supervision systems are still centralized. consequently, when communications are disrupted, it slows down or stops the supervision process. to allow the anytime supervision of such systems, we propose a distributed approch based on a multi-agent system where each supervision agent autonomously handles both diagnosis and repair on a given location. we demonstrate the advantage of our proposal and evaluates ads2 using an industrial case-study. experiments demonstrate the relevance of our approach with an overall reduction of the supervised system down-time of 34%.",nan
2014,an algorithm for the penalized multiple choice knapsack problem,"we present an algorithm for the penalized multiple choice knapsack problem (pmckp), a combination of the more common penalized knapsack problem (pkp) and multiple choice knapsack problem (mckp). our approach is to converts a pmckp into a pkp using a previously known transformation between mckp and kp, and then solve the pkp greedily. for pmckps with well-behaved penalty functions, our algorithm is optimal for the linear relaxation of the problem.",nan
2014,generation of relevant spreadsheet repair candidates,"spreadsheets are amongst the most successful examples of end user programming. because of their, still increasing, importance for companies, spreadsheets have drastic economical and societal impact. hence, locating and fixing spreadsheet faults is important and deserves attention from the research community. a state-of-the-art technique uses genetic programming for generating repair candidates, but a limitation that hinders real-world application is that it still computes too many repair candidates. in this paper, we discuss a novel technique based on constraint solving that uses distinguishing test cases to narrow down the number of repair candidates.",nan
2014,comparing models for spreadsheet fault localization,"locating faults in spreadsheets can be difficult. therefore, tools supporting the localization of faults are needed. this paper presents a novel dependency-based model that can be used in model-based software debugging (mbsd). this model allows improvements of the diagnostic accuracy while keeping the computation times short. in an empirical evaluation, we show that dependency-based models of spreadsheets whose value-based models are often not solvable in an acceptable amount of time can be solved in less than one second. furthermore, the amount of diagnoses is reduced by 15 % on average when using the novel instead of the original dependency-based model.",nan
2014,video event recognition by dempster-shafer theory,"this paper presents an event recognition framework, based on dempster-shafer theory, that combines evidence of events from low-level computer vision analytics. the proposed method employing evidential network modelling of composite events, is able to represent uncertainty of event output from low level video analysis and infer high-level events with semantic meaning along with degrees of belief. the method has been evaluated on videos taken of subjects entering and leaving a seated area. this has relevance to a number of transport scenarios, such as onboard buses and trains, and also in train stations and airports. recognition results of 78% and 100% for four composite events are encouraging.",nan
2014,probabilistic argumentation with incomplete information,"we consider augmenting abstract argumentation frame-works with probabilistic information and discuss different constraints to obtain meaningful probabilistic information. moreover, we investigate the problem of incomplete probability assignments and propose a solution for completing these assignments by applying the principle of maximum entropy.",nan
2014,multiobjective prices of stability and anarchy for multiobjective games,"we generalize the prices of stability and anarchy to multiobjective games. in the singleobjective case, the loss of overall efficiency induced by selfish behaviors is a deeply studied subject. while the price of anarchy bounds above the loss of efficiency of equilibria, the price of stability bounds it below. in a multiobjective game, each agent individually decides an action, creating an overall alternative which is valued by each agent using his multiobjective valuation function and compared by the pareto-preference. we issue the study of multiobjective prices of stability and anarchy by discussing several possible definitions.",nan
2014,prime implicates based inconsistency characterization,"measuring inconsistency is recognized as an important issue for handling inconsistencies [5, 6]. based on prime implicates canonical representation, we first characterize the conflicting variables allowing us to refine an existing inconsistency measure. secondly, we propose a new measure, to circumscribe the internal conflicts in a knowledge base. this measure is proved to satisfy a new but weaker form of dominance.",nan
2014,from default and autoepistemic logics to disjunctive answer set programs via the logic of gk,"we show how the pure logic of gk can be embedded into disjunctive logic programming. the translation we present is polynomial, but not modular, and introduces new variables. the result can then be used to compute the extension/expansion semantics of default and autoepistemic logics using disjunctive asp solvers.",nan
2014,estimating trust from agents' interactions via commitments,"how an agent trusts another naturally depends on the outcomes of their interactions. previous approaches have treated the outcomes in a domain-specific way. we propose an approach relating trust to the domain-independent notion of commitments. we conduct an empirical study to evaluate our approach, in which subjects read emails extracted from the enron dataset (augmented with some synthetic emails for completeness), and estimate trust between each pair of communicating participants. we propose a probabilistic model for trust based on commitment outcomes and show how to train its parameters for each subject based on the subject's trust assessments. the results are promising, though imperfect. our main contribution is to launch a research program into computing trust based on a semantically well-founded account of agent interactions.",nan
2014,on the usage of behavior models to detect atm fraud,"the detection of atm fraud is a key concern for both financial institutes and bank customers but also for atm suppliers. this paper deals with the algorithmic learning of an atm's behavior model given the data stream of status information produced by standard mechatronic devices embedded in modern atms. during operation, the observed status information is compared with the learned reference model to detect abnormal behavior—assuming that a significant anomaly is a strong indicator of a fraud attempt. in contrast to previous work on automatic atm fraud detection, we apply a class of models that also capture the timing behavior, thus covering a broader range of fraud and manipulation. in particular, we present an approach to learn a tailored behavior model, called probabilistic deterministic timed-transition automaton, in order to enable the detection of time-based anomalies. we also report on preliminary results of an empirical evaluation using a real-world data set recorded on a public atm, indicating the practical applicability of our approach.",nan
2014,on the use of target sets for move selection in multi-agent debates,"in debates, agents are faced with the problem of deciding how to best contribute to the current state of the debate in order to satisfy their own goals. target sets specify minimal changes on the current state of the debate that are required to achieve such goals, where changes are the addition and/or deletion of attacks among arguments. however, agents may not have the ability to implement all the actions prescribed by a target set, nor to rely on others to help them to do so. in this short paper we provide evidence that this notion is still a useful criterion for move selection.",nan
2014,probabilistic active learning: a short proposition,"active mining of big data requires fast approaches that ideally select for a user-specified performance measure and arbitrary classifier the optimal instance for improving the classification performance. existing generic approaches are either slow, like error reduction, or heuristics, like uncertainty sampling. we propose a novel, fast yet versatile approach that directly optimises any user-specified performance measure: probabilistic active learning (pal).pal follows a smoothness assumption and models for a candidate instance both the true posterior in its neighbourhood and its label as random variables. by computing for each candidate its expected gain in classification performance over both variables, pal selects the candidate for labelling that is optimal in expectation. pal shows comparable or better classification performance than error reduction and uncertainty sampling, has the same asymptotic linear time complexity as uncertainty sampling, and is faster than error reduction.",nan
2014,an optimal iterative algorithm for extracting mucs in a black-box constraint network,"we present a non-intrusive iterative algorithm for extracting minimal unsatisfiable cores in black-box constraint networks. the problem can be generalized as the one of finding a minimal subset satisfying an upward-closed property p. if performance is measured as the number of infeasibility property checks, we show that the proposed algorithm, adel, is optimal both for small and for large mucs and that it consistently outperforms existing approaches in between those two extremal cases.",nan
2014,heuristics to increase observability in spectrum-based fault localization,"the high abstraction level of spectrum-based fault localization (sfl) reasoning, on the one hand, offers the advantage of a model-free approach to diagnosis, while, on the other, reduces the inherently limited testability of many hardware and software systems. thus, along with substantial complexity gains, sfl exhibits limited diagnostic performance, compared to model-based diagnosis. this paper describes two algorithms (lion and tiger) that exploit low cost heuristics to determine the best location to insert additional test oracles (monitors, probes, invariants) so as to increase the observability within the systems. experiments show that even simple algorithms can considerably improve sfl's diagnostic accuracy.",nan
2014,a weakening of independence in judgment aggregation: agenda separability,"one of the better studied properties for operators in judgment aggregation is independence, which essentially dictates that the collective judgment on one issue should not depend on the individual judgments given on some other issue(s) in the same agenda. independence is a desirable property for various reasons, but unfortunately it is too strong, as, together with mild additional conditions, it implies dictatorship. we propose here a weakening of independence, named agenda separability and show that this property is discriminant, i.e., some judgment aggregation rules satisfy it, others do not.",nan
2014,a decomposition approach for discovering discriminative motifs in a sequence database,this paper addresses the discovery of discriminative nary motifs in databases of labeled sequences. we consider databases made up of positive and negative sequences and define a motif as a set of patterns embedded in all positive sequences and subject to alignment constraints. we formulate constraints to eliminate redundant motifs and present a general constraint optimization framework to compute motifs that are exclusive to the positive sequences. we cast the discovery of closed and replication-free motifs in this framework and propose a two-stage approach whose last stage reduces to a minimum set covering problem. experiments on protein sequence datasets demonstrate its efficiency.,nan
2014,width-based algorithms for classical planning: new results,"we have recently shown that classical planning problems can be characterized in terms of a width measure that is bounded and small for most planning benchmark domains when goals are restricted to single atoms. two simple algorithms have been devised for exploiting this structure: iterated width (iw) for achieving atomic goals, that runs in time exponential in the problem width by performing a sequence of pruned breadth first searches, and serialized iw (siw) that uses iw in a greedy search for achieving conjunctive goals one goal at a time. while siw does not use heuristic estimators of any sort, it manages to solve more problems than a greedy bfs using a heuristic like hadd. yet, it does not approach the performance of more recent planners like lama. in this short paper, we introduce two simple extension to iw and siw that narrow the performance gap with state-of-the-art planners. the first involves changing the greedy search for achieving the goals one at a time, by a depth-first search that is able to backtrack. the second involves computing a relaxed plan once before going to the next subgoal for making the pruning in the breadth-first procedure less agressive, while keeping iw exponential in the width parameter. the empirical results are interesting as they follow from ideas that are very different from those used in current planners.",nan
2014,an intelligent threat prevention framework with heterogeneous information,"three issues usually are associated with threat prevention intelligent surveillance systems. first, the fusion and interpretation of large scale incomplete heterogeneous information; second, the demand of effectively predicting suspects' intention and ranking the potential threats posed by each suspect; third, strategies of allocating limited security resources (e.g., the dispatch of security team) to prevent a suspect's further actions towards critical assets. however, in the literature, these three issues are seldomly considered together in a sensor network based intelligent surveillance framework. to address this problem, in this paper, we propose a multi-level decision support framework for in-time reaction in intelligent surveillance. more specifically, based on a multi-criteria event modeling framework, we design a method to predict the most plausible intention of a suspect. following this, a decision support model is proposed to rank each suspect based on their threat severity and to determine resource allocation strategies. finally, formal properties are discussed to justify our framework.",nan
2014,coordinated team learning and difference rewards for distributed intrusion response,"distributed denial of service attacks constitute a rapidly evolving threat in the current internet. multiagent router throttling is a novel approach to respond to such attacks. we demonstrate that our approach can significantly scale-up using hierarchical communication and coordinated team learning. furthermore, we incorporate a form of reward shaping called difference rewards and show that the scalability of our system is significantly improved in experiments involving over 100 reinforcement learning agents. we also demonstrate that difference rewards constitute an ideal online learning mechanism for network intrusion response. we compare our proposed approach against a popular state-of-the-art router throttling technique from the network security literature, and we show that our proposed approach significantly outperforms it. we note that our approach can be useful in other related multiagent domains.",nan
2014,how hard is it to control an election by breaking ties?,"we study the computational complexity of controlling the result of an election by breaking ties strategically. this problem is equivalent to the problem of deciding the winner of an election under parallel universes tie-breaking. when the chair of the election is only asked to break ties to choose between one of the co-winners, the problem is trivially easy. however, in multi-round elections, we prove that it can be np-hard for the chair to compute how to break ties to ensure a given result. additionally, we show that the form of the tie-breaking function can increase the opportunities for control.",nan
2014,modeling gaze mechanisms for grounding in hri,"grounding is essential in human interaction and crucial for social robots collaborating with humans. gaze plays versatile roles for establishing, maintaining and repairing the common ground. it is combined with parallel modalities and involved in several processes for behavior generation and recognition. we present a uniform modeling approach focusing on the multi-modal, parallel and bidirectional aspects of gaze and their interleaving with the dialog logic.",nan
2014,faustian dynamics in sarkar's social cycle,"recently bai and lagunoff [1] have studied the question of faustian dynamics (fd) of policy and political power using a formal game theoretic framework. specifically, they studied the conflict between implementing a (personally) optimal policy and maintaining political power. however, these works assumed that the policy makers come from the same population that empowers them. in contrast, in this paper we study a society that has a political class, hence policy makers are detached from the general population. specifically, we study a society where members of the political class compete via pre-election propaganda campaigns – a competition form characteristic to modern democracies. we assume that the society is characterised by an inherent cyclical faustian dynamics, such as the sarkar cycle, and concentrate on the strategic behaviour of the political class members. we show that their propaganda over time tends to become extreme (single issue oriented). in addition, the equilibrium behaviour of the political class members precludes them from adopting a persistent agenda. rather, to optimise their political gain over time, they must lack any permanent agenda or views.",nan
2014,trustworthy advice,"we propose a novel trust model for assessing the trustworthiness of advice. we say an advice is composed of a plan to execute and a goal to be fulfilled. the expectation of an advice's outcome is calculated by assessing the probabilities of commiting to and executing the plan, and the probability of the executed plan fulfiling the intended goal. the probabilities are learned from similar past experiences.",nan
2014,momentum online lda for large-scale datasets,"modeling large-scale document collections is a significant direction in machine learning research. online lda uses stochastic gradient optimization technology to speed the convergence; however the large noise of stochastic gradients leads to slower convergence and worse performance. in this paper, we employ the momentum term to smooth out the noise of stochastic gradients, and propose an extension of online lda, namely momentum online lda (molda). we collect a large-scale corpus consisting of 2m documents to evaluate our model. experimental results indicate that molda achieves faster convergence and better performance than the state-of-the-art.",nan
2014,imitative leadsheet generation with user constraints,"we introduce the problem of generating musical leadsheets, i.e. a melody with chord labels, in the style of an arbitrary composer, that satisfy arbitrary user constraints. the problem is justified by the very nature of musical creativity, as many composers create music precisely by imitating a given style to which they add their own constraints. we propose a solution of this problem by formulating it as a markov constraint problem. markov constraints enable users to create stylistically imitative leadsheets that satisfy a large palette of constraints. we show that generated leadsheets are stylistically consistent by reclassifying them using markov classifiers.",nan
2014,(co)evolution leads towards romas,"this paper investigates the dynamics of a simple coevolutionary system. it consists of a predator-prey system in which one population maximizes its distance to the members of the other population, while the second population tries to minimize the distance to the first population. this results in a coevolutionary pursuer-evader (pe) system whose dynamics can easily be visualised and studied.next, a simple genotype-phenotype mapping is added to the system. this mapping – as well as other sources of increased selection – push the system towards regions of maximum adaptability (romas). these romas are a generalization of the concept “evolution to the edge of chaos”.",nan
2014,a new study of two divergence metrics for change detection in data streams,"streaming data are dynamic in nature with frequent changes. to detect such changes, most methods measure the difference between the data distributions in a current time window and a reference window. divergence metrics and density estimation are required to measure the difference between the data distributions. our study shows that the kullback-leibler (kl) divergence, the most popular metric for comparing distributions, fails to detect certain changes due to its asymmetric property and its dependence on the variance of the data. we thus consider two metrics for detecting changes in univariate data streams: a symmetric kl-divergence and a divergence metric measuring the intersection area of two distributions. the experimental results show that these two metrics lead to more accurate results in change detection than baseline methods such as change finder and using conventional kl-divergence.",nan
2014,finding good stochastic factored policies for factored markov decision processes,"we propose a framework for approximate resolution of mdps with factored state space, factored action space and additive reward, based on (i) considering stochastic factored policies (sfps) with a given structure, (ii) using variational approximations to estimate sfp values and (iii) using local continuous optimization algorithms to compute “good” sfps. we have implemented and tested an algorithm (ca-lbp), involving a loopy belief propagation algorithm and a coordinate ascent procedure. experiments show that ca-lbp performs as well as a state-of-the-art algorithm dedicated to a specific sub-class of fa-fmdps, and that ca-lbp can be applied to general fa-fmdps with up to 100 binary state variables and 100 binary action variables.",nan
2014,multi agent learning of relational action models,"multi agent relational action learning considers a community of agents, each rationally acting following some relational action model. the observed effect of past actions that led an agent to revise its action model can be communicated, upon request, to another agent, speeding up its own revision. we present a frame-work for such collaborative relational action model revision.",nan
2014,exploiting the semantic web for systems diagnosis,"diagnosis is the task of explaining abnormal behaviors of systems like telecommunication, transportation or energy systems. given a sequence of observations the problem is to determine, online, all faults that are in line with these observations. many approaches tackle this problem but they either require domain expertise or a formal description of how observations and faults are connected. this limits their scope to the diagnosis of well-understood faults. we address the problem of diagnosing faults that may occur for the first time and present a new diagnosis approach that integrates techniques for analyzing semantic descriptions of observations and faults.",nan
2014,stit is dangerously undecidable,"stit is a potential logical framework to capture responsibility, counterfactual emotions and norms, which are main ingredients for specifying behaviors of virtual agents. we identify here a new fragment and its satisfiability problem is np-complete and in σ3 when the number of agents is unbounded. we also identify a slightly more expressive fragment which is undecidable.",nan
2014,comparing data distribution using fading histograms,"the emergence of real temporal applications under non-stationary scenarios has drastically altered the ability to generate and gather information. nowadays, under dynamic scenarios, potentially unbounded and massive amounts of information are generated at high-speed rate, known as data streams. dealing with evolving data streams imposes the online monitoring of data in order to detect changes. the contribution of this paper is to present the advantage of using fading histograms to compare data distribution for change detection purposes. in an windowing scheme, data distributions provided by the fading histograms are compared using the kullback-leibler divergence. the experimental results support that the detection delay time is smaller when using fading histograms to represent data instead of standard histograms.",nan
2014,"introducing hierarchical adversarial search, a scalable search procedure for real-time strategy games","real-time strategy (rts) video games have proven to be a very challenging application area for artificial intelligence research. existing ai solutions are limited by vast state and action spaces and real-time constraints. most implementations efficiently tackle various tactical or strategic sub-problems, but there is no single algorithm fast enough to be successfully applied to full rts games. this paper introduces a hierarchical adversarial search framework which implements a different abstraction at each level — from deciding how to win the game at the top of the hierarchy to individual unit orders at the bottom.",nan
2014,detection and quantification of hand eczema by visible spectrum skin pattern analysis,"hand eczema is a frequent dermatosis with severe health and financial consequences to patients and society. it follows a chronic course and persists up to 15 years after onset. early detection of an exacerbation followed by the application of specific drugs for a few days can considerably reduce disease activity and avoid temporary disability. however, dermatitis patients usually rely on their own perception in assessing their skin condition and therefore often miss the time point for effective treatment. in this paper we present a prototype-based feasibility study of automated detection and quantification of hand eczema using texton-based imaging and machine-learning techniques.",nan
2014,surrogate-agent modeling for improved training,"computer-aided practice can help improve personnel training for demanding scenarios in terms of time and quality. in this paper, we concentrate on asymmetrical conflicts, such as a unit that deals with hostile crowds robbing a store, with the aim of preventing further criminal activity and at the same time minimizing physical and emotional damage. we propose a surrogate-agent modeling approach based on execution of the following loop: (i) observe a human (the unit leader) playing a set of scenarios in a simulated environment and induce strategic patterns of human play; (ii) use patterns to construct a surrogate agent (digital clone); (iii) test the surrogate under all possible circumstances through data farming; and (iv) evaluate the performance and highlight deficiencies in the agent's responses, thereby enabling human improvements in new attempts. experiments on two domains indicate that the proposed approach could significantly improve the training procedure and help trainees to properly perceive the cognitive properties of the crowds.",nan
2014,simultaneous tracking and activity recognition (star) using advanced agent-based behavioral simulations,"tracking and understanding moving pedestrian behaviors is of major concern for a growing number of applications. classical approaches either consider both problems separately or treat them simultaneously on the basis of limited contextual graphical models. in this paper, we consider tackling both problems jointly based on richer contextual information issued from agent-based behavioral simulators designed for realistically reproducing human behaviors within complex environments. we focus on the single target case and experimentally show that the proposed approach keeps good performances even in case of long periods of occlusion.",nan
2014,probabilistic two-level anomaly detection for correlated systems,we propose a novel probabilistic semi-supervised anomaly detection framework for multi-dimensional systems with high correlation among variables. our method is able to identify both abnormal instances and abnormal variables of an instance.,nan
2014,generating multi-agent plans by distributed intersection of finite state machines,"deterministic multi-agent planning described by ma-strips formalism requires mixture of coordination and synthesis of local agents' plans. all agents' plans, as sequences of actions, can be implicitly described by an appropriate generative structure. having all local plans of all participating agents described by such a structure and having a merged process of such structures, we can induce a global multi-agent plan by successive elimination of unfeasible combinations of local agents' plans.we use nondeterministic finite state machines (nfss) as the generative structure for plans and a well-known process of intersection of nfss to prune the plan spaces towards globally feasible multi-agent plan. since the numbers of the plans can be large, we use iterative process of building of the nfss using a state-of-the-art classical planner interleaved with the nfs intersecting process. we have evaluated the algorithm on an extensive number of problems from international planning competition extended for multi-agent planning.",nan
2014,agdistis – agnostic disambiguation of named entities using linked open data,"over the last decades, several billion web pages have been made available on the web. the ongoing transition from the current web of unstructured data to the data web yet requires scalable and accurate approaches for the extraction of structured data in rdf (resource description framework) from these websites. one of the key steps towards extracting rdf from text is the disambiguation of named entities. we address this issue by presenting agdistis, a novel knowledge-base-agnostic approach for named entity disambiguation. our approach combines the hypertext-induced topic search (hits) algorithm with label expansion strategies and string similarity measures. based on this combination, agdistis can efficiently detect the correct uris for a given set of named entities within an input text.",nan
2014,community detection based on a naming game,"opinion exchange in a social network can profoundly affect its structure, given that agreeing with another person bounds he/she more closely and vice versa. in this work, we show that communication interactions can form and reveal the community structure of a network, as we present a new model where agents change their links and behaviors according to the local history of communication successes. our simulations show that a local node parameter based on such history changes relatively to the node proportion of extra-community connections, and that (adaptive) edge weights tend to get high for intra-community and low for extra-community connections, also as a consequence of the history of communication successes. in non-convergent executions, the model gets trapped on a regime where clusters of agents agreeing with different words emerge, corresponding to the existing communities in the network.",nan
2014,argumentation frameworks features: an initial study,"semantics extensions are the outcome of the argumentation reasoning process: enumerating them is generally an intractable problem. for preferred semantics two efficient algorithms have been recently proposed, prefsat and scc-p, with significant runtime variations. this preliminary work aims at investigating the reasons (argumentation framework features) for such variations. remarkably, we observed that few features have a strong impact, and those exploited by the most performing algorithm are not the most relevant.",nan
2014,unsupervised semantic clustering of twitter hashtags,"social micro-blogging networks such as twitter provide an enormous amount of information, and their automated and unsupervised analysis constitutes an exciting research challenge in artificial intelligence. this work presents a novel methodology, based on a semantic clustering of the set of hashtags, which permits to obtain automatically the topics associated to a given set of tweets. a case study on the field of oncology shows how the main topics of interest are successfully discovered.",nan
2014,fast instantiation of ggp game descriptions using prolog with tabling,we present a method to instantiate game descriptions used in general game playing with a prolog interpreter using tabling. instantiation is a crucial step for speeding up the interpretation of game descriptions and increasing the playing strength of general players. our method allows us to ground almost all of the game descriptions present on the ggp servers in a time that is compatible with the common time settings of the ggp competition. it instantiates more rapidly than previous published methods.,nan
2014,emotional trends in social media – a state space approach,"in this paper, a new modeling and learning approach is presented which is based on two assumptions from the field of psychology: 1. the number of tweets mainly depends on previous dynamics of the discussion, i.e. a state-space modeling approach is used for the first time. 2. humans mainly react to emotional stimuli, i.e. tweets are automatically characterized by their emotional content. therefore, the emotions of conversations are extracted and used for system identification and parameter estimation of a state space model, which deals with events and its transitions.the proposed approach is further evaluated with an example discussion about the spanish corruption affair held on twitter during summer 2013. the experimental results show a method to model and learn the evolution of social media discussions based on emotions.",nan
2014,local image descriptor inspired by visual cortex,"the ability of visual cortex to accomplish object recognition tasks accurately and effortlessly makes it an attractive goal of computer vision to emulate the mechanism of the cortex. the neural process of object recognition in the brain follows a hierarchical scheme. in this paper, we present a novel model inspired by the visual pathway in primate brains. this multi-layer neural network model imitates the hierarchical convergent processing mechanism of the visual pathway. we show experimentally that local image features generated by this model exhibit robust discrimination and even better generalization ability compared with some existing image descriptors. we also demonstrate the application of this model to object recognition tasks. the result provides strong support for the potential of this model.",nan
2014,a concise horn theory for rcc8,"rcc8 is a well-known constraint language for expressing and reasoning about spatial knowledge. we state a simple and concise horn theory for rcc8 analogous to the ord-horn theory for temporal reasoning. this theory allows for expressing rcc8 and retains tractability of the well-known horn reduct of rcc8. further, it is much more adequate for practical purposes in the area of logic programming and surpasses previous attempts.",nan
2014,noised diffusion dynamics with individual biased opinion,"in online social network, the personal information dissemination behavior is reported to be affected by the clash of social individuals' biased opinions. in this paper, we present a model to discuss the influence of individual biased opinion on diffusion dynamics. based on multi-agent simulations, we obtain some conclusions which are helpful for recommender systems and in controlling diffusion. in addition, our study offers potential avenues for the study of diffusion dynamics with personal biases.",nan
2014,election attacks with few candidates,"we investigate the parameterized complexity of strategic behaviors in generalized scoring rules. in particular, we prove that the manipulation, control (all the 22 standard types), and bribery problems are fixed-parameter tractable for many generalized scoring rules, with respect to the number of candidates. our results imply that all these strategic problems are fixed-parameter tractable for many common voting rules, such as plurality, r-approval, borda, copeland, maximin, bucklin, ranked pairs, schulze, etc., with respect to the number of candidates.",nan
2014,dynamic taxi pricing,"taxi journeys are usually priced according to the distance covered and time taken for the trip. such a fixed cost strategy is simple to understand, but does not take into account the likelihood that a taxi can pick up additional passengers at the original passenger's destination. in this paper we investigate dynamic taxi pricing strategies. by using domain knowledge, such strategies discount trips to locations containing many potential passengers, and increase fares to those areas with few potential passengers. identifying a closed form optimal dynamic pricing strategy is difficult, and by representing the domain as an mdp, we can identify an optimal strategy for specific domains. we empirically compare such dynamic pricing strategies with fixed cost strategies, and suggest future extensions to this work.",nan
2014,semantical information graph model toward fast information valuation in large teamwork,"sharing information is critical to large teamwork for cooperative decision making in dynamic and partially observable environments. to be effective, other than building a full information coverage, agents should valuate how a potential receiver could be benefited with a piece of given information. in this paper, we propose a fast valuation model with complex network based graph modeling and analysis, which help to indicate the information importance to a given information base. similar to vague information valuation by humans, the key is that important information always significantly changes their complex information graph with its incorporation. therefore, we calculate the semantic based value of this new information in a graph model and build a local graph evaluation algorithm to estimate information graph evolution, instead of performing expensive complete graph search. although the decision may be not precise, similar to human communication, it is good enough to disseminate valuable information around the team.",nan
2014,from disjunctive to normal logic programs via unfolding and shifting,"we show that every propositional disjunctive logic program under the answer set semantics can be equivalently transformed into a normal one via unfolding and shifting. more precisely, after iteratively applying the unfolding operator for some rules in a disjunctive program, its shifted program, which is a normal program, must have the same answer sets as the original disjunctive program.",nan
2014,nicta evacuation planner: actionable evacuation plans with contraflows,"evacuations are a critical aspect of disaster management, and generally the first prevention measure to ensure the safety of the population under threat. designing evacuation plans is a complex task that requires to take into account multiple factors in order to limit congestion and ensure that all evacuees reach safety in time. this paper proposes a conflict-based path-generation algorithm for evacuation planning and a web-based intelligent system targeted at local authorities and emergency services. the key contribution of this paper is to propose the first scalable approach to produce actionable evacuation plans that simultaneously schedules the evacuation and selects contraflow roads. the benefits of the approach are illustrated on two large-scale case studies. the resulting optimization model is integrated in nicta evacuation planner, a tool to model, plan, and simulate evacuations.",nan
2014,bicycle route planning with route choice preferences,"bicycle route planning is a challenging problem because of the diverse set of factors considered by cyclists in choosing their cycling routes. we provide a solution to this problem based on a formal model expressive enough to represent transport network features and cyclists' preferences grounded in the studies of real-world bicycle route choice behaviour. our solution employs the a* algorithm together with vectors of cost and heuristic functions – able to optimise routes for travel time, comfort, quietness, and flatness. we have implemented, practically deployed and experimentally evaluated our solution in the challenging setting of the city of prague. the experiments confirmed that the planner is able to return high-quality plans in less than 250 milliseconds per query.",nan
2014,no one is left “unwatched”: fairness in observation of crowds of mobile targets in active camera surveillance,"central to the problem of active multi-camera surveillance is the fundamental issue of fairness in the observation of crowds of targets such that no target is “starved” of observation by the cameras for a long time. this paper presents a principled decision-theoretic multi-camera coordination and control (mc2) algorithm called fair-mc2 that can coordinate and control the active cameras to achieve max-min fairness in the observation of crowds of targets moving stochastically. our fair-mc2 algorithm is novel in demonstrating how (a) the uncertainty in the locations, directions, speeds, and observation times of the targets arising from the stochasticity of their motion can be modeled probabilistically, (b) the notion of fairness in observing targets can be formally realized in the domain of multi-camera surveillance for the first time by exploiting the max-min fairness metric to formalize our surveillance objective, that is, to maximize the expected minimum observation time over all targets while guaranteeing a predefined image resolution of observing them, and (c) a structural assumption in the state transition dynamics of a surveillance environment can be exploited to improve its scalability to linear time in the number of targets to be observed during surveillance. empirical evaluation through extensive simulations in realistic surveillance environments shows that fair-mc2 outperforms the state-of-the-art and baseline mc2 algorithms. we have also demonstrated the feasibility of deploying our fair-mc2 algorithm on real axis 214 ptz cameras.",nan
2014,intellireq: intelligent techniques for software requirements engineering,"requirements engineering is considered as one of the most critical phases of a software development project. low-quality requirements are a major reason for the failure of a project. consequently, techniques are needed that help to improve the support of stakeholders in the development of requirements models as well as in the process of deciding about the corresponding release plans. in this paper we introduce the intellireq requirements engineering environment. this environment is based on different recommendation approaches that support stakeholders in requirements-related activities such as definition, quality assurance, reuse, and release planning. we provide an overview of recommendation approaches integrated in intellireq and report results of empirical studies that show in which way recommenders can improve the quality of requirements engineering processes.",nan
2014,efficient policy iteration for periodic markov decision processes,"we propose a solution to a new problem that is faced by steelworks, who own private thermal power-plants and plan to use batteries to absorb fluctuations in power demand. a major challenge is in controlling both the power generation and the use of batteries under such fluctuations. we formulate a markov decision process (mdp) and design the states of the mdp so that it has a periodic structure to avoid the explosion of its state space. we then develop a policy iteration algorithm that exploits the periodic structure for computational efficiency. numerical experiments suggest that the combination of the proposed mdp and the policy iteration allows us to find a control policy that can significantly reduce the electricity cost.",nan
2014,geometrical feature extraction for cuneiforms,"cuneiform writing is one of the earliest methods of writing in human history. it is based on pressing a stylus (reed) on clay tablets, resulting in wedge marks (cuneiforms) which, when combined, provide meaningful symbols. applying modern machine learning methods to the study of ancient cuneiform tablets is a fascinating task. in the present paper we describe a method for extracting geometrical features of the wedges imprinted by the stylus that is used for writing. we introduce two independent feature extraction methods to describe the wedges. the data for this study come from precise optical scans of three tablets, originating from different historical periods. we use these tablets to demonstrate the validity of our extracted features, and to demonstrate the accuracy of classifying the different tablets.",nan
2014,influence of internal values and social networks for achieving sustainable organizations,"the low carbon at work (locaw) project has studied the everyday behavior of employees in six organizations in order to achieve a more sustainable europe. of these six, four organizations were involved in backcasting workshops to obtain future scenarios aimed at significantly improving engagement with pro-environmental behaviors by 2050. from these scenarios policies were extracted from the workshop participants that achieve this aim in their organization. agent based models (abm) were designed to model the organizations using actual information from the organization; the design also placed special emphasis on the representation of the social network. abms were then used to simulate the effects of the different policies derived from the backcasting scenarios. in this paper, the results for two organizations, udc and aquatim, are shown. these experimental results demonstrate the influence of different social networks and internal motivations of employees to determine the effectiveness of a given policy.",nan
2014,combining engineering and qualitative models to fault diagnosis in air handling units,this paper presents a methodology for model-based fault localization and identification that exploits both numerical (modelica) models and a qualitative model-based approach to diagnosis. it has been applied to diagnosis of an air handling unit based on data recorded by a building management system. the main steps from model development to diagnosis based on the recorded data are discussed.,nan
2014,stop-free strategies for traffic networks: decentralized on-line optimization,"traffic management in large networks remains an important challenge in transportation systems. the best approach would be to use existing infrastructure and find a solution to manage the increasing flows of vehicles. multi-agent systems and autonomous vehicles are today considered as a promising approach to deal with traffic control. in this paper, we propose a two-level decentralized multi-agent system which allows autonomous vehicles crossing the network intersections without stopping. at the first level, we use a control agent at each intersection which (1) lets the vehicles from each road pass alternately, and (2) allows them to optimally regulate their speed in its vicinity. at the second level, each agent coordinates with its neighboring agents in order to optimize the flows inside the network. we evaluate this approach empirically, with a comparison with a more opportunistic first-come first-served strategy. experimental results (in simulation) are presented (measuring energy consumption), showing the advantages and disadvantages of each approach.",nan
2014,an integrated reconfigurable system for maritime situational awareness,"nowadays the maritime operational picture is characterised by a growing number of entities whose interactions and activities are constantly changing. to provide timely support in this dynamic environment, automated systems need to be equipped with tools— lacking in existing systems—for real-time prioritisation of the application tasks (missions), selection and alignment of relevant information, and efficient reasoning at a situation level. in this paper, we present metis—an industrial prototype system for supporting real-time, actionable maritime situational awareness. in particular, we focus on the innovation of metis, which lies in the employment and integration of several state-of-the-art ai technologies to build the overall system's intelligence. these include reconfiguration of multi-context systems, natural language processing of heterogeneous (un)structured data and probabilistic reasoning of uncertain information. the capabilities of the system have been demonstrated in a proof of concept, which is deployed as a situational awareness plugin in the tacticos command-and-control platform of our industrial partner. the principles exploited by metis are giving valuable insights into what is considered to become the next generation of situational awareness systems.",nan
2014,stochastic filtering methods for predicting agent performance in the smart grid,"a variety of multiagent systems methods has been proposed for forming cooperatives of interconnected agents representing electricity producers or consumers in the smart grid. one major problem that arises in this domain is assessing participating agents uncertainty, and correctly predicting their future behaviour. in this paper, we adopt two stochastic filtering techniques —the unscented kalman filter equipped with gaussian processes, and the histogram filter— and use these to effectively monitor the trustworthiness of agent statements regarding their final actions. the methods are incorporated within a directly applicable scheme for providing electricity demand management services. simulation results confirm that these techniques provide tangible benefits regarding enhanced consumption reduction performance, and increased financial gains.",nan
2014,clustering weather situations with respect to prediction of solar irradiance by multiple nwp models,"with the photovoltaic (pv) and concentrating solar power (csp) forming a growing portion of european power sources, there is a strong demand for a reliable prediction of solar power. such predictions are mostly provided by physical or statistical models, both of which rely on accurate forecast of solar irradiance. for the short-to-medium-term forecast horizon (hours to days), irradiance forecast is provided mostly by numerical weather prediction (nwp) models. however, in spite of a recent effort to improve irradiance prediction within current nwp models, its quality is still not satisfactory and it is responsible for a majority of uncertainty in photovoltaic power forecasting. a promising method of improving nwp solar irradiance prediction is multi-model approach. this paper presents preliminary results from a data mining approach to combining irradiance forecasts from multiple nwp models.",nan
2014,modular behavior trees: language for fast ai in open-world video games,"as the graphical representation of computer games gradually becomes comparable to cinema, previously neglected non-graphical game aspects start to play an important role in maintaining believability of the game world. one such aspect is the behavior of non-player characters (npcs), which should appear intelligent and purposeful in the ideal case or not completely stupid at the very least. a good and fast language to express the behaviors is vital for success. we present a visual agent language inspired by behavior trees that was approved for deployment at the core of ai system for an upcoming high-budget open world game.",nan
2014,extending semantic sensor networks for automatically tackling smart building problems,sensor systems are constantly growing in all application areas and become elements of our environment. semantic sensor networks (ssn) support this development and provide standardized semantic access for reasoning on this information. unfortunately they do not model internal system knowledge or simple correlations between sensors and hence they cannot be used to automatically perform analytics tasks based on sensor data only. we show how ssn ontology can be extended and demonstrate its benefits for the task of diagnosing smart building problems using real-world data.,nan
2014,parkinsoncheck smart phone app,"the paper introduces the parkinsoncheck application. it is an app for smart phones based on spirography (spiral drawing) intended to detect signs of parkinson's disease (pd) and essential tremor (et), which is the main differential diagnosis from pd in the early stage of the disease. the app is equipped with an expert system and is the first such app to be completely automated. its intended use is twofold: (a) to act as a standalone test for general population, advising potential patients to seek medical help as early as possible, and (b) to be used by neurologists as a portable and inexpensive fully digitalised clinical decision support system. parkinsoncheck is currently freely available in slovenia on four mobile platforms as a pilot study. after potentially upgrading its expert system with new learning data, the plan is for it to be translated into english and offered worldwide.",nan
2014,condition monitoring with incomplete observations,"we introduce an approach for predicting the behaviour of a machine during a production cycle. typical data analysis methods assume that continuous behaviour is (fully) observed. this assumption is unrealistic as monitored machines are often interrupted and restarted at irregular points in time. we study the resulting problem, propose a solution and report on a use-case in wire drawing.",nan
2014,real-time adaptive problem detection in poultry,"real-time identification of unexpected values upon monitoring the production parameters of egg laying hens is quite challenging, as the collected data includes natural variability in addition to chance fluctuation. we present an adaptive method for calculating residuals that reflect the latter type of fluctuation only, and thereby provide for more accurate detection of potential problems. we report on the application of our method to real-world poultry data.",nan
2014,the piano music companion,"we present a system that we call ‘the piano music companion’ and that is able to follow and understand (at least to some extent) a live piano performance. within a few seconds this system can identify the piece that is being played, and the position within the piece. it then tracks the progress of the performer over time via a robust score following algorithm. the companion is useful in multiple ways, e.g., it can be used for piece identification, music visualisation, during piano rehearsal and for automatic page turning.",nan
2014,vmap: a visual schema mapping tool,schema mapping languages are a key technique for data engineers to perform data exchange and integration in the context of the semantic web. visual schema mapping languages provide an extra benefit as they help to visually describe the flow of data between various sources and destinations in the internet. we present a tool called vmap for implementing visual schema mappings. its focus is on standardization and modularity.,nan
2014,personalized fully multimodal journey planner,"we present an advanced journey planner designed to help travellers to take full advantage of the increasingly rich, and consequently more complex offering of mobility services available in modern cities. in contrast to existing systems, our journey planner is capable of planning with the full spectrum of mobility services; combining individual and collective, fixed-schedule as well as on-demand modes of transport, while taking into account individual user preferences and the availability of transport services. furthermore, the planner is able to personalize journey planning for each individual user by employing a recommendation engine that builds a contextual model of the user from the observation of user's past travel choices. the planner has been deployed in four large european cities and positively evaluated by hundreds of users in field trials.",nan
2014,orwellian eye: video recommendation with microsoft kinect,"this paper demonstrates interest beat (inbeat.eu) as a recommender system for online videos, which determines user interest in the content based on gaze tracking with microsoft kinect in addition to explicit user feedback. content of the videos is represented using a semantic wikifier. user profile is constructed from preference rules, which are discovered with an association rule learner.",nan
2014,advanced public transport network analyser,"we present a web-based tool for a fine-grained analysis of the quality of public transport coverage. employing an efficient graph-based transport network representation and a fast, modified dijkstra-based journey planning algorithm, the tool calculates four public transport accessibility indices: journey duration, service frequency, the number of transfers, and a combined, overall index. together, the indices give an accurate picture of the user-perceived accessibility by public transport in the area and time of interest.",nan
2014,"mobile tourist guide: bridging the gap between recommending, planning and user-centered interaction","we present a mobile tourist guide for planning and conducting sightseeing day trips. the system combines a hybrid recommender system for sights, events and other points of interest with a tour planner for time-constrained activities taking additional constraints for public transport connections into account. a novelty of the implemented approach compared to existing solutions for tourists is that the user retains full control over the tour by being able to directly edit any detail at any time, even if there are existing constraints that hinder the direct execution of an edit operation. moreover, recommender and planner are closely interconnected by regarding the reachability of recommended items with respect to the current selection as well as filling unavoidable gaps with fitting recommendations. the application is tailored to the city of nuremberg, germany, but can be extended by additional data for other destinations as well.",nan
2016,when do rule changes count-as legal rule changes?,"institutions regulate societies. comprising searle's constitutive counts-as rules, “a counts-as b in context c”, an institution ascribes from brute and institutional facts (as), a social reality comprising institutional facts (bs) conditional on the social reality (contexts cs). when brute facts change an institution evolves from one social reality to the next. rule changes are also regulated by rule-modifying counts-as rules ascribing rule change in the past/present/future (e.g. a majority rule change vote counts-as a rule change). determining rule change legality is difficult, since changing counts-as rules both alters and is conditional on the social reality, and in some cases hypothetical rule-change effects (e.g. not retroactively criminalising people). however, without a rigorous account of rule change ascriptions, ai agents cannot support humans in understanding the laws imposed on them. moreover, advances in automated governance design for socio-technical systems, are limited by agents' ability to understand how and when to enact institutional changes. consequently, we answer “when do rule changes count-as legal rule changes?” in a temporal setting with a novel formal framework.",nan
2016,constant time expected similarity estimation for large-scale anomaly detection,"a new algorithm named expected similarity estimation (expose) was recently proposed to solve the problem of large-scale anomaly detection. it is a non-parametric and distribution free kernel method based on the hilbert space embedding of probability measures. given a dataset of n samples, expose takes [oscr    ](n) time to build a model and [oscr    ](1) time per prediction.in this work we describe and analyze a simple and effective stochastic optimization algorithm which allows us to drastically reduce the learning time of expose from previous linear to constant. it is crucial that this approach allows us to determine the number of iterations based on a desired accuracy, independent of the dataset size n. we will show that the proposed stochastic gradient descent algorithm works in general possible infinite-dimensional hilbert spaces, is easy to implement and requires no additional step-size parameters.",nan
2016,auc maximization in bayesian hierarchical models,"the area under the curve (auc) measures such as the area under the receiver operating characteristics curve (auroc) and the area under the precision-recall curve (aupr) are known to be more appropriate than the error rate, especially, for imbalanced data sets. there are several algorithms to optimize auc measures instead of minimizing the error rate. however, this idea has not been fully exploited in bayesian hierarchical models owing to the difficulties in inference. here, we formulate a general bayesian inference framework, called bayesian auc maximization (bam), to integrate auc maximization into bayesian hierarchical models by borrowing the pairwise and listwise ranking ideas from the information retrieval literature. to showcase our bam framework, we develop two bayesian linear classifier variants for two ranking approaches and derive their variational inference procedures. we perform validation experiments on four biomedical data sets to demonstrate the better predictive performance of our framework over its error-minimizing counterpart in terms of average auroc and aupr values.",nan
2016,validating cross-perspective topic modeling for extracting political parties' positions from parliamentary proceedings,"in the literature, different topic models have been introduced that target the task of viewpoint extraction. because, generally, these studies do not present thorough validations of the models they introduce, it is not clear in advance which topic modeling technique will work best for our use case of extracting viewpoints of political parties from parliamentary proceedings. we argue that the usefulness of methods like topic modeling depend on whether they yield valid and reliable results on real world data. this means that there is a need for validation studies. in this paper, we present such a study for an existing topic model for viewpoint extraction called cross-perspective topic modeling [11]. the model is applied to dutch parliamentary proceedings, and the resulting topics and opinions are validated using external data. the results of our validation show that the model yields valid topics (content and criterion validity), and opinions with content validity. we conclude that cross-perspective topic modeling is a promising technique for extracting political parties' positions from parliamentary proceedings. second, by exploring a number of validation methods, we demonstrate that validating topic models is feasible, even without extensive domain knowledge.",nan
2016,finite unary relations and qualitative constraint satisfaction,"extending qualitative csps with the ability of restricting selected variables to finite sets of possible values has been proposed as an important research direction with important applications. complexity results for this kind of formalisms have appeared in the literature but they focus on concrete examples and not on general principles. we propose three general methods. the first two methods are based on analysing the given csp from a model-theoretical perspective, while the third method is based on directly analysing the growth of the representation of solutions. we exemplify our methods on temporal and spatial formalisms including allen's algebra and rcc5.",nan
2016,dynamic choice of state abstraction in q-learning,"q-learning associates states and actions of a markov decision process to expected future reward through online learning. in practice, however, when the state space is large and experience is still limited, the algorithm will not find a match between current state and experience unless some details describing states are ignored. on the other hand, reducing state information affects long term performance because decisions will need to be made on less informative inputs. we propose a variation of q-learning that gradually enriches state descriptions, after enough experience is accumulated. this is coupled with an ad-hoc exploration strategy that aims at collecting key information that allows the algorithm to enrich state descriptions earlier. experimental results obtained by applying our algorithm to the arcade game pac-man show that our approach significantly outperforms q-learning during the learning process while not penalizing long-term performance.",nan
2016,a dialectical proof theory for universal acceptance in coherent logic-based argumentation frameworks,"given a logic-based argumentation framework built over a knowledge base in a logical language and a query in that language, the query is universally accepted if it is entailed from all extensions. as shown in [2, 14], universal acceptance is different from skeptical acceptance as a query may be entailed from different arguments distributed over all extensions but not necessarily skeptical ones. in this paper we provide a dialectical proof theory for universal acceptance in coherent logic-based argumentation frameworks. we prove its finiteness, soundness, completeness, consistency and study its dispute complexity. we give an exact characterization for non-universal acceptance and provide an upper-bound for universal acceptance.",nan
2016,adaptive binary quantization for fast nearest neighbor search,"hashing has been proved an attractive technique for fast nearest neighbor search over big data. compared to the projection based hashing methods, prototype based ones own stronger capability of generating discriminative binary codes for the data with complex inherent structure. however, our observation indicates that they still suffer from the insufficient coding that usually utilizes the complete binary codes in a hypercube. to address this problem, we propose an adaptive binary quantization method that learns a discriminative hash function with prototypes correspondingly associated with small unique binary codes. our alternating optimization adaptively discovers the prototype set and the code set of a varying size in an efficient way, which together robustly approximate the data relations. our method can be naturally generalized to the product space for long hash codes. we believe that our idea serves as a very helpful insight to hashing research. the extensive experiments on four large-scale (up to 80 million) datasets demonstrate that our method significantly outperforms state-of-the-art hashing methods, with up to 58.84% performance gains relatively.",nan
2016,exploring parallel tractability of ontology materialization,"materialization is an important reasoning service for applications built on the web ontology language (owl). to make materialization efficient in practice, current research focuses on deciding tractability of an ontology language and designing parallel reasoning algorithms. however, some well-known large-scale ontologies, such as yago, have been shown to have good performance for parallel reasoning, but they are expressed in ontology languages that are not parallelly tractable, i.e., the reasoning is inherently sequential in the worst case. this motivates us to study the problem of parallel tractability of ontology materialization from a theoretical perspective. that is, we aim to identify the ontologies for which materialization is parallelly tractable, i.e., in nc complexity. in this work, we focus on datalog rewritable ontology languages. we identify several classes of datalog rewritable ontologies (called parallelly tractable classes) such that materialization over them is parallelly tractable. we further investigate the parallel tractability of materialization of a datalog rewritable owl fragment dhl (description horn logic) and an extension of dhl that allows complex role inclusion axioms. based on the above results, we analyze real-world datasets and show that many ontologies expressed in dhl or its extension belong to the parallelly tractable classes.",nan
2016,student-t process regression with dependent student-t noise,"gaussian process regression (gpr) is a powerful non-parametric method. however, gpr may perform poorly if the data are contaminated by outliers. to address the issue, we replace the gaussian process with a student-t process and introduce dependent student-t noise in this paper, leading to a student-t process regression with dependent student-t noise model (tprd). closed form expressions for the marginal likelihood and predictive distribution of tprd are derived. besides, tprd gives a probabilistic interpretation to the student-t process regression with the noise incorporated into its kernel (tprk), which is a common approach for the student-t process regression. moreover, we analyze the influence of different kernels. if the kernel meets a condition, called β-property here, the maximum marginal likelihood estimation of tprd's hyperparameters is independent of the degrees of freedom ν of the student-t process, which implies that gpr, tprd and tprk have exactly the same predictive mean. empirically, the degrees of freedom ν could be regarded as a convergence accelerator, indicating that tprd with a suitable ν performs faster than gpr. if the kernel does not have the β-property, tprd has better performances than gpr, without additional computational cost. on benchmark datasets, the proposed results are verified.",nan
2016,an extension of the owen-value interaction index and its application to inter-links prediction,"link prediction is a key problem in social network analysis: it involves making suggestions about where to add new links in a network, based solely on the structure of the network. we address a special case of this problem, whereby the new links are supposed to connect different communities in the network; we call it the interlinks prediction problem. this is particularly challenging as there are typically very few links between different communities. to solve this problem, we propose a local node-similarity measure, inspired by the owen-value interaction index—a concept developed in cooperative game theory and fuzzy systems. although this index requires an exponential number of operations in the general case, we show that our local node-similarity measure is computable in polynomial time. we apply our measure to solve the inter-links prediction problem in a number of real-life networks, and show that it outperforms all other local similarity measures in the literature.",nan
2016,cluster-driven model for improved word and text embedding,"most of the existing word embedding models only consider the relationships between words and their local contexts (e.g. ten words around the target word). however, information beyond local contexts (global contexts), which reflect the rich semantic meanings of words, are usually ignored. in this paper, we present a general framework for utilizing global information to learn word and text representations. our models can be easily integrated into existing local word embedding models, and thus introduces global information of varying degrees according to different downstream tasks. moreover, we view our models in the co-occurrence matrix perspective, based on which a novel weighted term-document matrix is factorized to generate text representations. we conduct a range of experiments to evaluate word and text representations learned by our models. experimental results show that our models outperform or compete with state-of-the-art models. source code of the paper is available at https://github.com/zhezhaoa/cluster-driven.",nan
2016,learning temporal context for activity recognition,"we investigate how incremental learning of long-term human activity patterns improves the accuracy of activity classification over time. rather than trying to improve the classification methods themselves, we assume that they can take into account prior probabilities of activities occurring at a particular time. we use the classification results to build temporal models that can provide these priors to the classifiers. as our system gradually learns about typical patterns of human activities, the accuracy of activity classification improves, which results in even more accurate priors. two datasets collected over several months containing hand-annotated activity in residential and office environments were chosen to evaluate the approach. several types of temporal models were evaluated for each of these datasets. the results indicate that incremental learning of daily routines leads to a significant improvement in activity classification.",nan
2016,"leader-follower mdp models with factored state space and many followers – followers abstraction, structured dynamics and state aggregation","the leader-follower markov decision processes (lf-mdp) framework extends both markov decision processes (mdp) and stochastic games. it provides a model where an agent (the leader) can influence a set of other agents (the followers) which are playing a stochastic game, by modifying their immediate reward functions, but not their dynamics. it is assumed that all agents act selfishly and try to optimize their own long-term expected reward. finding equilibrium strategies in a lf-mdp is hard, especially when the joint state space of followers is factored. in this case, it takes exponential time in the number of followers. our theoretical contribution is threefold. first, we analyze a natural assumption (substitutability of followers), which holds in many applications. under this assumption, we show that a lf-mdp can be solved exactly in polynomial time, when deterministic equilibria exist for all games encountered in the lf-mdp. second, we show that an additional assumption of sparsity of the problem dynamics allows us to decrease the exponent of the polynomial. finally, we present a state-aggregation approximation, which decreases further the exponent and allows us to approximately solve large problems. we empirically validate the lf-mdp approach on a class of realistic animal disease control problems. for problems of this class, we find deterministic equilibria for all games. using our first two results, we are able to solve the exact lf-mdp problem with 15 followers (compared to 6 or 7 in the original model). using state-aggregation, problems with up to 50 followers can be solved approximately. the approximation quality is evaluated by comparison with the exact approach on problems with 12 and 15 followers.",nan
2016,cube: a cuda approach for bucket elimination on gpus,"we consider bucket elimination (be), a popular algorithmic framework to solve constraint optimisation problems (cops). we focus on the parallelisation of the most computationally intensive operations of be, i.e., join sum and maximisation, which are key ingredients in several close variants of the be framework (including belief propagation on junction trees and distributed cop techniques such as actiongdl and dpop). in particular, we propose cube, a highly-parallel gpu implementation of such operations, which adopts an efficient memory layout allowing all threads to independently locate their input and output addresses in memory, hence achieving a high computational throughput. we compare cube with the most recent gpu implementation of be. our results show that cube achieves significant speed-ups (up to two orders of magnitude) w.r.t. the counterpart approach, showing a dramatic decrease of the runtime w.r.t. the serial version (i.e., up to 652× faster). more important, such speed-ups increase when the complexity of the problem grows, showing that cube correctly exploits the additional degree of parallelism inherent in the problem.",nan
2016,managing energy markets in future smart grids using bilateral contracts,"future smart grids will empower home owners to buy energy from real-time markets, coalesce into energy cooperatives, and sell energy they generate from their local renewable energy sources. such interactions by large numbers of small prosumers (that both consume and produce) will engender potentially unpredictable fluctuations in energy prices which could be detrimental to all actors in the system. hence, in this paper, we propose negotiation mechanisms to orchestrate such interactions as well as pricing mechanisms to help stabilise energy prices on multiple time scales. we then prove 1) that our solution guarantees that, while prices fluctuations can be constrained, 2) that it is individually rational for agents to join energy cooperatives and 3) that the negotiation mechanisms we employ result in pareto-optimal solutions.",nan
2016,a rational account of classical logic argumentation for real-world agents,"classical logic based argumentation (clar) characterises single agent non-monotonic reasoning and enables distributed non-monotonic reasoning amongst agents in dialogues. however, features of clar that have been shown sufficient to ensure satisfaction of rationality postulates, preclude their use by resource bounded agents reasoning individually, or dialectically in real-world dialogue. this paper provides a new formalisation of clar that is both suitable for such uses and satisfies the rationality postulates. we illustrate by providing a rational dialectical characterisation of brewka's non-monotonic preferred subtheories defined under the assumption of restricted inferential capabilities.",nan
2016,two dimensional uncertainty in persuadee modelling in argumentation,"when attempting to persuade an agent to believe (or disbelieve) an argument, it can be advantageous for the persuader to have a model of the persuadee. models have been proposed for taking account of what arguments the persuadee believes and these can be used in a strategy for persuasion. however, there can be uncertainty as to the accuracy of such models. to address this issue, this paper introduces a two-dimensional model that accounts for the uncertainty of belief by a persuadee and for the confidence in that uncertainty evaluation. this gives a better modeling for using lotteries so that the outcomes involve statements about what the user believes/disbelieves, and the confidence value is the degree to which the user does indeed hold those outcomes (and this is a more refined and more natural modeling than found in [19]). this framework is also extended with a modelling of the risk of disengagement by the persuadee.",nan
2016,"a data driven similarity measure and example mapping function for general, unlabelled data sets","deep networks such as autoencoders and deep belief nets are able to construct alternative, and often informative, representations of unlabeled data by searching for (hidden) structure and correlations between the features chosen to represent the data and combining them into new features that allow sparse representations of the data. these representations have been chosen to often increase the accuracy of further classification or regression accuracy when compared to the original, often human chosen representations. in this work, we attempt an investigation of the relation between such discovered representations found using related but differently represented sets of examples. to this end, we combine the cross-domain comparison capabilities of unsupervised manifold alignment with the unsupervised feature construction of deep belief nets, resulting in an example mapping function that allows re-encoding examples from any source to any target task. using the t-distributed stochastic neighbour embedding technique to map translated and real examples to a lower dimensional space, we employ kl-divergence to define a dissimilarity measure between data sets enabling us to measure found representation similarities between domains.",nan
2016,on stochastic primal-dual hybrid gradient approach for compositely regularized minimization,"we consider a wide spectrum of regularized stochastic minimization problems, where the regularization term is composite with a linear function. examples of this formulation include graph-guided regularized minimization, generalized lasso and a class of ℓ1 regularized problems. the computational challenge is that the closed-form solution of the proximal mapping associated with the regularization term is not available due to the imposed linear composition. fortunately, the structure of the regularization term allows us to reformulate it as a new convex-concave saddle point problem which can be solved using the primal-dual hybrid gradient (pdhg) approach. however, this approach may be inefficient in realistic applications as computing the full gradient of the expected objective function could be very expensive when the number of input data samples is considerably large. to address this issue, we propose a stochastic pdhg (spdhg) algorithm with either uniformly or non-uniformly averaged iterates. through uniformly averaged iterates, the spdhg algorithm converges in expectation with  rate for general convex objectives and o(log (t)/t) rate for strongly convex objectives, respectively. while with non-uniformly averaged iterates, the spdhg algorithm is expected to converge with o(1/t) rate for strongly convex objectives. numerical experiments on different genres of datasets demonstrate that our proposed algorithm outperforms other competing algorithms.",nan
2016,decentralized large-scale electricity consumption shifting by prosumer cooperatives,"in this work we address the problem of coordinated consumption shifting for electricity prosumers. we show that individual optimization with respect to electricity prices does not always lead to minimized costs, thus necessitating a cooperative approach. a prosumer cooperative employs an internal cryptocurrency mechanism for coordinating members decisions and distributing the collectively generated profits. the mechanism generates cryptocoins in a distributed fashion, and awards them to participants according to various criteria, such as contribution impact and accuracy between stated and final shifting actions. in particular, when a scoring rules-based distribution method is employed, participants are incentivized to be accurate. when tested on a large dataset with real-world production and consumption data, our approach is shown to provide incentives for accurate statements and increased economic profits for the cooperative.",nan
2016,analysing approximability and heuristics in planning using the exponential-time hypothesis,"cost-optimal planning has become a very well-studied topic within planning. needless to say, cost-optimal planning has proven to be computationally hard both theoretically and in practice. since cost-optimal planning is an optimisation problem, it is natural to analyse it from an approximation point of view. even though such studies may be valuable in themselves, additional motivation is provided by the fact that there is a very close link between approximability and the performance of heuristics used in heuristic search. the aim of this paper is to analyse approximability (and indirectly the performance of heuristics) with respect to lower time bounds. that is, we are not content by merely classifying problems into complexity classes — we also study their time complexity. this is achieved by replacing standard complexity-theoretic assumptions (such as p ≠ np) with the exponential time hypothesis (eth). this enables us to analyse, for instance, the performance of the h+ heuristic and obtain general trade-off results that correlate approximability bounds with bounds on time complexity.",nan
2016,a simple account of multi-agent epistemic planning,"a realistic model of multi-agent planning must allow us to formalize notions which are absent in classical planning, such as communication and knowledge. we investigate multi-agent planning based on a simple logic of knowledge that is grounded on the visibility of propositional variables. using such a formal logic allows us to prove the existence of a plan given the description of the individual actions. we present an encoding of multi-agent planning problems expressed in this logic into the standard planning language pddl. the solvability of a planning task is reduced to a model checking problem in a dynamic extension of our logic, proving its complexity. feeding the resulting problem into a pddl planner provides a provably correct plan for the original multi-agent planning problem. we apply our method on several examples such as the gossip problem.",nan
2016,lexicographic refinements in possibilistic decision trees,"possibilistic decision theory has been proposed twenty years ago and has had several extensions since then. because of the lack of decision power of possibilistic decision theory, several refinements have then been proposed. unfortunately, these refinements do not allow to circumvent the difficulty when the decision problem is sequential. in this article, we propose to extend lexicographic refinements to possibilistic decision trees. we show, in particular, that they still benefit from an expected utility (eu) grounding. we also provide qualitative dynamic programming algorithms to compute lexicographic optimal strategies. the paper is completed with an experimental study that shows the feasibility and the interest of the approach.",nan
2016,"even angels need the rules: ai, roboethics, and the law","over the past years, scholars have increasingly debated over the reasons why we should, or should not, deploy specimens of ai technology, such as robots, on the battlefields, in the market, or at our homes. amongst the moral theories that discuss what is right, or what is wrong, about a robot's behaviour, virtue ethics, rather than utilitarianism and deontologism, offers a fruitful approach to the debate. the context sensitivity and bottom-up methodology of virtue ethics fits like hand to glove with the unpredictability of robotic behaviour, for it involves a trial-and-error learning of what makes the behaviour of that robot good, or bad. however, even advocates of virtue ethics admit the limits of their approach: all in all, the more societies become complex, the less shared virtues are effective, the more we need rules on rights and duties. by reversing the kantian idea that a nation of devils can establish a state of good citizens, if they “have understanding,” we can say that even a nation of angels would need the law in order to further their coordination and collaboration. accordingly, the aim of this paper is not only to show that a set of perfect moral agents, namely a bunch of angelic robots, need rules. also, no single moral theory can instruct us as to how to legally bind our artificial agents through ai research and robotic programming.",nan
2016,one-class to multi-class model update using the class-incremental optimum-path forest classifier,"incremental learning capabilities of classifiers is a relevant topic, specially when dealing with scenarios such as data stream mining, concept drift and active learning. we investigate the capabilities of an incremental version of the optimum-path forest classifier (opf-ci) in the context of learning new classes and compare its behavior against support vector machines and k nearest neighbours classifiers. the opf-ci classifier is a parameter-free, graph-based approach to incremental training that runs in linear time with respect to the number of instances. our results show opf-ci keeps the running time low while producing an accuracy behavior similar to the other classifiers for increments of instances. also, it is robust to variations on the order of the learned classes, demonstrating the applicability of the method.",nan
2016,more than a name? on implications of preconditions and effects of compound htn planning tasks,"there are several formalizations for hierarchical planning. many of them allow to specify preconditions and effects for compound tasks. they can be used, e.g., to assist during the modeling process by ensuring that the decomposition methods' plans “implement” the compound tasks' intended meaning. this is done based on so-called legality criteria that relate these preconditions and effects to the method's plans and pose further restrictions. despite the variety of expressive hierarchical planning formalisms, most theoretical investigations are only known for standard htn planning, where compound tasks are just names, i.e., no preconditions or effects can be specified. thus, up to know, a direct comparison to other hierarchical planning formalisms is hardly possible and fundamental theoretical properties are yet unknown. to enable a better comparison between such formalisms (in particular with respect to their computational expressivity), we first provide a survey on the different legality criteria known from the literature. then, we investigate the theoretical impact of these criteria for two fundamental problems to planning: plan verification and plan existence. we prove that the plan verification problem is at most np-complete, while the plan existence problem is in the general case both semi-decidable and undecidable, independent of the demanded criteria. finally, we discuss our theoretical findings and practical implications.",nan
2016,a probabilistic logic programming approach to automatic video montage,"hiring a professional camera crew to cover an event such as a lecture, sports game or musical performance may be prohibitively expensive. the cametron project aims at drastically reducing this cost by developing an (almost) fully automated system that can produce video recordings of such events with a quality similar to that of a professional crew. this system consists of different components, including intelligent pan-tilt-zoom cameras and uavs that act as “virtual camera men”. to combine the footage of these different cameras into a single coherent and pleasant-to-watch video, a “virtual editor” is needed. this paper describes the development of such a component. we adopt a declarative approach, in which we build a model of the decision process that a human editor might follow to edit a video. to achieve a montage that obeys various cinematographic rules while at the same time retaining a natural, non-mechanical feel, we construct this model in a probabilistic logic programming language. we demonstrate that the resulting system can be run in real-time and that it delivers montages that are almost indistinguishable from those made by a professional editor.",nan
2016,checking the conformance of requirements in agent designs using atl,"intelligent agent systems built using the bdi model of agency have grown in popularity for implementing complex systems such as uavs, military simulations, trading agents and intelligent games. the robust and flexible behaviours that these systems afford also makes testing the ‘correctness’ of these systems a non-trivial task. whilst the main focus on existing work has been on checking the correctness of agent-programs, in this work we present an approach to formally verify agent-based designs for a particular bdi agent design methodology. the focus is on verifying whether the detailed design of the agents conform to the requirements specification. we present a sound and complete approach, formally verifiable properties, and an evaluation with respect to time and effectiveness.",nan
2016,a uniform account of realizability in abstract argumentation,"we introduce a general framework for analyzing realizability in abstract dialectical frameworks (adfs) and various of its subclasses. in particular, the framework applies to dung argumentation frameworks, setafs by nielsen and parsons, and bipolar adfs. we present a uniform characterization method for the admissible, complete, preferred and model/stable semantics. we employ this method to devise an algorithm that decides realizability for the mentioned formalisms and semantics; moreover the algorithm allows for constructing a desired knowledge base whenever one exists. the algorithm is built in a modular way and thus easily extensible to new formalisms and semantics. we have implemented our approach in answer set programming, and used the implementation to obtain several novel results on the relative expressiveness of the abovemen-tioned formalisms.",nan
2016,a scalable clustering-based local multi-label classification method,"multi-label classification aims to assign multiple labels to a single test instance. recently, more and more multi-label classification applications arise as large-scale problems, where the numbers of instances, features and labels are either or all large. to tackle such problems, in this paper we develop a clustering-based local multi-label classification method, attempting to reduce the problem size in instances, features and labels. our method consists of low-dimensional data clustering and local model learning. specifically, the original dataset is firstly decomposed into several regular-scale parts by applying clustering analysis on the feature subspace, which is induced by a supervised multi-label dimension reduction technique; then, an efficient local multi-label model, meta-label classifier chains, is trained on each data cluster. given a test instance, only the local model belonging to the nearest cluster to it is activated to make the prediction. extensive experiments performed on eighteen benchmark datasets demonstrated the efficiency of the proposed method compared with the state-of-the-art algorithms.",nan
2016,multiscale triangular centroid distance for shape-based plant leaf recognition,"the shapes of plant leaves are very important to plant ecologists and botanists because these can help distinguish plant species as well as serve as health indicators. in this paper, we present a novel contour-based shape descriptor named multiscale triangular centroid distance (mtcd) for plant leaf recognition. mtcd features at different triangles are extracted from each contour point to provide a compact, multiscale shape descriptor. both local and global features of a plant leaf are effectively captured by the proposed method. a simple cosine distance is used to calculate the dissimilarity measurement between mtcd descriptors. therefore, mtcd is a rapid approach for shape matching and is suitable for real-time application. the proposed method has been evaluated using four publicly available plant leaf datasets, including the swedish leaf dataset, the smithsonian leaf dataset, the flavia leaf dataset, and the imageclef2012 leaf dataset. the experimental results show that this novel approach can achieve high recognition accuracy. comparisons with other state-of-the-art shape-based plant leaf recognition methods further demonstrate the effectiveness and efficiency of mtcd.",nan
2016,complexity of control by partitioning veto and maximin elections and of control by adding candidates to plurality elections,"control by partition refers to situations where an election chair seeks to influence the outcome of an election by partitioning either the candidates or the voters into two groups, thus creating two first-round subelections that determine who will take part in a final round. the model of partition-of-voters control attacks is remotely related to “gerrymandering” (maliciously resizing election districts). while the complexity of control by partition (and other control actions) has been studied thoroughly for many voting systems, there are no such results known for the important veto and maximin voting systems. we settle the complexity of control by partition for veto in a broad variety of models and for maximin with respect to destructive control by partition of candidates. we also observe that a reduction from the literature [8] showing the parameterized complexity of control by adding candidates to plurality elections, parameterized by the number of voters, is technically flawed by giving a counterexample, and we show how this reduction can be fixed.",nan
2016,agent-based refinement for predicate abstraction of multi-agent systems,"we put forward an agent-based refinement methodology for the verification of infinite-state multi-agent systems by predicate abstraction. we use specifications defined in a three-valued variant of the temporal epistemic logic atlk. we define “failure states” as candidates for refinement, and provide a sound automatic procedure for their identification. further, we introduce a methodology based on craig's interpolants for the refinement of the agent-specific predicates upon which the abstraction is built. we illustrate the refinement technique on an infinite-state auction scenario, and show that specifications of interest, that could not be checked by plain abstraction, can now be verified on the refined models.",nan
2016,combining deterministic and nondeterministic search for optimal journey planning under uncertainty,"optimal multi-modal journey planning under uncertainty is a challenging problem, due in part to an increased branching factor generated by nondeterministic actions. deterministic search, which ignores all uncertainty, can be much faster, but deterministic plans lack correctness and optimality guarantees in the uncertainty-aware domain.we present a novel approach that combines the strengths of both deterministic and nondeterministic search in order to achieve superior performance. initially, an a* search is used checking whether the resulting deterministic plan remains correct and optimal under uncertainty. when the plan is invalid, a backpropagation step through the a*'s search graph improves the initial heuristic while preserving its admissibility. after the backpropagation, an ao* search is run with the new improved heuristic. a theoretical analysis proves that our approach is sound and optimal. our backpropagation correctly handles a subtle issue arising in the presence of state-dominance pruning. this supports the use of these two powerful speedup techniques in combination, for a better overall performance.we empirically evaluate our solution in multi-modal journey planning under uncertainty, with realistic data from three european cities. our results show that our approach brings a significant performance improvement over a state-of-the-art, highly optimized journey planning engine based on ao* search.",nan
2016,robust real-time human perception with depth camera,"perception of the presence and position of human is crucial for many kinds of artificial intelligence (ai) applications. in this paper, we have developed a novel two-staged method for realtime human detection in depth image. the first stage is to quickly scan through the image to detect possible head-top locations in order to ensure all the candidate locations are included. the second stage is to use a novel head-shoulder descriptor (hsd) which jointly encodes the one-hot depth difference information and local geometric characteristics of human upper body to filter the detections so as to keep the genuine human locations and discard false positives. the results show that our approach using only depth data is superior to other methods using color and depth images on four datasets. in addition, our method performs well under weak illumination conditions or even total darkness. moreover, our system is also able to run in real-time on conventional pc without gpu acceleration.",nan
2016,a new kernelized associative memory and some of its applications,"the classical bidirectional associative memory (bam) allows for the storage of pairs of vectors, such that when either member of the pair is presented to the bam, the other member may be successfully recalled. this work presents a novel bam, improved with respect to its capacity and noise performance through the use of the kernel trick, a common technique in machine learning for transforming linear methods into nonlinear methods. by kernelizing the bam's energy function directly and defining new methods for recall, the kernel bam shows improved performance compared to both the original bam as well as a previously existing nonlinear bam. this is demonstrated with thorough experimentation on synthetic datasets, and several practical applications are given on real data.",nan
2016,strategical argumentative agent for human persuasion,"automated agents should be able to persuade people in the same way people persuade each other - via dialogs. today, automated persuasion modeling and research use unnatural assumptions regarding persuasive interaction, which creates doubt regarding their applicability for real-world deployment with people. in this work we present a novel methodology for persuading people through argumentative dialogs. our methodology combines theoretical argumentation modeling, machine learning and markovian optimization techniques that together result in an innovative agent named spa. two extensive field experiments, with more than 100 human subjects, show that spa is able to persuade people significantly more often than a baseline agent and no worse than people are able to persuade each other.",nan
2016,formalizing commitment-based deals in boolean games,"boolean games (bgs) are a strategic framework in which agents' goals are described using propositional logic. despite the popularity of bgs, the problem of how agents can coordinate with others to (at least partially) achieve their goals has hardly received any attention. however, negotiation protocols that have been developed outside the setting of bgs can be adopted for this purpose, provided that we can formalize (i) how agents can make commitments and (ii) how deals between coalitions of agents can be identified given a set of active commitments. in this paper, we focus on these two aims. first, we show how agents can formulate commitments that are in accordance with their goals, and what it means for the commitments of an agent to be consistent. second, we formalize deals in terms of coalitions who can achieve their goals without help from others. we show that verifying the consistency of a set of commitments of one agent is πp2-complete while checking the existence of a deal in a set of mutual commitments is σp2-complete. finally, we illustrate how the introduced concepts of commitments and deals can be used to achieve game-theoretical properties of the deals and to configure negotiation protocols.",nan
2016,a joint model for sentiment-aware topic detection on social media,"joint sentiment/topic models are widely applied in detecting sentiment-aware topics on the lengthy review data and they are achieved with latent dirichlet allocation (lda) based model. nowadays plenty of user-generated posts, e.g., tweets and e-commerce short reviews, are published on the social media and the posts imply the public's sentiments (i.e., positive and negative) towards various topics. however, the existing sentiment/topic models are not applicable to detect sentiment-aware topics on the posts, i.e., short texts, because applying the models to the short texts directly will suffer from the context sparsity problem. in this paper, we propose a time-user sentiment/topic latent dirichlet allocation (tus-lda) which aggregates posts in the same timeslice or user as a pseudo-document to alleviate the context sparsity problem. moreover, we design approaches for parameter inference and incorporating prior knowledge into tus-lda. experiments on the sentiment140 and tweets of electronic products from twitter7 show that tus-lda outperforms previous models in the tasks of sentiment classification and sentiment-aware topic extraction. finally, we visualize the sentiment-aware topics discovered by tus-lda.",nan
2016,a reinforcement learning framework for trajectory prediction under uncertainty and budget constraint,"we consider the problem of trajectory prediction, where a trajectory is an ordered sequence of location visits and corresponding timestamps. the problem arises when an agent makes sequential decisions to visit a set of spatial locations of interest. each location bears a stochastic utility and the agent has a limited budget to spend. given the agent's observed partial trajectory, our goal is to predict the agent's remaining trajectory. we propose a solution framework to the problem that incorporates both the stochastic utility of each location and the budget constraint. we first cluster the agents into groups of homogeneous behaviors called “agent types”. depending on its type, each agent's trajectory is then transformed into a discrete-state sequence representation. based on such representations, we use reinforcement learning (rl) to model the underlying decision processes and inverse rl to learn the utility distributions of the spatial locations. we finally propose two decision models to make predictions: one is based on long-term optimal planning of rl and another uses myopic heuristics. we apply the framework to predict real-world human trajectories collected in a large theme park and are able to explain the underlying processes of the observed actions.",nan
2016,person re-identification via multiple coarse-to-fine deep metrics,"person re-identification, aiming to identify images of the same person from various cameras views in different places, has attracted a lot of research interests in the field of artificial intelligence and multimedia. as one of its popular research directions, the metric learning method plays an important role for seeking a proper metric space to generate accurate feature comparison. however, the existing metric learning methods mainly aim to learn an optimal distance metric function through a single metric, making them difficult to consider multiple similar relationships between the samples. to solve this problem, this paper proposes a coarse-to-fine deep metric learning method equipped with multiple different stacked auto-encoder (sae) networks and classification networks. in the perspective of the human's visual mechanism, the multiple different levels of deep neural networks simulate the information processing of the brain's visual system, which employs different patterns to recognize the character of objects. in addition, a weighted assignment mechanism is presented to handle the different measure manners for final recognition accuracy. the experimental results conducted on two public datasets, i.e., viper and cuhk have shown the prospective performance of the proposed method.",nan
2016,how hard is bribery with distance restrictions?,"we study the complexity of the bribery problem with distance restrictions. in particular, in the bribery problem, we are given an election and a distinguished candidate p, and are asked whether we can make p win/not win the election by bribing at most k voters to recast their votes. in the bribery problem with distance restrictions, we require that the votes recast by the bribed voters are close to their original votes. to measure the closeness between two votes, we adopt the prevalent kendall-tau distance and the hamming distance. we achieve a wide range of complexity results for this problem under a variety of voting correspondences, including the borda, condorcet, copelandα for every 0≤α≤1 and maximin.",nan
2016,beyond ic postulates: classification criteria for merging operators,"merging is one of the central operations in the field of belief change, which is concerned with aggregating the opinions of individuals. representation theorems provide a family of merging operators satisfying some natural desiderata for merging beliefs. however, little is known about how these operators can be further distinguished. in the field of social choice, on the other hand, numerous properties have been proposed in order to classify voting rules. in this work, we adapt these properties to the context of merging and investigate how they relate to the standard postulates. our results thus lead to a more fine-grained classification of merging operators and shed light on the question of which particular merging operator is best suited in a concrete application domain.",nan
2016,schema-based debugging of federated data sources,"information explosion leads to continuous growth of data distributed over different data sources. however, the increasing number of data sources increases the risk of inconsistency. in such a federative setting, description logics can be applied to define a central schema that serves as a conceptual view comprising and extending the semantics of each data source. consequently, each data source is treated as a single knowledge base that is integrated in a federated knowledge base. following this idea, we propose an approach for automated debugging of federated knowledge bases that targets the identification and repair of inconsistency. we report on experiments with a large distributed dataset from the domain of library science.",nan
2016,belief contraction within fragments of propositional logic,"recently, belief change within the framework of fragments of propositional logic has gained attention. in the context of revision it has been proposed to refine existing operators so that they operate within propositional fragments, and that the result of revision remains in the fragment under consideration. in this paper we generalize this notion of refinement to belief change operators. whereas the notion of refinement allowed one to define concrete rational operators adapted to propositional fragments in the context of revision and update, it has to be specified for contraction. we propose a specific notion of refinement for contraction operators, called reasonable refinement. this allows us to provide refined contraction operators that satisfy the basic postulates for contraction. we study the logical properties of reasonable refinements of two well-known model-based contraction operators. our approach is not limited to the horn fragment but applicable to many fragments of propositional logic, like horn, krom and affine fragments.",nan
2016,a novel cross-modal topic correlation model for cross-media retrieval,"a novel cross-modal topic correlation model cmtcm is developed in this paper to facilitate more effective cross-modal analysis and cross-media retrieval for large-scale multimodal document collections. it can be modeled as a cross-modal topic correlation model which explores the inter-related correlation distribution over the deep representations of multimodal documents. it integrates the deep multimodal document representation, relational topic correlation modeling, and cross-modal topic correlation learning, which aims to characterize the correlations between the heterogeneous topic distributions of inter-related visual images and semantic texts, and measure their association degree more precisely. very positive results were obtained in our experiments using a large quantity of public data.",nan
2016,situation calculus game structures and gdl,"we present a situation calculus-based account of multi-players synchronous games in the style of general game playing. such games can be represented as action theories of a special form, situation calculus synchronous game structures (scsgss), in which we have a single action tick whose effects depend on the combination of moves selected by the players. then one can express properties of the game, e.g., winning conditions, playability, weak and strong winnability, etc. in a first-order alternating-time μ-calculus. we discuss verification in this framework considering computational effectiveness. we also show that scsgss can be considered as a first-order variant of the game description language (gdl) that supports infinite domains and possibly non-terminating games. we do so by giving a translation of gdl specifications into scsgss and showing its correctness. finally, we show how a player's possible moves can be specified in a golog-like programming language.",nan
2016,the game of reciprocation habits,"people often have reciprocal habits, almost automatically responding to others' actions. a robot who interacts with humans may also reciprocate, in order to come across natural and be predictable. we aim to facilitate decision support that advises on utility-efficient habits in these interactions. to this end, given a model for reciprocation behavior with parameters that represent habits, we define a game that describes what habit one should adopt to increase the utility of the process. this paper concentrates on two agents. the used model defines that an agent's action is a weighted combination of the other's previous actions (reacting) and either i) her innate kindness, or ii) her own previous action (inertia). in order to analyze what happens when everyone reciprocates rationally, we define a game where an agent may choose her habit, which is either her reciprocation attitude (i or ii), or both her reciprocation attitude and weight. we characterize the nash equilibria of these games and consider their efficiency. we find that the less kind agents should adjust to the kinder agents to improve both their own utility as well as the social welfare. this constitutes advice on improving cooperation and explains real life phenomena in human interaction, such as the societal benefits from adopting the behavior of the kindest person, or becoming more polite as one grows up.",nan
2016,randomized distribution feature for image classification,"local image features can be assumed to be drawn from an unknown distribution. for image classification, such features are compared through the histogram-based model or the metric-based model. by quantizing these local features into a set of histograms, the histogram-based model is convenient and has vectorial representation of image but information could be lost in vector quantization. unlike the histogram-based model, the metric-based model estimates the metrics over the underlying distribution of local features immediately, achieving better predictive performance. however, the model requires higher computational cost and loses the benefit of vectorial representation of image.to retain the advantages of these two models, this paper proposes the (doubly) randomized distribution features that represent the underlying distribution of local features in each image as a vectorial feature by utilizing random fourier feature. we prove the convergences of the similarity and distance based on the randomized distribution feature. remarkable advantages of the randomized distribution feature are that it has vectorial representation and thus computes efficiently as the histogram-based model. besides, it provides rigorous theory guarantee and competitive performance as the metric-based model. compared with several state-of-the-art algorithms, experiments in three real-world datasets justify that our proposed approaches attain competitive classification accuracy with faster computational speed. furthermore, we indicate that our proposed features can utilize the methods in learning based on vectors, which are broadly studied in traditional machine learning domain, to deal with the problems in learning based on distribution.",nan
2016,shapelearner: towards shape-based visual knowledge harvesting,"the deluge of images on the web has led to a number of efforts to organize images semantically and mine visual knowledge. despite enormous progress on categorizing entire images or bounding boxes, only few studies have targeted fine-grained image understanding at the level of specific shape contours. for instance, beyond recognizing that an image portrays a cat, we may wish to distinguish its legs, head, tail, and so on. to this end, we present shapelearner, a system that acquires such visual knowledge about object shapes and their parts in a semantic taxonomy, and then is able to exploit this hierarchy in order to analyze new kinds of objects that it has not observed before. shapelearner jointly learns this knowledge from sets of segmented images. the space of label and segmentation hypotheses is pruned and then evaluated using integer linear programming. experiments on a variety of shape classes show the accuracy and effectiveness of our method.",nan
2016,observation-based multi-agent planning with communication,"models of decentralized online planning vary in the information that individual agents use to make local action decisions. some models consider only local observations, eschewing coordination through communication. others use communication to ensure that all agents are aware of the action decisions of others, but assume costless and delay-free communication. in this paper, we propose a model of online planning (ob-map) that uses estimates of the value of communicating to manage coordination through communication as costs vary. we compare this approach to existing models in widely employed benchmark problems, demonstrating that ob-map performs significantly better in many scenarios regardless of varying (including infinite) cost of communication.",nan
2016,a framework for actionable clustering using constraint programming,"consider if you wish to cluster your ego network in facebook so as to find several useful groups each of which you can invite to a different dinner party. you may require that each cluster must contain equal number of males and females, that the width of a cluster in terms of age is at most 10 and that each person in a cluster should have at least r other people with the same hobby. these are examples of cardinality, geometric and density requirements/constraints respectfully that can make the clustering useful for a given purpose. however existing formulations of constrained clustering were not designed to handle these constraints since they typically deal with low-level, instance-level constraints. we formulate a constraint programming (cp) languages formulation of clustering with these cluster-level styles of constraints which we call actionable clustering. experimental results show the potential uses of this work to make clustering more actionable. we also show that these constraints can be used to improve the accuracy of semi-supervised clustering.",nan
2016,repetitive branch-and-bound using constraint programming for constrained minimum sum-of-squares clustering,"minimum sum-of-squares clustering (mssc) is a widely studied task and numerous approximate as well as a number of exact algorithms have been developed for it. recently the interest of integrating prior knowledge in data mining has been shown, and much attention has gone into incorporating user constraints into clustering algorithms in a generic way.exact methods for mssc using integer linear programming or constraint programming have been shown to be able to incorporate a wide range of constraints. however, a better performing method for unconstrained exact clustering is the repetitive branch-and-bound algorithm (rbba) algorithm. in this paper we show that both approaches can be combined. the key idea is to replace the internal branch-and-bound of rbba by a constraint programming solver, and use it to compute tight lower and upper bounds. to achieve this, we integrate the computed bounds into the solver using a novel constraint. our method combines the best of both worlds, and is generic as well as performing better than other exact constrained methods. furthermore, we show that our method can be used for multi-objective mssc clustering, including constrained multi-objective clustering.",nan
2016,is spearman's law of diminishing returns (slodr) meaningful for artificial agents?,"the progress of artificial intelligence is reaching a point that some research questions that were only relevant for human and other animal agents are becoming relevant for artificial agents as well. one of those questions comes from human intelligence research and is known as spearman's law of diminishing returns (slodr). charles spearman, the father of factor analysis and the g factor (a dominant factor explaining most of the variance in cognitive tests for human populations), observed that when the analysis was restricted to the subpopulation of most able subjects, the relevance of this dominant factor diminished, as if the power of general intelligence were saturated or not fully used by the most able individuals. in about a century, there have been numerous theoretical explanations and experiments to confirm or reject spearman's hypothesis. however, all of them have been based on human or animal populations. in this paper, we analyse for the first time whether the slodr makes sense for artificial agents and what its role should be in the analysis of general-purpose ai. we use a synthetic scenario based on modified elementary cellular automata (eca) where the eca rules work as tasks and the population of agents is generated with an agent policy language. different slices of the population by ability and of the tasks by difficulty are analysed, showing that slodr does not really appear. indeed, even if very slightly, we find the reverse, i.e., that more correlation takes place for more able subpopulations, what we conjecture as the universal law of augmenting returns (uloar).",nan
2016,efficient computation of exact irv margins,"computing the margin of victory (mov) in an instant runoff voting (irv) election is np-hard. in an irv election with winning candidate w, the mov defines the smallest number of cast votes that, if modified, result in the election of a candidate other than w. the ability to compute such margins has significant value. arguments over the correctness of an election outcome usually rely on the size of the electoral margin. risk-limiting audits use the size of this margin to determine how much post-election auditing is required. we present an efficient branch-and-bound algorithm for computing exact margins that substantially improves on the current best-known approach. although exponential in the worst case, our algorithm runs efficiently in practice, computing margins in instances that could not be solved by the current state-of-the-art in a reasonable time frame.",nan
2016,on labelling statements in multi-labelling argumentation,"in computational models of argumentation, argument justification has attracted more attention than statement justification, and significant sensitivity losses are identifiable when dealing with the justification of statements by otherwise appealing formalisms. this paper reappraises statement justification as a formalism-independent component in argument-based reasoning. we introduce a novel general model of argument-based reasoning based on multiple stages of labellings, the last one being devoted to statement justification, identify two alternative paths from argument acceptance to statement justification, and compare their expressiveness. we then show that this model encompasses several prominent literature proposals as special cases, thereby enabling a systematic comparison of existing approaches to statement justification, evidencing their merits and limits. finally we illustrate our model by specifying a generic ignorance-aware statement justification and showing how it can be seamlessly integrated into different formalisms.",nan
2016,annotate-sample-average (asa): a new distant supervision approach for twitter sentiment analysis,"the classification of tweets into polarity classes is a popular task in sentiment analysis. state-of-the-art solutions to this problem are based on supervised machine learning models trained from manually annotated examples. a drawback of these approaches is the high cost involved in data annotation. two freely available resources that can be exploited to solve the problem are: 1) large amounts of unlabelled tweets obtained from the twitter api and 2) prior lexical knowledge in the form of opinion lexicons. in this paper, we propose annotate-sample-average (asa), a distant supervision method that uses these two resources to generate synthetic training data for twitter polarity classification. positive and negative training instances are generated by sampling and averaging unlabelled tweets containing words with the corresponding polarity. polarity of words is determined from a given polarity lexicon. our experimental results show that the training data generated by asa (after tuning its parameters) produces a classifier that performs significantly better than a classifier trained from tweets annotated with emoticons and a classifier trained, without any sampling and averaging, from tweets annotated according to the polarity of their words.",nan
2016,"semi-supervised group sparse representation: model, algorithm and applications","group sparse representation (gsr) exploits group structure in data and works well on many problems. however, the group structure must be manually given in advance. in many practical scenarios such as classification, samples are grouped according to their labels. constructing a consistent group structure in such cases is not easy. the reasons are: 1) samples may be incorrectly labeled; and 2) label assigning in big data is time-consuming and expensive. in this paper, we propose and formulate a new problem, semi-supervised group sparse representation (ss-gsr) to support group sparse representation among both labeled and unlabeled data, while learning a more robust group structure, which can be further exploited to more effectively represent other unlabeled data. we develop a model to tackle the ss-gsr problem, based on the manifold assumption in subspace segmentation that samples in the same group lie close in feature space and span the same subspace. we also propose an alternating algorithm to solve the model. finally, we validate the model via extensive experiments.",nan
2016,modeling bounded rationality for sponsored search auctions,"sponsored search auctions (ssas) have attracted a lot of research attention in recent years and different equilibrium concepts have been studied in order to understand advertisers' bidding strategies. however, the assumption that advertisers are perfectly rational in these studies is unrealistic in the real world. in this work, we apply the quantal response equilibrium (qre), which is powerful in modeling bounded rationality, to ssas. due to high computational complexity, existing methods for qre computation have very poor scalability for ssas. through exploiting the structures of qre for ssas, this paper presents an efficient homotopy-based algorithm to compute the qre for large-size ssas, which features the following two novelties: 1) we represent the ssas as an action graph game (agg) which can compute the expected utilities in polynomial time; 2) we further significantly reduce redundant calculations by leveraging the underlying relations between advertisers' utilities. we also develop an estimator to infer parameters of ssas and fit the qre model into a dataset from a commercial search engine. our experimental results indicate that the algorithm can significantly improve the scalability of qre computation for ssas and the qre model can describe the real-world bidding behaviors in a very accurate manner.",nan
2016,an improved state filter algorithm for sir epidemic forecasting,"in epidemic modeling, state filtering is an excellent tool for enhancing the performance of traditional epidemic models. we introduce a novel state filter algorithm to further improve the performance of state-of-the-art approaches based on susceptible-infected-recovered (sir) models. the proposed algorithm merges two techniques, which are typically used separately: linear correction, as seen in the ensemble kalman filter (enkf), and resampling, as used in the particle filter (pf). we compare the inferential accuracy of our approach against the enkf and the ensemble adjustment kalman filter (eakf), using algorithms employing both an uncentered covariance matrix (ucm) and the standard column-centered covariance matrix (ccm). our algorithm requires o(dn) more time than enkf does, where d is the ensemble dimension and n denotes the ensemble size. we demonstrate empirically that our algorithm with ucm achieves the lowest root-mean-square-error (rmse) and the highest correlation coefficient (corr) amongst the selected methods, in 11 out of 14 major real-world scenarios. we show that the enkf with ucm outperforms the enkf with ccm, while the eakf gains better accuracy with ccm in most scenarios.",nan
2016,socially-aware multiagent learning: towards socially optimal outcomes,"in multiagent systems the capability of learning is important for an agent to behave appropriately in face of unknown opponents and a dynamic environment. from the system designer's perspective, it is desirable if the agents can learn to coordinate towards socially optimal outcomes, while also avoiding being exploited by selfish opponents. to this end, we propose a novel gradient ascent based algorithm (sa-iga) which augments the basic gradient-ascent algorithm by incorporating social awareness into the policy update process. we theoretically analyze the learning dynamics of sa-iga using dynamical system theory, and sa-iga is shown to have linear dynamics for a wide range of games including symmetric games. the learning dynamics of two representative games (the prisoner's dilemma game and coordination game) are analyzed in detail. based on the idea of sa-iga, we further propose a practical multiagent learning algorithm, called sa-pga, based on the q-learning update rule. simulation results show that an sa-pga agent can achieve higher social welfare than previous social-optimality oriented conditional joint action learner (cjal) and also is robust against individually rational opponents by reaching nash equilibrium solutions.",nan
2016,factors of collective intelligence: how smart are agent collectives?,"the dynamics and characteristics behind intelligent cognitive systems lie at the heart of understanding, and devising, successful solutions to a variety of multiagent problems. despite the extant literature on collective intelligence, important questions like “how does the effectiveness of a collective compare to its isolated members?” and “are there some general rules or properties shaping the spread of intelligence across various cognitive systems and environments?” remain somewhat of a mystery. in this paper we develop the idea of collective intelligence by giving some insight into a range of factors hindering and influencing the effectiveness of interactive cognitive systems. we measure the influence of each examined factor on intelligence independently, and empirically show that collective intelligence is a function of them all simultaneously. we further investigate how the organisational structure of equally sized groups shapes their effectiveness. the outcome is fundamental to the understanding and prediction of the collective performance of multiagent systems, and for quantifying the emergence of intelligence over different environmental settings.",nan
2016,synthesizing argumentation frameworks from examples,"argumentation is nowadays a core topic in ai research. understanding computational and representational aspects of abstract argumentation frameworks (afs) is a central topic in the study of argumentation. the study of realizability of afs aims at understanding the expressive power of afs under different semantics. we propose and study the af synthesis problem as a natural extension of realizability, addressing some of the shortcomings arising from the relatively stringent definition of realizability. specifically, af synthesis seeks to construct, or synthesize, afs that are semantically closest to the knowledge at hand even when no afs exactly representing the knowledge exist. going beyond defining the af synthesis problem, we (i) prove np-completeness of af synthesis under several semantics, (ii) study basic properties of the problem in relation to realizability, (iii) develop algorithmic solutions to af synthesis using constrained optimization, (iv) empirically evaluate our algorithms on different forms of af synthesis instances, as well as (v) discuss variants and generalization of af synthesis.",nan
2016,budgeted multi–armed bandit in continuous action space,"multi–armed bandits (mabs) have been widely considered in the last decade to model settings in which an agent wants to learn the action providing the highest expected reward among a fixed set of available actions during the operational life of a system. classical techniques provide solutions that minimize the regret due to learning in settings where selecting an arm has no cost. though, in many real world applications the learner has to pay some cost for pulling each arm and the learning process is constrained by a fixed budget b. this problem is addressed in the literature as the budgeted mab (bmab). in this paper, for the first time, we study the problem of budgeted continuous–armed bandit (bcab), where the set of the possible actions consists in a continuous set (e.g., a range of prices) and the learner suffers from a random reward and cost at each round. we provide a novel algorithm, named b–zoom, which suffers a regret of , where d is the zooming dimension of the problem. finally, we provide an empirical analysis showing that, despite a lower average performance, the proposed approach is more robust to adverse settings as compared to existing algorithms designed for bmab.",nan
2016,a framework for automatic debugging of functional and degradation failures,"software diagnosis is a particularly challenging problem for modern systems, which may consist of dozens, if not hundreds, of components computing on concurrent and potentially distributed platforms, and using infrastructure and services built by many organizations. we propose a framework that generalizes state-of-the-art classical reasoning-based fault diagnosis which tolerates observation uncertainty and addresses degradation of quality of service. empirical evaluation involving 27000 highly realistic synthetic scenarios demonstrates an average accuracy improvement of 20% (with 99% statistical significance) which is considerable in the domain of software fault localization (sfl). we measure the improvement in accuracy on well-established sfl performance metrics.",nan
2016,online adaptation of deep architectures with reinforcement learning,"online learning has become crucial to many problems in machine learning. as more data is collected sequentially, quickly adapting to changes in the data distribution can offer several competitive advantages such as avoiding loss of prior knowledge and more efficient learning. however, adaptation to changes in the data distribution (also known as covariate shift) needs to be performed without compromising past knowledge already built in into the model to cope with voluminous and dynamic data. in this paper, we propose an online stacked denoising autoencoder whose structure is adapted through reinforcement learning. our algorithm forces the network to exploit and explore favourable architectures employing an estimated utility function that maximises the accuracy of an unseen validation sequence. different actions, such as pool, increment and merge are available to modify the structure of the network. as we observe through a series of experiments, our approach is more responsive, robust, and principled than its counterparts for non-stationary as well as stationary data distributions. experimental results indicate that our algorithm performs better at preserving gained prior knowledge and responding to changes in the data distribution.",nan
2016,multi-class probabilistic active learning,"this work addresses active learning for multi-class classification. active learning algorithms optimize classifier performance by successively selecting the most beneficial instances from a pool of unlabeled instances to be labeled by an oracle. in this work, we study the influence of the following factors for active learning: (1) an instance's impact, (2) its posterior, and (3) the reliability of this posterior. to do so, we propose a new decision-theoretic approach, called multi-class probabilistic active learning (mcpal). building on a probabilistic active learning framework, our approach is non-myopic, fast, and optimizes a performance measure (like accuracy) directly. considering all influence factors, mcpal determines the expected gain in performance to compare the usefulness of instances. for this purpose, it calculates the density weighted expectation over the true posterior and over all possible labeling combinations in a closed-form solution. thus, in contrast to other multi-class algorithms, it considers the posterior's reliability which improved the performance. in our experimental evaluation, we show that the combination of the selected influence factors works best and that mcpal is superior in comparison to various other multi-class active learning algorithms on six datasets.",nan
2016,online prediction of exponential decay time series with human-agent application,"exponential decay time series are prominent in many fields. in some applications, the time series behavior can change over time due to a change in the user's preferences or a change of environment. in this paper we present an innovative online learning algorithm, which we name exponentron, for the prediction of exponential decay time series. we state a regret bound for our setting, which theoretically compares the performance of our online algorithm relative to the performance of the best batch prediction mechanism, which can be chosen in hindsight from a class of hypotheses after observing the entire time series. in experiments with synthetic and real-world data sets, we found that the proposed algorithm compares favorably with the classic time series prediction methods by providing up to 41% improvement in prediction accuracy. furthermore, we used the proposed algorithm for the design of a novel automated agent for the improvement of the communication process between a driver and its automotive climate control system. throughout extensive human study with 24 drivers we show that our agent improves the communication process and increases drivers' satisfaction, exemplifying the exponentron's applicative benefit.",nan
2016,set-valued conditioning in a possibility theory setting,possibilistic logic is a well-known framework for dealing with uncertainty and reasoning under inconsistent or prioritized knowledge bases. this paper deals with conditioning uncertain information where the weights associated with formulas are in the form of sets of uncertainty degrees. the first part of the paper studies set-valued possibility theory where we provide a characterization of set-valued possibilistic logic bases and set-valued possibility distributions by means of the concepts of compatible possibilistic logic bases and compatible possibility distributions respectively. the second part of the paper addresses conditioning set-valued possibility distributions. we first propose a set of three natural postulates for conditioning set-valued possibility distributions. we then show that any set-valued conditioning satisfying these three postulates is necessarily based on conditioning the set of compatible standard possibility distributions. the last part of the paper shows how one can efficiently compute set-valued conditioning over possibilistic knowledge bases.,nan
2016,an improved cnf encoding scheme for probabilistic inference,"we present and evaluate a new cnf encoding scheme for reducing probabilistic inference from a graphical model to weighted model counting. this new encoding scheme elaborates on the cnf encoding scheme enc4 introduced by chavira and darwiche, and improves it by taking advantage of log encodings of the elementary variable/value assignments and of the implicit encoding of the most frequent probability value per conditional probability table. from the theory side, we show that our encoding scheme is faithful, and that for each input network, the cnf formula it leads to contains less variables and less clauses than the cnf formula obtained using enc4. from the practical side, we show that the c2d compiler empowered by our encoding scheme performs in many cases significantly better than when enc4 is used, or when the state-of-the-art ace compiler is considered instead.",nan
2016,a bayesian approach to norm identification,"when entering a system, an agent should be aware of the obligations and prohibitions (collectively norms) that affect it. existing solutions to this norm identification problem make use of observations of either norm compliant, or norm violating, behaviour. thus, they assume an extreme situation where norms are typically violated, or complied with. in this paper we propose a bayesian approach to norm identification which operates by learning from both norm compliant and norm violating behaviour. we evaluate our approach's effectiveness empirically and compare its accuracy to existing approaches. by utilising both types of behaviour, we not only overcome a major limitation of such approaches, but also obtain improved performance over the state of the art, allowing norms to be learned with fewer observations.",nan
2016,subsumed label elimination for maximum satisfiability,"we propose subsumed label elimination (sle), a socalled label-based preprocessing technique for the boolean optimization paradigm of maximum satisfiability (maxsat). we formally show that sle is orthogonal to previously proposed sat-based preprocessing techniques for maxsat in that it can simplify the underlying minimal unsatisfiable core structure of maxsat instances. we also formally show that sle can considerably reduce the number of internal sat solver calls within modern core-guided maxsat solvers. empirically, we show that combining sle with sat-based preprocessing improves the performance of various state-of-the-art maxsat solvers on standard industrial weighted partial maxsat benchmarks.",nan
2016,vertical optimization of resource dependent flight paths,"flight routes are paths calculated on a network of waypoints representing 3d-coordinates. a common approach is first to calculate a path in a 2d-network, taking into account feasibility constraints, and then to optimize the altitude of the flight.we focus on the problem of determining the vertical trajectory defined by an altitude at each waypoint of a 2d-route. the cost of an airway depends, directly, on fuel consumption and on flying time, and, indirectly, on weight and on weather conditions. these dependencies invalidate the fifo property, which is commonly assumed for time-dependent networks. moreover, the amount of fuel at departure has an impact on the weight and depends on the length of the route. this, therefore, needs to be decided upon for our problem. we aim at minimizing the total cost.we study path-finding algorithms, both exact and heuristic, that we iterate in a line-search procedure to decide the fuel amount at departure. we use real-life data for the experimental analysis and conclude that on those data the assumption of the fifo property remains a good heuristic choice.",nan
2016,exploiting bayesian network sensitivity functions for inference in credal networks,"a bayesian network is a concise representation of a joint probability distribution, which can be used to compute any probability of interest for the represented distribution. credal networks were introduced to cope with the inevitable inaccuracies in the parametrisation of such a network. where a bayesian network is parametrised by defining unique local distributions, in a credal network sets of local distributions are given. from a credal network, lower and upper probabilities can be inferred. such inference, however, is often problematic since it may require a number of bayesian network computations exponential in the number of credal sets. in this paper we propose a preprocessing step that is able to reduce this complexity. we use sensitivity functions to show that for some classes of parameter in bayesian networks the qualitative effect of a parameter change on an outcome probability of interest is independent of the exact numerical specification. we then argue that credal sets associated with such parameters can be replaced by a single distribution.",nan
2016,interval-based relaxation for general numeric planning,"we generalise the interval-based relaxation to sequential numeric planning problems with non-linear conditions and effects, and cyclic dependencies. this effectively removes all the limitations on the problem placed in previous work on numeric planning heuristics, and even allows us to extend the planning language with a wider set of mathematical functions. heuristics obtained from the generalised relaxation are pruning-safe. we derive one such heuristic and use it to solve discrete-time control-like planning problems with autonomous processes. few planners can solve such problems, and search with our new heuristic compares favourably with them.",nan
2016,randomized canonical correlation discriminant analysis for face recognition,"as an important technique in multivariate statistical analysis, canonical correlation analysis (cca) has been widely used in face recognition. but existing cca based face recognition methods need two kinds of expression for the same face sample, and usually suffers high computational complexity in dealing with large samples. in this paper, we present a supervised method called randomized canonical correlation discriminant analysis (rccda) based on randomized non-linear canonical correlation analysis (rcca) to make up for the shortage of cca based face recognition methods. we first obtain basis vectors approximately with random features instead of the calculation of kernel matrix to improve the efficiency of computation, then we use these basis vectors to compute random optimal discriminant features which can reduce the dimension of face features while preserving as much discriminatory information as possible. the result of experiments on extended yale b, ar, orl and feret face databases demonstrates that the performance of our method compares favorably with some state-of-the-art algorithms.",nan
2016,reconsidering agm-style belief revision in the context of logic programs,"belief revision has been studied mainly with respect to background logics that are monotonic in character. in this paper we study belief revision when the underlying logic is non-monotonic instead—an inherently interesting problem that is under explored. in particular, we will focus on the revision of a body of beliefs that is represented as a logic program under the answer set semantics, while the new information is also similarly represented as a logic program. our approach is driven by the observation that unlike in a monotonic setting where, when necessary, consistency in a revised body of beliefs is maintained by jettisoning some old beliefs, in a non-monotonic setting consistency can be restored by adding new beliefs as well. we will define two revision functions through syntactic and model-theoretic methods respectively and subsequently provide representation theorems for characterising them.",nan
2016,value based reasoning and the actions of others,"practical reasoning, reasoning about what actions should be chosen, is highly dependent both on the individual values of the agent concerned and on what others choose to do. we discuss how value based argumentation about what to do can be performed without making assumptions about the preferences of the other agents. we then show how expected utility calculations relate to the value-based argumentation approach, and express the reasoning as arguments and objections, so that they can be integrated value-based practical reasoning. we illustrate our discussion with examples of value based reasoning in public goods games as used in experimental economics and present an initial evaluation of the approach in terms of these experiments.",nan
2016,analogical classifiers: a theoretical perspective,"in recent works, analogy-based classifiers have been proved quite successful. they exhibit good accuracy rates when compared with standard classification methods. nevertheless, a theoretical study of their predictive power has not been done so far. one of the main barriers has been the lack of functional definition: analogical learners have only algorithmic definitions. the aim of our paper is to complement the empirical studies with a theoretical perspective. using a simplified framework, we first provide a concise functional definition of the output of an analogical learner. two versions of the definition are considered, a strict and a relaxed one. as far as we know, this is the first definition of this kind for analogical learner. then, taking inspiration from results in k-nn studies, we examine some analytic properties such as convergence and vc-dimension, which are among the basic markers in terms of machine learning expressiveness. we then look at what could be expected in terms of theoretical accuracy from such a learner, in a boolean setting. we examine learning curves for artificial domains, providing experimental results that illustrate our formulas, and empirically validate our functional definition of analogical classifiers.",nan
2016,uncertainty-sensitive reasoning for inferring sameas facts in linked data,"discovering whether or not two uris described in linked data—in the same or different rdf datasets—refer to the same real-world entity is crucial for building applications that exploit the cross-referencing of open data. a major challenge in data interlinking is to design tools that effectively deal with incomplete and noisy data, and exploit uncertain knowledge. in this paper, we model data interlinking as a reasoning problem with uncertainty. we introduce a probabilistic framework for modelling and reasoning over uncertain rdf facts and rules that is based on the semantics of probabilistic datalog. we have designed an algorithm, probfr, based on this framework. experiments on real-world datasets have shown the usefulness and effectiveness of our approach for data linkage and disambiguation.",nan
2016,can a condorcet rule have a low coalitional manipulability?,"we investigate the possibility of designing a voting rule that both meets the condorcet criterion and has a low vulnerability to coalitional manipulation. for this purpose, we examine the condorcification of a voting rule, where the original rule is altered to elect the condorcet winner when one exists, and we study its impact on coalitional manipulability. a recent result states that for a large class of voting rules based on strict total orders, their condorcification is at most as coalitionally manipulable as the original rule. we show that for most of them, the improvement is strict. we extend these results to a broader framework that includes weak orders and cardinal voting rules. these results support the main message of this paper: when searching for a “reasonable” voting rule with minimal coalitional manipulability, investigations can be restricted to condorcet rules. in other words, in a class of “reasonnable” voting rules, it is possible to have both the condorcet criterion and a minimal vulnerability to coalitional manipulation.",nan
2016,upper and lower time and space bounds for planning,"there is an extensive literature on the complexity of planning, but explicit bounds on time and space complexity are very rare. on the other hand, problems like the constraint satisfaction problem have been thoroughly analysed in this respect. we provide a number of upper and lower bound results for both plan satisfiability (psat) and length-optimal planning (lop), with an emphasis on monotone planning (where actions have only positive effects) which is used in, for instance, h+ and similar heuristics. let v and a be the number of variables and actions, respectively. we consider both restrictions on the number and polarity of preconditions and effects of actions and the pubs restrictions in sas+. for all such classes, we show that psat and lop is either tractable or cannot be solved in subexponential time 2o(v) or time 2o(a), unless the so-called exponential time hypothesis (eth) is false. there is also a sharp transition: monotone lop can be solved in time 2o(v) if  but not if a∊ω(v). we also study upper bounds and discuss the trade-off between time and space, providing a polynomial-space algorithm for monotone lop that beats depth-first search in most cases. this raises the important question how lower bounds are affected by polynomial space restrictions.",nan
2016,abstraction-based verification of infinite-state reactive modules,"we introduce the formalism of infinite-state reactive modules to reason about the strategic behaviour of autonomous agents in a setting where data are explicitly exhibited in the systems description and in the specification language. technically, we endow reactive modules with an infinite domain of interpretation for individual variables, and introduce fo-atl, a first-order version of alternating time temporal logic, for the specification of properties of interest. we show that their verification is decidable for classes of data types of interest. this result is proved by defining a first-order version of alternating bisimulations and finite bisimilar abstractions. we illustrate the formal machinery by applying it to english and sealed bid auctions. in particular, we show that strategic properties of agents in auctions, including manipulability and collusion, can be expressed and verified in this framework.",nan
2016,translation-based revision and merging for minimal horn reasoning,"in this paper we introduce a new approach for revising and merging consistent horn formulae under minimal model semantics. our approach is translation-based in the following sense: we generate a propositional encoding capturing both the syntax of the original horn formulae (the clauses which appear or not in them) and their semantics (their minimal models). we can then use any classical revision or merging operator to perform belief change on the encoding. the resulting propositional theory is then translated back into a horn formula. we identify some specific operators which guarantee a particular kind of minimal change. a unique feature of our approach is that it allows us to control whether minimality of change primarily relates to the syntax or to the minimal model semantics of the horn formula. we give an axiomatic characterization of minimal change on the minimal model for this new setting, and we show that some specific translation-based revision and merging operators satisfy our postulates.",nan
2016,parallel filter-based feature selection based on balanced incomplete block designs,"in this paper we propose a method for scaling up filter-based feature selection in classification problems. we use the conditional mutual information as filter measure and show how the required statistics can be computed in parallel avoiding unnecessary calculations. the distribution of the calculations between the available computing units is determined based on balanced incomplete block designs, a strategy first developed within the area of statistical design of experiments. we show the scalability of our method through a series of experiments on synthetic and real-world datasets.",nan
2016,distributed controllers for norm enforcement,"this paper focuses on computational mechanisms that control the behavior of autonomous systems at runtime without necessarily restricting their autonomy. we build on existing approaches from runtime verification, control automata, and norm-based systems, and define norm-based controllers that enforce norms by modifying system behavior at runtime to make it norm compliant. for many applications, an autonomous system should comply with a set of norms. we extend our approach to a distributed setting, where a set of norm-based controllers jointly modify the runtime behavior of an autonomous system. the norms that a set of norm-based controllers jointly enforce are investigated and characterized in terms of the norms that are enforced by individual norm-based controllers. we show that a set of norm-based controllers is able to modify the runtime behavior of an autonomous system to make it compliant with all norms that the individual norm-based controllers aim at enforcing.",nan
2016,automatic verification of golog programs via predicate abstraction,"golog is a logic programming language for high-level agent control. in a recent paper, we proposed a sound but incomplete method for automatic verification of partial correctness of golog programs where we give a number of heuristic methods to strengthen given formulas in order to discover loop invariants. however, our method does not work on arithmetic domains. on the other hand, the method of predicate abstraction is widely used in the software engineering community for model checking and partial correctness verification of programs. intuitively, the predicate abstraction task is to find a formula consisting of a given set of predicates to approximate a given first-order formula. in this paper, we propose a method for automatic verification of partial correctness of golog programs which use predicate abstraction as a uniform method to strengthen given formulas. we implement a system based on the proposed method, conduct experiments on arithmetical domains and examples from the paper by li and liu. also, we apply our method to the verification of winning strategies for combinatorial games.",nan
2016,an assessment study of features and meta-level features in twitter sentiment analysis,"sentiment analysis is the task of determining the opinion expressed on subjective data, which may include microblog messages, such as tweets. this type of message has been considered the target of sentiment analysis in many recent studies, since they represent a rich source of opinionated texts. thus, in order to determine the opinion expressed in tweets, different studies have employed distinct strategies, which mainly include supervised machine learning methods. for this purpose, different kinds of features have been evaluated. despite that, none of the state-of-the-art studies has evaluated distinct categories of features, regarding their similar characteristics. in this context, this work presents a literature review of the most common feature representation in twitter sentiment analysis. we propose to group features sharing similar aspects into specific categories. we also evaluate the relevance of these categories of features, including meta-level features, using a significant number of twitter datasets. furthermore, we apply important and well-known feature selection strategies in order to identify relevant subsets of features for each dataset. we show in the experimental evaluation that the results achieved in this study, using feature selection strategies, outperform the results reported in previous works for the most of the assessed datasets.",nan
2016,crowdfunding public projects with provision point: a prediction market approach,"crowdfunding is emerging as a popular means to generate funding from citizens for public projects. this is popularly known as civic crowdfunding. in this paper, we focus on crowdfunding public projects with provision point: these are projects in which contributions must reach a predetermined threshold in order for the project to be provisioned. on web based civic crowdfunding platforms, the success of crowdfunding public projects has been somewhat mixed. in this paper, our objective is to design a mechanism that improves the success of crowdfunding public projects. in particular, we propose a class of mechanisms for crowdfunding platforms with sequentially arriving agents. this class of mechanisms induces an extensive form game for agents arriving on the platform and we show that the game has a non-empty set of sub-game perfect equilibria at which the project is fully funded. we call this new class of mechanisms provision point mechanism with securities (pps). the novelty of pps lies in the use of a prediction market to incentivize agents to contribute in proportion to their true value for the project and to contribute as soon as they arrive at the crowdfunding platform. different variations of pps are possible depending on the underlying prediction market. in this paper, we use a cost function (or equivalently, scoring rule) based prediction market; in fact, we specify the requirements that a cost function should satisfy to be used in pps. we study and compare two specific instances of pps: (1) logarithmic market scoring rule based and (2) quadratic scoring rule based. we also discuss the considerations that should guide the choice of the cost function when deploying our mechanism on crowdfunding platforms.",nan
2016,welfare of sequential allocation mechanisms for indivisible goods,"sequential allocation is a simple and attractive mechanism for the allocation of indivisible goods used in a number of real world settings. in sequential allocation, agents pick items according to a policy, the order in which agents take turns. sequential allocation will return an allocation which is pareto efficient – no agent can do better without others doing worse. however, sequential allocation may not return the outcome that optimizes the social welfare. we consider therefore the relationship between the welfare and the efficiency of the allocations returned by sequential allocation mechanisms. we then study some simple computational questions about what welfare is possible or necessary depending on the choice of policy. over half the problems we study turn out to be tractable, and we give polynomial time algorithms to compute them. we also consider a novel control problem in which the chair chooses a policy to improve social welfare. again, many of the control problems we study turn out to be tractable, and our results give polynomial time algorithms. in this case, tractability is a good thing so that the chair can improve the social welfare of the allocation.",nan
2016,a computational approach to consensus-finding,"consensus-finding plays a ubiquitous role in a.i. in this paper, a consensus among agents is defined as a non-contradictory fragment of all the information conveyed by the agents such that this fragment does not logically conflict with any of the agents. this concept is investigated in modal logic s5 in order to meet representation needs that are put in light by this concept of consensus itself. interestingly, an optimization-based approach to compute maximal consensuses is developed and shown experimentally efficient very often for both the standard boolean and s5 frameworks.",nan
2016,complexity and tractability islands for combinatorial auctions on discrete intervals with gaps,"combinatorial auctions are mechanisms for allocating bundles of goods to agents who each have preferences over these goods. finding an economically efficient allocation, the so-called winner determination problem, is computationally intractable in the general case, which is why it is important to identify special cases that are tractable but also sufficiently expressive for applications. we introduce a family of auction problems in which the goods on auction can be rearranged into a sequence, and each bid submitted concerns a bundle of goods corresponding to an interval on this sequence, possibly with multiple gaps of bounded length. we investigate the computational complexity of the winner determination problem for such auctions and explore the frontier between tractability and intractability in detail, identifying tractable, intractable, and fixed-parameter tractable cases.",nan
2016,efficient sat approach to multi-agent path finding under the sum of costs objective,"in the multi-agent path finding (mapf) the task is to find non-conflicting paths for multiple agents. in this paper we present the first sat-solver for the sum-of-costs variant of mapf which was previously only solved by search-based methods. using both a lower bound on the sum-of-costs and an upper bound on the makespan, we are able to have a reasonable number of variables in our sat encoding. we then further improve the encoding by borrowing ideas from icts, a search-based solver. experimental evaluation on several domains showed that there are many scenarios where the new sat-based method outperforms the best variants of previous sum-of-costs search solvers - the icts and icbs algorithms.",nan
2016,fixed-domain reasoning for description logics,"after decades of fruitful research, description logics (dls) have evolved into a de facto standard in logic-based knowledge representation. in particular, they serve as the formal basis of the standardized and very popular web ontology language (owl), which also comes with the advantage of readily available user-friendly modeling tools and optimized reasoning engines. in the course of the wide-spread adoption of owl and dls, situations have been observed where logically less skilled practitioners are (ab)using these formalisms as constraint languages adopting a closed-world assumption, contrary to the open-world semantics imposed by the classical definitions and the standards. to provide a clear theoretical basis and inferencing support for this often practically reasonable “off-label use” we propose an alternative formal semantics reflecting the intuitive understanding of such scenarios. to that end, we introduce the fixed-domain semantics and argue that this semantics gives rise to an interesting new inferencing task: model enumeration. we describe how the new semantics can be axiomatized in very expressive dls. we thoroughly investigate the complexities for standard reasoning as well as query answering under the fixed-domain semantics for a wide range of dls. further, we present an implementation of a fixed-domain dl reasoner based on a translation into answer set programming (asp) which is competitive with alternative approaches for standard reasoning tasks and provides the added functionality of model enumeration.",nan
2016,on redundancy in simple temporal networks,"the simple temporal problem (stp) has been widely used in various applications to schedule tasks. for dynamical systems, scheduling needs to be efficient and flexible to handle uncertainty and perturbation. to this end, modern approaches usually encode the temporal information as an stp instance. this representation contains redundant information, which can not only take a significant amount of storage space, but also make scheduling inefficient due to the non-concise representation. in this paper, we investigate the problem of simplifying an stp instance by removing redundant information. we show that such a simplification can result in a unique minimal representation without loss of temporal information, and present an efficient algorithm to achieve this task. evaluation on a large benchmark dataset of stp exhibits a significant reduction in redundant information for the involved instances.",nan
2016,on metric temporal description logics,"we introduce metric temporal description logics (mtdls) as combinations of the classical description logic alc with (a) ltlbin, an extension of the temporal logic ltl with succinctly represented intervals, and (b) metric temporal logic mtl, extending ltlbin with capabilities to quantitatively reason about time delays. our main contributions are algorithms and tight complexity bounds for the satisfiability problem in these mtdls: for mtdls based on (fragments of) ltlbin, we establish complexity bounds ranging from exptime to 2expspace. for mtdls based on (fragments of) mtl interpreted over the naturals, we establish complexity bounds ranging from expspace to 2expspace.",nan
2016,plan-based narrative generation with coordinated subplots,"despite recent progress in plan-based narrative generation, one major limitation is that systems tend to produce a single plotline whose progression entirely determines the narrative experience. however, for certain narrative genres such as serial dramas and soaps, multiple interleaved subplots are expected by the audience, as this tends to be the norm in real-world, human-authored narratives. current narrative generation techniques have overlooked this important requirement, something which could improve the perceived quality of generated stories. to this end, we have developed a flexible plan-based approach to multiplot narrative generation, that successfully generates narratives conforming to different subplot profiles, in terms of the number of subplots interleaved and the relative time spent on each presentation. we have identified specific challenges such as: distribution of virtual characters across subplots; length of each subplot presentation; and transitioning between subplots.in this paper, we overview this approach and describe its operation in a prototype interactive storytelling (is) system set in the serial drama genre. results of experiments with the system demonstrate its usability. furthermore, results of a user study highlight the potential of the approach, with clear user preference for presentations that feature interleaved multiple subplots.",nan
2016,hybrid gaussian and von mises model-based clustering,"data collected about a phenomenon often measures its magnitude and direction. the most common approach to clustering this data assumes that directional data can be modeled as gaussian. however, directional data has special properties that conventional statistics cannot handle. to deal with them, other approaches like the von mises distribution must be applied. in this paper we present a new model based on mixtures of bayesian networks to simultaneously cluster both linear and directional data.",nan
2016,adaptive symbiotic collaboration for targeted complex manipulation tasks,"this paper addresses the problem of human-robot collaboration in the context of manipulation tasks. in particular, we focus on tasks where a robot must perform some complex manipulation that is successfully completed only upon reaching some target pose provided by a human user. we propose an approach in which the robot explicitly reasons about its ability to complete the task and proactively requests the assistance of the human teammate when necessary. our approach effectively trades-off the benefits arising from the human assistance with the cost of disturbing the user. we also propose an adaptation mechanism that enables the robot to adjust its behavior to the particular manner by which the human user responds to the requests made by the robot. we test our approach in a simple illustrative scenario and in two real interaction scenarios involving the baxter robot.",nan
2016,attuning ontology alignments to semantically heterogeneous multi-agent interactions,"in this paper we tackle the problem of semantic heterogeneity in multi-agent communication, i.e., when agents in a multi-agent system use different vocabularies for message passing, or might interpret shared vocabulary in varying ways. the problem of achieving meaningful communication in such semantically heterogeneous multi-agent interactions has been mainly tackled either by using ontology alignments to translate vocabularies, or by using methods that learn an alignment by observing how the utterance of particular terms affects the unfolding of an interaction. we propose solutions that combine these approaches and study how agents can use external alignments with possibly incomplete or erroneous mappings when communicating with each other in the context of a multi-agent interaction. we further show experimentally that with the experience gained through repeated interactions and by using simple learning techniques agents can find and repair those mappings of an ontology alignment that lead to unsuccessful interactions, thus improving the success rate of their future interactions.",nan
2016,on the construction of high-dimensional simple games,"voting is a commonly applied method for the aggregation of the preferences of multiple agents into a joint decision. if preferences are binary, i.e., “yes” and “no”, every voting system can be described by a (monotone) boolean function χ:{0,1}n→{0,1}. however, its naive encoding needs 2n bits. the subclass of threshold functions, which is sufficient for homogeneous agents, allows a more succinct representation using n weights and one threshold. for heterogeneous agents, one can represent χ as an intersection of k threshold functions. taylor and zwicker have constructed a sequence of examples requiring  and provided a construction guaranteeing . the magnitude of the worst-case situation was to be determined by elkind et al. in 2008, but the analysis unfortunately turned out to be wrong. here we uncover a relation to coding theory that allows the determination of the minimum number k for a subclass of voting systems. as an application, we give a construction for k≥2n−o(n), i.e., there is no gain from a representation complexity point of view.",nan
2016,a dynamic logic of norm change,"norms are effective and flexible means to control and regulate the behaviour of autonomous systems. adding norms to a system changes its specification which may in turn ensure desirable system properties. as of yet, there is no generally agreed formal methodology to represent and reason about the dynamics of norms and their impacts on system specifications. in this paper, we introduce various types of norms, such as state-based or action-based norms, and gradually develop a dynamic modal logic to characterize the dynamics of such norms in a formal way. the logic can be used to prove various properties of norm dynamics and their impacts on system specification. moreover, we show that this logic is sound and complete.",nan
2016,h-index manipulation by undoing merges,"the h-index is an important bibliographic measure used to assess the performance of researchers. van bevern et al. [artif. intel., to appear] showed that, despite computational worst-case hardness results, substantial manipulation of the h-index of google scholar author profiles is possible by merging articles. complementing this work, we study the opposite operation, the splitting of articles, which is arguably the more natural operation for manipulation and which is also allowed within google scholar. we present numerous results on computational complexity (from linear-time algorithms to parameterized computational hardness results) and empirically indicate that at least small improvements of the h-index by splitting merged articles are easily achievable.",nan
2016,planning under uncertainty for aggregated electric vehicle charging with renewable energy supply,"renewable energy sources introduce uncertainty regarding generated power in smart grids. for instance, power that is generated by wind turbines is time-varying and dependent on the weather. electric vehicles will become increasingly important in the development of smart grids with a high penetration of renewables, because their flexibility makes it possible to charge their batteries when renewable supply is available. charging of electric vehicles can be challenging, however, because of uncertainty in renewable supply and the potentially large number of vehicles involved. in this paper we propose a vehicle aggregation framework which uses markov decision processes to control electric vehicles and deals with uncertainty in renewable supply. we present a grouping technique to address the scalability aspects of our framework. in experiments we show that the aggregation framework maximizes the profit of the aggregator, reduces cost of customers and reduces consumption of conventionally-generated power.",nan
2016,on the computation of top-k extensions in abstract argumentation frameworks,"formal argumentation has received a lot of attention during the last two decades, since abstract argumentation framework provides the basis for various reasoning problems in artificial intelligence. unfortunately, the exponential number of its possible semantics extensions makes some reasoning problems intractable in this framework. in this paper, we investigate the pivotal issue of efficient computation of acceptable arguments called extensions according to a given semantics. in particular, we address this aspect by applying a strategy of how to use preferences at the semantics level in order to determine what are “desirable” outcomes of the argumentation process. then, we present a new approach for computing the top-k extensions of an abstract argumentation framework, according to a user-specified preference relation. indeed, an extension is a top-k extension for a given semantics if it admits less than k extensions preferred to it with respect to a preference relation. our experiments on various datasets demonstrate the effectiveness and scalability of our approach and the accuracy of the proposed enumeration method.",nan
2016,on revision of partially specified convex probabilistic belief bases,"we propose a method for an agent to revise its incomplete probabilistic beliefs when a new piece of propositional information is observed. in this work, an agent's beliefs are represented by a set of probabilistic formulae – a belief base. the method involves determining a representative set of ‘boundary’ probability distributions consistent with the current belief base, revising each of these probability distributions and then translating the revised information into a new belief base. we use a version of lewis imaging as the revision operation. the correctness of the approach is proved. an analysis of the approach is done against six rationality postulates. the expressivity of the belief bases under consideration are rather restricted, but has some applications. we also discuss methods of belief base revision employing the notion of optimum entropy, and point out some of the benefits and difficulties in those methods. both the boundary distribution method and the optimum entropy methods are reasonable, yet yield different results.",nan
2016,solving dynamic controllability problem of multi-agent plans with uncertainty using mixed integer linear programming,"executing multi-agent missions requires managing the uncertainty about uncontrollable events. when communications are intermittent, it additionally requires for each agent to act only based on its local view of the problem, that is independently of events which are controlled or observed by the other agents. in this paper, we propose a new framework for dealing with such contexts, with a focus on mission plans involving temporal constraints. this framework, called multi-agent simple temporal network with uncertainty (mastnu), is a combination between multi-agent simple temporal network (mastn) and simple temporal network with uncertainty (stnu). we define the dynamic controllability property for mastnu, and a method for computing offline valid execution strategies which are then dispatched between agents. this method is based on a mixed-integer linear programming formulation and can also be used to optimize criteria such as the temporal flexibility of multi-agent plans.",nan
2016,combining efficient preprocessing and incremental maxsat reasoning for maxclique in large graphs,"we describe a new exact algorithm for maxclique, called lmc (short for large maxclique), that is especially suited for large sparse graphs. lmc is competitive because it combines an efficient preprocessing procedure and incremental maxsat reasoning in a branch-and-bound scheme. the empirical results show that lmc outperforms existing exact maxclique algorithms on large sparse graphs from real-world applications.",nan
2016,an efficient approach for the generation of allen relations,"event data is increasingly being represented according to the linked data principles. the need for large-scale machine learning on data represented in this format has thus led to the need for efficient approaches to compute rdf links between resources based on their temporal properties. time-efficient approaches for computing links between rdf resources have been developed over the last years. however, dedicated approaches for linking resources based on temporal relations have been paid little attention to. in this paper, we address this research gap by presenting aegle, a novel approach for the efficient computation of links between events according to allen's interval algebra. we study allen's relations and show that we can reduce all thirteen relations to eight simpler relations. we then present an efficient algorithm with a complexity of o(nlog n) for computing these eight relations. our evaluation of the runtime of our algorithms shows that we outperform the state of the art by up to 4 orders of magnitude while maintaining a precision and a recall of 1.",nan
2016,you can't always forget what you want: on the limits of forgetting in answer set programming,"selectively forgetting information while preserving what matters the most is becoming an increasingly important issue in many areas, including in knowledge representation and reasoning. depending on the application at hand, forgetting operators are defined to obey different sets of desirable properties. of the myriad of desirable properties discussed in the context of forgetting in answer set programming, strong persistence, which imposes certain conditions on the correspondence between the answer sets of the program pre-and post-forgetting, and a certain independence from non-forgotten atoms, seems to best capture its essence, and be desirable in general. however, it has remained an open problem whether it is always possible to forget a set of atoms from a program while obeying strong persistence. in this paper, after showing that it is not always possible to forget a set of atoms from a program while obeying this property, we move forward and precisely characterise what can and cannot be forgotten from a program, by presenting a necessary and sufficient criterion. this characterisation allows us to draw some important conclusions regarding the existence of forgetting operators for specific classes of logic programs, to characterise the class of forgetting operators that achieve the correct result whenever forgetting is possible, and investigate the related question of determining what we can forget from some specific logic program.",nan
2016,solving set optimization problems by cardinality optimization with an application to argumentation,"optimization—minimization or maximization—in the lattice of subsets is a frequent operation in artificial intelligence tasks. examples are subset-minimal model-based diagnosis, nonmonotonic reasoning by means of circumscription, or preferred extensions in abstract argumentation. finding the optimum among many admissible solutions is often harder than finding admissible solutions with respect to both computational complexity and methodology. this paper addresses the former issue by means of an effective method for finding subset-optimal solutions. it is based on the relationship between cardinality-optimal and subset-optimal solutions, and the fact that many logic-based declarative programming systems provide constructs for finding cardinality-optimal solutions, for example maximum satisfiability (maxsat) or weak constraints in answer set programming (asp). clearly each cardinality-optimal solution is also a subset-optimal one, and if the language also allows for the addition of particular restricting constructs (both maxsat and asp do) then all subset-optimal solutions can be found by an iterative computation of cardinality-optimal solutions. as a showcase, the computation of preferred extensions of abstract argumentation frameworks using the proposed method is studied.",nan
2016,the need for knowledge extraction: understanding harmful gambling behavior with neural networks,"responsible gambling is a field of study that involves supporting gamblers so as to reduce the harm that their gambling activity might cause. recently in the literature, machine learning algorithms have been introduced as a way to predict potentially harmful gambling based on patterns of gambling behavior, such as trends in amounts wagered and the time spent gambling. in this paper, neural network models are analyzed to help predict the outcome of a partial proxy for harmful gambling behavior: when a gambler “self-excludes”, requesting a gambling operator to prevent them from accessing gambling opportunities. drawing on survey and interview insights from industry and public officials as to the importance of interpretability, a variant of the knowledge extraction algorithm trepan is proposed which can produce compact, human-readable logic rules efficiently, given a neural network trained on gambling data. to the best of our knowledge, this paper reports the first industrial-strength application of knowledge extraction from neural networks, which otherwise are black-boxes unable to provide the explanatory insights which are crucially required in this area of application. we show that through knowledge extraction one can explore and validate the kinds of behavioral and demographic profiles that best predict self-exclusion, while developing a machine learning approach with greater potential for adoption by industry and treatment providers. experimental results reported in this paper indicate that the rules extracted can achieve high fidelity to the trained neural network while maintaining competitive accuracy and providing useful insight to domain experts in responsible gambling.",nan
2016,strategy representation and reasoning in the situation calculus,"strategy representation and reasoning has been one of the most active research areas in ai and multi-agent systems. representative strategic logics are atl and the more expressive strategy logic sl which reasons about strategies explicitly. in this paper, by a simple extension of the situation calculus with a strategy sort, we develop a general framework for strategy representation and reasoning for complete information games. this framework can be used to compactly represent both concurrent and turn-based possibly infinite game structures, specify the internal structure of strategies, reason about strategies explicitly, and reason about strategic abilities of coalitions under commitments to strategy specifications. we show that our framework is strictly more expressive than sl, and inspired by the work of de giacomo et al. on bounded action theories, give a decidable fragment of our framework.",nan
2016,exploiting mus structure to measure inconsistency of knowledge bases,"measuring inconsistency is recognized as an important research issue for quantifying and handling inconsistencies in knowledge bases. several logic-based inconsistency measures have been proposed. minimal unsatisfiable and maximal satisfiable subsets are at the heart of the syntactic measures, while semantic inconsistency measures are often based on some paraconsistent semantics. in order to design interesting measures faithful to human rationality, many properties have been introduced to reach this goal. in this paper, we propose a new property called sub-additivity allowing to push further the ability to reorder knowledge bases according to their inconsistency degree. after pointing out the limitations of several measures to satisfy the sub-additivity property, we present a new measure based on a fine exploitation of the internal structure of the knowledge base, namely the structure of its associated minimal unsatisfiable subsets. then, we show how its computation can be formulated as a nonlinear optimization problem. finally, we prove that the new measure satisfies all the required properties while highlighting its interesting features.",nan
2016,gaining insight by structural knowledge extraction,"the availability of increasingly larger and more complex datasets has boosted the demand for systems able to analyze them automatically. the design and implementation of effective systems requires coding knowledge about the application domain inside the system itself; however, the designer is expected to intuitively grasp the most relevant features of the raw data as a preliminary step.in this paper we propose a framework to get useful insight about a set of complex data, and we claim that a shift in perspective may be of help to tackle with the unaddressed goal of representing knowledge by means of the structure inferred from the collected samples. we will present a formulation of knowledge extraction in terms of grammatical inference (gi), an inductive process able to select the best grammar consistent with the samples, and a proof-of-concept application in a scenario of mobility data.",nan
2016,complexity of threshold query answering in probabilistic ontological data exchange,"we study the complexity of threshold query answering in the logical framework for probabilistic ontological data exchange, which is an extension of the classical probabilistic data exchange framework with (1) probabilistic databases compactly encoded with several different annotations according to three different probability models used and (2) existential rules of different expressiveness. the ontological data exchange framework provides a logical formalization of exchanging probabilistic data and knowledge from one ontology to another via either deterministic or probabilistic mappings. we define the threshold query answering task in this framework and provide a thorough analysis of its computational complexity for different classes of existential rules and types of complexity. we also delineate several classes of existential rules and a probability model along with a compact encoding in which the threshold query answering problem can be solved in polynomial time in the data complexity.",nan
2016,markov logic networks with numerical constraints,"markov logic networks (mlns) have proven to be useful tools for reasoning about uncertainty in complex knowledge bases. in this paper, we extend mlns with numerical constraints and present an efficient implementation in terms of a cutting plane method. this extension is useful for reasoning over uncertain temporal data. to show the applicability of this extension, we enrich log-linear description logics (dls) with concrete domains (datatypes). thereby, allowing to reason over weighted dls with datatypes. moreover, we use the resulting formalism to reason about temporal assertions in db-pedia, thus illustrating its practical use.",nan
2016,revisiting the cross entropy method with applications in stochastic global optimization and reinforcement learning,"in this paper, we provide a new algorithm for the problem of stochastic global optimization where only noisy versions of the objective function are available. the algorithm is inspired by the well known cross entropy (ce) method. the algorithm takes the shape of a multi-timescale stochastic approximation algorithm, where we reuse the previous samples based on discounted averaging, and hence it saves the overall computational and storage cost. we provide proof of the stability and the global optimization property of our algorithm. the algorithm shows good performance on the noisy versions of global optimization benchmarks and outperforms a state-of-the-art algorithm for non-linear function approximation in reinforcement learning.",nan
2016,online auctions for dynamic assignment: theory and empirical evaluation,"dynamic resource assignment is a common problem in multi-agent systems. we consider scenarios in which dynamic agents have preferences about assignments and the resources that can be assigned using online auctions. we study the trade-off between the following online auction properties: (i) truthfulness, (ii) expressiveness, (iii) efficiency, and (iv) average case performance. we theoretically and empirically compare four different online auctions: (i) arrival priority serial dictatorship, (ii) split dynamic vcg, (iii) e-action, and (iv) online ranked competition auction. the latter is a novel design based on the competitive secretary problem. we show that, in addition to truthfulness and algorithmic efficiency, the degree of competition also plays an important role in selecting the best algorithm for a given context.",nan
2016,mathematical programming models for optimizing partial-order plan flexibility,"a partial-order plan (pop) compactly encodes a set of sequential plans that can be dynamically chosen by an agent at execution time. one natural measure of the quality of a pop is its flexibility, which is defined to be the total number of sequential plans it embodies (i.e., its linearizations). as this criteria is hard to optimize, existing work has instead optimized proxy functions that are correlated with the number of linearizations. in this paper, we develop and strengthen mixed-integer linear programming (milp) models for three proxy functions: two from the pop literature and a third novel function based on the temporal flexibility criteria from the scheduling literature. we show theoretically and empirically that none of the three proxy measures dominate the others in terms of number of sequential plans. compared to the state-of-the-art maxsat model for the problem, we empirically demonstrate that two of our milp models result in equivalent or slightly better solution quality with savings of approximately one order of magnitude in computation time.",nan
2016,on distances between kd45n kripke models and their use for belief revision,"in this paper, some distances between kd45n kripke models are introduced and investigated. we define several distances between kripke models, based on different criteria, inspired by various concepts such as bisimulation and propositional distances between valuations for different modal degrees. we study the properties of these distances. such distances are useful for defining belief change operators in multi-agent scenarios. we show that they can be used to define belief revision operators based on the standard agm framework and suited to kd45n kripke models.",nan
2016,unsupervised activity recognition using latent semantic analysis on a mobile robot,"we show that by using qualitative spatio-temporal abstraction methods, we can learn common human movements and activities from long term observation by a mobile robot. our novel framework encodes multiple qualitative abstractions of rgbd video from detected activities performed by a human as encoded by a skeleton pose estimator. analogously to informational retrieval in text corpora, we use latent semantic analysis (lsa) to uncover latent, semantically meaningful, concepts in an unsupervised manner, where the vocabulary is occurrences of qualitative spatio-temporal features extracted from video clips, and the discovered concepts are regarded as activity classes. the limited field of view of a mobile robot represents a particular challenge, owing to the obscured, partial and noisy human detections and skeleton pose-estimates from its environment. we show that the abstraction into a qualitative space helps the robot to generalise and compare multiple noisy and partial observations in a real world dataset and that a vocabulary of latent activity classes (expressed using qualitative features) can be recovered.",nan
2016,dichotomy for pure scoring rules under manipulative electoral actions,"scoring systems are an extremely important class of election systems. we study the complexity of manipulation, constructive control by deleting voters (ccdv), and bribery for scoring systems. for manipulation, we show that for all scoring rules with a constant number of different coefficients, manipulation is in p. and we conjecture that there is no dichotomy theorem.on the other hand, we obtain dichotomy theorems for ccdv and bribery. more precisely, we show that both of these problems are easy for 1-approval, 2-approval, 1-veto, 2-veto, 3-veto, generalized 2-veto, and (2, 1, ..., 1, 0), and hard in all other cases. these results are the “dual” of the dichotomy theorem for the constructive control by adding voters (ccav) problem from [14], but do not at all follow from that result. in particular, proving hardness for ccdv is harder than for ccav since we do not have control over what the controller can delete, and proving easiness for bribery tends to be harder than for control, since bribery can be viewed as control followed by manipulation.",nan
2016,higher-order correlation coefficient analysis for eeg-based brain-computer interface,"electroencephalogram (eeg) based brain-computer interface (bci) has been proved to be an effective communication way between human brain and external devices. in order to effectively recover the cortical dynamics from the eeg signals and improve the classification performance, plenty of studies focused on constructing subject-specific spatial and spectral filters, achieving considerable improvement in classification accuracy. however, almost all the approaches aimed to find one common subspace for projection of all the samples in different classes. studies have shown that active channels and frequency information were not only subject-dependent but also class-dependent. thus the variety of class-dependent spatial and spectral characteristics can provide further discriminative information for classification. in this paper, we proposed a tensor-based method which attempted to seek individual spatial and spectral subspaces for each class by which each class was projected into its own subspace separately such that they were easily to be classified. finally, we added a regularization term in this model to avoid overfitting. we evaluated the effectiveness and robustness of the proposed method on two different datasets including one widely-used benchmark eeg dataset collected from healthy subjects and one self-collected eeg dataset collected from stroke patients. the results demonstrated its superior performance.",nan
2016,classtering: joint classification and clustering with mixture of factor analysers,"in this work we propose a novel parametric bayesian model for the problem of semi-supervised classification and clustering. standard approaches of semi-supervised classification can recognize classes but cannot find groups of data. on the other hand, semi-supervised clustering techniques are able to discover groups of data but cannot find the associations between clusters and classes. the proposed model can classify and cluster samples simultaneously, allowing the analysis of data in the presence of an unknown number of classes and/or an arbitrary number of clusters per class. experiments on synthetic and real world data show that the proposed model compares favourably to state-of-the-art approaches for semi-supervised clustering and that the discovered clusters can help to enhance classification performance, even in cases where the cluster and the low density separation assumptions do not hold. we finally show that when applied to a challenging real-world problem of subgroup discovery in breast cancer, the method is capable of maximally exploiting the limited information available and identifying highly promising subgroups.",nan
2016,extending the description logic  with acyclic tboxes,"in a previous paper, we have introduced an extension of the lightweight description logic el that allows us to define concepts in an approximate way. for this purpose, we have defined a graded membership function deg, which for each individual and concept yields a number in the interval [0, 1] expressing the degree to which the individual belongs to the concept. threshold concepts c~t for ~∊{<,≤,>,≥} then collect all the individuals that belong to c with degree ~t. we have then investigated the complexity of reasoning in the description logic , which is obtained from  by adding such threshold concepts. in the present paper, we extend these results, which were obtained for reasoning without tboxes, to the case of reasoning w.r.t. acyclic tboxes. surprisingly, this is not as easy as might have been expected. on the one hand, one must be quite careful to define acyclic tboxes such that they still just introduce abbreviations for complex concepts, and thus can be unfolded. on the other hand, it turns out that, in contrast to the case of el, adding acyclic tboxes to  increases the complexity of reasoning by at least on level of the polynomial hierarchy.",nan
2016,clique-width and directed width measures for answer-set programming,"disjunctive answer set programming (asp) is a powerful declarative programming paradigm whose main decision problems are located on the second level of the polynomial hierarchy. identifying tractable fragments and developing efficient algorithms for such fragments are thus important objectives in order to complement the sophisticated asp systems available to date. hard problems can become tractable if some problem parameter is bounded by a fixed constant; such problems are then called fixed-parameter tractable (fpt). while several fpt results for asp exist, parameters that relate to directed or signed graphs representing the program at hand have been neglected so far. in this paper, we first give some negative observations showing that directed width measures on the dependency graph of a program do not lead to fpt results. we then consider the graph parameter of signed clique-width and present a novel dynamic programming algorithm that is fpt w.r.t. this parameter. clique-width is more general than the well-known treewidth, and, to the best of our knowledge, ours is the first fpt algorithm for bounded clique-width for reasoning problems beyond sat.",nan
2016,emotion analysis as a regression problem – dimensional models and their implications on emotion representation and metrical evaluation,"emotion analysis (ea) and sentiment analysis are closely related tasks differing in the psychological phenomenon they aim to catch. we address fine-grained models for ea which treat the computation of the emotional status of narrative documents as a regression rather than a classification problem, as performed by coarse-grained approaches. we introduce ekman's basic emotions (be) and russell and mehrabian's valence-arousal-dominance (vad) model—two major schemes of emotion representation following opposing lines of psychological research, i.e., categorical and dimensional models—and discuss problems when bes are used in a regression approach. we present the first natural language system thoroughly evaluated for fine-grained emotion analysis using the vad scheme. although we only employ simple bow features, we reach correlation values up until r = .65 with human annotations. furthermore, we show that the prevailing evaluation methodology relying solely on pearson's correlation coefficient r is deficient which leads us to the introduction of a complementary error-based metric. due to the lack of comparable (vad-based) systems, we, finally, introduce a novel method of mapping between vad and be emotion representations to create a reasonable basis for comparison. this enables us to evaluate vad output against human be judgments and, thus, allows for a more direct comparison with existing be-based emotion analysis systems. even with this, admittedly, error-prone transformation step our vad-based system achieves state-of-the-art performance in three out of six emotion categories, out-performing all existing be-based systems but one.",nan
2016,aspect-based relational sentiment analysis using a stacked neural network architecture,"sentiment analysis can be regarded as a relation extraction problem in which the sentiment of some opinion holder towards a certain aspect of a product, theme or event needs to be extracted. we present a novel neural architecture for sentiment analysis as a relation extraction problem that addresses this problem by dividing it into three subtasks: i) identification of aspect and opinion terms, ii) labeling of opinion terms with a sentiment, and iii) extraction of relations between opinion terms and aspect terms. for each subtask, we propose a neural network based component and combine all of them into a complete system for relational sentiment analysis. the component for aspect and opinion term extraction is a hybrid architecture consisting of a recurrent neural network stacked on top of a convolutional neural network. this approach outperforms a standard convolutional deep neural architecture as well as a recurrent network architecture and performs competitively compared to other methods on two datasets of annotated customer reviews. to extract sentiments for individual opinion terms, we propose a recurrent architecture in combination with word distance features and achieve promising results, outperforming a majority baseline by 18% accuracy and providing the first results for the usage dataset. our relation extraction component outperforms the current state-of-the-art in aspect-opinion relation extraction by 15% f-measure.",nan
2016,learning invariant representation for malicious network traffic detection,"statistical learning theory relies on an assumption that the joint distributions of observations and labels are the same in training and testing data. however, this assumption is violated in many real world problems, such as training a detector of malicious network traffic that can change over time as a result of attacker's detection evasion efforts. we propose to address this problem by creating an optimized representation, which significantly increases the robustness of detectors or classifiers trained under this distributional shift. the representation is created from bags of samples (e.g. network traffic logs) and is designed to be invariant under shifting and scaling of the feature values extracted from the logs and under permutation and size changes of the bags. the invariance is achieved by combining feature histograms with feature self-similarity matrices computed for each bag and significantly reduces the difference between the training and testing data. the parameters of the representation, such as histogram bin boundaries, are learned jointly with the classifier. we show that the representation is effective for training a detector of malicious traffic, achieving 90% precision and 67% recall on samples of previously unseen malware variants.",nan
2016,making sense of item response theory in machine learning,"item response theory (irt) is widely used to measure latent abilities of subjects (specially for educational testing) based on their responses to items with different levels of difficulty. the adaptation of irt has been recently suggested as a novel perspective for a better understanding of the results of machine learning experiments and, by extension, other artificial intelligence experiments. for instance, irt suits classification tasks perfectly, where instances correspond to items and classifiers correspond to subjects. by adopting irt, item (i.e., instance) characteristic curves can be estimated using logistic models, for which several parameters characterise each dataset instance: difficulty, discrimination and guessing. irt looks promising for the analysis of instance hardness, noise, classifier dominances, etc. however, some caveats have been found when trying to interpret the irt parameters in a machine learning setting, especially when we include some artificial classifiers in the pool of classifiers to be evaluated: the optimal and pessimal classifiers, a random classifier and the majority and minority classifiers. in this paper we perform a series of experiments with a range of datasets and classification methods to fully understand how irt works and what their parameters really mean in the context of machine learning. this better understanding will hopefully pave the way to a myriad of potential applications in machine learning and artificial intelligence.",nan
2016,"skeptical, weakly skeptical, and credulous inference based on preferred ranking functions","while the axiomatic system p is an important standard for plausible nonmonotonic reasoning, inference relations obtained from system z or from c-representations have been designed which go beyond system p. in this paper, we propose the new concept of weakly skeptical inference that properly extends the recently introduced skeptical c-inference, but avoids disadvantages of a too liberal credulous inference. we extend the concepts of skeptical, weakly skeptical, and credulous c-inference by taking preferred models obtained from different minimality criteria into account. we illustrate the usefulness of the obtained inference relations, show that they fulfill various desirable properties, and elaborate on their interrelationships.",nan
2016,real-time timeline summarisation for high-impact events in twitter,"twitter has become a valuable source of event-related information, namely, breaking news and local event reports. due to its capability of transmitting information in real-time, twitter is further exploited for timeline summarisation of high-impact events, such as protests, accidents, natural disasters or disease outbreaks. such summaries can serve as important event digests where users urgently need information, especially if they are directly affected by the events. in this paper, we study the problem of timeline summarisation of high-impact events that need to be generated in real-time. our proposed approach includes four stages: classification of real-world events reporting tweets, online incremental clustering, post-processing and sub-events summarisation. we conduct a comprehensive evaluation of different stages on the “ebola outbreak” tweet stream, and compare our approach with several baselines, to demonstrate its effectiveness. our approach can be applied as a replacement of a manually generated timeline and provides early alarms for disaster surveillance.",nan
2016,towards better models of externalities in sponsored search auctions,"sponsored search auctions (ssas) arguably represent the problem at the intersection of computer science and economics with the deepest applications in real life. within the realm of ssas, the study of the effects that showing one ad has on the other ads, a.k.a. externalities in economics, is of utmost importance and has so far attracted the attention of much research. however, even the basic question of modeling the problem has so far escaped a definitive answer. the popular cascade model is arguably too idealized to really describe the phenomenon yet it allows a good comprehension of the problem. other models, instead, describe the setting more adequately but are too complex to permit a satisfactory theoretical analysis. in this work, we attempt to get the best of both approaches: firstly, we define a number of general mathematical formulations for the problem in the attempt to have a rich description of externalities in ssas and, secondly, prove a host of results drawing a nearly complete picture about the computational complexity of the problem. we complement these approximability results with some considerations about mechanism design in our context.",nan
2016,"a computational method for extracting, representing, and predicting social closeness","identifying the social closeness between two individuals is a key skill in any situation where interpersonal relations play a role. consequently, such capacity is needed to enable ai systems to understand human interactions and interact naturally in social situations. however, current research has relied on simple proxies for social closeness, and richer models are difficult to achieve. this paper presents an approach to predicting social closeness based on linguistic interactions and demonstrates an ability to predict social closeness with high accuracy.",nan
2016,planning using actions with control parameters,"although pddl is an expressive modelling language, a significant limitation is imposed on the structure of actions: the parameters of actions are restricted to values from finite (in fact, explicitly enumerated) domains. there is one exception to this, introduced in pddl2.1, which is that durative actions may have durations that are chosen (possibly subject to explicit constraints in the action models) by the planner. a motivation for this limitation is that it ensures that the set of grounded actions is finite and, ignoring duration, the branching factor of action choices at a state is therefore finite. although the duration parameter can make this choice infinite, very few planners support this possibility, but restrict themselves to durative actions with fixed durations. in this paper we motivate a proposed extension to pddl to allow actions with infinite domain parameters, which we call control parameters. we illustrate reasons for using this modelling feature and then describe a planning approach that can handle domains that exploit it, implemented in a new planner, popcorn (partial-order planning with constrained real numerics). we show that this approach scales to solve interesting problems.",nan
2016,fixed-parameter tractable optimization under dnnf constraints,"minimizing a cost function under a set of combinatorial constraints is a fundamental, yet challenging problem in ai. fortunately, in various real-world applications, the set of constraints describing the problem structure is much less susceptible to change over time than the cost function capturing user's preferences. in such situations, compiling the set of feasible solutions during an offline step can make sense, especially when the target compilation language renders computationally easier the generation of optimal solutions for cost functions supplied “on the fly”, during the online step. in this paper, the focus is laid on boolean constraints compiled into dnnf representations. we study the complexity of the minimization problem for several families of cost functions subject to dnnf constraints. beyond linear minimization which is already known to be tractable in the dnnf language, we show that both quadratic minimization and submodular minization are fixed-parameter tractable for various subsets of dnnf. in particular, the fixed-parameter tractability of constrained submodular minimization is established using a natural parameter capturing the structural dissimilarity between the submodular cost function and the dnnf representation.",nan
2016,preference modeling with possibilistic networks and symbolic weights: a theoretical study,"the use of possibilistic networks for representing conditional preference statements on discrete variables has been proposed only recently. the approach uses non-instantiated possibility weights to define conditional preference tables. moreover, additional information about the relative strengths of these symbolic weights can be taken into account. the fact that at best we have some information about the relative values of these weights acknowledges the qualitative nature of preference specification. these conditional preference tables give birth to vectors of symbolic weights that reflect the preferences that are satisfied and those that are violated in a considered situation. the comparison of such vectors may rely on different orderings: the ones induced by the product-based, or the minimum-based chain rule underlying the possibilistic network, the discrimin, or leximin refinements of the minimum-based ordering, as well as pareto ordering, and the symmetric pareto ordering that refines it. a thorough study of the relations between these orderings in presence of vector components that are symbolic rather numerical is presented. in particular, we establish that the product-based ordering and the symmetric pareto ordering coincide in presence of constraints comparing pairs of symbolic weights. this ordering agrees in the boolean case with the inclusion between the sets of preference statements that are violated. the symmetric pareto ordering may be itself refined by the leximin ordering. the paper highlights the merits of product-based possibilistic networks for representing preferences and provides a comparative discussion with cp-nets and ocf-networks.",nan
2016,leveraging stratification in twitter sampling,"with tweet volumes reaching 500 million a day, sampling is inevitable for any application using twitter data. realizing this, data providers such as twitter, gnip and boardreader license sampled data streams priced in accordance with the sample size. big data applications working with sampled data would be interested in working with a large enough sample that is representative of the universal dataset. previous work focusing on the representativeness issue has considered ensuring that global occurrence rates of key terms, be reliably estimated from the sample. present technology allows sample size estimation in accordance with probabilistic bounds on occurrence rates for the case of uniform random sampling. in this paper, we consider the problem of further improving sample size estimates by leveraging stratification in twitter data. we analyze our estimates through an extensive study using simulations and real-world data, establishing the superiority of our method over uniform random sampling. our work provides the technical know-how for data providers to expand their portfolio to include stratified sampled datasets, whereas applications are benefited by being able to monitor more topics/events at the same data and computing cost.",nan
2016,a minimization-based approach to iterated multi-agent belief change,"we investigate minimization-based approaches to iterated belief change in multi-agent systems. a network of agents is represented by an undirected graph, where propositional formulas are associated with vertices. information is shared between vertices via a procedure where each vertex minimizes disagreement with other vertices in the graph. each iterative approach takes into account the proximity between vertices, with the underlying assumption that information from nearby sources is given higher priority than information from more distant sources. we have identified two main approaches to iteration: in the first approach, a vertex takes into account the information at its immediate neighbours only, and information from more distant vertices is propagated via iteration; in the second approach, a vertex first takes into account information from distance-1 neighbours, then from distance-2 neighbours, and so on, in a prioritized fashion. there prove to be three distinct ways to define the second approach, so in total we have four types of iteration. we define these types formally, find relationships between them, and investigate their basic logical properties. we also implemented the approaches in a software system called equibel.",nan
2016,parameterised model checking for alternating-time temporal logic,"we investigate the parameterised model checking problem for specifications expressed in alternating-time temporal logic. we introduce parameterised concurrent game structures representing infinitely many games with different number of agents. we introduce a parametric variant of atl to express properties of the system irrespectively of the number of agents present in the system. while the parameterised model checking problem is undecidable, we define a special class of systems on which we develop a sound and complete counter abstraction technique. we illustrate the methodology here devised on the prioritised version of the train-gate-controller.",nan
2016,interpretable encoding of densities using possibilistic logic,"probability density estimation from data is a widely studied problem. often, the primary goal is to faithfully mimic the underlying empirical density. having an interpretable model that allows insight into why certain predictions were made is often of secondary importance. using logic-based formalisms, such as markov logic, can help with interpretability, but even in markov logic it can be difficult to gain insight into a model's behavior due to interactions between the logical formulas used to specific the model. this paper explores an alternative approach to representing densities that makes use of possibilistic logic. concretely, we propose a novel way to transform a learned density tree into a possibilistic logic theory. an advantage of our transformation is that it permits performing both map and, surprisingly, marginal inference, with the converted possibilistic logic theory. at the same time, we still retain the benefits conferred by using possibilistic logic, such as the ability to compact the theory and the interpretability of the model.",nan
2016,unsupervised ranking of knowledge bases for named entity recognition,"with the continuous growth of freely accessible knowledge bases and the heterogeneity of textual corpora, selecting the most adequate knowledge base for named entity recognition is becoming a challenge in itself. in this paper, we propose an unsupervised method to rank knowledge bases according to their adequacy for the recognition of named entities in a given corpus. building on a state-of-the-art, unsupervised entity linking approach, we propose several evaluation metrics to measure the lexical and structural adequacy of a knowledge base for a given corpus. we study the correlation between these metrics and three standard performance measures: precision, recall and f1 score. our multi-domain experiments on 9 different corpora with 6 knowledge bases show that three of the proposed metrics are strong performance predictors having 0.62 to 0.76 pearson correlation with precision and 0.96 correlation with both recall and f1 score.",nan
2016,skeleton-based orienteering for level set estimation,"in recent years, the use of unmanned vehicles for monitoring spatial environmental phenomena has gained increasing attention. within this context, an interesting problem is level set estimation, where the goal is to identify regions of space where the analyzed phenomena (for example the ph value in a body of water) is above or below a given threshold level. typically, in the literature this problem is approached with techniques which search for the most interesting sampling locations to collect the desired information (i.e., locations where we can gain the most information by sampling). however, the common assumption underlying this class of approaches is that acquiring a sample is expensive (e.g., in terms of consumed energy and time). in this paper, we take a different perspective on the same problem by considering the case where a mobile vehicle can continuously acquire measurements with a negligible cost, through high rate sampling sensors. in this scenario, it is crucial to reduce the path length that the mobile platform executes to collect the data. to address this issue, we propose a novel algorithm, called skeleton-based orienteering for level set estimation (sbolse). our approach starts from the lse formulation introduced in [10] and formulates the level set estimation problem as an orienteering problem. this allows one to determine informative locations while considering the length of the path. to combat the complexity associated with the orienteering approach, we propose a heuristic approach based on the concept of topological skeletonization. we evaluate our algorithm by comparing it with the state of the art approaches (i.e., lse and lse-batch) both on a real world dataset extracted from mobile platforms and on a synthetic dataset extracted from co2 maps. results show that our approach achieves a near optimal classification accuracy while significantly reducing the travel distance (up to 70% w.r.t lse and 30% w.r.t. lse-batch).",nan
2016,interruptible task execution with resumption in golog,"mobile robots should perform a growing number of tasks and react to time-critical events. thus, the ability to interrupt a task and resume it later is crucial. while interleaved execution occurs often in robotics, existing approaches do not consider the fact that interrupting a task and resuming an interrupted task often requires intermediate steps. in this paper we present an approach to interruptible task execution with resumption. we propose intrgolog which extends indigolog by task interruption and resumption through introducing new constructs to determine and fulfill the requirements of tasks. our experiments on a service robot and in simulation show that the ability to switch to another task enables a robot to react in a swift and reliable fashion to new events.",nan
2016,constructing hierarchical task models using invariance analysis,"hierarchical task networks (htns) are a common model for encoding knowledge about planning domains in the form of task decompositions. we present a novel algorithm that uses invariant analysis to construct an htn from the pddl description of a planning domain and a single representative instance. the algorithm defines two types of composite tasks that interact to achieve the goal of a planning instance. one type of task achieves fluents by traversing invariants in which only one fluent can be true at a time. the other type of task applies a single action, which first involves ensuring that the precondition of the action holds. the resulting htn can be applied to any instance of the planning domain, and is provably sound. we show that the performance of our algorithm is comparable to algorithms that learn htns from examples and use added knowledge.",nan
2016,learning the structure of dynamic hybrid relational models,"typical approaches to relational mdps consider only discrete variables or else discretize the continuous variables prior to inference or learning. in contrast, we consider hybrid relational mdps, which are represented as probabilistic programs and specify the probability density function of the continuous variables. our key contribution is that we introduce a technique for learning their structure (and parameters) from data. the learned models contain rich relational descriptions as well as mathematical equations. we demonstrate the utility of our approach by learning a model that accurately predicts the effects of robot-arm actions. the learned model is then used for planning tasks.",nan
2016,a distributed asynchronous solver for nash equilibria in hypergraphical games,"hypergraphical games provides a compact model of a network of self-interested agents, each involved in simultaneous subgames with its neighbors. the overall aim is for the agents in the network to reach a nash equilibrium, in which no agent has an incentive to change their response, but without revealing all their private information. asymmetric distributed constraint satisfaction (adiscsp) has been proposed as a solution to this search problem. in this paper, we propose a new model of hypergraphical games as an adiscsp based on a new global constraint, and a new asynchronous algorithm for solving adiscsp that is able to find a nash equilibrium. we show empirically that we significantly reduce both message passing and computation time, achieving an order of magnitude improvement in messaging and in non-concurrent computation time on dense problems compared to state-of-the art algorithms.",nan
2016,summary information for reasoning about hierarchical plans,"hierarchically structured agent plans are important for efficient planning and acting, and they also serve (among other things) to produce “richer” classical plans, composed not just of a sequence of primitive actions, but also “abstract” ones representing the supplied hierarchies. a crucial step for this and other approaches is deriving precondition and effect “summaries” from a given plan hierarchy. this paper provides mechanisms to do this for more pragmatic and conventional hierarchies than in the past. to this end, we formally define the notion of a precondition and an effect for a hierarchical plan; we present data structures and algorithms for automatically deriving this information; and we analyse the properties of the presented algorithms. we conclude the paper by detailing how our algorithms may be used together with a classical planner in order to obtain abstract plans.",nan
2016,knowledge-based programs with defaults in a modal situation calculus,"we consider the realistic case of a golog agent that only possesses incomplete knowledge about the state of its environment and has to resort to sensing in order to gather additional information at runtime, and where the agent is controlled by a knowledge-based program in which test conditions explicitly refer to the agent's knowledge (or lack thereof). in this paper, we propose a formalization of knowledge-based agents that extends earlier proposals by a form of non-monotonic reasoning that includes reiter-style defaults. we present a reasoning mechanism that enables us to reduce projection queries about future states of the agent's knowledge (including nesting of epistemic modalities) to classical default logic, and provide a corresponding representation theorem. we thus obtain the theoretical foundation for an implementation where reasoning subtasks can be handed to an embedded off-the-shelf reasoner for default logic, and that supports a (in some respects) more expressive epistemic action language than previous solutions.",nan
2016,solving multi-agent knapsack problems using incremental approval voting,"in this paper, we study approval voting for multi-agent knapsack problems under incomplete preference information. the agents consider the same set of feasible knapsacks, implicitly defined by a budget constraint, but they possibly diverge in the utilities they attach to items. individual utilities being difficult to assess precisely and to compare, we collect approval statements on knapsacks from the agents with the aim of determining the optimal solutions by approval voting. we first propose a search procedure based on mixed-integer programming to explore the space of utilities compatible with the known part of preferences in order to determine or approximate the set of possible approval winners. then, we propose an incremental procedure combining preference elicitation and search in order to determine the set of approval winners without requiring the full elicitation of the agents' preferences. finally, the practical efficiency of these procedures is illustrated by various numerical tests.",nan
2016,propositional abduction with implicit hitting sets,"logic-based abduction finds important applications in artificial intelligence and related areas. one application example is in finding explanations for observed phenomena. propositional abduction is a restriction of abduction to the propositional domain, and complexity-wise is in the second level of the polynomial hierarchy. recent work has shown that exploiting implicit hitting sets and propositional satisfiability (sat) solvers provides an efficient approach for propositional abduction. this paper investigates this earlier work and proposes a number of algorithmic improvements. these improvements are shown to yield exponential reductions in the number of sat solver calls. more importantly, the experimental results show significant performance improvements compared to the the best approaches for propositional abduction.",nan
2016,improved multi-label classification using inter-dependence structure via a generative mixture model,"single-label classification associates each instance with a single label, while multi-label classification (mlc), assigns multiple labels to instances. simple mlc systems assume that labels are independent of one another, while more complex approaches capture inter-dependencies among labels. experiments comparing performance of mlc systems demonstrate that there is much room for improvement.notably, when an instance is associated with multiple labels, a feature-value of the instance may depend only on a subset of these labels and thus be conditionally independent of the others given the label-subset. current systems do not account for such conditional independence. moreover, dependence of a feature-value on a label is likely to imply its dependence on other inter-dependent labels. our hypothesis is that by explicitly modeling the dependence between feature values and specific subsets of inter-dependent labels, the assignment of multi-labels to instances can be done more accurately.we present a probabilistic generative model that captures dependencies among labels as well as between features and labels, by means of a bayesian network. we introduce the concept of label dependency sets as a basis for a new mixture model that represents conditional independencies between features and labels given subsets of inter-dependent labels. experimental results show that the performance of the system we have developed based on our model for mlc significantly improves upon results obtained by current mlc systems that are based on probabilistic models.",nan
2016,accelerating norm emergence through hierarchical heuristic learning,"social norms serve as an important mechanism to regulate the behaviours of agents and to facilitate coordination among them in multiagent systems. one important research question is how a norm can rapidly emerge through repeated local interaction within agent societies under different environments when their coordination space becomes large. to address this problem, we propose a hierarchically heuristic learning strategy (hhls) under the hierarchical social learning framework. subordinate agents report their information to their supervisors, while supervisors can generate instructions (rules and suggestions) based on the information collected from their subordinates. subordinate agents heuristically update their strategies based on both their own experience and the instructions from their supervisors. extensive experiment evaluations show that hhls can support the emergence of desirable social norms more efficiently and can be applicable in a much wider range of multiagent interaction scenarios compared with previous work. the influence of key related factors (e.g., different topologies, population, neighbourhood and action space size, cluster size) are also investigated and new insights are obtained as well.",nan
2016,entity embeddings with conceptual subspaces as a basis for plausible reasoning,"conceptual spaces are geometric representations of conceptual knowledge in which entities correspond to points, natural properties correspond to convex regions, and the dimensions of the space correspond to salient features. while conceptual spaces enable elegant models of various cognitive phenomena, the lack of automated methods for constructing such representations have so far limited their application in artificial intelligence. to address this issue, we propose a method which learns a vector-space embedding of entities from wikipedia and constrains this embedding such that entities of the same semantic type are located in some lower-dimensional subspace. we experimentally demonstrate the usefulness of these subspaces as approximate conceptual space representations by showing, among others, that important features can be modelled as directions and that natural properties tend to correspond to convex regions.",nan
2016,automatic bridge bidding using deep reinforcement learning,"bridge is among the zero-sum games for which artificial intelligence has not yet outperformed expert human players. the main difficulty lies in the bidding phase of bridge, which requires cooperative decision making under partial information. existing artificial intelligence systems for bridge bidding rely on and are thus restricted by human-designed bidding systems or features. in this work, we propose a pioneering bridge bidding system without the aid of human domain knowledge. the system is based on a novel deep reinforcement learning model, which extracts sophisticated features and learns to bid automatically based on raw card data. the model includes an upper-confidence-bound algorithm and additional techniques to achieve a balance between exploration and exploitation. our experiments validate the promising performance of our proposed model. in particular, the model advances from having no knowledge about bidding to achieving superior performance when compared with a champion-winning computer bridge program that implements a human-designed bidding system.",nan
2016,hierarchical strategy synthesis for pursuit-evasion problems,"we present a novel approach for solving pursuit-evasion problems where multiple pursuers with limited sensing capabilities are used to detect all possible mobile evaders in a given environment. we make no assumptions about the number, the speed, or the maneuverability of evaders. our algorithm takes as input a map of the environment and sensor models for the pursuers. we then obtain a graph representation of an environment using a čech complex. even with such a representation, the configuration space grows exponentially with the number of pursuers. in order to address this challenge, we propose an abstraction framework to partition the configuration space into sets of topologically similar configurations that preserve the space of possible evader locations. we validate our approach on several simulated environments with varying topologies and numbers of pursuers.",nan
2016,decidable reasoning in a first-order logic of limited conditional belief,"in a series of papers, liu, lakemeyer, and levesque address the problem of decidable reasoning in expressive first-order knowledge bases. here, we extend their ideas to accommodate conditional beliefs, as in “if she is australian, then she presumably eats kangaroo meat.” perhaps the most prevalent semantics of a conditional belief is to evaluate the consequent in the most-plausible worlds consistent with the premise. in this paper, we devise a technique to approximate this notion of plausibility, and complement it with liu, lakemeyer, and levesque's weak inference. based on these ideas, we develop a logic of limited conditional belief, and provide soundness, decidability, and (for the propositional case) tractability results.",nan
2016,a temporal-causal modelling approach to integrated contagion and network change in social networks,"this paper introduces an integrated adaptive temporal-causal network model for dynamics in networks of social interactions addressing contagion between states, and changing connections within these social networks by two principles: the homophily principle and the more-becomes-more principle. the model has been evaluated in three different manners: by simulation experiments, by verification based on mathematical analysis, and by validation against an empirical data set.",nan
2016,exact particle filter modularization improves runtime performance,"bayesian filters provide a robust and powerful technique for integrating noisy information in dynamic environments. however, the computational cost of the filtering algorithm depends on the size of the problem, and an effective solution may be constrained by execution time. this paper applies basic concepts of clustering and message passing to particle filters making them substantially faster to compute, while still maintaining the original accuracy. an example from vehicle state estimation is provided to illustrate how to implement the technique. our results indicate that modularization can produce a speed up of over 28 times even on this small problem.",nan
2016,using the sugeno integral in optimal assignment problems with qualitative utilities,"this paper is devoted to the assignment problem when the preferences of the agents are defined by qualitative utilities. in this setting, it is not possible to compare assignments by summing up individual utilities because the sum operation becomes meaningless. we study here the optimization of a sugeno integral of the individual utilities. we show that the problem is np-hard in the general case, but we also identify special cases that are solvable in polynomial time. furthermore, we provide a mixed integer programming formulation in the general case, which leads to a compact formulation for k-minitive capacities.",nan
2016,complexity results for probabilistic datalog±,"we study the query evaluation problem in probabilistic databases in the presence of probabilistic existential rules. our focus is on the datalog± family of languages for which we define the probabilistic counterpart using a flexible and compact encoding of probabilities. this formalism can be viewed as a generalization of probabilistic databases, as it allows to generate new facts from the given ones, using so-called tuple-generating dependencies, or existential rules. we study the computational cost of this additional expressiveness under two different semantics. first, we use a conventional approach and assume that the probabilistic knowledge base is consistent and employ the standard possible world semantics. thereafter, we introduce a probabilistic inconsistency-tolerant semantics, which we call inconsistency-tolerant possible world semantics. for both of these cases, we provide a thorough complexity analysis relative to different languages, drawing a complete picture of the complexity of probabilistic query answering in this family.",nan
2016,strategic voting in a social context: considerate equilibria,"in a voting system, voters may adopt a strategic behaviour in order to manipulate the outcome of the election. this naturally entails a game theoretic conception of voting. the specificity of our work is that we embed the voting game into a social context where agents and their relations are given by a graph, i.e. a social network. we aim at integrating the information provided by the graph in a refinement of the game-theotical analysis of an election. we consider coalitional equilibria immune to deviations performed by realistic coalitions based on the social network, namely the cliques of the graph. agents are not fully selfish as they have consideration for their relatives. the corresponding notion of equilibrium was introduced by hoefer et al. [12] and called considerate equilibrium. we propose to study its existence and the ability of the agents to converge to such an equilibrium in strategic voting games using well-known voting rules: plurality, antiplurality, plurality with runoff, borda, k-approval, stv, maximin and copeland.",nan
2016,the complexity of deciding legality of a single step of magic: the gathering,"magic: the gathering is a game about magical combat for any number of players. formally it is a zero-sum, imperfect information stochastic game that consists of a potentially unbounded number of steps. we consider the problem of deciding if a move is legal in a given single step of magic. we show that the problem is (a) conp-complete in general; and (b) in p if either of two small sets of cards are not used. our lower bound holds even for single-player magic games. the significant aspects of our results are as follows: first, in most real-life game problems, the task of deciding whether a given move is legal in a single step is trivial, and the computationally hard task is to find the best sequence of legal moves in the presence of multiple players. in contrast, quite uniquely our hardness result holds for single step and with only one-player. second, we establish efficient algorithms for important special cases of magic.",nan
2016,description logics reasoning w.r.t. general tboxes is decidable for concrete domains with the ehd-property,"reasoning for description logics with concrete domains and w.r.t. general tboxes easily becomes undecidable. however, with some restriction on the concrete domain, decidability can be regained. we introduce a novel way to integrate concrete domains [dscr    ] into the well-known description logic , we call the resulting logic . we then identify sufficient conditions on [dscr    ] that guarantee decidability of the satisfiability problem, even in the presence of general tboxes. in particular, we show decidability of  for several domains over the integers, for which decidability was open. more generally, this result holds for all negation-closed concrete domains with the ehd-property, which stands for ‘the existence of a homomorphism is definable’. such technique has recently been used to show decidability of ctl* with local constraints over the integers.",nan
2016,realisability of production recipes,"there is a rising demand for customised products with a high degree of complexity. to meet these demands, manufacturing lines are increasingly becoming autonomous, networked, and intelligent, with production lines being virtualised into a manufacturing cloud, and advertised either internally to a company, or externally in a public cloud. in this paper, we present a novel approach to two key problems in such future manufacturing systems: the realisability problem (whether a product can be manufactured by a set of manufacturing resources) and the control problem (how a particular product should be manufactured). we show how both production recipes specifying the steps necessary to manufacture a particular product, and manufacturing resources and their topology can be formalised as labelled transition systems, and define a novel simulation relation which captures what it means for a recipe to be realisable on a production topology. we show how a controller that can orchestrate the resources in order to manufacture the product on the topology can be extracted from the simulation relation, and give an algorithm to compute a simulation relation and a controller.",nan
2016,towards lifelong object learning by integrating situated robot perception and semantic web mining,"autonomous robots that are to assist humans in their daily lives are required, among other things, to recognize and understand the meaning of task-related objects. however, given an open-ended set of tasks, the set of everyday objects that robots will encounter during their lifetime is not foreseeable. that is, robots have to learn and extend their knowledge about previously unknown objects on-the-job. our approach automatically acquires parts of this knowledge (e.g., the class of an object and its typical location) in form of ranked hypotheses from the semantic web using contextual information extracted from observations and experiences made by robots. thus, by integrating situated robot perception and semantic web mining, robots can continuously extend their object knowledge beyond perceptual models which allows them to reason about task-related objects, e.g., when searching for them, robots can infer the most likely object locations. an evaluation of the integrated system on long-term data from real office observations, demonstrates that generated hypotheses can effectively constrain the meaning of objects. hence, we believe that the proposed system can be an essential component in a lifelong learning framework which acquires knowledge about objects from real world observations.",nan
2016,declaratively capturing local label correlations with multi-label trees,"the goal of multi-label classification is to predict multiple labels per data point simultaneously. real-world applications tend to have high-dimensional label spaces, employing hundreds or even thousands of labels. while these labels could be predicted separately, by capturing label correlation we might achieve better predictive performance. in contrast with previous attempts in the literature that have modelled label correlations globally, this paper proposes a novel algorithm to model correlations and cluster labels locally. lacovac is a multi-label decision tree classifier that clusters labels into several dependent subsets at various points during training. the clusters are obtained locally by identifying the conditionally-dependent labels in localised regions of the feature space using the label correlation matrix. lacovac interleaves between two main decisions on the label matrix with training instances in rows and labels in columns: splitting this matrix vertically by partitioning the labels into subsets, or splitting it horizontally using features in the conventional way. experiments on 13 benchmark datasets demonstrate that our proposal achieves competitive performance over a wide range of evaluation metrics when compared with the state-of-the-art multi-label classifiers.",nan
2016,get me to my gate on time: efficiently solving general-sum bayesian threat screening games,"threat screening games (tsgs) are used in domains where there is a set of individuals or objects to screen with a limited amount of screening resources available to screen them. tsgs are broadly applicable to domains like airport passenger screening, stadium screening, cargo container screening, etc. previous work on tsgs focused only on the bayesian zero-sum case and provided the mga algorithm to solve these games. in this paper, we solve bayesian general-sum tsgs which we prove are np-hard even when exploiting a compact marginal representation. we also present an algorithm based upon a adversary type hierarchical tree decomposition and an efficient branch-and-bound search to solve bayesian generalsum tsgs. with this we provide four contributions: (1) gate, the first algorithm for solving bayesian general-sum tsgs, which uses hierarchical type trees and a novel branch-and-bound search, (2) the branch-and-guide approach which combines branch-and-bound search with the mga algorithm for the first time, (3) heuristics based on properties of tsgs for accelerated computation of gate, and (4) experimental results showing the scalability of gate needed for real-world domains.",nan
2016,false-name-proof mechanisms for path auctions in social networks,"we study path auction mechanisms for buying path between two given nodes in a social network, where edges are owned by strategic agents. the well known vcg mechanism is the unique solution that guarantees both truthfulness and efficiency. however, in social network environments, the mechanism is vulnerable to false-name manipulations where agents can profit from placing multiple bids under fictitious names. moreover, the vcg mechanism often leads to high overpayment. in this paper, we present core-selecting path mechanisms that are robust against false-name bids and address the overpayment problem. specifically, we provide a new formulation for the core, which greatly reduces the number of core constraints. based on the new formulation, we present a vickery-nearest pricing rule, which finds the core payment profile that minimizes the l∞ distance to the vcg payment profile. we prove that the vickery-nearest core payments can be computed in polynomial time by solving linear programs. our experiment results on real network datasets and reported cost dataset show that our vickery-nearest core-selecting path mechanism can reduce vcg's overpayment by about 20%.",nan
2016,multi-robot adversarial coverage,"this work discusses the problem of adversarial coverage, in which one or more robots are required to visit every point of a given area, which contains threats that might stop the robots. the objective of the robots is to cover the target area as quickly as possible, while maximizing the percentage of covered area before they are stopped. this problem has many real-world applications, from performing coverage missions in hazardous fields such as nuclear power plants, to surveillance of enemy forces in the battlefield and field demining. previous studies of the problem dealt with single-robot coverage. using a multi-robot team for the coverage has clear advantages in terms of both coverage time and robustness: even if one robot is totally damaged, others may take over its coverage subtask. hence, in this paper we describe a multi-robot coverage algorithm for adversarial environments that tries to maximize the percentage of covered area before the team is stopped, while minimizing the coverage time. we analytically show that the algorithm is robust, in that as long as a single robot is able to move, the coverage will be completed. we also establish theoretical bounds on the minimum covered area guaranteed by the algorithm and on the coverage time. lastly, we evaluate the effectiveness of the algorithm in an extensive set of environments and settings.",nan
2016,parameterized complexity results for the kemeny rule in judgment aggregation,"we investigate the parameterized complexity of computing an outcome of the kemeny rule in judgment aggregation, providing the first parameterized complexity results for this problem for any judgment aggregation procedure. as parameters, we consider (i) the number of issues, (ii) the maximum size of formulas used to represent issues, (iii) the size of the integrity constraint used to restrict the set of feasible opinions, (iv) the number of individuals, and (v) the maximum hamming distance between any two individual opinions, as well as all possible combinations of these parameters. we provide parameterized complexity results for two judgment aggregation frameworks: formula-based judgment aggregation and constraint-based judgment aggregation. whereas the classical complexity of computing an outcome of the kemeny rule in these two frameworks coincides, the parameterized complexity results differ.",nan
2016,agm-style revision of beliefs and intentions,"we introduce a logic for temporal beliefs and intentions based on shoham's database perspective and we formalize his coherence conditions on beliefs and intentions. in order to do this we separate strong beliefs from weak beliefs. strong beliefs are independent from intentions, while weak beliefs are obtained by adding intentions to strong beliefs and everything that follows from that. we provide agm-style postulates for the revision of strong beliefs and intentions: strong belief revision may trigger intention revision, but intention revision may only trigger revision of weak beliefs. after revision, the strong beliefs are coherent with the intentions. we show in a representation theorem that a revision operator satisfying our postulates can be represented by a pre-order on interpretations of the beliefs, together with a selection function for the intentions.",nan
2016,facility location games with optional preference,"in this paper, we propose the optional preference model for the facility location game with two heterogeneous facilities on a line. agents in this new model are allowed to have optional preference, which gives more flexibility for agents to report. aiming at minimizing maximum cost or sum cost of agents, we propose different deterministic strategy-proof mechanisms without monetary transfers. depending on which facility the agent with optional preference cares for, we consider two variants of the optional preference model: min (caring for the closer one) and max (caring for the further one). for the min variant, we propose a 2-approximation mechanism for the maximum cost objective, as well as a lower bound of 4/3, and a (n/2+1)-approximation mechanism for the sum cost objective, as well as a lower bound of 2. for max variant, we propose an optimal mechanism for the maximum cost objective and a 2-approximation mechanism for the sum cost objective.",nan
2016,iterative judgment aggregation,"judgment aggregation problems form a class of collective decision-making problems represented in an abstract way, subsuming some well known problems such as voting. a collective decision can be reached in many ways, but a direct one-step aggregation of individual decisions is arguably most studied. another way to reach collective decisions is by iterative consensus building – allowing each decision-maker to change their individual decision in response to the choices of the other agents until a consensus is reached. iterative consensus building has so far only been studied for voting problems. here we propose an iterative judgment aggregation algorithm, based on movements in an undirected graph, and we study for which instances it terminates with a consensus. we also compare the computational complexity of our itterative procedure with that of related judgment aggregation operators.",nan
2016,partial order temporal plan merging for mobile robot tasks,"for many mobile service robot applications, planning problems are based on deciding how and when to navigate to certain locations and execute certain tasks. typically, many of these tasks are independent from one another, and the main objective is to obtain plans that efficiently take into account where these tasks can be executed and when execution is allowed. in this paper, we present an approach, based on merging of partial order plans with durative actions, that can quickly and effectively generate a plan for a set of independent goals. this plan exploits some of the synergies of the plans for each single task, such as common locations where certain actions should be executed. we evaluate our approach in benchmarking domains, comparing it with state-of-the-art planners and showing how it provides a good trade-off between the approach of sequencing the plans for each task (which is fast but produces poor results), and the approach of planning for a conjunction of all the goals (which is slow but produces good results).",nan
2016,towards online concept drift detection with feature selection for data stream classification,"data streams are unbounded, sequential data instances that are generated very rapidly. the storage, querying and mining of such rapid flows of data is computationally very challenging. data stream mining (dsm) is concerned with the mining of such data streams in real-time using techniques that require only one pass through the data. dsm techniques need to be adaptive to reflect changes of the pattern encoded in the stream (concept drift). the relevance of features for a dsm classification task may change due to concept drifts and this paper describes the first step towards a concept drift detection method with online feature tracking capabilities.",nan
2016,towards sparql-based induction for large-scale rdf data sets,"we show how to convert owl class expressions to sparql queries where the instances of that concept are with a specific abox equal to the sparql query result. furthermore, we implement and integrate our converter into the celoe algorithm (class expression learning for ontology engineering), where it replaces the position of a traditional owl reasoner. this will foster the application of structured machine learning to the semantic web, since most data is readily available in triple stores. we provide experimental evidence for the usefulness of the bridge. in particular, we show that we can improve the run time of machine learning approaches by several orders of magnitude.",nan
2016,burg matrix divergence based multi-metric learning,"the basic idea of most distance metric learning methods is to find a space that can optimally classify data points belong to different categories. however, current methods only learn one mahalanobis distance for each data set, which actually fails to perfectly classify different categories in most real world applications. to improve the classification accuracy of k-nearest-neighbour algorithm, a multi-metric learning method is proposed in this paper to completely classify different categories by sequentially learning sub-metrics. the proposed algorithm is based on minimizing the burg matrix divergence between metrics. the experiments on five uci data sets demonstrate the improved performance of multi-metric learning when comparing with the state-of-the-art methods.",nan
2016,a novel approach of applying the differential evolution to spatial discrete data,"the differential evolution (de) is a powerful bio-inspired algorithm searching optimal solutions. the actual de modifications can handle the real, integer and discrete valued problems. the values of the discrete-valued variables represent the integer indices addressing the discrete samples in the ordered array. the optimization in unordered samples leads to a random search. this paper proposes a novel modification dealing with d-dimensional discrete vertices. a vertex hashing is used to strengthen the local properties of a dataset and to improve the spatial convergence of the evolution.",nan
2016,topic-level influencers identification in the microblog sphere,"this paper studies the problem of identifying influencers on specific topics in the microblog sphere. prior works usually use the cumulative number of social links to measure users' topic-level influence, which ignores the dynamics of influence. as a result, they usually find faded influencers. to address the limitations of prior methods, we propose a novel probabilistic generative model to capture the variation of influence over time. then a influence decay method is proposed to measure users' current topic-level influence.",nan
2016,multilevel agent-based modelling for assignment or matching problems,"assignment or matching problems have been addressed by various multi-agent methods, focused on enhancing privacy and distribution. nevertheless, they little rely on the organisational structure provided by multi-agent systems (mas). we rather start from the intrinsic ability of multilevel mas to represent intermediate points of view between the individual and the collective levels, to express matching or assignment problems in a homogeneous formalism. this model allows to define relevant metrics to assess the satisfaction of agent groups and allow them to build solutions that improve the overall well-being without disclosing all their individual information.",nan
2016,simple epistemic planning: generalised gossiping,"the gossip problem, in which information (secrets) must be shared among a certain number of agents using the minimum number of calls, is of interest in the conception of communication networks and protocols. we extend the gossip problem to arbitrary epistemic depths. for example, we may require not only that all agents know all secrets but also that all agents know that all agents know all secrets. we give optimal protocols for the generalised gossip problem, in the case of two-way communications, one-way communications and parallel communication. in the presence of negative goals testing the existence of a successful protocol is np-complete.",nan
2016,a general characterization of model-based diagnosis,"the model-based diagnosis (mbd) framework developed by reiter has been a strong theoretical foundation for mbd, yet is limited to models that are described in terms of logical sentences. we propose a more general framework that covers a wide range of modelling languages, ranging from ai-based languages (e.g., logic and bayesian networks) to fdi-based languages (e.g., linear gaussian models). we show that a graph-theoretic basis for decomposable system models can be augmented with several languages and corresponding inference algorithms based on valuation algebras.",nan
2016,on the impact of subproblem orderings on anytime and/or best-first search for lower bounds,"best-first search can be regarded as anytime scheme for producing lower bounds on the optimal solution, a characteristic that is mostly overlooked. we explore this topic in the context of and/or best-first search, guided by the mbe heuristic, when solving graphical models. in that context, the impact of the secondary heuristic for subproblem ordering may be significant, especially in the anytime context. indeed, our paper illustrates this, showing that the new concept of bucket errors can advise in providing effective subproblem orderings in and/or search.",nan
2016,salient region detection based on the global contrast combining background measure for indoor robots,"in this paper, we propose a new method of salient region detection for indoor robots, which integrate the background distribution into the primary saliency. region roundness is proposed to describe the compactness of a region to measure background distribution more robustly. in order to validate the proposed method, several influential ones are compared on the dsd dataset. the results demonstrate that the proposed approach outperforms existing methods and is useful for indoor robots.",nan
2016,semi-supervised learning on an augmented graph with class labels,"in this paper, we propose a novel graph-based method for semi-supervised learning. our method runs a diffusion-based affinity learning algorithm on an augmented graph consisting of not only the nodes of labeled and unlabeled data but also artificial nodes representing class labels. the learned affinities between unlabeled data and class labels are used for classification. our method achieves superior results on many standard data sets.",nan
2016,identifying and rewarding subcrowds in crowdsourcing,"identifying and rewarding truthful workers are key to the sustainability of crowdsourcing platforms. in this paper, we present a clustering based rewarding mechanism that rewards workers based on their truthfulness while accommodating the differences in workers' preferences. experimental results show that the proposed approach can effectively discover subcrowds under various conditions, and truthful workers are better rewarded than less truthful ones.",nan
2016,data set operations to hide decision tree rules,this paper focuses on preserving the privacy of sensitive patterns when inducing decision trees. our record augmentation approach for hiding sensitive classification rules in binary datasets is preferred over other heuristic solutions like output perturbation or cryptographic techniques since the raw data itself is readily available for public use. we describe the process and an indicative experiment using a prototype hiding tool.,nan
2016,evolutionary agent-based modeling of past societies' organization structure,"in this work, we extend a generic agent-based model for simulating ancient societies, by blending, for the first time, evolutionary game theory with multiagent systems' self-organization. our approach models the evolution of social behaviours in a population of strategically interacting agents corresponding to households in the early minoan era. to this end, agents participate in repeated games by means of which they exchange utility (corresponding to resources) with others. the results of the games contribute to both the continuous re-organization of the social structure, and the progressive adoption of the most successful agent strategies. agent population is not fixed, but fluctuates over time. the particularity of the domain necessitates that agents in our games receive non-static payoffs, in contrast to most games studied in the literature; and that the evolutionary dynamics are formulated via assessing the perceived fitness of the agents, defined in terms of how successful they are in accumulating utility. our results show that societies of strategic agents that self-organize via adopting the aforementioned evolutionary approach, demonstrate a sustainability that largely matches that of self-organizing societies of more cooperative agents; and that strategic cooperation is in fact, in many instances, an emergent behaviour in this domain.",nan
2016,strategic path planning allowing on-the-fly updates,"this work deals with the problem of strategic path planning while avoiding detection by a mobile adversary. in this problem, an evading agent is placed on a graph, where one or more nodes are defined as safehouses. the agent's goal is to find a path from its current location to a safehouse, while minimizing the probability of meeting a mobile adversarial agent at a node along its path (i.e., being captured). we examine several models of this problem, where each one has different assumptions on what the agents know about their opponent, all using a framework for computing node utility. we use several risk attitudes for computing the utility values, whose impact on the actual performance of the path planning algorithms is highlighted by an empirical analysis. furthermore, we allow the agents to use information gained along their movement, in order to efficiently update their motion strategies on-the-fly. analytic and empiric analysis show that on-the-fly updates increase the probability that our agent reaches its destination safely.",nan
2016,finding diverse high-quality plans for hypothesis generation,"in this paper, we address the problem of finding diverse high-quality plans motivated by the hypothesis generation problem. to this end, we present a planner called tk* that first efficiently solves the “top-k” cost-optimal planning problem to find k best plans, followed by clustering to produce diverse plans as cluster representatives.",nan
2016,using a deep understanding of network activities for network vulnerability assessment,"in data-communication networks, network reliability is of great concern to both network operators and customers. therefore, network operators want to determine what services could be affected by software vulnerabilities being exploited that are present within their data-communication network. to determine what services could be affected by a software vulnerability being exploited, it is fundamentally important to know the ongoing tasks in a network. a particular task may depend on multiple network services, spanning many network devices. unfortunately, dependency details are often not documented and are difficult to discover by relying on human expert knowledge. in monitored networks huge amounts of data are available and by applying data mining techniques, we are able to extract information of ongoing network activities. from a data mining perspective, we are interested to test the potential of applying data mining techniques to real-life applications.",nan
2016,all-transfer learning for deep neural networks and its application to sepsis classification,"in this article, we propose a transfer learning method for deep neural networks (dnns). deep learning has been widely used in many applications. however, applying deep learning is problematic when a large amount of training data are not available. one of the conventional methods for solving this problem is transfer learning for dnns. in the field of image recognition, state-of-the-art transfer learning methods for dnns re-use parameters trained on source domain data except for the output layer. however, this method may result in poor classification performance when the amount of target domain data is significantly small. to address this problem, we propose a method called all-transfer deep learning, which enables the transfer of all parameters of a dnn. with this method, we can compute the relationship between the source and target labels by the source domain knowledge. we applied our method to actual two-dimensional electrophoresis image (tdei) classification for determining if an individual suffers from sepsis; the first attempt to apply a classification approach to tdeis for proteomics, which has attracted considerable attention as an extension beyond genomics. the results suggest that our proposed method outperforms conventional transfer learning methods for dnns.",nan
2016,computing extensions' probabilities in probabilistic abstract argumentation: beyond independence,"we characterize the complexity of the problem of computing the probabilities of the extensions in probabilistic abstract argumentation. we consider all the most popular semantics of extensions (admissible, stable, preferred, complete, grounded, ideal-set, ideal and semi-stable) and different forms of correlations that can be defined between arguments and defeats. we show that the complexity of the problem ranges from fp to fp#p-complete, with fp||np-complete cases, depending on the semantics of the extensions and the imposed correlations.",nan
2016,explained activity recognition with computational assumption-based argumentation,"activity recognition is a key problem in multi-sensor systems. in this work, we introduce computational assumption-based argumentation, an argumentation approach that seamlessly combines sensor data processing with high-level inference. our method gives classification results comparable to machine learning based approaches with reduced training time while also giving explanations.",nan
2016,encoding cryptographic functions to sat using transalg system,"in this paper we propose the technology for constructing propositional encodings of discrete functions. it is aimed at solving inversion problems of considered functions using state-of-the-art sat solvers. we implemented this technology in the form of the software system called transalg, and used it to construct sat encodings for a number of cryptanalysis problems. by applying sat solvers to these encodings we managed to invert several cryptographic functions. in particular, we used the sat encodings produced by transalg to construct the family of two-block md5 collisions in which the first 10 bytes are zeros. in addition to that we used transalg encoding for the widely known a5/1 keystream generator to solve several dozen of its cryptanalysis instances in a distributed computing environment. also in the present paper we compare the functionality of transalg with that of similar software systems.",nan
2016,explanatory diagnosis of an ontology stream via reasoning about actions,"explanatory diagnosis of an ontology stream aims to explain the changes hidden in the ontology stream by a sequence of actions. in this paper, we present a framework for explanatory diagnosis of an ontology stream, which allows the actions to be uncertain. in order to capture the semantics of actions, we introduce a new update operator and effect-guided bold-repair. by combining these operators with a query mechanism of description logics  supporting inconsistency-tolerant semantics, we present a formal definition for the explanatory diagnosis problem of ontology streams.",nan
2016,dardis: distributed and randomized dispatching and scheduling,"scheduling and dispatching are critical enabling technologies in supercomputing and grid computing. in these contexts, scalability is an issue: we have to allocate and schedule up to tens of thousands of tasks on tens of thousands of resources. this problem scale is out of reach for complete and centralized scheduling approaches.we propose a distributed allocation and scheduling paradigm called dardis that is lightweight, scalable and fully customizable in many domains. in dardis each task offloads to the available resources the computation of a probability index associated with each possible start time for the given task on the specific resource. the task then selects the proper resource and start time on the basis of the above probability.",nan
2016,scaling structure learning of probabilistic logic programs by mapreduce,"probabilistic logic programming is a promising formalism for dealing with uncertainty. learning probabilistic logic programs has been receiving an increasing attention in inductive logic programming: for instancethe system slipcover learns high quality theories in a variety of domains. howeverslipcover is computationally expensivewith a running time of the order of hours. in order to apply slipcover to big data, we present sempre, for “structure learning by mapreduce”, that scales slipcover by following a mapreduce strategy, directly implemented with the message passing interface.",nan
2016,employing hypergraphs for efficient coalition formation with application to the v2g problem,"this paper proposes, for the first time in the literature, the use of hypergraphs for the efficient formation of effective coalitions. we put forward several formation methods that build on existing hypergraph algorithms, and exploit hypergraph structure to identify agents with desirable characteristics. our approach allows the near-instantaneous formation of high quality coalitions, while adhering to multiple stated requirements regarding coalition quality. moreover, our methods are shown to scale to dozens of thousands of agents within fractions of a second; with one of them scaling to even millions of agents within seconds. we apply our approach to the problem of forming coalitions to provide (electric) vehicle-to-grid (v2g) services. ours is the first approach able to deal with large-scale, realtime coalition formation for the v2g problem, while taking multiple criteria into account for creating electric vehicle coalitions.",nan
2016,increasing coalition stability in large-scale coalition formation with self-interested agents,"in coalition formation with self-interested agents both social welfare of the multi-agent system and stability of individual coalitions must be taken into account. however, in large-scale systems with thousands of agents, finding an optimal solution with respect to both metrics is infeasible.in this paper we propose an approach for finding coalition structures with suboptimal social welfare and coalition stability in large-scale multi-agent systems. our approach uses multi-agent simulation to model a dynamic coalition formation process. agents are allowed to deviate from unstable coalitions, thus increasing the coalition stability. furthermore we present an approach for estimating coalition stability, which alleviates exponential complexity of coalition stability computation. this approach is used for estimating stability of multiple coalition structures generated by the multi-agent simulation, which enables us to select a solution with high values of both social welfare and coalition stability. we experimentally show that our algorithms cause a major increase in coalition stability compared to a baseline social welfare-maximizing algorithm, while maintaining a very small decrease in social welfare.",nan
2016,bagged boosted trees for classification of ecological momentary assessment data,"ecological momentary assessment (ema) data is organized in multiple levels (per-subject, per-day, etc.) and this particular structure should be taken into account in machine learning algorithms used in ema like decision trees and its variants. we propose a new algorithm called bbt (standing for bagged boosted trees) that is enhanced by a over/under sampling method and can provide better estimates for the conditional class probability function. experimental results on a real-world dataset show that bbt can benefit ema data classification and performance.",nan
2016,reputation in the academic world,"this paper proposes a computational model based on peer reviews for assessing the reputation of researchers and research work. we argue that by relying on peer opinions, we address some of the pitfalls of current approaches for calculating the reputation of authors and papers. we also introduce a much needed feature for review management: calculating the reputation of reviews and reviewers.",nan
2016,collective future orientation and stock markets,"web search query logs can be used to track and, in some cases, anticipate the dynamics of individual behavior which is the smallest building block of the economy. we study aol query logs and introduce a collective future intent index to measure the degree to which internet users seek more information about the future than the past and the present. we have asked the question whether there is link between the collective future intent index and financial market fluctuations on a weekly time scale, and found a clear indication that the weekly transaction volume of s&p 500 index is correlated with the collective intent of the public to look forward.",nan
2016,learning of classification models from noisy soft-labels,we develop and test a new classification model learning algorithm that relies on the soft-label information and that is able to learn classification models more rapidly and with a smaller number of labeled instances than existing approaches.,nan
2016,distributed learning in expert referral networks,"human experts or autonomous agents in a referral network must decide whether to accept a task or refer to a more appropriate expert, and if so to whom. in order for the referral network to improve over time, the experts must learn to estimate the topical expertise of other experts. this paper extends concepts from reinforcement learning and active learning to referral networks, to learn how to refer at the network level, based on the proposed distributed interval estimation learning (diel) algorithm. diverse monte carlo simulations reveal that diel improves network performance significantly over both greedy and q-learning baselines [3], approaching optimal given enough data.",nan
2016,transfer learning for automatic short answer grading,"automatic short answer grading (asag) is the task of automatically grading students answers which are a few words to a few sentences long. while supervised machine learning techniques (classification, regression) have been successfully applied for asag, they suffer from the constant need of instructor graded answers as labelled data. in this paper, we propose a transfer learning based technique for asag built on an ensemble of text classifier of student answers and a classifier using numeric features derived from various similarity measures with respect to instructor provided model answers. we present preliminary empirical results to demonstrate efficacy of the proposed technique.",nan
2016,long-time sensor data analysis for estimation of physical capacity,"in this paper, we present a feature learning method for long-time sensor data. although feature learning methods have been successfully used in many applications, they cannot extract features efficiently when the dimension of training data is quite large. to address this problem, we propose a method to search effective features from long-time sensor data. the important characteristic of our method is that it searches the features based on the gradient of input vectors to minimize the objective function of the learning algorithm. we apply our method to the estimation of physical capacity from wearable sensor data. the experimental results show that our method can estimate leg muscle strength more accurately than conventional methods using a feature learning method and current clinical index.",nan
2016,enhancing sketch-based image retrieval via deep discriminative representation,in this paper we aim to employ deep learning to enhance sbir via deep discriminative representation. our main contributions focus on: 1) the deep discriminative representation is established to bridge both the visual appearance gap and the semantic gap between sketches and images; 2) the deep learning pattern is applied to our sbir model through training on our transformed sketch-like images to overcome the rarity of training sketches. our experiments on a large number of public sketch and image data have obtained very positive results.,nan
2016,structure in the value function of two-player zero-sum games of incomplete information,"in this paper, we introduce a new formulation for the value function of a zero-sum partially observable stochastic game (zs-posg) in terms of a ‘plan-time sufficient statistic’, a distribution over joint sets of information. we prove that this value function exhibits concavity and convexity with respect to appropriately chosen subspaces of the statistic space. we anticipate that this result is a key pre-cursor for developing solution methods that exploit such structure. finally, we show that the formulation allow us to reduce a finite zs-posg to a ‘centralized’ model with shared observations, thereby transferring results for the latter (narrower) class of games to games with individual observations.",nan
2016,gdl-iii: a proposal to extend the game description language to general epistemic games,"we propose an extension of the standard game description language for general game playing to include epistemic games, which are characterised by rules that depend on the knowledge of players. a single additional keyword suffices to define gdl-iii, a general description language for games with imperfect information and introspection. we present an answer set program for automatically reasoning about gdl-iii games. our extended language along with a suitable basic reasoning system can also be used to formalise and solve general epistemic puzzles.",nan
2016,impact of automated action labeling in classification of human actions in rgb-d videos,"for many applications it is important to be able to detect what a human is currently doing. this ability is useful for applications such as surveillance, human computer interfaces, games and health-care. in order to recognize a human action, the typical approach is to use manually labeled data to perform supervised training. this paper aims to compare the performance of several supervised classifiers trained with manually labeled data versus the same classifiers trained with data automatically labeled. in this paper we propose a framework capable of recognizing human actions using supervised classifiers trained with automatically labeled data in rgb-d videos.",nan
2016,transductive learning for the identification of word sense temporal orientation,"the ability to capture the time information conveyed in natural language is essential to many natural language processing applications such as information retrieval, question answering, automatic summarization, targeted marketing, loan repayment forecasting, and understanding economic patterns. in this paper, we propose a graph-based semi-supervised classification strategy that makes use of wordnet definitions or ‘glosses’, its conceptual-semantic and lexical relations to supplement wordnet entries with information on the temporality of its word senses. intrinsic evaluation results show that the proposed approach outperforms prior semi-supervised, non-graph classification approaches to the temporality recognition of word senses, and confirm the soundness of the proposed approach.",nan
2016,learning a bayesian network classifier by jointly maximizing accuracy and information,"although recent studies have shown that a bayesian network classifier (bnc) that maximizes the classification accuracy (i.e., minimizes the 0/1 loss function) is a powerful tool in knowledge representation and classification, this classifier focuses on the majority class, is usually uninformative about the distribution of misclassifications, and is insensitive to error severity (making no distinction between misclassification types). we propose to learn a bnc using an information measure (im) that jointly maximizes classification and information, and evaluate this measure using various databases. we show that an im-based bnc is superior to bncs learned using other measures, especially for ordinal classification and imbalanced problems, and does not fall behind state-of-the-art algorithms with respect to accuracy and amount of information provided.",nan
2016,transfer of reinforcement learning negotiation policies: from bilateral to multilateral scenarios,"trading and negotiation dialogue capabilities have been identified as important in a variety of ai application areas. in prior work, it was shown how reinforcement learning (rl) agents in bilateral negotiations can learn to use manipulation in dialogue to deceive adversaries in non-cooperative trading games. in this paper we show that such trained policies can also be used effectively for multilateral negotiations, and can even outperform those which are trained in these multilateral environments. ultimately, it is shown that training in simple bilateral environments (e.g. a generic version of “catan”) may suffice for complex multilateral non-cooperative trading scenarios (e.g. the full version of catan).",nan
2016,substantive irrationality in cognitive systems,in this paper we approach both procedural and substantive irrationality of artificial agent cognitive systems and consider that when it is not possible for an agent to make a logical inference (too expensive cognitive effort or not enough knowledge) she might replace certain parts of the logical reasoning with mere associations.,nan
2016,a history tree heuristic to generate better initiation sets for options in reinforcement learning,"options framework is a prominent way to improve learning speed by means of temporally extended actions, called options. although various attempts focusing on how to derive high quality termination conditions for options exist, the impact of initiation set generation of an option is relatively unexplored. in this work, we propose an effective heuristic method to derive useful initiation set elements via an analysis of the recent history of events.",nan
2016,on truthful auction mechanisms for electricity allocation,"as technology evolves and electricity demand rises, more and more research focus on the efficient electricity allocation mechanisms so as to make consumer demand adaptive to the supply of electricity at all times. in this paper, we formulate the problem of electricity allocation as a novel combinatorial auction model, and then put forward a directly applicable mechanisms. it is proven that the proposed mechanism is equipped with some useful economic properties and computational traceability. our works offer potential avenues for the stduy about efficient electricity allocation methods in smart grid.",nan
2016,towards a bdi player model for interactive narratives,"player modelling is one of the challenges in interactive narratives (ins), where a precise representation of the players mental state is needed to provide a personalised experience. however, how to represent the interaction of the player with the game to make the appropriate decision in the story is still an open question. in this paper, we aim to bridge this gap identifying the information needed to capture the players interaction with an in using the belief-desire-intention (bdi) model of agency. we present a bdi design to mimic a players interaction with a simplified version of the interactive fiction anchorhead.",nan
2016,a typicality-based revision to handle exceptions in description logics,"we propose a methodology to revise a description logic knowledge base when detecting exceptions. our approach relies on the methodology for debugging a description logic terminology, addressing the problem of diagnosing inconsistent ontologies by identifying a minimal subset of axioms responsible for an inconsistency. in the approach we propose, once the source of the inconsistency has been localized, the identified axioms are revised in order to obtain a consistent knowledge base including the detected exception about an individual x. to this aim, we make use of a nonmonotonic extension of the description logic alc based on the combination of a typicality operator and the well established nonmonotonic mechanism of rational closure, which allows to deal with prototypical properties and defeasible inheritance.",nan
2016,a new stochastic local search approach for computing preferred extensions of abstract argumentation,"in this paper, we proposed a new stochastic local search algorithm inc-ccaep for computing the preferred extensions in (abstract) argumentation frameworks (af). inc-ccaep realizes an incremental version of swcca, specially designed for computing the preferred extensions in af. experiments show that, inc-ccaep notably outperforms the state-of-the-art solvers consistently on random benchmarks with non-empty preferred extensions.",nan
2016,crowdsourced referral auctions,"motivated by web based marketplaces where the number of bidders in an auction is a small subset of potential bidders, we consider auctions where the auctioneer (seller) wishes to increase her revenue and/or social welfare by expanding the pool of participants. to this end, the seller crowdsources this task by offering a referral bonus to the participants. with the introduction of referrals, a participant can now bid and/or refer other agents to bid. we call our auctions crowdsourced referral auctions since the seller exploits the knowledge that agents have about other potential participants in the crowd. we introduce the notion of price of locality to quantify the loss in social welfare due to restricted (local) access of the seller to potential bidders. we introduce the notion of crowdsourced referral auction mechanisms (crams), propose two novel versions of crams and study the induced referral game in the canonical context of an auction for selling a single indivisible item. we compare their revenue performance and game theoretic properties and show that both of them outperform the baseline auction without referrals.",nan
2016,minisum and minimax committee election rules for general preference types,"in committee elections it is often assumed that voters only (dis)approve of each candidate or that they rank all candidates, as it is common for single-winner elections. we suggest an intermediate approach, where the voters rank the candidates into a fixed number of groups. this allows more diverse votes than approval votes, but leaves more freedom than in a linear order. a committee is then elected by applying the minisum or minimax approach to minimize the voters' dissatisfaction. we study the axiomatic properties of these committee election rules as well as the complexity of winner determination and show fixed-parameter tractability for our minimax rules.",nan
2016,space debris removal: a game theoretic analysis,"we analyse active space debris removal efforts from a strategic, game-theoretic perspective. an active debris removal mission is a costly endeavour that has a positive effect (or risk reduction) for all satellites in the same orbital band. this leads to a dilemma: each actor (space agency, private stakeholder, etc.) has an incentive to delay its actions and wait for others to respond. the risk of the latter action is that, if everyone waits the joint outcome will be catastrophic leading to what in game theory is referred to as the ‘tragedy of the commons’. we introduce and thoroughly analyse this dilemma using simulation and empirical game theory in a two player setting.",nan
2016,"cuilt: a scalable, mix-and-match framework for local iterative approximate best-response algorithms","we implement cuilt, a scalable mix-and-match framework for local iterative approximate best-response algorithms for dcops, using the graph processing framework signal/collect, where each agent is modeled as a vertex and communication pathways are represented as edges. choosing this abstraction allows us to exploit the generic graph-oriented distribution/optimization heuristics and makes our proposed framework scalable, configurable, as well as extensible. we found that this approach allows us to scale to problems more than 3 orders of magnitude larger than results commonly published so far, to easily create hybrid algorithms by mixing components, and to run the algorithms fast, in a parallel fashion.",nan
2016,not being at odds with a class: a new way of exploiting neighbors for classification,"classification can be viewed as a matter of associating a new item with the class where it is the least at odds w.r.t. the other elements. a recently proposed oddness index applied to pairs or triples (rather than larger subsets of elements in a class), when summed up over all such subsets, provides an accurate estimate of a global oddness of an item w.r.t. a class. rather than considering all pairs in a class, one can only deal with pairs containing one of the nearest neighbors of the item in the target class. taking a step further, we choose the second element in the pair as another nearest neighbor in the class. the oddness w.r.t. a class computed on the basis of pairs made of two nearest neighbors leads to low complexity classifiers, still competitive in terms of accuracy w.r.t. classical approaches.",nan
2016,value-based reasoning and norms,"norms are designed to guide choice of actions. value-based practical reasoning is an approach to explaining and justifying choice of actions in terms of value preferences. here we explore how value-based practical reasoning can be related to norms and their evolution. starting from a basic model of a society and the norms that can arise from it, we consider how additional values, and a more sophisticated model, with more detailed states and a history, and a finer grained description of actions, can accommodate more complex norms, and a correspondingly more complex social order.",nan
2016,using recursive neural networks to detect and classify drug-drug interactions from biomedical texts,"the purpose of this paper is to explore in detail how a recursive neural network can be applied to classify drug-drug interactions from biomedical texts. the system is based on mv-rnn, a matrix-vector recursive neural network, built from the stanford constituency trees of sentences. drug-drug interactions are usually described by long sentences with complex structures (such as subordinate clauses, oppositions, and coordinate structures, among others). our experiments show a low performance that may be probably due to the parser not being able to capture the structural complexity of sentences in the biomedical domain.",nan
2016,efficient computation of deterministic extensions for dynamic abstract argumentation frameworks,"we address the problem of efficiently computing the extensions of abstract argumentation frameworks (afs) which are updated by adding/deleting arguments or attacks. we focus on the two most popular ‘deterministic’ semantics (namely, grounded and ideal) and present two approaches for their incremental computation, well-suited to dynamic applications where updates to an initial af are frequently performed to take into account new available knowledge.",nan
2016,the post-modern homunculus,"throughout the ages, magicians, scientists and charlatans have created life-like artifacts, some purported to be intelligent. in one famous case, the chess player, the intelligence was a little person hidden inside doing the thinking. analogously, throughout the history of philosophy, and cognition, theories have arisen to explain intelligence in humans, but a philosophical problem with many such explanations is that they use what is called a homunculus argument – the explanation, upon scrutiny reveals a “little one” (homunculus) in the proposed mental apparatus that is responsible for thinking. for most of the era of computing, the imitation game, as so simply yet subtly put forward by alan turing, has been considered the gold standard for measuring this mysterious quantity, though recently hector levesque has pointedly argued that the time has come to abandon turing's test for a better one of his own design, which he describes in a series of acclaimed papers. in particular, we argue that levesque, who has cleverly found the ‘homunculus’ in the arguments of others, has essentially regressed the problem of intelligence to a homunculus in his own system.",nan
2016,an efficient and expressive similarity measure for relational clustering using neighbourhood trees,"clustering is an underspecified task: there are no universal criteria for what makes a good clustering. this is especially true for relational data, where similarity can be based on the features of individuals, the relationships between them, or a mix of both. existing methods for relational clustering have strong and often implicit biases in this respect. in this paper, we introduce a novel similarity measure for relational data. it is the first measure to incorporate a wide variety of types of similarity, including similarity of attributes, similarity of relational context, and proximity in a hypergraph. we experimentally evaluate how using this similarity affects the quality of clustering on very different types of datasets. the experiments demonstrate that (a) using this similarity in standard clustering methods consistently gives good results, whereas other measures work well only on datasets that match their bias; and (b) on most datasets, the novel similarity outperforms even the best among the existing ones.",nan
2016,on inconsistency measuring and resolving,"in dealing with the field of inconsistency, measuring and resolving conflicts are two interesting concepts allowing to make a better reasoning. in this paper, we address these two essential questions. firstly, we propose a new framework for inconsistency handling based on independent sets of muses. in fact, we propose to rank the knowledge bases using a sequence of number that represents independent sets of muses. such sequence is able to catch more finely the structure of muses. moreover, an inconsistency measure is instantiated based on such sequence. secondly, we address the question of inconsistency resolving through hitting set deletion. in this case we show the interesting points of our new approach which selects hitting sets covering minimum subset of muses. indeed, a framework is defined for the computation of such hitting sets. furthermore, a derived underlying decision problem is defined. a special case of such decision problem is known to be defined in the literature as sim-unsat and its complexity is still open.",nan
2016,a fuzzy semantic cep model for situation identification in smart homes,"uncertainty is an essential issue for smart home applications. events generated from sensors can be outdated, inaccurate, imprecise or in contradiction with other ones. these unreliable data can lead to dysfunction in smart home applications. to tackle these challenges, we propose a new model named fscep (fuzzy semantic complex event processing) that integrates fuzzy logic paradigm, semantic features through an ontology and traditional cep. we confronted fscep with other works tackling uncertainty for cep and experimented it through simulation with early but promising result.",nan
2016,multi-context systems in time,"in this paper we consider how to enhance flexibility and generality in multi-context systems (mcs) by considering that contexts can evolve over time, that bridge-rule application can be proactive (according to a context's specific choice), and not instantaneous but requiring an execution mechanism. we introduce bridge-rule patterns to make bridge-rules parametric w.r.t. the involved contexts.",nan
2016,speech emotion recognition using voiced segment selection algorithm,"speech emotion recognition (ser) poses one of the major challenges in human-machine interaction. we propose a new algorithm, the voiced segment selection (vss) algorithm, which can produce an accurate segmentation of speech signals. the vss algorithm deals with the voiced signal segment as the texture image processing feature which is different from the traditional method. it uses the log-gabor filters to extract the voiced and unvoiced features from spectrogram to make the classification. the finding shows that the vss method is a more accurate algorithm for voiced segment detection. therefore, it has potential to improve performance of emotion recognition from speech.",nan
2016,scalable exact map inference in graphical models,"this paper presents parallel dovetailing in a distributed-memory environment for exact map inference in graphical models. parallel dovetailing is a simple procedure which performs multiple searches in parallel with different parameter configurations. we evaluate empirically the performance of parallel dovetailing with three state-of-the-art and/or search algorithms in solving various map inference benchmarks. our results clearly show that parallel dovetailing is effective, yielding considerable speedups and improving the solving abilities of these state-of-the-art baseline methods.",nan
2016,hiding actions in concurrent games,"we study a class of determined two-player reachability games, played by player0 and player1 under imperfect information. precisely, we consider the case in which player0 wins the game if player1 cannot prevent him from reaching a target state. we show that the problem of deciding such a game is exptime-complete.",nan
2016,shape invariant formulation for change point models in multiple dimensions,"artificial intelligence has achieved superhuman performance in a variety of tasks. unfortunately, this is often done without interpretable methods. in medicine, it is not sufficient to have an algorithm with maximum accuracy. the methods must be evaluated by experts. our motivating task is to detect features of the eye using unlabeled data. we detect features using a bayesian changepoint model. changepoint detection can perform object recognition when applied to two dimensional feature spaces such as images. we present a formula for detecting any shape with the changepoint model. we then extend it to multiple features in order to capture the color of an object. the work presented here has the ability to incorporate prior information, the ability to handle images of varying granularity, can provide confidence estimates on object features in an image. it will serve as the foundation for a classification method with interpretable results.",nan
2016,shaping proto-value functions using rewards,"in reinforcement learning (rl), an important sub-problem is learning the value function, which is chiefly influenced by the architecture used to represent value functions. is often expressed as a linear combination of a pre-selected set of basis functions. these basis functions are either selected in an ad-hoc manner or are tailored to the rl task using the domain knowledge. selecting basis functions in an ad-hoc manner does not give a good approximation of value function while choosing functions using domain knowledge introduces dependency on the task. thus, a desirable scenario is to have a method to choose basis functions that are task independent, but which also provide a good approximation for the value function. in this paper, we propose a novel task-independent basis function construction method that uses the topology of the underlying state space and the reward structure to build the reward-based proto value functions (rpvfs). the approach we propose gives good approximation for the value function and enhanced learning performance. the performance is demonstrated via experiments on grid-world tasks.",nan
2016,heuristic constraint answer set programming,"constraint answer set programming (casp) is a family of hybrid approaches integrating answer set programming (asp) and constraint programming (cp). these hybrid approaches have already proven to be very successful in various domains. in this paper we present first evaluation results for the casp solver ascass, which provides novel methods for defining and exploiting problem-dependent search heuristics. beyond the possibility of using already built-in problem-independent heuristics, ascass allows on the asp level the definition of problem-dependent variable selection, value selection and pruning strategies, which guide the search of the cp solver. the proof-of-concept evaluation was carried out on benchmark instances of the real world partner units problem (pup). due to a sophisticated heuristic, which cannot be represented by other asp or casp solvers, ascass shows superior performance.",nan
2016,non-deterministic planning with numeric uncertainty,"uncertainty arises in many compelling real-world applications of planning. there is a large body of work on propositional uncertainty where actions have non-deterministic outcomes. however handling numeric uncertainty has been given less consideration. in this paper, we present a novel offline policy-building approach for problems with numeric uncertainty. in particular, inspired by the planner prp, we define a numeric constraint representation that captures only relevant numeric information, supporting a more compact policy representation. we also show how numeric dead ends can be generalised to avoid redundant search. empirical results show we can substantially reduce the time taken to build a policy.",nan
2016,how good is predictive routing in the online version of the braess paradox?,"with the online routing game model we can investigate the online routing problem, where each subsequent agent of the traffic flow may select different route based on real-time data. recent investigations proved that if the agents of such system use the selfish shortest path search strategy, then in some situations sometimes the multi-agent system may be worse off with real-time data than without real-time data, even if anticipatory techniques are applied to predict the future state of the environment. we investigate the online braess paradox, where each subsequent agent of the traffic flow may select different route, using anticipatory techniques.",nan
2016,delete-free reachability analysis for temporal and hierarchical planning,"reachability analysis is a crucial part of the heuristic computation for many state of the art classical and temporal planners. in this paper, we study the difficulty that arises in assessing the reachability of actions in planning problems containing sets of interdependent actions, notably including problems with required concurrency as well as hierarchical planning problems. we show the limitation of state-of-the-art techniques and propose a new method suitable for both temporal and hierarchical planning problems. our proposal is evaluated on fape, a constraint-based temporal planner.a long version of this paper was presented at the hsdip workshop [1].",nan
2016,adaptive condorcet-based stopping rules can be efficient,"a crowdsourcing project is usually comprised of many unit tasks known as human intelligence tasks (hits). as answers to each hit varies between workers, each hit is often contracted to more than one worker to obtain a reliable and consistent enough answer. when implementing a project, an important design decision is how to formulate hits and how to aggregate workers' answers. these decisions have strong impact on the quality of results and cost of elicitation process. one way to design an efficient elicitation procedure is to use adaptive stopping rules, which allows terminating the elicitation process as soon as a high quality result is guaranteed.adaptively deciding how many times to issue a hit is mostly well understood for the case of binary-answer hits, thanks to the work of abraham et al. [3, 2, 1]. in this line of work the authors focused on plurality-based stopping rules and provided their theoretical analysis. as a decision rule (when many alternatives are offered), it is well known that plurality may be inferior to other rules, such as the condorcet method. we argue that for large number of possible answers, plurality-based stopping rules may also be terribly inefficient. in other words, one may need to elicit answers from many workers (at least linear in the number of answers) in order to get any reasonable approximation of the plurality answer. somewhat surprisingly, we show that condorset-based stopping rules may be much more efficient (with the number of workers to find the approximate condorset winner depending only logarithmically on the number of answers). moreover, in an important case of restricted domains, namely single-peaked domains, we show that the stopping time to find an approximate condorcet-based winner does not depend on the number of answers at all. overall, our results suggest that both crowdsourcing platform developers and hit designers, should consider condorcet-based adaptive stopping rules as a useful tool in their toolboxes.",nan
2016,multi-level semantics with vertical integrity constraints,"operational semantics is a fundamental approach to the formalisation of programming languages and almost a standard when it comes to agent-oriented programming languages. it helps ensure the correctness of interpreters, facilitates their implementation, and supports proofs of important properties. multi-agent oriented systems are a particular kind of distributed systems and through the semantics of agent languages, operational semantics ended up playing an important role towards ensuring their desired behaviour, even though the operational semantics becomes more involved than originally intended. this work presents a new style for the operational semantics of systems with multiple levels of abstractions (such as multi-agent systems), by providing multi-level transitions (i.e., multiple hierarchical transition systems) with vertical (i.e., inter-level) integrity constraints to ensure consistency of interrelated transitions.",nan
2016,supervised graph-based term weighting scheme for effective text classification,"due to the increase in electronic documents, automatic text classification has gained a lot of importance as manual classification of documents is time-consuming. machine learning is the main approach for automatic text classification, where texts are represented, terms are weighted on the basis of the chosen representation and a classification model is built. vector space model is the dominant text representation largely due to its simplicity. graphs are becoming an alternative text representation that have the ability to capture important information in text such as term order, term co-occurrence and term relationships that are not considered by the vector space model. substantially better text classification performance has been demonstrated for term weighting schemes which use a graph representation. in this paper, we introduce a graph-based term weighting scheme, tw-srw, which is an effective supervised term weighting method that considers the co-occurrence information in text for increasing text classification accuracy. experimental results show that it outperforms the state-of-the-art unsupervised term weighting schemes.",nan
2016,secure multi-agent planning algorithms,"multi-agent planning (map) is often motivated by the preservation of private information. such motivation is not only natural for multi-agent systems, but is one of the main reasons, why map problems cannot be solved centrally.in this paper, we analyze privacy leakage of the most common map paradigms. then, we propose a new class secmap of secure map algorithms and show how the existing techniques can be modified to fall in the proposed class.",nan
2016,analysis of swarm communication models,the biological swarm literature presents communication models that attempt to capture the nature of interactions among the swarm's individuals. the evaluated hypothesis that the choice of a biologically inspired communication model can affect an artificial swarm's performance for a given task was supported.,nan
2016,fault manifestability verification for discrete event systems,"fault diagnosis is a crucial and challenging task in the automatic control of complex systems, whose efficiency depends on the diagnosability property of a system. diagnosability describes the system ability to determine whether a given fault has effectively occurred based on the observations. however, this is a very strong property that requires generally high number of sensors to be satisfied. consequently, it is not rare that developing a diagnosable system is too expensive. to solve this problem, in this paper, we first define a new system property called manifestability that represents the weakest requirement on faults and observations for having a chance to identify on line fault occurrences and can be verified at design stage. then, we propose an algorithm with pspace complexity to automatically verify it.",nan
2016,using petri net plans for modeling uav-ugv cooperative landing,"use of cooperative multi vehicle team including aerial and ground vehicles has been growing rapidly over the last years, ranging from search and rescue to logistics. in this paper, we consider a cooperative landing task problem, where an unmanned aerial vehicle (uav) must land on an unmanned ground vehicle (ugv) while such ground vehicle is moving in the environment to execute its own mission. to solve this challenging problem we consider the petri net plans (pnps) framework, an advanced planning specification framework, to effectively use different controllers in different conditions and to monitor the evolution of the system during mission execution so that the best controller is always used even in face of unexpected situations. empirical simulation results show that our system can properly monitor the joint mission carried out by the uav/ugv team, hence confirming that the use of a formal planning language significantly helps in the design of such complex scenarios.",nan
2016,decoupling a resource constraint through fictitious play in multi-agent sequential decision making,"when multiple independent agents use a limited shared resource, they need to coordinate and thereby their planning problems become coupled. we present a resource assignment strategy that decouples agents using marginal utility cost, allowing them to plan individually. we show that agents converge to an expected cost curve by keeping a history of plans, inspired by fictitious play. this performs slightly better than a state-of-the-art best-response approach and is significantly more scalable than a preallocation mixed-integer linear programming formulation, providing a good trade-off between performance and quality.",nan
2016,sets of contrasting rules to identify trigger factors,"in this paper we introduce a new pattern, referred to as “set of contrasting rules”. the main originality of this pattern is that it allows to easily identify trigger factors: factors that can bring some event state changes. in real applications this pattern can thus be used to influence the values of some attributes, what was shown through the experiments conducted on a real dataset of census data.",nan
2016,link prediction by incidence matrix factorization,"link prediction suffers from the data sparsity problem. this paper presents and validates our hypothesis that, for sparse networks, incidence matrix factorization (imf) could perform better than adjacency matrix factorization (amf), which has been used in many previous studies. a key observation supporting the hypothesis is that imf models a partially-observed graph more accurately than amf. a technical challenge for validating our hypothesis is that, unlike amf approach, there does not exist an obvious method to make predictions using a factorized incidence matrix. to this end, we newly develop an optimization-based link prediction method adopting imf. we have conducted thorough experiments using synthetic and real-world datasets to investigate the relationship between the sparsity of a network and the performance of the aforementioned two methods. the experimental results show that imf performs better than amf as networks become sparser, which strongly validates our hypothesis.",nan
2016,dialogues as social practices for serious games,"the paper describes an architecture for a social conversational agent. the aim is to use the agent in a serious game to improve the social and communicative skills of the players, showing the social effects of conversational choices on the emotions and behavioural changes of the interlocutors.",nan
2016,abducing workflow traces: a general framework to manage incompleteness in business processes,"the capability to store data about business process executions in so-called event logs has brought to the identification of a range of key reasoning services (consistency, compliance, runtime monitoring, prediction) for the analysis of process executions and process models. tools for the provision of these services typically focus on one form of reasoning alone. moreover, they are often very rigid in dealing with forms of incomplete information about the process execution. while this enables the development of ad hoc solutions, it also poses an obstacle for the adoption of reasoning-based solutions. in this paper we exploit the power of abduction to provide a flexible, and yet computationally effective framework able to reinterpret key reasoning services in terms of incompleteness and observability in a uniform and effective way.",nan
2016,non-utilitarian coalition structure generation,"the coalition structure generation problem is one of the key challenges in multi-agent coalition formation. it involves partitioning a set of agents into coalitions so that system performance is optimized. to date, the multi-agent systems literature has focused exclusively on the utilitarian version of this problem which seeks to maximize the sum of the values of the coalitions involved. however, there are many examples of situations in which other performance metrics are of interest. in particular, in games with non-transferable utility, we may be more interested in an egalitarian optimal coalition structure, or in minimizing the difference between the utilities of the most affluent and poorest agents. in this paper, we present a number of exact algorithms to solve such non-utilitarian formulations of the coalition structure generation problem.",nan
2016,pa*: optimal path planning for perception tasks,"in this paper we introduce the problem of planning for perception of a target position. given a sensing target, the robot has to move to a goal position from where the target can be perceived. our algorithm minimizes the overall path cost as a function of both motion and perception costs, given an initial robot position and a sensing target. we contribute a heuristic search method, pa*, that efficiently searches for an optimal path. we prove the proposed heuristic is admissible, and introduce a new goal state stopping condition.",nan
2016,cross-domain error correction in personality prediction,"in this paper, we analyze domain bias in automated text-based personality prediction, and proposes a novel method to correct domain bias. the proposed approach is very general since it requires neither retraining a personality prediction system using examples from a new domain, nor any knowledge of the original training data used to develop the system. we conduct several experiments to evaluate the effectiveness of the method, and the findings indicate a significant improvement of prediction accuracy.",nan
2016,classical planning with communicative actions,"explicit communication planning is an increasing necessity for agent systems. furthermore, there has also been a renewed interest in using classical planning to perform this planning in addition to physical actions in a goal-directed way. existing approaches, however, are not applicable to a broad spectrum of domains. we present several generic pre-processing strategies and adaptations of the fast-forward (ff) planner that we compare in two different domains in terms of time complexity.",nan
2016,mixed strategy extraction from uct tree in security games,in this paper a simulation-based approach to finding optimal defender strategy in multi-act security games (sg) played on a graph is proposed. the method employs the upper confidence bounds applied to trees (uct) algorithm which relies on massive simulations of possible game scenarios. three different variants of the algorithm are presented and compared with each other as well as against the mixed integer linear program (milp) exact solution in terms of computational efficiency and memory requirements. experimental evaluation shows that the method has a few times lower memory demands and is faster than milp approach in majority of test cases while preserving quality of the resulting mixed strategies.,nan
2016,boolean negotiation games,"we propose a new strategic model of negotiation, called boolean negotiation games. our model is inspired by boolean games and the alternating offers model of bargaining. it offers a computationally grounded model for studying properties of negotiation protocols in a qualitative setting. boolean negotiation games can yield agreements that are more beneficial than stable solutions (nash equilibria) of the underlying boolean game.",nan
2016,toward addressing collusion among human adversaries in security games,"security agencies including the us coast guard, the federal air marshal service and the los angeles airport police are several major domains that have been deploying stackelberg security games and related algorithms to protect against a single adversary or multiple, independent adversaries strategically. however, there are a variety of real-world security domains where adversaries may benefit from colluding in their actions against the defender. given the potential negative effect of these collusive actions, the defender has an incentive to break up collusion by playing off the self-interest of individual adversaries. this paper deals with problem of collusive security games for rational and bounded rational adversaries. the theoretical results verified with human subject experiments showed that behavior model which optimizes against bounded rational adversaries provides demonstrably better performing defender strategies against human subjects.",nan
2016,detecting communities using coordination games: a short paper,"communities typically capture homophily as people of the same community share many common features. this paper is motivated by the problem of community detection in social networks, as it can help improve our understanding of the network topology. given the selfish nature of humans to align with like-minded people, we employ game theoretic models and algorithms to detect communities in this paper. specifically, we employ coordination games to represent interactions between individuals in a social network. we provide a novel and scalable two phased algorithm nashoverlap to compute an accurate overlapping community structure in the given network. we evaluate our algorithm against the best existing methods for community detection and show that our algorithm improves significantly on benchmark networks with respect to standard normalised mutual information measure.",nan
2016,pricing options with portfolio-holding trading agents in direct double auction,"options constitute integral part of modern financial trades, and are priced according to the risk associated with buying or selling certain asset in future. financial literature mostly concentrates on risk-neutral methods of pricing options such as black-scholes model. however, it is an emerging field in option pricing theory to use trading agents with utility functions to determine the option's potential payoff for the agent. in this paper, we use one of such methodologies developed by othman and sandholm to design portfolio-holding agents that are endowed with popular option portfolios such as bullish spread, butterfly spread, straddle, etc to price options. agents use their portfolios to evaluate how buying or selling certain option would change their current payoff structure, and form their orders based on this information. we also simulate these agents in a multi-unit direct double auction. the emerging prices are compared to risk-neutral prices under different market conditions. through an appropriate endowment of option portfolios to agents, we can also mimic market conditions where the population of agents are bearish, bullish, neutral or non-neutral in their beliefs.",nan
2016,case-based classification on hierarchical structure of formal concept analysis,"we propose a novel hierarchical cbc model (hcbc) based on formal concept analysis (fca). firstly, concept lattice (cl), the hierarchical and conceptual structure in fca, is adopted to represent cases. thus a novel dynamic weight model is proposed from cl to measure similarities between cases and concepts. then the similarity metric is applied to retrieve the top-k similar concepts which are used to vote for adaptive solutions for new cases by majority voting in case adaption. experiments show our model shows good performance in terms of accuracy and outperforms the other classification methods.",nan
2016,stochastic area pooling for generic convolutional neural network,"this paper proposes a novel sapnet model that incorporates a stochastic area pooling (sap) method with a generic stacked t-shaped cnn architecture. in our sap method, pooling area is randomly transformed and max pooling operation is then conducted on such areas, which are no longer regular identical fixed upright squares. it can be viewed as feature-level augmentation, substantially reducing model parameters while keeping generalization ability of cnn almost unchanged. furthermore, we present a generic cnn architecture that structurally resembles three stacked t-shaped cubes. in such architecture, the number of kernels in convolutional layer preceding any pooling layer is doubled and all learnable weight layers are combined with batch normalization and dropout with a small ratio. finally, on cifar-10, cifar-100, mnist, and svhn datasets, the experimental results show that our sapnet requires fewer parameters than regular cnn models and still achieves superior recognition performances for all the four benchmarks.",nan
2016,hole in one: using qualitative reasoning for solving hard physical puzzle problems,"the capability of determining the right sequence of physical actions to achieve a given task is essential for ai that interacts with the physical world. the great difficulty in developing this capability has two main causes: (1) the world is continuous and therefore the action space is infinite, (2) due to noisy perception, we do not know the exact physical properties of our environment and therefore cannot precisely simulate the consequences of a physical action.in this paper we define a realistic physical action selection problem that has many features common to these kind of problems, the minigolf hole-in-one problem: given a two-dimensional minigolf-like obstacle course, a ball and a hole, determine a single shot that hits the ball into the hole. we assume gravity as well as noisy perception of the environment. we present a method that solves this problem similar to how humans are approaching these problems, by using qualitative reasoning and mental simulation, combined with sampling of actions in the real environment and adjusting the internal knowledge based on observing the actual outcome of sampled actions. we evaluate our method using difficult minigolf levels that require the ball to bounce at several objects in order to hit the hole and compare with existing methods.",nan
2016,relational grounded language learning,"in the past, research on learning language models mainly used syntactic information during the learning process but in recent years, researchers began to also use semantic information. this paper presents such an approach where the input of our learning algorithm is a dataset of pairs made up of sentences and the contexts in which they are produced. the system we present is based on inductive logic programming techniques that aim to learn a mapping between n-grams and a semantic representation of their associated meaning. experiments have shown that we can learn such a mapping that made it possible later to generate relevant descriptions of images or learn the meaning of words without any linguistic resource.",nan
2016,learning the repair urgency for a decision support system for tunnel maintenance,"the transport network in many countries relies on extended portions which run underground in tunnels. as tunnels age, repairs are required to prevent dangerous collapses. however repairs are expensive and will affect the operational efficiency of the tunnel. we present a decision support system (dss) based on supervised machine learning methods that learns to predict the risk factor and the resulting repair urgency in the tunnel maintenance planning of a european national rail operator. the data on which the prototype has been built consists of 47 tunnels of varying lengths. for each tunnel, periodic survey inspection data is available for multiple years, as well as other data such as the method of construction of the tunnel. expert annotations are also available for each 10m tunnel segment for each survey as to the degree of repair urgency which are used for both training and model evaluation. we show that good predictive power can be obtained and discuss the relative merits of a number of learning methods.",nan
2016,one – a personalized wellness system,"as the world becomes increasingly digitally readable through a variety of sensors, digital services will play a key role in advising and supporting people towards a variety of goals. in this paper, we present a personalized wellness system that leverages techniques from cognitive science and machine learning to improve a user's well-being by suggesting daily micro-goals (e.g., “bring a healthy snack to work”), and by enabling social sharing of individual achievements. specifically, we propose a method for estimating a user's likelihood of successfully completing a given micro-goal (“one”) and study the correlation between ones and users' actions to improve their chances of reaching their wellness objectives.",nan
2016,planning search and rescue missions for uav teams,"the coordination of multiple unmanned aerial vehicles (uavs) to carry out aerial surveys is a major challenge for emergency responders. in particular, uavs have to fly over kilometre-scale areas while trying to discover casualties as quickly as possible. to aid in this process, it is desirable to exploit the increasing availability of data about a disaster from sources such as crowd reports, satellite remote sensing, or manned reconnaissance. in particular, such information can be a valuable resource to drive the planning of uav flight paths over a space in order to discover people who are in danger. however challenges of computational tractability remain when planning over the very large action spaces that result. to overcome these, we introduce the survivor discovery problem and present as our solution, the first example of a continuous factored coordinated monte carlo tree search algorithm. our evaluation against state of the art benchmarks show that our algorithm, co-cmcts, is able to localise more casualties faster than standard approaches by 7% or more on simulations with real-world data.",nan
2016,integrating arima and spatiotemporal bayesian networks for high resolution malaria prediction,"since malaria is prevalent in less developed and more remote areas in which public health resources are often scarce, targeted intervention is essential in allocating resources for effective malaria control. to effectively support targeted intervention, predictive models must be not only accurate but they must also have high temporal and spatial resolution to help determine when and where to intervene. in this paper we take the first essential step towards a system to support targeted intervention in thailand by developing a high resolution prediction model through the combination of bayes nets and arima. bayes nets and arima have complementary strengths, with the bayes nets better able to represent the effect of environmental variables and arima better able to capture the characteristics of the time series of malaria cases. leveraging these complementary strengths, we develop an ensemble predictor from the two that has significantly better accuracy that either predictor alone. we build and test the models with data from tha song yang district in northern thailand, creating village-level models with weekly temporal resolution.",nan
2016,rapid adaptation of air combat behaviour,"adaptive behaviour for computer generated forces enriches training simulations with appropriate challenge levels. for adequate insight into the range of possible behaviour, the adaptation has to take place in a rapid fashion. ideally, each new behaviour model should remain readable by (and thereby under the control of) human experts. although various attempts have been made at creating adaptive behaviour, current solutions require large numbers of simulations. moreover, usability by end users has been of subordinate interest, as is compliance with doctrine and ethics. in this work, we present a machine learning method that enables fast behaviour adaptation, while keeping the behaviour models in a human-readable format. we demonstrate the effectiveness of the proposed method in beyond-visual-range air combat simulations.",nan
2016,an intelligent system for personalized conference event recommendation and scheduling,"many conference mobile apps today lack the intelligent feature to automatically generates optimal schedules based on delegates' preferences. this entails two major challenges: (a) identifying preferences of users; and (b) given the preferences, generating a schedule that optimizes his preferences. in this paper, we specifically focus on academic conferences, where users are prompted to input their preferred keywords. our key contribution is an integrated conference scheduling agent that automatically recognizes user preferences based on keywords, provides a list of recommended talks and optimizes user schedule based on these preferences. to demonstrate the utility of our integrated conference scheduling agent, we first demonstrated the app in the international conference on autonomous agents and multi-agent systems (aamas 2015) and conducted a survey to collect some data, which are used to verify the results presented in this paper. it is able to provide well calibrated results with respect to precision, accuracy and recall. we also tested the app in the 2015 wi-iat international conference (singapore). the android and web-based apps have been demonstrated and deployed in aamas 2016 (singapore) with positive responses from the users.",nan
2016,continuous live stress monitoring with a wristband,"in this paper we propose a method for continuous stress monitoring using data provided by a commercial wrist device equipped with common physiological sensors and an accelerometer. the method consists of three machine-learning components: a laboratory stress-detector that detects short-term stress every 2 minutes; an activity recognizer that continuously recognizes user's activity and thus provides context information; and a context-based stress detector that first aggregates the predictions of the laboratory detector, and then exploits the user's context in order to provide the final decision in a 20 minute interval. the method was trained on 21 subjects in a laboratory setting and tested on 5 subjects in a real-life setting. the accuracy on 55 days of real-life data was 92%. the method is currently being implemented as a smartphone application, which will be demonstrated at the conference.",nan
2016,an intelligent system for aggression de-escalation training,"artificial intelligence techniques are increasingly being used to develop smart training applications for professionals in various domains. this paper presents an intelligent training system that enables professionals in the public domain to practice their aggression de-escalation skills. the system is one of the main products of the stress project, an interdisciplinary research project involving partners from academia, industry and society. the system makes use of a variety of ai-related techniques, including simulation, virtual agents, sensor fusion, model-based analysis and adaptive support. a preliminary evaluation of the system has been conducted with two groups of potential end users, namely tram conductors and police academy students.",nan
2016,a practical approach to fuse shape and appearance information in a gaussian facial action estimation framework,"in many domains of computer vision, such as medical imaging and facial image analysis, it is necessary to combine shape (geometric) and appearance (texture) information. in this paper, we describe a method for combining geometric and texture-based evidence for facial actions within a kalman filter framework. the geometric evidence is provided by a face alignment method. the texture-based evidence is provided by a set of support vector machines (svm) for various action units (au). the proposed method is a practical solution to the problem of fusing categorical probabilities within a kalman filter based state estimation framework. a first performance evaluation on upper face aus demonstrates the practical applicability of the proposed fusion method. the method is applicable to arbitrary imaging domains, apart from facial action estimation.",nan
2016,planning tourist agendas for different travel styles,"this paper describes e-tourism2.0, a web-based recommendation and planning system for tourism activities that takes into account the preferences that define the travel style of the user. e-tourism2.0 features a recommender system with access to various web services in order to obtain updated information about locations, monuments, opening hours, or transportation modes. the planning system of e-tourism2.0 models the taste and travel style preferences of the user and creates a planning problem which is later solved by a planner, returning a personalized plan (agenda) for the tourist. e-tourism2.0 contributes with a special module that calculates the recommendable duration of a visit for a user and the modeling of preferences into a planning problem.",nan
